<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3D%28temporal%20OR%20spatiotemp%2A%20OR%20sequential%2A%29%20AND%20%28action%2A%20OR%20behavio%2A%20OR%20activit%2A%29%20AND%20%28detect%2A%20OR%20localiz%2A%29%26id_list%3D%26start%3D0%26max_results%3D1000" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=(temporal OR spatiotemp* OR sequential*) AND (action* OR behavio* OR activit*) AND (detect* OR localiz*)&amp;id_list=&amp;start=0&amp;max_results=1000</title>
  <id>http: //arxiv.org/api/Ko3qoJJCi+aWAbyW9h/yxUBGnww</id>
  <updated>2025-04-26T00: 00: 00-04: 00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1837</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1000</opensearch:itemsPerPage>
  <entry>
    <id>http: //arxiv.org/abs/2202.07925v2</id>
    <updated>2022-08-28T19: 46: 29Z</updated>
    <published>2022-02-16T08: 34: 11Z</published>
    <title>ActionFormer: Localizing Moments of Actions with Transformers</title>
    <summary>  Self-attention based Transformer models have demonstrated impressive results
for image classification and object detection, and more recently for video
understanding. Inspired by this success, we investigate the application of
Transformer networks for temporal action localization in videos. To this end,
we present ActionFormer -- a simple yet powerful model to identify actions in
time and recognize their categories in a single shot, without using action
proposals or relying on pre-defined anchor windows. ActionFormer combines a
multiscale feature representation with local self-attention, and uses a
light-weighted decoder to classify every moment in time and estimate the
corresponding action boundaries. We show that this orchestrated design results
in major improvements upon prior works. Without bells and whistles,
ActionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outperforming the best
prior model by 14.1 absolute percentage points. Further, ActionFormer
demonstrates strong results on ActivityNet 1.3 (36.6% average mAP) and
EPIC-Kitchens 100 (+13.5% average mAP over prior works). Our code is available
at http: //github.com/happyharrycn/actionformer_release.
</summary>
    <author>
      <name>Chenlin Zhang</name>
    </author>
    <author>
      <name>Jianxin Wu</name>
    </author>
    <author>
      <name>Yin Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ECCV 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.07925v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.07925v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2004.06971v2</id>
    <updated>2020-11-10T16: 43: 56Z</updated>
    <published>2020-04-15T09: 36: 37Z</published>
    <title>ActionSpotter: Deep Reinforcement Learning Framework for Temporal Action
  Spotting in Videos</title>
    <summary>  Summarizing video content is an important task in many applications. This
task can be defined as the computation of the ordered list of actions present
in a video. Such a list could be extracted using action detection algorithms.
However, it is not necessary to determine the temporal boundaries of actions to
know their existence. Moreover, localizing precise boundaries usually requires
dense video analysis to be effective. In this work, we propose to directly
compute this ordered list by sparsely browsing the video and selecting one
frame per action instance, task known as action spotting in literature. To do
this, we propose ActionSpotter, a spotting algorithm that takes advantage of
Deep Reinforcement Learning to efficiently spot actions while adapting its
video browsing speed, without additional supervision. Experiments performed on
datasets THUMOS14 and ActivityNet show that our framework outperforms state of
the art detection methods. In particular, the spotting mean Average Precision
on THUMOS14 is significantly improved from 59.7% to 65.6% while skipping 23% of
video.
</summary>
    <author>
      <name>Guillaume Vaudaux-Ruth</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISIR, PIROS, SU</arxiv:affiliation>
    </author>
    <author>
      <name>Adrien Chan-Hon-Tong</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISIR, PIROS, SU</arxiv:affiliation>
    </author>
    <author>
      <name>Catherine Achard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISIR, PIROS, SU</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2004.06971v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.06971v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2006.03732v2</id>
    <updated>2021-05-18T18: 19: 25Z</updated>
    <published>2020-06-05T23: 08: 41Z</published>
    <title>WOAD: Weakly Supervised Online Action Detection in Untrimmed Videos</title>
    <summary>  Online action detection in untrimmed videos aims to identify an action as it
happens, which makes it very important for real-time applications. Previous
methods rely on tedious annotations of temporal action boundaries for training,
which hinders the scalability of online action detection systems. We propose
WOAD, a weakly supervised framework that can be trained using only video-class
labels. WOAD contains two jointly-trained modules, i.e., temporal proposal
generator (TPG) and online action recognizer (OAR). Supervised by video-class
labels, TPG works offline and targets at accurately mining pseudo frame-level
labels for OAR. With the supervisory signals from TPG, OAR learns to conduct
action detection in an online fashion. Experimental results on THUMOS'14,
ActivityNet1.2 and ActivityNet1.3 show that our weakly-supervised method
largely outperforms weakly-supervised baselines and achieves comparable
performance to the previous strongly-supervised methods. Beyond that, WOAD is
flexible to leverage strong supervision when it is available. When strongly
supervised, our method obtains the state-of-the-art results in the tasks of
both online per-frame action recognition and online detection of action start.
</summary>
    <author>
      <name>Mingfei Gao</name>
    </author>
    <author>
      <name>Yingbo Zhou</name>
    </author>
    <author>
      <name>Ran Xu</name>
    </author>
    <author>
      <name>Richard Socher</name>
    </author>
    <author>
      <name>Caiming Xiong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.03732v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.03732v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2411.00883v1</id>
    <updated>2024-10-31T14: 16: 56Z</updated>
    <published>2024-10-31T14: 16: 56Z</published>
    <title>Technical Report for ActivityNet Challenge 2022 -- Temporal Action
  Localization</title>
    <summary>  In the task of temporal action localization of ActivityNet-1.3 datasets, we
propose to locate the temporal boundaries of each action and predict action
class in untrimmed videos. We first apply VideoSwinTransformer as feature
extractor to extract different features. Then we apply a unified network
following Faster-TAD to simultaneously obtain proposals and semantic labels.
Last, we ensemble the results of different temporal action detection models
which complement each other. Faster-TAD simplifies the pipeline of TAD and gets
remarkable performance, obtaining comparable results as those of multi-step
approaches.
</summary>
    <author>
      <name>Shimin Chen</name>
    </author>
    <author>
      <name>Wei Li</name>
    </author>
    <author>
      <name>Jianyang Gu</name>
    </author>
    <author>
      <name>Chen Chen</name>
    </author>
    <author>
      <name>Yandong Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv: 2204.02674</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.00883v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.00883v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2305.04186v3</id>
    <updated>2023-12-25T07: 10: 21Z</updated>
    <published>2023-05-07T04: 18: 22Z</published>
    <title>Video-Specific Query-Key Attention Modeling for Weakly-Supervised
  Temporal Action Localization</title>
    <summary>  Weakly-supervised temporal action localization aims to identify and localize
the action instances in the untrimmed videos with only video-level action
labels. When humans watch videos, we can adapt our abstract-level knowledge
about actions in different video scenarios and detect whether some actions are
occurring. In this paper, we mimic how humans do and bring a new perspective
for locating and identifying multiple actions in a video. We propose a network
named VQK-Net with a video-specific query-key attention modeling that learns a
unique query for each action category of each input video. The learned queries
not only contain the actions' knowledge features at the abstract level but also
have the ability to fit this knowledge into the target video scenario, and they
will be used to detect the presence of the corresponding action along the
temporal dimension. To better learn these action category queries, we exploit
not only the features of the current input video but also the correlation
between different videos through a novel video-specific action category query
learner worked with a query similarity loss. Finally, we conduct extensive
experiments on three commonly used datasets (THUMOS14, ActivityNet1.2, and
ActivityNet1.3) and achieve state-of-the-art performance.
</summary>
    <author>
      <name>Xijun Wang</name>
    </author>
    <author>
      <name>Aggelos K. Katsaggelos</name>
    </author>
    <link href="http://arxiv.org/abs/2305.04186v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.04186v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1807.00686v1</id>
    <updated>2018-06-29T07: 49: 08Z</updated>
    <published>2018-06-29T07: 49: 08Z</published>
    <title>YH Technologies at ActivityNet Challenge 2018</title>
    <summary>  This notebook paper presents an overview and comparative analysis of our
systems designed for the following five tasks in ActivityNet Challenge 2018:
temporal action proposals, temporal action localization, dense-captioning
events in videos, trimmed action recognition, and spatio-temporal action
localization.
</summary>
    <author>
      <name>Ting Yao</name>
    </author>
    <author>
      <name>Xue Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Rank 2 in both Temporal Activity Detection Task &amp; Kinetics Task @
  ActivityNet 2018. arXiv admin note: substantial text overlap with
  arXiv: 1710.08011 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2007.09883v2</id>
    <updated>2020-08-26T01: 51: 02Z</updated>
    <published>2020-07-20T04: 35: 40Z</published>
    <title>Complementary Boundary Generator with Scale-Invariant Relation Modeling
  for Temporal Action Localization: Submission to ActivityNet Challenge 2020</title>
    <summary>  This technical report presents an overview of our solution used in the
submission to ActivityNet Challenge 2020 Task 1 (\textbf{temporal action
localization/detection
}). Temporal action localization requires to not only
precisely locate the temporal boundaries of action instances, but also
accurately classify the untrimmed videos into specific categories. In this
paper, we decouple the temporal action localization task into two stages (i.e.
proposal generation and classification) and enrich the proposal diversity
through exhaustively exploring the influences of multiple components from
different but complementary perspectives. Specifically, in order to generate
high-quality proposals, we consider several factors including the video feature
encoder, the proposal generator, the proposal-proposal relations, the scale
imbalance, and ensemble strategy. Finally, in order to obtain accurate
detections, we need to further train an optimal video classifier to recognize
the generated proposals. Our proposed scheme achieves the state-of-the-art
performance on the temporal action localization task with \textbf{
    42.26
}
average mAP on the challenge testing set.
</summary>
    <author>
      <name>Haisheng Su</name>
    </author>
    <author>
      <name>Jinyuan Feng</name>
    </author>
    <author>
      <name>Hao Shao</name>
    </author>
    <author>
      <name>Zhenyu Jiang</name>
    </author>
    <author>
      <name>Manyuan Zhang</name>
    </author>
    <author>
      <name>Wei Wu</name>
    </author>
    <author>
      <name>Yu Liu</name>
    </author>
    <author>
      <name>Hongsheng Li</name>
    </author>
    <author>
      <name>Junjie Yan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to CVPR workshop of ActivityNet Challenge 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.09883v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.09883v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1907.12223v1</id>
    <updated>2019-07-29T06: 10: 51Z</updated>
    <published>2019-07-29T06: 10: 51Z</published>
    <title>Multi-Granularity Fusion Network for Proposal and Activity Localization:
  Submission to ActivityNet Challenge 2019 Task 1 and Task 2</title>
    <summary>  This technical report presents an overview of our solution used in the
submission to ActivityNet Challenge 2019 Task 1 (\textbf{temporal action
proposal generation
}) and Task 2 (\textbf{temporal action
localization/detection
}). Temporal action proposal indicates the temporal
intervals containing the actions and plays an important role in temporal action
localization. Top-down and bottom-up methods are the two main categories used
for proposal generation in the existing literature. In this paper, we devise a
novel Multi-Granularity Fusion Network (MGFN) to combine the proposals
generated from different frameworks for complementary filtering and confidence
re-ranking. Specifically, we consider the diversity comprehensively from
multiple perspectives, e.g. the characteristic aspect, the data aspect, the
model aspect and the result aspect. Our MGFN achieves the state-of-the-art
performance on the temporal action proposal task with 69.85 AUC score and the
temporal action localization task with 38.90 mAP on the challenge testing set.
</summary>
    <author>
      <name>Haisheng Su</name>
    </author>
    <author>
      <name>Xu Zhao</name>
    </author>
    <author>
      <name>Shuming Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1907.12223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.12223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2409.00449v1</id>
    <updated>2024-08-31T13: 03: 26Z</updated>
    <published>2024-08-31T13: 03: 26Z</published>
    <title>ActionPose: Pretraining 3D Human Pose Estimation with the Dark Knowledge
  of Action</title>
    <summary>  2D-to-3D human pose lifting is an ill-posed problem due to depth ambiguity
and occlusion. Existing methods relying on spatial and temporal consistency
alone are insufficient to resolve these problems because they lack semantic
information of the motions. To overcome this, we propose ActionPose, a
framework that leverages action knowledge by aligning motion embeddings with
text embeddings of fine-grained action labels. ActionPose operates in two
stages: pretraining and fine-tuning. In the pretraining stage, the model learns
to recognize actions and reconstruct 3D poses from masked and noisy 2D poses.
During the fine-tuning stage, the model is further refined using real-world 3D
human pose estimation datasets without action labels. Additionally, our
framework incorporates masked body parts and masked time windows in motion
modeling to mitigate the effects of ambiguous boundaries between actions in
both temporal and spatial domains. Experiments demonstrate the effectiveness of
ActionPose, achieving state-of-the-art performance in 3D pose estimation on
public datasets, including Human3.6M and MPI-INF-3DHP. Specifically, ActionPose
achieves an MPJPE of 36.7mm on Human3.6M with detected 2D poses as input and
15.5mm on MPI-INF-3DHP with ground-truth 2D poses as input.
</summary>
    <author>
      <name>Longyun Liao</name>
    </author>
    <author>
      <name>Rong Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/2409.00449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.00449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2411.10922v1</id>
    <updated>2024-11-17T00: 39: 59Z</updated>
    <published>2024-11-17T00: 39: 59Z</published>
    <title>Exploiting VLM Localizability and Semantics for Open Vocabulary Action
  Detection</title>
    <summary>  Action detection aims to detect (recognize and localize) human actions
spatially and temporally in videos. Existing approaches focus on the closed-set
setting where an action detector is trained and tested on videos from a fixed
set of action categories. However, this constrained setting is not viable in an
open world where test videos inevitably come beyond the trained action
categories. In this paper, we address the practical yet challenging
Open-Vocabulary Action Detection (OVAD) problem. It aims to detect any action
in test videos while training a model on a fixed set of action categories. To
achieve such an open-vocabulary capability, we propose a novel method OpenMixer
that exploits the inherent semantics and localizability of large
vision-language models (VLM) within the family of query-based detection
transformers (DETR). Specifically, the OpenMixer is developed by spatial and
temporal OpenMixer blocks (S-OMB and T-OMB), and a dynamically fused alignment
(DFA) module. The three components collectively enjoy the merits of strong
generalization from pre-trained VLMs and end-to-end learning from DETR design.
Moreover, we established OVAD benchmarks under various settings, and the
experimental results show that the OpenMixer performs the best over baselines
for detecting seen and unseen actions. We release the codes, models, and
dataset splits at https: //github.com/Cogito2012/OpenMixer.
</summary>
    <author>
      <name>Wentao Bao</name>
    </author>
    <author>
      <name>Kai Li</name>
    </author>
    <author>
      <name>Yuxiao Chen</name>
    </author>
    <author>
      <name>Deep Patel</name>
    </author>
    <author>
      <name>Martin Renqiang Min</name>
    </author>
    <author>
      <name>Yu Kong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">WACV 2025 Accepted</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.10922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.10922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2407.12987v1</id>
    <updated>2024-07-17T20: 07: 05Z</updated>
    <published>2024-07-17T20: 07: 05Z</published>
    <title>ActionSwitch: Class-agnostic Detection of Simultaneous Actions in
  Streaming Videos</title>
    <summary>  Online Temporal Action Localization (On-TAL) is a critical task that aims to
instantaneously identify action instances in untrimmed streaming videos as soon
as an action concludes -- a major leap from frame-based Online Action Detection
(OAD). Yet, the challenge of detecting overlapping actions is often overlooked
even though it is a common scenario in streaming videos. Current methods that
can address concurrent actions depend heavily on class information, limiting
their flexibility. This paper introduces ActionSwitch, the first class-agnostic
On-TAL framework capable of detecting overlapping actions. By obviating the
reliance on class information, ActionSwitch provides wider applicability to
various situations, including overlapping actions of the same class or
scenarios where class information is unavailable. This approach is complemented
by the proposed "conservativeness loss", which directly embeds a conservative
decision-making principle into the loss function for On-TAL. Our ActionSwitch
achieves state-of-the-art performance in complex datasets, including
Epic-Kitchens 100 targeting the challenging egocentric view and FineAction
consisting of fine-grained actions.
</summary>
    <author>
      <name>Hyolim Kang</name>
    </author>
    <author>
      <name>Jeongseok Hyun</name>
    </author>
    <author>
      <name>Joungbin An</name>
    </author>
    <author>
      <name>Youngjae Yu</name>
    </author>
    <author>
      <name>Seon Joo Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.12987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.12987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2211.14905v2</id>
    <updated>2023-03-27T08: 39: 13Z</updated>
    <published>2022-11-27T18: 13: 05Z</published>
    <title>Multi-Modal Few-Shot Temporal Action Detection</title>
    <summary>  Few-shot (FS) and zero-shot (ZS) learning are two different approaches for
scaling temporal action detection (TAD) to new classes. The former adapts a
pretrained vision model to a new task represented by as few as a single video
per class, whilst the latter requires no training examples by exploiting a
semantic description of the new class. In this work, we introduce a new
multi-modality few-shot (MMFS) TAD problem, which can be considered as a
marriage of FS-TAD and ZS-TAD by leveraging few-shot support videos and new
class names jointly. To tackle this problem, we further introduce a novel
MUlti-modality PromPt mETa-learning (MUPPET) method. This is enabled by
efficiently bridging pretrained vision and language models whilst maximally
reusing already learned capacity. Concretely, we construct multi-modal prompts
by mapping support videos into the textual token space of a vision-language
model using a meta-learned adapter-equipped visual semantics tokenizer. To
tackle large intra-class variation, we further design a query feature
regulation scheme. Extensive experiments on ActivityNetv1.3 and THUMOS14
demonstrate that our MUPPET outperforms state-of-the-art alternative methods,
often by a large margin. We also show that our MUPPET can be easily extended to
tackle the few-shot object detection problem and again achieves the
state-of-the-art performance on MS-COCO dataset. The code will be available in
https: //github.com/sauradip/MUPPET
</summary>
    <author>
      <name>Sauradip Nag</name>
    </author>
    <author>
      <name>Mengmeng Xu</name>
    </author>
    <author>
      <name>Xiatian Zhu</name>
    </author>
    <author>
      <name>Juan-Manuel Perez-Rua</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <author>
      <name>Yi-Zhe Song</name>
    </author>
    <author>
      <name>Tao Xiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.14905v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.14905v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2302.11027v1</id>
    <updated>2023-02-21T22: 02: 39Z</updated>
    <published>2023-02-21T22: 02: 39Z</published>
    <title>Analysis of Real-Time Hostile Activitiy Detection from Spatiotemporal
  Features Using Time Distributed Deep CNNs, RNNs and Attention-Based
  Mechanisms</title>
    <summary>  Real-time video surveillance, through CCTV camera systems has become
essential for ensuring public safety which is a priority today. Although CCTV
cameras help a lot in increasing security, these systems require constant human
interaction and monitoring. To eradicate this issue, intelligent surveillance
systems can be built using deep learning video classification techniques that
can help us automate surveillance systems to detect violence as it happens. In
this research, we explore deep learning video classification techniques to
detect violence as they are happening. Traditional image classification
techniques fall short when it comes to classifying videos as they attempt to
classify each frame separately for which the predictions start to flicker.
Therefore, many researchers are coming up with video classification techniques
that consider spatiotemporal features while classifying. However, deploying
these deep learning models with methods such as skeleton points obtained
through pose estimation and optical flow obtained through depth sensors, are
not always practical in an IoT environment. Although these techniques ensure a
higher accuracy score, they are computationally heavier. Keeping these
constraints in mind, we experimented with various video classification and
action recognition techniques such as ConvLSTM, LRCN (with both custom CNN
layers and VGG-16 as feature extractor) CNNTransformer and C3D. We achieved a
test accuracy of 80% on ConvLSTM,
83.33% on CNN-BiLSTM,
70% on VGG16-BiLstm
,
76.76% on CNN-Transformer and 80% on C3D.
</summary>
    <author>
      <name>Labib Ahmed Siddique</name>
    </author>
    <author>
      <name>Rabita Junhai</name>
    </author>
    <author>
      <name>Tanzim Reza</name>
    </author>
    <author>
      <name>Salman Sayeed Khan</name>
    </author>
    <author>
      <name>Tanvir Rahman</name>
    </author>
    <link href="http://arxiv.org/abs/2302.11027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.11027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2006.07526v2</id>
    <updated>2020-06-24T04: 22: 52Z</updated>
    <published>2020-06-13T01: 05: 51Z</published>
    <title>CBR-Net: Cascade Boundary Refinement Network for Action Detection:
  Submission to ActivityNet Challenge 2020 (Task 1)</title>
    <summary>  In this report, we present our solution for the task of temporal action
localization (detection) (task 1) in ActivityNet Challenge 2020. The purpose of
this task is to temporally localize intervals where actions of interest occur
and predict the action categories in a long untrimmed video. Our solution
mainly includes three components: 1) feature encoding: we apply three kinds of
backbones, including TSN [
    7
], Slowfast[
    3
] and I3d[
    1
], which are both pretrained
on Kinetics dataset[
    2
]. Applying these models, we can extract snippet-level
video representations; 2) proposal generation: we choose BMN [
    5
] as our
baseline, base on which we design a Cascade Boundary Refinement Network
(CBR-Net) to conduct proposal detection. The CBR-Net mainly contains two
modules: temporal feature encoding, which applies BiLSTM to encode long-term
temporal information; CBR module, which targets to refine the proposal
precision under different parameter settings; 3) action localization: In this
stage, we combine the video-level classification results obtained by the fine
tuning networks to predict the category of each proposal. Moreover, we also
apply to different ensemble strategies to improve the performance of the
designed solution, by which we achieve 42.788% on the testing set of
ActivityNet v1.3 dataset in terms of mean Average Precision metrics.
</summary>
    <author>
      <name>Xiang Wang</name>
    </author>
    <author>
      <name>Baiteng Ma</name>
    </author>
    <author>
      <name>Zhiwu Qing</name>
    </author>
    <author>
      <name>Yongpeng Sang</name>
    </author>
    <author>
      <name>Changxin Gao</name>
    </author>
    <author>
      <name>Shiwei Zhang</name>
    </author>
    <author>
      <name>Nong Sang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ActivityNet Challenge 2020 Temporal Action Localization (Task 1)
  Champion Solution (Rank 1)</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.07526v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.07526v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2310.17493v2</id>
    <updated>2023-10-30T15: 47: 44Z</updated>
    <published>2023-10-26T15: 49: 35Z</published>
    <title>A Hybrid Graph Network for Complex Activity Detection in Video</title>
    <summary>  Interpretation and understanding of video presents a challenging computer
vision task in numerous fields - e.g. autonomous driving and sports analytics.
Existing approaches to interpreting the actions taking place within a video
clip are based upon Temporal Action Localisation (TAL), which typically
identifies short-term actions. The emerging field of Complex Activity Detection
(CompAD) extends this analysis to long-term activities, with a deeper
understanding obtained by modelling the internal structure of a complex
activity taking place within the video. We address the CompAD problem using a
hybrid graph neural network which combines attention applied to a graph
encoding the local (short-term) dynamic scene with a temporal graph modelling
the overall long-duration activity. Our approach is as follows: i) Firstly, we
propose a novel feature extraction technique which, for each video snippet,
generates spatiotemporal `tubes' for the active elements (`agents') in the
(local) scene by detecting individual objects, tracking them and then
extracting 3D features from all the agent tubes as well as the overall scene.
ii) Next, we construct a local scene graph where each node (representing either
an agent tube or the scene) is connected to all other nodes. Attention is then
applied to this graph to obtain an overall representation of the local dynamic
scene. iii) Finally, all local scene graph representations are interconnected
via a temporal graph, to estimate the complex activity class together with its
start and end time. The proposed framework outperforms all previous
state-of-the-art methods on all three datasets including ActivityNet-1.3,
Thumos-14, and ROAD.
</summary>
    <author>
      <name>Salman Khan</name>
    </author>
    <author>
      <name>Izzeddin Teeti</name>
    </author>
    <author>
      <name>Andrew Bradley</name>
    </author>
    <author>
      <name>Mohamed Elhoseiny</name>
    </author>
    <author>
      <name>Fabio Cuzzolin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is Accepted at WACV 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.17493v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.17493v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1811.07460v1</id>
    <updated>2018-11-19T02: 12: 06Z</updated>
    <published>2018-11-19T02: 12: 06Z</published>
    <title>Segregated Temporal Assembly Recurrent Networks for Weakly Supervised
  Multiple Action Detection</title>
    <summary>  This paper proposes a segregated temporal assembly recurrent (STAR) network
for weakly-supervised multiple action detection. The model learns from
untrimmed videos with only supervision of video-level labels and makes
prediction of intervals of multiple actions. Specifically, we first assemble
video clips according to class labels by an attention mechanism that learns
class-variable attention weights and thus helps the noise relieving from
background or other actions. Secondly, we build temporal relationship between
actions by feeding the assembled features into an enhanced recurrent neural
network. Finally, we transform the output of recurrent neural network into the
corresponding action distribution. In order to generate more precise temporal
proposals, we design a score term called segregated temporal gradient-weighted
class activation mapping (ST-GradCAM) fused with attention weights. Experiments
on THUMOS'14 and ActivityNet1.3 datasets show that our approach outperforms the
state-of-the-art weakly-supervised method, and performs at par with the
fully-supervised counterparts.
</summary>
    <author>
      <name>Yunlu Xu</name>
    </author>
    <author>
      <name>Chengwei Zhang</name>
    </author>
    <author>
      <name>Zhanzhan Cheng</name>
    </author>
    <author>
      <name>Jianwen Xie</name>
    </author>
    <author>
      <name>Yi Niu</name>
    </author>
    <author>
      <name>Shiliang Pu</name>
    </author>
    <author>
      <name>Fei Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Proc. AAAI Conference on Artificial Intelligence 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.07460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.07460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2008.03270v1</id>
    <updated>2020-08-07T17: 08: 24Z</updated>
    <published>2020-08-07T17: 08: 24Z</published>
    <title>Multi-Level Temporal Pyramid Network for Action Detection</title>
    <summary>  Currently, one-stage frameworks have been widely applied for temporal action
detection, but they still suffer from the challenge that the action instances
span a wide range of time. The reason is that these one-stage detectors, e.g.,
Single Shot Multi-Box Detector (SSD), extract temporal features only applying a
single-level layer for each head, which is not discriminative enough to perform
classification and regression. In this paper, we propose a Multi-Level Temporal
Pyramid Network (MLTPN) to improve the discrimination of the features.
Specially, we first fuse the features from multiple layers with different
temporal resolutions, to encode multi-layer temporal information. We then apply
a multi-level feature pyramid architecture on the features to enhance their
discriminative abilities. Finally, we design a simple yet effective feature
fusion module to fuse the multi-level multi-scale features. By this means, the
proposed MLTPN can learn rich and discriminative features for different action
instances with different durations. We evaluate MLTPN on two challenging
datasets: THUMOS'14 and Activitynet v1.3, and the experimental results show
that MLTPN obtains competitive performance on Activitynet v1.3 and outperforms
the state-of-the-art approaches on THUMOS'14 significantly.
</summary>
    <author>
      <name>Xiang Wang</name>
    </author>
    <author>
      <name>Changxin Gao</name>
    </author>
    <author>
      <name>Shiwei Zhang</name>
    </author>
    <author>
      <name>Nong Sang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by PRCV2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.03270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.03270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2504.00647v1</id>
    <updated>2025-04-01T10: 57: 37Z</updated>
    <published>2025-04-01T10: 57: 37Z</published>
    <title>FDDet: Frequency-Decoupling for Boundary Refinement in Temporal Action
  Detection</title>
    <summary>  Temporal action detection aims to locate and classify actions in untrimmed
videos. While recent works focus on designing powerful feature processors for
pre-trained representations, they often overlook the inherent noise and
redundancy within these features. Large-scale pre-trained video encoders tend
to introduce background clutter and irrelevant semantics, leading to context
confusion and imprecise boundaries. To address this, we propose a
frequency-aware decoupling network that improves action discriminability by
filtering out noisy semantics captured by pre-trained models. Specifically, we
introduce an adaptive temporal decoupling scheme that suppresses irrelevant
information while preserving fine-grained atomic action details, yielding more
task-specific representations. In addition, we enhance inter-frame modeling by
capturing temporal variations to better distinguish actions from background
redundancy. Furthermore, we present a long-short-term category-aware relation
network that jointly models local transitions and long-range dependencies,
improving localization precision. The refined atomic features and
frequency-guided dynamics are fed into a standard detection head to produce
accurate action predictions. Extensive experiments on THUMOS14, HACS, and
ActivityNet-1.3 show that our method, powered by InternVideo2-6B features,
achieves state-of-the-art performance on temporal action detection benchmarks.
</summary>
    <author>
      <name>Xinnan Zhu</name>
    </author>
    <author>
      <name>Yicheng Zhu</name>
    </author>
    <author>
      <name>Tixin Chen</name>
    </author>
    <author>
      <name>Wentao Wu</name>
    </author>
    <author>
      <name>Yuanjie Dang</name>
    </author>
    <link href="http://arxiv.org/abs/2504.00647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.00647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2209.02250v2</id>
    <updated>2022-10-25T08: 36: 29Z</updated>
    <published>2022-09-06T06: 55: 26Z</published>
    <title>Spatio-Temporal Action Detection Under Large Motion</title>
    <summary>  Current methods for spatiotemporal action tube detection often extend a
bounding box proposal at a given keyframe into a 3D temporal cuboid and pool
features from nearby frames. However, such pooling fails to accumulate
meaningful spatiotemporal features if the position or shape of the actor shows
large 2D motion and variability through the frames, due to large camera motion,
large actor shape deformation, fast actor action and so on. In this work, we
aim to study the performance of cuboid-aware feature aggregation in action
detection under large action. Further, we propose to enhance actor feature
representation under large motion by tracking actors and performing temporal
feature aggregation along the respective tracks. We define the actor motion
with intersection-over-union (IoU) between the boxes of action tubes/tracks at
various fixed time scales. The action having a large motion would result in
lower IoU over time, and slower actions would maintain higher IoU. We find that
track-aware feature aggregation consistently achieves a large improvement in
action detection performance, especially for actions under large motion
compared to the cuboid-aware baseline. As a result, we also report
state-of-the-art on the large-scale MultiSports dataset. The Code is available
at https: //github.com/gurkirt/ActionTrackDetectron.
</summary>
    <author>
      <name>Gurkirt Singh</name>
    </author>
    <author>
      <name>Vasileios Choutas</name>
    </author>
    <author>
      <name>Suman Saha</name>
    </author>
    <author>
      <name>Fisher Yu</name>
    </author>
    <author>
      <name>Luc Van Gool</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,
5 figures,
5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.02250v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.02250v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2303.11732v1</id>
    <updated>2023-03-21T10: 40: 13Z</updated>
    <published>2023-03-21T10: 40: 13Z</published>
    <title>Multi-modal Prompting for Low-Shot Temporal Action Localization</title>
    <summary>  In this paper, we consider the problem of temporal action localization under
low-shot (zero-shot &amp; few-shot) scenario, with the goal of detecting and
classifying the action instances from arbitrary categories within some
untrimmed videos, even not seen at training time. We adopt a Transformer-based
two-stage action localization architecture with class-agnostic action proposal,
followed by open-vocabulary classification. We make the following
contributions. First, to compensate image-text foundation models with temporal
motions, we improve category-agnostic action proposal by explicitly aligning
embeddings of optical flows, RGB and texts, which has largely been ignored in
existing low-shot methods. Second, to improve open-vocabulary action
classification, we construct classifiers with strong discriminative power,
i.e., avoid lexical ambiguities. To be specific, we propose to prompt the
pre-trained CLIP text encoder either with detailed action descriptions
(acquired from large-scale language models), or visually-conditioned
instance-specific prompt vectors. Third, we conduct thorough experiments and
ablation studies on THUMOS14 and ActivityNet1.3, demonstrating the superior
performance of our proposed model, outperforming existing state-of-the-art
approaches by one significant margin.
</summary>
    <author>
      <name>Chen Ju</name>
    </author>
    <author>
      <name>Zeqian Li</name>
    </author>
    <author>
      <name>Peisen Zhao</name>
    </author>
    <author>
      <name>Ya Zhang</name>
    </author>
    <author>
      <name>Xiaopeng Zhang</name>
    </author>
    <author>
      <name>Qi Tian</name>
    </author>
    <author>
      <name>Yanfeng Wang</name>
    </author>
    <author>
      <name>Weidi Xie</name>
    </author>
    <link href="http://arxiv.org/abs/2303.11732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.11732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1806.02964v3</id>
    <updated>2018-09-26T10: 48: 22Z</updated>
    <published>2018-06-08T04: 22: 54Z</published>
    <title>BSN: Boundary Sensitive Network for Temporal Action Proposal Generation</title>
    <summary>  Temporal action proposal generation is an important yet challenging problem,
since temporal proposals with rich action content are indispensable for
analysing real-world videos with long duration and high proportion irrelevant
content. This problem requires methods not only generating proposals with
precise temporal boundaries, but also retrieving proposals to cover truth
action instances with high recall and high overlap using relatively fewer
proposals. To address these difficulties, we introduce an effective proposal
generation method, named Boundary-Sensitive Network (BSN), which adopts "local
to global" fashion. Locally, BSN first locates temporal boundaries with high
probabilities, then directly combines these boundaries as proposals. Globally,
with Boundary-Sensitive Proposal feature, BSN retrieves proposals by evaluating
the confidence of whether a proposal contains an action within its region. We
conduct experiments on two challenging datasets: ActivityNet-1.3 and THUMOS14,
where BSN outperforms other state-of-the-art temporal action proposal
generation methods with high recall and high temporal precision. Finally,
further experiments demonstrate that by combining existing action classifiers,
our method significantly improves the state-of-the-art temporal action
detection performance.
</summary>
    <author>
      <name>Tianwei Lin</name>
    </author>
    <author>
      <name>Xu Zhao</name>
    </author>
    <author>
      <name>Haisheng Su</name>
    </author>
    <author>
      <name>Chongjing Wang</name>
    </author>
    <author>
      <name>Ming Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.02964v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02964v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1911.11306v2</id>
    <updated>2020-03-06T06: 05: 01Z</updated>
    <published>2019-11-26T01: 59: 54Z</published>
    <title>SRG: Snippet Relatedness-based Temporal Action Proposal Generator</title>
    <summary>  Recent temporal action proposal generation approaches have suggested
integrating segment- and snippet score-based methodologies to produce proposals
with high recall and accurate boundaries. In this paper, different from such a
hybrid strategy, we focus on the potential of the snippet score-based approach.
Specifically, we propose a new snippet score-based method, named Snippet
Relatedness-based Generator (SRG), with a novel concept of "snippet
relatedness". Snippet relatedness represents which snippets are related to a
specific action instance. To effectively learn this snippet relatedness, we
present "pyramid non-local operations" for locally and globally capturing
long-range dependencies among snippets. By employing these components, SRG
first produces a 2D relatedness score map that enables the generation of
various temporal intervals reliably covering most action instances with high
overlap. Then, SRG evaluates the action confidence scores of these temporal
intervals and refines their boundaries to obtain temporal action proposals. On
THUMOS-14 and ActivityNet-1.3 datasets, SRG outperforms state-of-the-art
methods for temporal action proposal generation. Furthermore, compared to
competing proposal generators, SRG leads to significant improvements in
temporal action detection.
</summary>
    <author>
      <name>Hyunjun Eun</name>
    </author>
    <author>
      <name>Sumin Lee</name>
    </author>
    <author>
      <name>Jinyoung Moon</name>
    </author>
    <author>
      <name>Jongyoul Park</name>
    </author>
    <author>
      <name>Chanho Jung</name>
    </author>
    <author>
      <name>Changick Kim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCSVT.2019.2953187</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCSVT.2019.2953187" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in TCSVT</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.11306v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.11306v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2008.01432v1</id>
    <updated>2020-08-04T09: 35: 11Z</updated>
    <published>2020-08-04T09: 35: 11Z</published>
    <title>Boundary Content Graph Neural Network for Temporal Action Proposal
  Generation</title>
    <summary>  Temporal action proposal generation plays an important role in video action
understanding, which requires localizing high-quality action content precisely.
However, generating temporal proposals with both precise boundaries and
high-quality action content is extremely challenging. To address this issue, we
propose a novel Boundary Content Graph Neural Network (BC-GNN) to model the
insightful relations between the boundary and action content of temporal
proposals by the graph neural networks. In BC-GNN, the boundaries and content
of temporal proposals are taken as the nodes and edges of the graph neural
network, respectively, where they are spontaneously linked. Then a novel graph
computation operation is proposed to update features of edges and nodes. After
that, one updated edge and two nodes it connects are used to predict boundary
probabilities and content confidence score, which will be combined to generate
a final high-quality proposal. Experiments are conducted on two mainstream
datasets: ActivityNet-1.3 and THUMOS14. Without the bells and whistles, BC-GNN
outperforms previous state-of-the-art methods in both temporal action proposal
and temporal action detection tasks.
</summary>
    <author>
      <name>Yueran Bai</name>
    </author>
    <author>
      <name>Yingying Wang</name>
    </author>
    <author>
      <name>Yunhai Tong</name>
    </author>
    <author>
      <name>Yang Yang</name>
    </author>
    <author>
      <name>Qiyue Liu</name>
    </author>
    <author>
      <name>Junhui Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2008.01432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.01432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1607.01979v2</id>
    <updated>2016-07-12T22: 53: 46Z</updated>
    <published>2016-07-07T12: 08: 04Z</published>
    <title>Untrimmed Video Classification for Activity Detection: submission to
  ActivityNet Challenge</title>
    <summary>  Current state-of-the-art human activity recognition is focused on the
classification of temporally trimmed videos in which only one action occurs per
frame. We propose a simple, yet effective, method for the temporal detection of
activities in temporally untrimmed videos with the help of untrimmed
classification. Firstly, our model predicts the top k labels for each untrimmed
video by analysing global video-level features. Secondly, frame-level binary
classification is combined with dynamic programming to generate the temporally
trimmed activity proposals. Finally, each proposal is assigned a label based on
the global label, and scored with the score of the temporal activity proposal
and the global score. Ultimately, we show that untrimmed video classification
models can be used as stepping stone for temporal detection.
</summary>
    <author>
      <name>Gurkirt Singh</name>
    </author>
    <author>
      <name>Fabio Cuzzolin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, Presented at ActivityNet Large Scale Activity Recognition
  Challenge workshop at CVPR 2016, Second position in ActivityNet Detection
  challenge 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.01979v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01979v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2106.11812v1</id>
    <updated>2021-06-20T02: 51: 34Z</updated>
    <published>2021-06-20T02: 51: 34Z</published>
    <title>Proposal Relation Network for Temporal Action Detection</title>
    <summary>  This technical report presents our solution for temporal action detection
task in AcitivityNet Challenge 2021. The purpose of this task is to locate and
identify actions of interest in long untrimmed videos. The crucial challenge of
the task comes from that the temporal duration of action varies dramatically,
and the target actions are typically embedded in a background of irrelevant
activities. Our solution builds on BMN, and mainly contains three steps: 1)
action classification and feature encoding by Slowfast, CSN and ViViT; 2)
proposal generation. We improve BMN by embedding the proposed Proposal Relation
Network (PRN), by which we can generate proposals of high quality; 3) action
detection. We calculate the detection results by assigning the proposals with
corresponding classification results. Finally, we ensemble the results under
different settings and achieve 44.7% on the test set, which improves the
champion result in ActivityNet 2020 by 1.9% in terms of average mAP.
</summary>
    <author>
      <name>Xiang Wang</name>
    </author>
    <author>
      <name>Zhiwu Qing</name>
    </author>
    <author>
      <name>Ziyuan Huang</name>
    </author>
    <author>
      <name>Yutong Feng</name>
    </author>
    <author>
      <name>Shiwei Zhang</name>
    </author>
    <author>
      <name>Jianwen Jiang</name>
    </author>
    <author>
      <name>Mingqian Tang</name>
    </author>
    <author>
      <name>Changxin Gao</name>
    </author>
    <author>
      <name>Nong Sang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR-2021 ActivityNet Temporal Action Localization Challenge champion
  solution (1st Place)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CVPRW-2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2106.11812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.11812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1908.00707v1</id>
    <updated>2019-08-02T05: 49: 37Z</updated>
    <published>2019-08-02T05: 49: 37Z</published>
    <title>Scale Matters: Temporal Scale Aggregation Network for Precise Action
  Localization in Untrimmed Videos</title>
    <summary>  Temporal action localization is a recently-emerging task, aiming to localize
video segments from untrimmed videos that contain specific actions. Despite the
remarkable recent progress, most two-stage action localization methods still
suffer from imprecise temporal boundaries of action proposals. This work
proposes a novel integrated temporal scale aggregation network (TSA-Net). Our
main insight is that ensembling convolution filters with different dilation
rates can effectively enlarge the receptive field with low computational cost,
which inspires us to devise multi-dilation temporal convolution (MDC) block.
Furthermore, to tackle video action instances with different durations, TSA-Net
consists of multiple branches of sub-networks. Each of them adopts stacked MDC
blocks with different dilation parameters, accomplishing a temporal receptive
field specially optimized for specific-duration actions. We follow the
formulation of boundary point detection, novelly detecting three kinds of
critical points (ie, starting / mid-point / ending) and pairing them for
proposal generation. Comprehensive evaluations are conducted on two challenging
video benchmarks, THUMOS14 and ActivityNet-1.3. Our proposed TSA-Net
demonstrates clear and consistent better performances and re-calibrates new
state-of-the-art on both benchmarks. For example, our new record on THUMOS14 is
46.9% while the previous best is 42.8% under mAP@0.5.
</summary>
    <author>
      <name>Guoqiang Gong</name>
    </author>
    <author>
      <name>Liangfeng Zheng</name>
    </author>
    <author>
      <name>Kun Bai</name>
    </author>
    <author>
      <name>Yadong Mu</name>
    </author>
    <link href="http://arxiv.org/abs/1908.00707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.00707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2204.02674v1</id>
    <updated>2022-04-06T08: 55: 35Z</updated>
    <published>2022-04-06T08: 55: 35Z</published>
    <title>Faster-TAD: Towards Temporal Action Detection with Proposal Generation
  and Classification in a Unified Network</title>
    <summary>  Temporal action detection (TAD) aims to detect the semantic labels and
boundaries of action instances in untrimmed videos. Current mainstream
approaches are multi-step solutions, which fall short in efficiency and
flexibility. In this paper, we propose a unified network for TAD, termed
Faster-TAD, by re-purposing a Faster-RCNN like architecture. To tackle the
unique difficulty in TAD, we make important improvements over the original
framework. We propose a new Context-Adaptive Proposal Module and an innovative
Fake-Proposal Generation Block. What's more, we use atomic action features to
improve the performance. Faster-TAD simplifies the pipeline of TAD and gets
remarkable performance on lots of benchmarks, i.e., ActivityNet-1.3 (40.01%
mAP), HACS Segments (38.39% mAP), SoccerNet-Action Spotting (54.09% mAP). It
outperforms existing single-network detector by a large margin.
</summary>
    <author>
      <name>Shimin Chen</name>
    </author>
    <author>
      <name>Chen Chen</name>
    </author>
    <author>
      <name>Wei Li</name>
    </author>
    <author>
      <name>Xunqiang Tao</name>
    </author>
    <author>
      <name>Yandong Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages,
5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.02674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.02674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1904.09288v1</id>
    <updated>2019-04-19T17: 59: 39Z</updated>
    <published>2019-04-19T17: 59: 39Z</published>
    <title>STEP: Spatio-Temporal Progressive Learning for Video Action Detection</title>
    <summary>  In this paper, we propose Spatio-TEmporal Progressive (STEP) action
detector---a progressive learning framework for spatio-temporal action
detection in videos. Starting from a handful of coarse-scale proposal cuboids,
our approach progressively refines the proposals towards actions over a few
steps. In this way, high-quality proposals (i.e., adhere to action movements)
can be gradually obtained at later steps by leveraging the regression outputs
from previous steps. At each step, we adaptively extend the proposals in time
to incorporate more related temporal context. Compared to the prior work that
performs action detection in one run, our progressive learning framework is
able to naturally handle the spatial displacement within action tubes and
therefore provides a more effective way for spatio-temporal modeling. We
extensively evaluate our approach on UCF101 and AVA, and demonstrate superior
detection results. Remarkably, we achieve mAP of 75.0% and 18.6% on the two
datasets with 3 progressive steps and using respectively only 11 and 34 initial
proposals.
</summary>
    <author>
      <name>Xitong Yang</name>
    </author>
    <author>
      <name>Xiaodong Yang</name>
    </author>
    <author>
      <name>Ming-Yu Liu</name>
    </author>
    <author>
      <name>Fanyi Xiao</name>
    </author>
    <author>
      <name>Larry Davis</name>
    </author>
    <author>
      <name>Jan Kautz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2019 (Oral)</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.09288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.09288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2011.11893v2</id>
    <updated>2021-02-18T10: 23: 05Z</updated>
    <published>2020-11-24T04: 45: 17Z</published>
    <title>Temporal Action Detection with Multi-level Supervision</title>
    <summary>  Training temporal action detection in videos requires large amounts of
labeled data, yet such annotation is expensive to collect. Incorporating
unlabeled or weakly-labeled data to train action detection model could help
reduce annotation cost. In this work, we first introduce the Semi-supervised
Action Detection (SSAD) task with a mixture of labeled and unlabeled data and
analyze different types of errors in the proposed SSAD baselines which are
directly adapted from the semi-supervised classification task. To alleviate the
main error of action incompleteness (i.e., missing parts of actions) in SSAD
baselines, we further design an unsupervised foreground attention (UFA) module
utilizing the "independence" between foreground and background motion. Then we
incorporate weakly-labeled data into SSAD and propose Omni-supervised Action
Detection (OSAD) with three levels of supervision. An information bottleneck
(IB) suppressing the scene information in non-action frames while preserving
the action information is designed to help overcome the accompanying
action-context confusion problem in OSAD baselines. We extensively benchmark
against the baselines for SSAD and OSAD on our created data splits in THUMOS14
and ActivityNet1.2, and demonstrate the effectiveness of the proposed UFA and
IB methods. Lastly, the benefit of our full OSAD-IB model under limited
annotation budgets is shown by exploring the optimal annotation strategy for
labeled, unlabeled and weakly-labeled data.
</summary>
    <author>
      <name>Baifeng Shi</name>
    </author>
    <author>
      <name>Qi Dai</name>
    </author>
    <author>
      <name>Judy Hoffman</name>
    </author>
    <author>
      <name>Kate Saenko</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Huijuan Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2011.11893v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.11893v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.19542v1</id>
    <updated>2024-04-30T13: 14: 28Z</updated>
    <published>2024-04-30T13: 14: 28Z</published>
    <title>One-Stage Open-Vocabulary Temporal Action Detection Leveraging Temporal
  Multi-scale and Action Label Features</title>
    <summary>  Open-vocabulary Temporal Action Detection (Open-vocab TAD) is an advanced
video analysis approach that expands Closed-vocabulary Temporal Action
Detection (Closed-vocab TAD) capabilities. Closed-vocab TAD is typically
confined to localizing and classifying actions based on a predefined set of
categories. In contrast, Open-vocab TAD goes further and is not limited to
these predefined categories. This is particularly useful in real-world
scenarios where the variety of actions in videos can be vast and not always
predictable. The prevalent methods in Open-vocab TAD typically employ a 2-stage
approach, which involves generating action proposals and then identifying those
actions. However, errors made during the first stage can adversely affect the
subsequent action identification accuracy. Additionally, existing studies face
challenges in handling actions of different durations owing to the use of fixed
temporal processing methods. Therefore, we propose a 1-stage approach
consisting of two primary modules: Multi-scale Video Analysis (MVA) and
Video-Text Alignment (VTA). The MVA module captures actions at varying temporal
resolutions, overcoming the challenge of detecting actions with diverse
durations. The VTA module leverages the synergy between visual and textual
modalities to precisely align video segments with corresponding action labels,
a critical step for accurate action identification in Open-vocab scenarios.
Evaluations on widely recognized datasets THUMOS14 and ActivityNet-1.3, showed
that the proposed method achieved superior results compared to the other
methods in both Open-vocab and Closed-vocab settings. This serves as a strong
demonstration of the effectiveness of the proposed method in the TAD task.
</summary>
    <author>
      <name>Trung Thanh Nguyen</name>
    </author>
    <author>
      <name>Yasutomo Kawanishi</name>
    </author>
    <author>
      <name>Takahiro Komamizu</name>
    </author>
    <author>
      <name>Ichiro Ide</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 18th IEEE International Conference on Automatic Face and Gesture
  Recognition (FG 2024)</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.19542v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.19542v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2408.13777v1</id>
    <updated>2024-08-25T09: 07: 06Z</updated>
    <published>2024-08-25T09: 07: 06Z</published>
    <title>Towards Completeness: A Generalizable Action Proposal Generator for
  Zero-Shot Temporal Action Localization</title>
    <summary>  To address the zero-shot temporal action localization (ZSTAL) task, existing
works develop models that are generalizable to detect and classify actions from
unseen categories. They typically develop a category-agnostic action detector
and combine it with the Contrastive Language-Image Pre-training (CLIP) model to
solve ZSTAL. However, these methods suffer from incomplete action proposals
generated for \textit{unseen
} categories, since they follow a frame-level
prediction paradigm and require hand-crafted post-processing to generate action
proposals. To address this problem, in this work, we propose a novel model
named Generalizable Action Proposal generator (GAP), which can interface
seamlessly with CLIP and generate action proposals in a holistic way. Our GAP
is built in a query-based architecture and trained with a proposal-level
objective, enabling it to estimate proposal completeness and eliminate the
hand-crafted post-processing. Based on this architecture, we propose an
Action-aware Discrimination loss to enhance the category-agnostic dynamic
information of actions. Besides, we introduce a Static-Dynamic Rectifying
module that incorporates the generalizable static information from CLIP to
refine the predicted proposals, which improves proposal completeness in a
generalizable manner. Our experiments show that our GAP achieves
state-of-the-art performance on two challenging ZSTAL benchmarks, i.e.,
Thumos14 and ActivityNet1.3. Specifically, our model obtains significant
performance improvement over previous works on the two benchmarks, i.e., +3.2%
and +3.4% average mAP, respectively.
</summary>
    <author>
      <name>Jia-Run Du</name>
    </author>
    <author>
      <name>Kun-Yu Lin</name>
    </author>
    <author>
      <name>Jingke Meng</name>
    </author>
    <author>
      <name>Wei-Shi Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICPR 2024. Code is available at
  https: //github.com/Run542968/GAP</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.13777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.13777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2205.07134v2</id>
    <updated>2022-11-28T11: 43: 24Z</updated>
    <published>2022-05-14T21: 16: 21Z</published>
    <title>ETAD: Training Action Detection End to End on a Laptop</title>
    <summary>  Temporal action detection (TAD) with end-to-end training often suffers from
the pain of huge demand for computing resources due to long video duration. In
this work, we propose an efficient temporal action detector (ETAD) that can
train directly from video frames with extremely low GPU memory consumption. Our
main idea is to minimize and balance the heavy computation among features and
gradients in each training iteration. We propose to sequentially forward the
snippet frame through the video encoder, and backward only a small necessary
portion of gradients to update the encoder. To further alleviate the
computational redundancy in training, we propose to dynamically sample only a
small subset of proposals during training. Moreover, various sampling
strategies and ratios are studied for both the encoder and detector. ETAD
achieves state-of-the-art performance on TAD benchmarks with remarkable
efficiency. On ActivityNet-1.3, training ETAD in 18 hours can reach 38.25%
average mAP with only 1.3 GB memory consumption per video under end-to-end
training. Our code will be publicly released.
</summary>
    <author>
      <name>Shuming Liu</name>
    </author>
    <author>
      <name>Mengmeng Xu</name>
    </author>
    <author>
      <name>Chen Zhao</name>
    </author>
    <author>
      <name>Xu Zhao</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <link href="http://arxiv.org/abs/2205.07134v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.07134v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1810.11794v1</id>
    <updated>2018-10-28T11: 10: 33Z</updated>
    <published>2018-10-28T11: 10: 33Z</published>
    <title>Cascaded Pyramid Mining Network for Weakly Supervised Temporal Action
  Localization</title>
    <summary>  Weakly supervised temporal action localization, which aims at temporally
locating action instances in untrimmed videos using only video-level class
labels during training, is an important yet challenging problem in video
analysis. Many current methods adopt the "localization by classification"
framework: first do video classification, then locate temporal area
contributing to the results most. However, this framework fails to locate the
entire action instances and gives little consideration to the local context. In
this paper, we present a novel architecture called Cascaded Pyramid Mining
Network (CPMN) to address these issues using two effective modules. First, to
discover the entire temporal interval of specific action, we design a two-stage
cascaded module with proposed Online Adversarial Erasing (OAE) mechanism, where
new and complementary regions are mined through feeding the erased feature maps
of discovered regions back to the system. Second, to exploit hierarchical
contextual information in videos and reduce missing detections, we design a
pyramid module which produces a scale-invariant attention map through combining
the feature maps from different levels. Final, we aggregate the results of two
modules to perform action localization via locating high score areas in
temporal Class Activation Sequence (CAS). Extensive experiments conducted on
THUMOS14 and ActivityNet-1.3 datasets demonstrate the effectiveness of our
method.
</summary>
    <author>
      <name>Haisheng Su</name>
    </author>
    <author>
      <name>Xu Zhao</name>
    </author>
    <author>
      <name>Tianwei Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ACCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.11794v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.11794v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2211.04905v1</id>
    <updated>2022-11-08T04: 50: 54Z</updated>
    <published>2022-11-08T04: 50: 54Z</published>
    <title>SimOn: A Simple Framework for Online Temporal Action Localization</title>
    <summary>  Online Temporal Action Localization (On-TAL) aims to immediately provide
action instances from untrimmed streaming videos. The model is not allowed to
utilize future frames and any processing techniques to modify past predictions,
making On-TAL much more challenging. In this paper, we propose a simple yet
effective framework, termed SimOn, that learns to predict action instances
using the popular Transformer architecture in an end-to-end manner.
Specifically, the model takes the current frame feature as a query and a set of
past context information as keys and values of the Transformer. Different from
the prior work that uses a set of outputs of the model as past contexts, we
leverage the past visual context and the learnable context embedding for the
current query. Experimental results on the THUMOS14 and ActivityNet1.3 datasets
show that our model remarkably outperforms the previous methods, achieving a
new state-of-the-art On-TAL performance. In addition, the evaluation for Online
Detection of Action Start (ODAS) demonstrates the effectiveness and robustness
of our method in the online setting. The code is available at
https: //github.com/TuanTNG/SimOn
</summary>
    <author>
      <name>Tuan N. Tang</name>
    </author>
    <author>
      <name>Jungin Park</name>
    </author>
    <author>
      <name>Kwonyoung Kim</name>
    </author>
    <author>
      <name>Kwanghoon Sohn</name>
    </author>
    <link href="http://arxiv.org/abs/2211.04905v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.04905v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2407.03197v1</id>
    <updated>2024-07-03T15: 29: 10Z</updated>
    <published>2024-07-03T15: 29: 10Z</published>
    <title>DyFADet: Dynamic Feature Aggregation for Temporal Action Detection</title>
    <summary>  Recent proposed neural network-based Temporal Action Detection (TAD) models
are inherently limited to extracting the discriminative representations and
modeling action instances with various lengths from complex scenes by
shared-weights detection heads. Inspired by the successes in dynamic neural
networks, in this paper, we build a novel dynamic feature aggregation (DFA)
module that can simultaneously adapt kernel weights and receptive fields at
different timestamps. Based on DFA, the proposed dynamic encoder layer
aggregates the temporal features within the action time ranges and guarantees
the discriminability of the extracted representations. Moreover, using DFA
helps to develop a Dynamic TAD head (DyHead), which adaptively aggregates the
multi-scale features with adjusted parameters and learned receptive fields
better to detect the action instances with diverse ranges from videos. With the
proposed encoder layer and DyHead, a new dynamic TAD model, DyFADet, achieves
promising performance on a series of challenging TAD benchmarks, including
HACS-Segment, THUMOS14, ActivityNet-1.3, Epic-Kitchen 100, Ego4D-Moment
QueriesV1.0, and FineAction. Code is released to
https: //github.com/yangle15/DyFADet-pytorch.
</summary>
    <author>
      <name>Le Yang</name>
    </author>
    <author>
      <name>Ziwei Zheng</name>
    </author>
    <author>
      <name>Yizeng Han</name>
    </author>
    <author>
      <name>Hao Cheng</name>
    </author>
    <author>
      <name>Shiji Song</name>
    </author>
    <author>
      <name>Gao Huang</name>
    </author>
    <author>
      <name>Fan Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.03197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.03197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2008.05721v1</id>
    <updated>2020-08-13T06: 56: 52Z</updated>
    <published>2020-08-13T06: 56: 52Z</published>
    <title>Learning Temporally Invariant and Localizable Features via Data
  Augmentation for Video Recognition</title>
    <summary>  Deep-Learning-based video recognition has shown promising improvements along
with the development of large-scale datasets and spatiotemporal network
architectures. In image recognition, learning spatially invariant features is a
key factor in improving recognition performance and robustness. Data
augmentation based on visual inductive priors, such as cropping, flipping,
rotating, or photometric jittering, is a representative approach to achieve
these features. Recent state-of-the-art recognition solutions have relied on
modern data augmentation strategies that exploit a mixture of augmentation
operations. In this study, we extend these strategies to the temporal dimension
for videos to learn temporally invariant or temporally localizable features to
cover temporal perturbations or complex actions in videos. Based on our novel
temporal data augmentation algorithms, video recognition performances are
improved using only a limited amount of training data compared to the
spatial-only data augmentation algorithms, including the 1st Visual Inductive
Priors (VIPriors) for data-efficient action recognition challenge. Furthermore,
learned features are temporally localizable that cannot be achieved using
spatial augmentation algorithms. Our source code is available at
https: //github.com/taeoh-kim/temporal_data_augmentation.
</summary>
    <author>
      <name>Taeoh Kim</name>
    </author>
    <author>
      <name>Hyeongmin Lee</name>
    </author>
    <author>
      <name>MyeongAh Cho</name>
    </author>
    <author>
      <name>Ho Seong Lee</name>
    </author>
    <author>
      <name>Dong Heon Cho</name>
    </author>
    <author>
      <name>Sangyoun Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">European Conference on Computer Vision (ECCV) 2020,
1st Visual
  Inductive Priors for Data-Efficient Deep Learning Workshop (Oral)</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.05721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.05721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2106.10271v4</id>
    <updated>2022-08-11T14: 04: 47Z</updated>
    <published>2021-06-18T17: 58: 34Z</published>
    <title>End-to-end Temporal Action Detection with Transformer</title>
    <summary>  Temporal action detection (TAD) aims to determine the semantic label and the
temporal interval of every action instance in an untrimmed video. It is a
fundamental and challenging task in video understanding. Previous methods
tackle this task with complicated pipelines. They often need to train multiple
networks and involve hand-designed operations, such as non-maximal suppression
and anchor generation, which limit the flexibility and prevent end-to-end
learning. In this paper, we propose an end-to-end Transformer-based method for
TAD, termed TadTR. Given a small set of learnable embeddings called action
queries, TadTR adaptively extracts temporal context information from the video
for each query and directly predicts action instances with the context. To
adapt Transformer to TAD, we propose three improvements to enhance its locality
awareness. The core is a temporal deformable attention module that selectively
attends to a sparse set of key snippets in a video. A segment refinement
mechanism and an actionness regression head are designed to refine the
boundaries and confidence of the predicted instances, respectively. With such a
simple pipeline, TadTR requires lower computation cost than previous detectors,
while preserving remarkable performance. As a self-contained detector, it
achieves state-of-the-art performance on THUMOS14 (56.7% mAP) and HACS Segments
(32.09% mAP). Combined with an extra action classifier, it obtains 36.75% mAP
on ActivityNet-1.3. Code is available at https: //github.com/xlliu7/TadTR.
</summary>
    <author>
      <name>Xiaolong Liu</name>
    </author>
    <author>
      <name>Qimeng Wang</name>
    </author>
    <author>
      <name>Yao Hu</name>
    </author>
    <author>
      <name>Xu Tang</name>
    </author>
    <author>
      <name>Shiwei Zhang</name>
    </author>
    <author>
      <name>Song Bai</name>
    </author>
    <author>
      <name>Xiang Bai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2022.3195321</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2022.3195321" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IEEE Transactions on Image Processing (TIP). Code:
  https: //github.com/xlliu7/TadTR</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.10271v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.10271v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2211.05299v1</id>
    <updated>2022-11-10T02: 27: 30Z</updated>
    <published>2022-11-10T02: 27: 30Z</published>
    <title>Prior-enhanced Temporal Action Localization using Subject-aware Spatial
  Attention</title>
    <summary>  Temporal action localization (TAL) aims to detect the boundary and identify
the class of each action instance in a long untrimmed video. Current approaches
treat video frames homogeneously, and tend to give background and key objects
excessive attention. This limits their sensitivity to localize action
boundaries. To this end, we propose a prior-enhanced temporal action
localization method (PETAL), which only takes in RGB input and incorporates
action subjects as priors. This proposal leverages action subjects' information
with a plug-and-play subject-aware spatial attention module (SA-SAM) to
generate an aggregated and subject-prioritized representation. Experimental
results on THUMOS-14 and ActivityNet-1.3 datasets demonstrate that the proposed
PETAL achieves competitive performance using only RGB features, e.g., boosting
mAP by 2.41% or 0.25% over the state-of-the-art approach that uses RGB features
or with additional optical flow features on the THUMOS-14 dataset.
</summary>
    <author>
      <name>Yifan Liu</name>
    </author>
    <author>
      <name>Youbao Tang</name>
    </author>
    <author>
      <name>Ning Zhang</name>
    </author>
    <author>
      <name>Ruei-Sung Lin</name>
    </author>
    <author>
      <name>Haoqian Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages,
2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.05299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.05299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2311.00729v2</id>
    <updated>2023-11-04T23: 41: 21Z</updated>
    <published>2023-11-01T00: 17: 37Z</published>
    <title>ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot
  End-to-End Temporal Action Detection</title>
    <summary>  Temporal action detection (TAD) involves the localization and classification
of action instances within untrimmed videos. While standard TAD follows fully
supervised learning with closed-set setting on large training data, recent
zero-shot TAD methods showcase the promising open-set setting by leveraging
large-scale contrastive visual-language (ViL) pretrained models. However,
existing zero-shot TAD methods have limitations on how to properly construct
the strong relationship between two interdependent tasks of localization and
classification and adapt ViL model to video understanding. In this work, we
present ZEETAD, featuring two modules: dual-localization and zero-shot proposal
classification. The former is a Transformer-based module that detects action
events while selectively collecting crucial semantic embeddings for later
recognition. The latter one, CLIP-based module, generates semantic embeddings
from text and frame inputs for each temporal unit. Additionally, we enhance
discriminative capability on unseen classes by minimally updating the frozen
CLIP encoder with lightweight adapters. Extensive experiments on THUMOS14 and
ActivityNet-1.3 datasets demonstrate our approach's superior performance in
zero-shot TAD and effective knowledge transfer from ViL models to unseen action
categories.
</summary>
    <author>
      <name>Thinh Phan</name>
    </author>
    <author>
      <name>Khoa Vo</name>
    </author>
    <author>
      <name>Duy Le</name>
    </author>
    <author>
      <name>Gianfranco Doretto</name>
    </author>
    <author>
      <name>Donald Adjeroh</name>
    </author>
    <author>
      <name>Ngan Le</name>
    </author>
    <link href="http://arxiv.org/abs/2311.00729v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.00729v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1907.09702v1</id>
    <updated>2019-07-23T05: 53: 28Z</updated>
    <published>2019-07-23T05: 53: 28Z</published>
    <title>BMN: Boundary-Matching Network for Temporal Action Proposal Generation</title>
    <summary>  Temporal action proposal generation is an challenging and promising task
which aims to locate temporal regions in real-world videos where action or
event may occur. Current bottom-up proposal generation methods can generate
proposals with precise boundary, but cannot efficiently generate adequately
reliable confidence scores for retrieving proposals. To address these
difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate
confidence scores of densely distributed proposals, which denote a proposal as
a matching pair of starting and ending boundaries and combine all densely
distributed BM pairs into the BM confidence map. Based on BM mechanism, we
propose an effective, efficient and end-to-end proposal generation method,
named Boundary-Matching Network (BMN), which generates proposals with precise
temporal boundaries as well as reliable confidence scores simultaneously. The
two-branches of BMN are jointly trained in an unified framework. We conduct
experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where
BMN shows significant performance improvement with remarkable efficiency and
generalizability. Further, combining with existing action classifier, BMN can
achieve state-of-the-art temporal action detection performance.
</summary>
    <author>
      <name>Tianwei Lin</name>
    </author>
    <author>
      <name>Xiao Liu</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Errui Ding</name>
    </author>
    <author>
      <name>Shilei Wen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accepted by ICCV 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.09702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.09702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2207.06659v1</id>
    <updated>2022-07-14T05: 13: 50Z</updated>
    <published>2022-07-14T05: 13: 50Z</published>
    <title>Forcing the Whole Video as Background: An Adversarial Learning Strategy
  for Weakly Temporal Action Localization</title>
    <summary>  With video-level labels, weakly supervised temporal action localization
(WTAL) applies a localization-by-classification paradigm to detect and classify
the action in untrimmed videos. Due to the characteristic of classification,
class-specific background snippets are inevitably mis-activated to improve the
discriminability of the classifier in WTAL. To alleviate the disturbance of
background, existing methods try to enlarge the discrepancy between action and
background through modeling background snippets with pseudo-snippet-level
annotations, which largely rely on artificial hypotheticals. Distinct from the
previous works, we present an adversarial learning strategy to break the
limitation of mining pseudo background snippets. Concretely, the background
classification loss forces the whole video to be regarded as the background by
a background gradient reinforcement strategy, confusing the recognition model.
Reversely, the foreground(action) loss guides the model to focus on action
snippets under such conditions. As a result, competition between the two
classification losses drives the model to boost its ability for action
modeling. Simultaneously, a novel temporal enhancement network is designed to
facilitate the model to construct temporal relation of affinity snippets based
on the proposed strategy, for further improving the performance of action
localization. Finally, extensive experiments conducted on THUMOS14 and
ActivityNet1.2 demonstrate the effectiveness of the proposed method.
</summary>
    <author>
      <name>Ziqiang Li</name>
    </author>
    <author>
      <name>Yongxin Ge</name>
    </author>
    <author>
      <name>Jiaruo Yu</name>
    </author>
    <author>
      <name>Zhongming Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages,
5 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.06659v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.06659v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2501.13795v1</id>
    <updated>2025-01-23T16: 13: 58Z</updated>
    <published>2025-01-23T16: 13: 58Z</published>
    <title>Training-Free Zero-Shot Temporal Action Detection with Vision-Language
  Models</title>
    <summary>  Existing zero-shot temporal action detection (ZSTAD) methods predominantly
use fully supervised or unsupervised strategies to recognize unseen activities.
However, these training-based methods are prone to domain shifts and require
high computational costs, which hinder their practical applicability in
real-world scenarios. In this paper, unlike previous works, we propose a
training-Free Zero-shot temporal Action Detection (FreeZAD) method, leveraging
existing vision-language (ViL) models to directly classify and localize unseen
activities within untrimmed videos without any additional fine-tuning or
adaptation. We mitigate the need for explicit temporal modeling and reliance on
pseudo-label quality by designing the LOGarithmic decay weighted
Outer-Inner-Contrastive Score (LogOIC) and frequency-based Actionness
Calibration. Furthermore, we introduce a test-time adaptation (TTA) strategy
using Prototype-Centric Sampling (PCS) to expand FreeZAD, enabling ViL models
to adapt more effectively for ZSTAD. Extensive experiments on the THUMOS14 and
ActivityNet-1.3 datasets demonstrate that our training-free method outperforms
state-of-the-art unsupervised methods while requiring only 1/13 of the runtime.
When equipped with TTA, the enhanced method further narrows the gap with fully
supervised methods.
</summary>
    <author>
      <name>Chaolei Han</name>
    </author>
    <author>
      <name>Hongsong Wang</name>
    </author>
    <author>
      <name>Jidong Kuang</name>
    </author>
    <author>
      <name>Lei Zhang</name>
    </author>
    <author>
      <name>Jie Gui</name>
    </author>
    <link href="http://arxiv.org/abs/2501.13795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.13795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1906.06496v4</id>
    <updated>2020-04-24T06: 35: 10Z</updated>
    <published>2019-06-15T08: 35: 34Z</published>
    <title>Accelerating temporal action proposal generation via high performance
  computing</title>
    <summary>  Temporal action recognition always depends on temporal action proposal
generation to hypothesize actions and algorithms usually need to process very
long video sequences and output the starting and ending times of each potential
action in each video suffering from high computation cost. To address this,
based on boundary sensitive network we propose a new temporal convolution
network called Multipath Temporal ConvNet (MTN), which consists of two parts
i.e. Multipath DenseNet and SE-ConvNet. In this work, one novel high
performance ring parallel architecture based on Message Passing Interface (MPI)
is further introduced into temporal action proposal generation, which is a
reliable communication protocol, in order to respond to the requirements of
large memory occupation and a large number of videos. Remarkably, the total
data transmission is reduced by adding a connection between multiple computing
load in the newly developed architecture. It is found that, compared to the
traditional Parameter Server architecture, our parallel architecture has higher
efficiency on temporal action detection task with multiple GPUs, which is
suitable for dealing with the tasks of temporal action proposal generation,
especially for large datasets of millions of videos. We conduct experiments on
ActivityNet-1.3 and THUMOS14, where our method outperforms other state-of-art
temporal action detection methods with high recall and high temporal precision.
In addition, a time metric is further proposed here to evaluate the speed
performance in the distributed training process.
</summary>
    <author>
      <name>Tian Wang</name>
    </author>
    <author>
      <name>Shiye Lei</name>
    </author>
    <author>
      <name>Youyou Jiang</name>
    </author>
    <author>
      <name>Choi Chang</name>
    </author>
    <author>
      <name>Hichem Snoussi</name>
    </author>
    <author>
      <name>Guangcun Shan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11704-021-0173-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11704-021-0173-7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages,
12 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Frontiers of Computer Science volume 16, Article number: 164317
  (2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.06496v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.06496v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2103.05270v1</id>
    <updated>2021-03-09T07: 34: 01Z</updated>
    <published>2021-03-09T07: 34: 01Z</published>
    <title>PcmNet: Position-Sensitive Context Modeling Network for Temporal Action
  Localization</title>
    <summary>  Temporal action localization is an important and challenging task that aims
to locate temporal regions in real-world untrimmed videos where actions occur
and recognize their classes. It is widely acknowledged that video context is a
critical cue for video understanding, and exploiting the context has become an
important strategy to boost localization performance. However, previous
state-of-the-art methods focus more on exploring semantic context which
captures the feature similarity among frames or proposals, and neglect
positional context which is vital for temporal localization. In this paper, we
propose a temporal-position-sensitive context modeling approach to incorporate
both positional and semantic information for more precise action localization.
Specifically, we first augment feature representations with directed temporal
positional encoding, and then conduct attention-based information propagation,
in both frame-level and proposal-level. Consequently, the generated feature
representations are significantly empowered with the discriminative capability
of encoding the position-aware context information, and thus benefit boundary
detection and proposal evaluation. We achieve state-of-the-art performance on
both two challenging datasets, THUMOS-14 and ActivityNet-1.3, demonstrating the
effectiveness and generalization ability of our method.
</summary>
    <author>
      <name>Xin Qin</name>
    </author>
    <author>
      <name>Hanbin Zhao</name>
    </author>
    <author>
      <name>Guangchen Lin</name>
    </author>
    <author>
      <name>Hao Zeng</name>
    </author>
    <author>
      <name>Songcen Xu</name>
    </author>
    <author>
      <name>Xi Li</name>
    </author>
    <link href="http://arxiv.org/abs/2103.05270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.05270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2103.16024v1</id>
    <updated>2021-03-30T02: 01: 03Z</updated>
    <published>2021-03-30T02: 01: 03Z</published>
    <title>Augmented Transformer with Adaptive Graph for Temporal Action Proposal
  Generation</title>
    <summary>  Temporal action proposal generation (TAPG) is a fundamental and challenging
task in video understanding, especially in temporal action detection. Most
previous works focus on capturing the local temporal context and can well
locate simple action instances with clean frames and clear boundaries. However,
they generally fail in complicated scenarios where interested actions involve
irrelevant frames and background clutters, and the local temporal context
becomes less effective. To deal with these problems, we present an augmented
transformer with adaptive graph network (ATAG) to exploit both long-range and
local temporal contexts for TAPG. Specifically, we enhance the vanilla
transformer by equipping a snippet actionness loss and a front block, dubbed
augmented transformer, and it improves the abilities of capturing long-range
dependencies and learning robust feature for noisy action instances.Moreover,
an adaptive graph convolutional network (GCN) is proposed to build local
temporal context by mining the position information and difference between
adjacent features. The features from the two modules carry rich semantic
information of the video, and are fused for effective sequential proposal
generation. Extensive experiments are conducted on two challenging datasets,
THUMOS14 and ActivityNet1.3, and the results demonstrate that our method
outperforms state-of-the-art TAPG methods. Our code will be released soon.
</summary>
    <author>
      <name>Shuning Chang</name>
    </author>
    <author>
      <name>Pichao Wang</name>
    </author>
    <author>
      <name>Fan Wang</name>
    </author>
    <author>
      <name>Hao Li</name>
    </author>
    <author>
      <name>Jiashi Feng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pagess,
4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.16024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.16024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2312.01897v2</id>
    <updated>2024-04-15T07: 15: 43Z</updated>
    <published>2023-12-04T13: 51: 16Z</published>
    <title>Adapting Short-Term Transformers for Action Detection in Untrimmed
  Videos</title>
    <summary>  Vision Transformer (ViT) has shown high potential in video recognition, owing
to its flexible design, adaptable self-attention mechanisms, and the efficacy
of masked pre-training. Yet, it remains unclear how to adapt these pre-trained
short-term ViTs for temporal action detection (TAD) in untrimmed videos. The
existing works treat them as off-the-shelf feature extractors for each
short-trimmed snippet without capturing the fine-grained relation among
different snippets in a broader temporal context. To mitigate this issue, this
paper focuses on designing a new mechanism for adapting these pre-trained ViT
models as a unified long-form video transformer to fully unleash its modeling
power in capturing inter-snippet relation, while still keeping low computation
overhead and memory consumption for efficient TAD. To this end, we design
effective cross-snippet propagation modules to gradually exchange short-term
video information among different snippets from two levels. For inner-backbone
information propagation, we introduce a cross-snippet propagation strategy to
enable multi-snippet temporal feature interaction inside the backbone.For
post-backbone information propagation, we propose temporal transformer layers
for further clip-level modeling. With the plain ViT-B pre-trained with
VideoMAE, our end-to-end temporal action detector (ViT-TAD) yields a very
competitive performance to previous temporal action detectors, riching up to
69.5 average mAP on THUMOS14,
37.40 average mAP on ActivityNet-1.3 and 17.20
average mAP on FineAction.
</summary>
    <author>
      <name>Min Yang</name>
    </author>
    <author>
      <name>Huan Gao</name>
    </author>
    <author>
      <name>Ping Guo</name>
    </author>
    <author>
      <name>Limin Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVPR2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.01897v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.01897v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2110.11474v2</id>
    <updated>2021-10-25T01: 12: 47Z</updated>
    <published>2021-10-21T20: 43: 42Z</published>
    <title>AEI: Actors-Environment Interaction with Adaptive Attention for Temporal
  Action Proposals Generation</title>
    <summary>  Humans typically perceive the establishment of an action in a video through
the interaction between an actor and the surrounding environment. An action
only starts when the main actor in the video begins to interact with the
environment, while it ends when the main actor stops the interaction. Despite
the great progress in temporal action proposal generation, most existing works
ignore the aforementioned fact and leave their model learning to propose
actions as a black-box. In this paper, we make an attempt to simulate that
ability of a human by proposing Actor Environment Interaction (AEI) network to
improve the video representation for temporal action proposals generation. AEI
contains two modules, i.e., perception-based visual representation (PVR) and
boundary-matching module (BMM). PVR represents each video snippet by taking
human-human relations and humans-environment relations into consideration using
the proposed adaptive attention mechanism. Then, the video representation is
taken by BMM to generate action proposals. AEI is comprehensively evaluated in
ActivityNet-1.3 and THUMOS-14 datasets, on temporal action proposal and
detection tasks, with two boundary-matching architectures (i.e., CNN-based and
GCN-based) and two classifiers (i.e., Unet and P-GCN). Our AEI robustly
outperforms the state-of-the-art methods with remarkable performance and
generalization for both temporal action proposal generation and temporal action
detection.
</summary>
    <author>
      <name>Khoa Vo</name>
    </author>
    <author>
      <name>Hyekang Joo</name>
    </author>
    <author>
      <name>Kashu Yamazaki</name>
    </author>
    <author>
      <name>Sang Truong</name>
    </author>
    <author>
      <name>Kris Kitani</name>
    </author>
    <author>
      <name>Minh-Triet Tran</name>
    </author>
    <author>
      <name>Ngan Le</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in BMVC 2021 (Oral Session)</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.11474v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.11474v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2504.13460v2</id>
    <updated>2025-04-23T10: 12: 52Z</updated>
    <published>2025-04-18T04: 35: 35Z</published>
    <title>Chain-of-Thought Textual Reasoning for Few-shot Temporal Action
  Localization</title>
    <summary>  Traditional temporal action localization (TAL) methods rely on large amounts
of detailed annotated data, whereas few-shot TAL reduces this dependence by
using only a few training samples to identify unseen action categories.
However, existing few-shot TAL methods typically focus solely on video-level
information, neglecting textual information, which can provide valuable
semantic support for the localization task. Therefore, we propose a new
few-shot temporal action localization method by Chain-of-Thought textual
reasoning to improve localization performance. Specifically, we design a novel
few-shot learning framework that leverages textual semantic information to
enhance the model's ability to capture action commonalities and variations,
which includes a semantic-aware text-visual alignment module designed to align
the query and support videos at different levels. Meanwhile, to better express
the temporal dependencies and causal relationships between actions at the
textual level to assist action localization, we design a Chain of Thought
(CoT)-like reasoning method that progressively guides the Vision Language Model
(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for
videos. The generated texts can capture more variance of action than visual
features. We conduct extensive experiments on the publicly available
ActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named
Human-related Anomaly Localization and explore the application of the TAL task
in human anomaly detection. The experimental results demonstrate that our
proposed method significantly outperforms existing methods in single-instance
and multi-instance scenarios. We will release our code, data and benchmark.
</summary>
    <author>
      <name>Hongwei Ji</name>
    </author>
    <author>
      <name>Wulian Yun</name>
    </author>
    <author>
      <name>Mengshi Qi</name>
    </author>
    <author>
      <name>Huadong Ma</name>
    </author>
    <link href="http://arxiv.org/abs/2504.13460v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.13460v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2109.08847v1</id>
    <updated>2021-09-18T06: 15: 19Z</updated>
    <published>2021-09-18T06: 15: 19Z</published>
    <title>Towards High-Quality Temporal Action Detection with Sparse Proposals</title>
    <summary>  Temporal Action Detection (TAD) is an essential and challenging topic in
video understanding, aiming to localize the temporal segments containing human
action instances and predict the action categories. The previous works greatly
rely upon dense candidates either by designing varying anchors or enumerating
all the combinations of boundaries on video sequences; therefore, they are
related to complicated pipelines and sensitive hand-crafted designs. Recently,
with the resurgence of Transformer, query-based methods have tended to become
the rising solutions for their simplicity and flexibility. However, there still
exists a performance gap between query-based methods and well-established
methods. In this paper, we identify the main challenge lies in the large
variants of action duration and the ambiguous boundaries for short action
instances; nevertheless, quadratic-computational global attention prevents
query-based methods to build multi-scale feature maps. Towards high-quality
temporal action detection, we introduce Sparse Proposals to interact with the
hierarchical features. In our method, named SP-TAD, each proposal attends to a
local segment feature in the temporal feature pyramid. The local interaction
enables utilization of high-resolution features to preserve action instances
details. Extensive experiments demonstrate the effectiveness of our method,
especially under high tIoU thresholds. E.g., we achieve the state-of-the-art
performance on THUMOS14 (45.7% on mAP@0.6,
33.4% on mAP@0.7 and 53.5% on
mAP@Avg) and competitive results on ActivityNet-1.3 (32.99% on mAP@Avg). Code
will be made available at https: //github.com/wjn922/SP-TAD.
</summary>
    <author>
      <name>Jiannan Wu</name>
    </author>
    <author>
      <name>Peize Sun</name>
    </author>
    <author>
      <name>Shoufa Chen</name>
    </author>
    <author>
      <name>Jiewen Yang</name>
    </author>
    <author>
      <name>Zihao Qi</name>
    </author>
    <author>
      <name>Lan Ma</name>
    </author>
    <author>
      <name>Ping Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,
5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.08847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.08847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2307.07869v1</id>
    <updated>2023-07-15T18: 57: 27Z</updated>
    <published>2023-07-15T18: 57: 27Z</published>
    <title>Custom DNN using Reward Modulated Inverted STDP Learning for Temporal
  Pattern Recognition</title>
    <summary>  Temporal spike recognition plays a crucial role in various domains, including
anomaly detection, keyword spotting and neuroscience. This paper presents a
novel algorithm for efficient temporal spike pattern recognition on sparse
event series data. The algorithm leverages a combination of reward-modulatory
behavior, Hebbian and anti-Hebbian based learning methods to identify patterns
in dynamic datasets with short intervals of training. The algorithm begins with
a preprocessing step, where the input data is rationalized and translated to a
feature-rich yet sparse spike time series data. Next, a linear feed forward
spiking neural network processes this data to identify a trained pattern.
Finally, the next layer performs a weighted check to ensure the correct pattern
has been detected.To evaluate the performance of the proposed algorithm, it was
trained on a complex dataset containing spoken digits with spike information
and its output compared to state-of-the-art.
</summary>
    <author>
      <name>Vijay Shankaran Vivekanand</name>
    </author>
    <author>
      <name>Rajkumar Kubendran</name>
    </author>
    <link href="http://arxiv.org/abs/2307.07869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.07869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2102.01894v3</id>
    <updated>2021-08-19T13: 42: 57Z</updated>
    <published>2021-02-03T06: 29: 28Z</published>
    <title>Relaxed Transformer Decoders for Direct Action Proposal Generation</title>
    <summary>  Temporal action proposal generation is an important and challenging task in
video understanding, which aims at detecting all temporal segments containing
action instances of interest. The existing proposal generation approaches are
generally based on pre-defined anchor windows or heuristic bottom-up boundary
matching strategies. This paper presents a simple and efficient framework
(RTD-Net) for direct action proposal generation, by re-purposing a
Transformer-alike architecture. To tackle the essential visual difference
between time and space, we make three important improvements over the original
transformer detection framework (DETR). First, to deal with slowness prior in
videos, we replace the original Transformer encoder with a boundary attentive
module to better capture long-range temporal information. Second, due to the
ambiguous temporal boundary and relatively sparse annotations, we present a
relaxed matching scheme to relieve the strict criteria of single assignment to
each groundtruth. Finally, we devise a three-branch head to further improve the
proposal confidence estimation by explicitly predicting its completeness.
Extensive experiments on THUMOS14 and ActivityNet-1.3 benchmarks demonstrate
the effectiveness of RTD-Net, on both tasks of temporal action proposal
generation and temporal action detection. Moreover, due to its simplicity in
design, our framework is more efficient than previous proposal generation
methods, without non-maximum suppression post-processing. The code and models
are made available at https: //github.com/MCG-NJU/RTD-Action.
</summary>
    <author>
      <name>Jing Tan</name>
    </author>
    <author>
      <name>Jiaqi Tang</name>
    </author>
    <author>
      <name>Limin Wang</name>
    </author>
    <author>
      <name>Gangshan Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV 2021 camera ready version</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.01894v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.01894v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2212.09335v1</id>
    <updated>2022-12-19T10: 02: 50Z</updated>
    <published>2022-12-19T10: 02: 50Z</published>
    <title>Distilling Vision-Language Pre-training to Collaborate with
  Weakly-Supervised Temporal Action Localization</title>
    <summary>  Weakly-supervised temporal action localization (WTAL) learns to detect and
classify action instances with only category labels. Most methods widely adopt
the off-the-shelf Classification-Based Pre-training (CBP) to generate video
features for action localization. However, the different optimization
objectives between classification and localization, make temporally localized
results suffer from the serious incomplete issue. To tackle this issue without
additional annotations, this paper considers to distill free action knowledge
from Vision-Language Pre-training (VLP), since we surprisingly observe that the
localization results of vanilla VLP have an over-complete issue, which is just
complementary to the CBP results. To fuse such complementarity, we propose a
novel distillation-collaboration framework with two branches acting as CBP and
VLP respectively. The framework is optimized through a dual-branch alternate
training strategy. Specifically, during the B step, we distill the confident
background pseudo-labels from the CBP branch; while during the F step, the
confident foreground pseudo-labels are distilled from the VLP branch. And as a
result, the dual-branch complementarity is effectively fused to promote a
strong alliance. Extensive experiments and ablation studies on THUMOS14 and
ActivityNet1.2 reveal that our method significantly outperforms
state-of-the-art methods.
</summary>
    <author>
      <name>Chen Ju</name>
    </author>
    <author>
      <name>Kunhao Zheng</name>
    </author>
    <author>
      <name>Jinxiang Liu</name>
    </author>
    <author>
      <name>Peisen Zhao</name>
    </author>
    <author>
      <name>Ya Zhang</name>
    </author>
    <author>
      <name>Jianlong Chang</name>
    </author>
    <author>
      <name>Yanfeng Wang</name>
    </author>
    <author>
      <name>Qi Tian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The first two authors share the same contribution</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.09335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.09335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2206.11493v1</id>
    <updated>2022-06-23T06: 30: 08Z</updated>
    <published>2022-06-23T06: 30: 08Z</published>
    <title>Learning to Refactor Action and Co-occurrence Features for Temporal
  Action Localization</title>
    <summary>  The main challenge of Temporal Action Localization is to retrieve subtle
human actions from various co-occurring ingredients, e.g., context and
background, in an untrimmed video. While prior approaches have achieved
substantial progress through devising advanced action detectors, they still
suffer from these co-occurring ingredients which often dominate the actual
action content in videos. In this paper, we explore two orthogonal but
complementary aspects of a video snippet, i.e., the action features and the
co-occurrence features. Especially, we develop a novel auxiliary task by
decoupling these two types of features within a video snippet and recombining
them to generate a new feature representation with more salient action
information for accurate action localization. We term our method RefactorNet,
which first explicitly factorizes the action content and regularizes its
co-occurrence features, and then synthesizes a new action-dominated video
representation. Extensive experimental results and ablation studies on THUMOS14
and ActivityNet v1.3 demonstrate that our new representation, combined with a
simple action detector, can significantly improve the action localization
performance.
</summary>
    <author>
      <name>Kun Xia</name>
    </author>
    <author>
      <name>Le Wang</name>
    </author>
    <author>
      <name>Sanping Zhou</name>
    </author>
    <author>
      <name>Nanning Zheng</name>
    </author>
    <author>
      <name>Wei Tang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVPR 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.11493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.11493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2103.13183v2</id>
    <updated>2021-03-31T02: 34: 57Z</updated>
    <published>2021-03-24T13: 34: 42Z</published>
    <title>The Blessings of Unlabeled Background in Untrimmed Videos</title>
    <summary>  Weakly-supervised Temporal Action Localization (WTAL) aims to detect the
action segments with only video-level action labels in training. The key
challenge is how to distinguish the action of interest segments from the
background, which is unlabelled even on the video-level. While previous works
treat the background as "curses", we consider it as "blessings". Specifically,
we first use causal analysis to point out that the common localization errors
are due to the unobserved confounder that resides ubiquitously in visual
recognition. Then, we propose a Temporal Smoothing PCA-based (TS-PCA)
deconfounder, which exploits the unlabelled background to model an observed
substitute for the unobserved confounder, to remove the confounding effect.
Note that the proposed deconfounder is model-agnostic and non-intrusive, and
hence can be applied in any WTAL method without model re-designs. Through
extensive experiments on four state-of-the-art WTAL methods, we show that the
deconfounder can improve all of them on the public datasets: THUMOS-14 and
ActivityNet-1.3.
</summary>
    <author>
      <name>Yuan Liu</name>
    </author>
    <author>
      <name>Jingyuan Chen</name>
    </author>
    <author>
      <name>Zhenfang Chen</name>
    </author>
    <author>
      <name>Bing Deng</name>
    </author>
    <author>
      <name>Jianqiang Huang</name>
    </author>
    <author>
      <name>Hanwang Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVPR2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.13183v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.13183v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1811.11524v2</id>
    <updated>2019-04-12T06: 06: 42Z</updated>
    <published>2018-11-28T12: 47: 16Z</published>
    <title>Multi-granularity Generator for Temporal Action Proposal</title>
    <summary>  Temporal action proposal generation is an important task, aiming to localize
the video segments containing human actions in an untrimmed video. In this
paper, we propose a multi-granularity generator (MGG) to perform the temporal
action proposal from different granularity perspectives, relying on the video
visual features equipped with the position embedding information. First, we
propose to use a bilinear matching model to exploit the rich local information
within the video sequence. Afterwards, two components, namely segment proposal
producer (SPP) and frame actionness producer (FAP), are combined to perform the
task of temporal action proposal at two distinct granularities. SPP considers
the whole video in the form of feature pyramid and generates segment proposals
from one coarse perspective, while FAP carries out a finer actionness
evaluation for each video frame. Our proposed MGG can be trained in an
end-to-end fashion. By temporally adjusting the segment proposals with
fine-grained frame actionness information, MGG achieves the superior
performance over state-of-the-art methods on the public THUMOS-14 and
ActivityNet-1.3 datasets. Moreover, we employ existing action classifiers to
perform the classification of the proposals generated by MGG, leading to
significant improvements compared against the competing methods for the video
detection task.
</summary>
    <author>
      <name>Yuan Liu</name>
    </author>
    <author>
      <name>Lin Ma</name>
    </author>
    <author>
      <name>Yifeng Zhang</name>
    </author>
    <author>
      <name>Wei Liu</name>
    </author>
    <author>
      <name>Shih-Fu Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CVPR 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.11524v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.11524v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2109.02613v1</id>
    <updated>2021-09-06T17: 22: 46Z</updated>
    <published>2021-09-06T17: 22: 46Z</published>
    <title>Class Semantics-based Attention for Action Detection</title>
    <summary>  Action localization networks are often structured as a feature encoder
sub-network and a localization sub-network, where the feature encoder learns to
transform an input video to features that are useful for the localization
sub-network to generate reliable action proposals. While some of the encoded
features may be more useful for generating action proposals, prior action
localization approaches do not include any attention mechanism that enables the
localization sub-network to attend more to the more important features. In this
paper, we propose a novel attention mechanism, the Class Semantics-based
Attention (CSA), that learns from the temporal distribution of semantics of
action classes present in an input video to find the importance scores of the
encoded features, which are used to provide attention to the more useful
encoded features. We demonstrate on two popular action detection datasets that
incorporating our novel attention mechanism provides considerable performance
gains on competitive action detection models (e.g., around 6.2% improvement
over BMN action detection baseline to obtain 47.5% mAP on the THUMOS-14
dataset), and a new state-of-the-art of 36.25% mAP on the ActivityNet v1.3
dataset. Further, the CSA localization model family which includes BMN-CSA, was
part of the second-placed submission at the 2021 ActivityNet action
localization challenge. Our attention mechanism outperforms prior
self-attention modules such as the squeeze-and-excitation in action detection
task. We also observe that our attention mechanism is complementary to such
self-attention modules in that performance improvements are seen when both are
used together.
</summary>
    <author>
      <name>Deepak Sridhar</name>
    </author>
    <author>
      <name>Niamul Quader</name>
    </author>
    <author>
      <name>Srikanth Muralidharan</name>
    </author>
    <author>
      <name>Yaoxin Li</name>
    </author>
    <author>
      <name>Peng Dai</name>
    </author>
    <author>
      <name>Juwei Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICCV 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.02613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.02613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1812.10000v1</id>
    <updated>2018-12-25T00: 35: 31Z</updated>
    <published>2018-12-25T00: 35: 31Z</published>
    <title>Similarity R-C3D for Few-shot Temporal Activity Detection</title>
    <summary>  Many activities of interest are rare events, with only a few labeled examples
available. Therefore models for temporal activity detection which are able to
learn from a few examples are desirable. In this paper, we present a
conceptually simple and general yet novel framework for few-shot temporal
activity detection which detects the start and end time of the few-shot input
activities in an untrimmed video. Our model is end-to-end trainable and can
benefit from more few-shot examples. At test time, each proposal is assigned
the label of the few-shot activity class corresponding to the maximum
similarity score. Our Similarity R-C3D method outperforms previous work on
three large-scale benchmarks for temporal activity detection (THUMOS14,
ActivityNet1.2, and ActivityNet1.3 datasets) in the few-shot setting. Our code
will be made available.
</summary>
    <author>
      <name>Huijuan Xu</name>
    </author>
    <author>
      <name>Bingyi Kang</name>
    </author>
    <author>
      <name>Ximeng Sun</name>
    </author>
    <author>
      <name>Jiashi Feng</name>
    </author>
    <author>
      <name>Kate Saenko</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <link href="http://arxiv.org/abs/1812.10000v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.10000v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2110.01774v2</id>
    <updated>2024-11-22T22: 49: 52Z</updated>
    <published>2021-10-05T01: 18: 15Z</published>
    <title>HighlightMe: Detecting Highlights from Human-Centric Videos</title>
    <summary>  We present a domain- and user-preference-agnostic approach to detect
highlightable excerpts from human-centric videos. Our method works on the
graph-based representation of multiple observable human-centric modalities in
the videos, such as poses and faces. We use an autoencoder network equipped
with spatial-temporal graph convolutions to detect human activities and
interactions based on these modalities. We train our network to map the
activity- and interaction-based latent structural representations of the
different modalities to per-frame highlight scores based on the
representativeness of the frames. We use these scores to compute which frames
to highlight and stitch contiguous frames to produce the excerpts. We train our
network on the large-scale AVA-Kinetics action dataset and evaluate it on four
benchmark video highlight datasets: DSH, TVSum, PHD2, and SumMe. We observe a
4-12% improvement in the mean average precision of matching the human-annotated
highlights over state-of-the-art methods in these datasets, without requiring
any user-provided preferences or dataset-specific fine-tuning.
</summary>
    <author>
      <name>Uttaran Bhattacharya</name>
    </author>
    <author>
      <name>Gang Wu</name>
    </author>
    <author>
      <name>Stefano Petrangeli</name>
    </author>
    <author>
      <name>Viswanathan Swaminathan</name>
    </author>
    <author>
      <name>Dinesh Manocha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICCV48922.2021.00805</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICCV48922.2021.00805" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,
5 figures,
5 tables. In Proceedings of the IEEE/CVF
  International Conference on Computer Vision (ICCV),
2021</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV),
2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2110.01774v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.01774v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1911.11462v2</id>
    <updated>2020-04-02T21: 48: 21Z</updated>
    <published>2019-11-26T11: 27: 09Z</published>
    <title>G-TAD: Sub-Graph Localization for Temporal Action Detection</title>
    <summary>  Temporal action detection is a fundamental yet challenging task in video
understanding. Video context is a critical cue to effectively detect actions,
but current works mainly focus on temporal context, while neglecting semantic
context as well as other important context properties. In this work, we propose
a graph convolutional network (GCN) model to adaptively incorporate multi-level
semantic context into video features and cast temporal action detection as a
sub-graph localization problem. Specifically, we formulate video snippets as
graph nodes, snippet-snippet correlations as edges, and actions associated with
context as target sub-graphs. With graph convolution as the basic operation, we
design a GCN block called GCNeXt, which learns the features of each node by
aggregating its context and dynamically updates the edges in the graph. To
localize each sub-graph, we also design an SGAlign layer to embed each
sub-graph into the Euclidean space. Extensive experiments show that G-TAD is
capable of finding effective video context without extra supervision and
achieves state-of-the-art performance on two detection benchmarks. On
ActivityNet-1.3, it obtains an average mAP of 34.09%; on THUMOS14, it reaches
51.6% at IoU@0.5 when combined with a proposal processing method. G-TAD code is
publicly available at https: //github.com/frostinassiky/gtad.
</summary>
    <author>
      <name>Mengmeng Xu</name>
    </author>
    <author>
      <name>Chen Zhao</name>
    </author>
    <author>
      <name>David S. Rojas</name>
    </author>
    <author>
      <name>Ali Thabet</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVPR2020. 8 pages,
9 figures,
2 pages appendix</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.11462v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.11462v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2004.00137v1</id>
    <updated>2020-03-31T22: 02: 38Z</updated>
    <published>2020-03-31T22: 02: 38Z</published>
    <title>Revisiting Few-shot Activity Detection with Class Similarity Control</title>
    <summary>  Many interesting events in the real world are rare making preannotated
machine learning ready videos a rarity in consequence. Thus, temporal activity
detection models that are able to learn from a few examples are desirable. In
this paper, we present a conceptually simple and general yet novel framework
for few-shot temporal activity detection based on proposal regression which
detects the start and end time of the activities in untrimmed videos. Our model
is end-to-end trainable, takes into account the frame rate differences between
few-shot activities and untrimmed test videos, and can benefit from additional
few-shot examples. We experiment on three large scale benchmarks for temporal
activity detection (ActivityNet1.2, ActivityNet1.3 and THUMOS14 datasets) in a
few-shot setting. We also study the effect on performance of different amount
of overlap with activities used to pretrain the video classification backbone
and propose corrective measures for future works in this domain. Our code will
be made available.
</summary>
    <author>
      <name>Huijuan Xu</name>
    </author>
    <author>
      <name>Ximeng Sun</name>
    </author>
    <author>
      <name>Eric Tzeng</name>
    </author>
    <author>
      <name>Abir Das</name>
    </author>
    <author>
      <name>Kate Saenko</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <link href="http://arxiv.org/abs/2004.00137v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.00137v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2003.07734v1</id>
    <updated>2020-03-17T14: 11: 24Z</updated>
    <published>2020-03-17T14: 11: 24Z</published>
    <title>A Novel Online Action Detection Framework from Untrimmed Video Streams</title>
    <summary>  Online temporal action localization from an untrimmed video stream is a
challenging problem in computer vision. It is challenging because of i) in an
untrimmed video stream, more than one action instance may appear, including
background scenes, and ii) in online settings, only past and current
information is available. Therefore, temporal priors, such as the average
action duration of training data, which have been exploited by previous action
detection methods, are not suitable for this task because of the high
intra-class variation in human actions. We propose a novel online action
detection framework that considers actions as a set of temporally ordered
subclasses and leverages a future frame generation network to cope with the
limited information issue associated with the problem outlined above.
Additionally, we augment our data by varying the lengths of videos to allow the
proposed method to learn about the high intra-class variation in human actions.
We evaluate our method using two benchmark datasets, THUMOS'14 and ActivityNet,
for an online temporal action localization scenario and demonstrate that the
performance is comparable to state-of-the-art methods that have been proposed
for offline settings.
</summary>
    <author>
      <name>Da-Hye Yoon</name>
    </author>
    <author>
      <name>Nam-Gyu Cho</name>
    </author>
    <author>
      <name>Seong-Whan Lee</name>
    </author>
    <link href="http://arxiv.org/abs/2003.07734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.07734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2405.08204v1</id>
    <updated>2024-05-13T21: 47: 35Z</updated>
    <published>2024-05-13T21: 47: 35Z</published>
    <title>A Semantic and Motion-Aware Spatiotemporal Transformer Network for
  Action Detection</title>
    <summary>  This paper presents a novel spatiotemporal transformer network that
introduces several original components to detect actions in untrimmed videos.
First, the multi-feature selective semantic attention model calculates the
correlations between spatial and motion features to model spatiotemporal
interactions between different action semantics properly. Second, the
motion-aware network encodes the locations of action semantics in video frames
utilizing the motion-aware 2D positional encoding algorithm. Such a
motion-aware mechanism memorizes the dynamic spatiotemporal variations in
action frames that current methods cannot exploit. Third, the sequence-based
temporal attention model captures the heterogeneous temporal dependencies in
action frames. In contrast to standard temporal attention used in natural
language processing, primarily aimed at finding similarities between linguistic
words, the proposed sequence-based temporal attention is designed to determine
both the differences and similarities between video frames that jointly define
the meaning of actions. The proposed approach outperforms the state-of-the-art
solutions on four spatiotemporal action datasets: AVA 2.2, AVA 2.1, UCF101-24,
and EPIC-Kitchens.
</summary>
    <author>
      <name>Matthew Korban</name>
    </author>
    <author>
      <name>Peter Youngs</name>
    </author>
    <author>
      <name>Scott T. Acton</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TPAMI.2024.3377192</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TPAMI.2024.3377192" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Pattern Analysis and Machine Intelligence (2024)</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.08204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.08204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1906.08547v1</id>
    <updated>2019-06-20T10: 43: 00Z</updated>
    <published>2019-06-20T10: 43: 00Z</published>
    <title>vireoJD-MM at Activity Detection in Extended Videos</title>
    <summary>  This notebook paper presents an overview and comparative analysis of our
system designed for activity detection in extended videos (ActEV-PC) in
ActivityNet Challenge 2019. Specifically, we exploit person/vehicle detections
in spatial level and action localization in temporal level for action detection
in surveillance videos. The mechanism of different tubelet generation and model
decomposition methods are studied as well. The detection results are finally
predicted by late fusing the results from each component.
</summary>
    <author>
      <name>Fuchen Long</name>
    </author>
    <author>
      <name>Qi Cai</name>
    </author>
    <author>
      <name>Zhaofan Qiu</name>
    </author>
    <author>
      <name>Zhijian Hou</name>
    </author>
    <author>
      <name>Yingwei Pan</name>
    </author>
    <author>
      <name>Ting Yao</name>
    </author>
    <author>
      <name>Chong-Wah Ngo</name>
    </author>
    <link href="http://arxiv.org/abs/1906.08547v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.08547v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2204.10160v2</id>
    <updated>2022-04-25T23: 37: 46Z</updated>
    <published>2022-04-21T15: 14: 02Z</published>
    <title>A Multi-Person Video Dataset Annotation Method of Spatio-Temporally
  Actions</title>
    <summary>  Spatio-temporal action detection is an important and challenging problem in
video understanding. However, the application of the existing large-scale
spatio-temporal action datasets in specific fields is limited, and there is
currently no public tool for making spatio-temporal action datasets, it takes a
lot of time and effort for researchers to customize the spatio-temporal action
datasets, so we propose a multi-Person video dataset Annotation Method of
spatio-temporally actions.First, we use ffmpeg to crop the videos and frame the
videos; then use yolov5 to detect human in the video frame, and then use deep
sort to detect the ID of the human in the video frame. By processing the
detection results of yolov5 and deep sort, we can get the annotation file of
the spatio-temporal action dataset to complete the work of customizing the
spatio-temporal action dataset.
https: //github.com/Whiffe/Custom-ava-dataset_Custom-Spatio-Temporally-Action-Video-Dataset
</summary>
    <author>
      <name>Fan Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2204.10160v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.10160v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1704.06228v2</id>
    <updated>2017-09-18T08: 43: 11Z</updated>
    <published>2017-04-20T16: 51: 45Z</published>
    <title>Temporal Action Detection with Structured Segment Networks</title>
    <summary>  Detecting actions in untrimmed videos is an important yet challenging task.
In this paper, we present the structured segment network (SSN), a novel
framework which models the temporal structure of each action instance via a
structured temporal pyramid. On top of the pyramid, we further introduce a
decomposed discriminative model comprising two classifiers, respectively for
classifying actions and determining completeness. This allows the framework to
effectively distinguish positive proposals from background or incomplete ones,
thus leading to both accurate recognition and localization. These components
are integrated into a unified network that can be efficiently trained in an
end-to-end fashion. Additionally, a simple yet effective temporal action
proposal scheme, dubbed temporal actionness grouping (TAG) is devised to
generate high quality action proposals. On two challenging benchmarks, THUMOS14
and ActivityNet, our method remarkably outperforms previous state-of-the-art
methods, demonstrating superior accuracy and strong adaptivity in handling
actions with various temporal structures.
</summary>
    <author>
      <name>Yue Zhao</name>
    </author>
    <author>
      <name>Yuanjun Xiong</name>
    </author>
    <author>
      <name>Limin Wang</name>
    </author>
    <author>
      <name>Zhirong Wu</name>
    </author>
    <author>
      <name>Xiaoou Tang</name>
    </author>
    <author>
      <name>Dahua Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ICCV2017. Code &amp; models available at
  http: //yjxiong.me/others/ssn</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.06228v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.06228v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1807.04821v2</id>
    <updated>2018-07-18T20: 25: 26Z</updated>
    <published>2018-07-12T21: 07: 01Z</published>
    <title>CTAP: Complementary Temporal Action Proposal Generation</title>
    <summary>  Temporal action proposal generation is an important task, akin to object
proposals, temporal action proposals are intended to capture "clips" or
temporal intervals in videos that are likely to contain an action. Previous
methods can be divided to two groups: sliding window ranking and actionness
score grouping. Sliding windows uniformly cover all segments in videos, but the
temporal boundaries are imprecise; grouping based method may have more precise
boundaries but it may omit some proposals when the quality of actionness score
is low. Based on the complementary characteristics of these two methods, we
propose a novel Complementary Temporal Action Proposal (CTAP) generator.
Specifically, we apply a Proposal-level Actionness Trustworthiness Estimator
(PATE) on the sliding windows proposals to generate the probabilities
indicating whether the actions can be correctly detected by actionness scores,
the windows with high scores are collected. The collected sliding windows and
actionness proposals are then processed by a temporal convolutional neural
network for proposal ranking and boundary adjustment. CTAP outperforms
state-of-the-art methods on average recall (AR) by a large margin on THUMOS-14
and ActivityNet 1.3 datasets. We further apply CTAP as a proposal generation
method in an existing action detector, and show consistent significant
improvements.
</summary>
    <author>
      <name>Jiyang Gao</name>
    </author>
    <author>
      <name>Kan Chen</name>
    </author>
    <author>
      <name>Ram Nevatia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2018 main conference paper (camera ready version). Code is
  available in http: //www.github.com/jiyanggao/CTAP</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04821v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04821v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2112.03612v1</id>
    <updated>2021-12-07T10: 14: 26Z</updated>
    <published>2021-12-07T10: 14: 26Z</published>
    <title>DCAN: Improving Temporal Action Detection via Dual Context Aggregation</title>
    <summary>  Temporal action detection aims to locate the boundaries of action in the
video. The current method based on boundary matching enumerates and calculates
all possible boundary matchings to generate proposals. However, these methods
neglect the long-range context aggregation in boundary prediction. At the same
time, due to the similar semantics of adjacent matchings, local semantic
aggregation of densely-generated matchings cannot improve semantic richness and
discrimination. In this paper, we propose the end-to-end proposal generation
method named Dual Context Aggregation Network (DCAN) to aggregate context on
two levels, namely, boundary level and proposal level, for generating
high-quality action proposals, thereby improving the performance of temporal
action detection. Specifically, we design the Multi-Path Temporal Context
Aggregation (MTCA) to achieve smooth context aggregation on boundary level and
precise evaluation of boundaries. For matching evaluation, Coarse-to-fine
Matching (CFM) is designed to aggregate context on the proposal level and
refine the matching map from coarse to fine. We conduct extensive experiments
on ActivityNet v1.3 and THUMOS-14. DCAN obtains an average mAP of 35.39% on
ActivityNet v1.3 and reaches mAP 54.14% at IoU@0.5 on THUMOS-14, which
demonstrates DCAN can generate high-quality proposals and achieve
state-of-the-art performance. We release the code at
https: //github.com/cg1177/DCAN.
</summary>
    <author>
      <name>Guo Chen</name>
    </author>
    <author>
      <name>Yin-Dong Zheng</name>
    </author>
    <author>
      <name>Limin Wang</name>
    </author>
    <author>
      <name>Tong Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2022 camera ready version</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.03612v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.03612v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1807.02929v2</id>
    <updated>2018-07-18T08: 12: 36Z</updated>
    <published>2018-07-09T03: 33: 54Z</published>
    <title>Step-by-step Erasion, One-by-one Collection: A Weakly Supervised
  Temporal Action Detector</title>
    <summary>  Weakly supervised temporal action detection is a Herculean task in
understanding untrimmed videos, since no supervisory signal except the
video-level category label is available on training data. Under the supervision
of category labels, weakly supervised detectors are usually built upon
classifiers. However, there is an inherent contradiction between classifier and
detector; i.e., a classifier in pursuit of high classification performance
prefers top-level discriminative video clips that are extremely fragmentary,
whereas a detector is obliged to discover the whole action instance without
missing any relevant snippet. To reconcile this contradiction, we train a
detector by driving a series of classifiers to find new actionness clips
progressively, via step-by-step erasion from a complete video. During the test
phase, all we need to do is to collect detection results from the one-by-one
trained classifiers at various erasing steps. To assist in the collection
process, a fully connected conditional random field is established to refine
the temporal localization outputs. We evaluate our approach on two prevailing
datasets, THUMOS'14 and ActivityNet. The experiments show that our detector
advances state-of-the-art weakly supervised temporal action detection results,
and even compares with quite a few strongly supervised methods.
</summary>
    <author>
      <name>Jia-Xing Zhong</name>
    </author>
    <author>
      <name>Nannan Li</name>
    </author>
    <author>
      <name>Weijie Kong</name>
    </author>
    <author>
      <name>Tao Zhang</name>
    </author>
    <author>
      <name>Thomas H. Li</name>
    </author>
    <author>
      <name>Ge Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To Appear in ACM Multimedia 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.02929v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02929v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2210.02578v1</id>
    <updated>2022-10-05T21: 57: 25Z</updated>
    <published>2022-10-05T21: 57: 25Z</published>
    <title>AOE-Net: Entities Interactions Modeling with Adaptive Attention
  Mechanism for Temporal Action Proposals Generation</title>
    <summary>  Temporal action proposal generation (TAPG) is a challenging task, which
requires localizing action intervals in an untrimmed video. Intuitively, we as
humans, perceive an action through the interactions between actors, relevant
objects, and the surrounding environment. Despite the significant progress of
TAPG, a vast majority of existing methods ignore the aforementioned principle
of the human perceiving process by applying a backbone network into a given
video as a black-box. In this paper, we propose to model these interactions
with a multi-modal representation network, namely, Actors-Objects-Environment
Interaction Network (AOE-Net). Our AOE-Net consists of two modules, i.e.,
perception-based multi-modal representation (PMR) and boundary-matching module
(BMM). Additionally, we introduce adaptive attention mechanism (AAM) in PMR to
focus only on main actors (or relevant objects) and model the relationships
among them. PMR module represents each video snippet by a visual-linguistic
feature, in which main actors and surrounding environment are represented by
visual information, whereas relevant objects are depicted by linguistic
features through an image-text model. BMM module processes the sequence of
visual-linguistic features as its input and generates action proposals.
Comprehensive experiments and extensive ablation studies on ActivityNet-1.3 and
THUMOS-14 datasets show that our proposed AOE-Net outperforms previous
state-of-the-art methods with remarkable performance and generalization for
both TAPG and temporal action detection. To prove the robustness and
effectiveness of AOE-Net, we further conduct an ablation study on egocentric
videos, i.e. EPIC-KITCHENS 100 dataset. Source code is available upon
acceptance.
</summary>
    <author>
      <name>Khoa Vo</name>
    </author>
    <author>
      <name>Sang Truong</name>
    </author>
    <author>
      <name>Kashu Yamazaki</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <author>
      <name>Minh-Triet Tran</name>
    </author>
    <author>
      <name>Ngan Le</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in International Journal of Computer Vision</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.02578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.02578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2106.08061v2</id>
    <updated>2021-06-16T07: 00: 12Z</updated>
    <published>2021-06-15T11: 40: 18Z</published>
    <title>Relation Modeling in Spatio-Temporal Action Localization</title>
    <summary>  This paper presents our solution to the AVA-Kinetics Crossover Challenge of
ActivityNet workshop at CVPR 2021. Our solution utilizes multiple types of
relation modeling methods for spatio-temporal action detection and adopts a
training strategy to integrate multiple relation modeling in end-to-end
training over the two large-scale video datasets. Learning with memory bank and
finetuning for long-tailed distribution are also investigated to further
improve the performance. In this paper, we detail the implementations of our
solution and provide experiments results and corresponding discussions. We
finally achieve 40.67 mAP on the test set of AVA-Kinetics.
</summary>
    <author>
      <name>Yutong Feng</name>
    </author>
    <author>
      <name>Jianwen Jiang</name>
    </author>
    <author>
      <name>Ziyuan Huang</name>
    </author>
    <author>
      <name>Zhiwu Qing</name>
    </author>
    <author>
      <name>Xiang Wang</name>
    </author>
    <author>
      <name>Shiwei Zhang</name>
    </author>
    <author>
      <name>Mingqian Tang</name>
    </author>
    <author>
      <name>Yue Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2021 ActivityNet Workshop Report</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.08061v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.08061v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2206.10095v1</id>
    <updated>2022-06-21T03: 40: 58Z</updated>
    <published>2022-06-21T03: 40: 58Z</published>
    <title>Pyramid Region-based Slot Attention Network for Temporal Action Proposal
  Generation</title>
    <summary>  It has been found that temporal action proposal generation, which aims to
discover the temporal action instances within the range of the start and end
frames in the untrimmed videos, can largely benefit from proper temporal and
semantic context exploitation. The latest efforts were dedicated to considering
the temporal context and similarity-based semantic contexts through
self-attention modules. However, they still suffer from cluttered background
information and limited contextual feature learning. In this paper, we propose
a novel Pyramid Region-based Slot Attention (PRSlot) module to address these
issues. Instead of using the similarity computation, our PRSlot module directly
learns the local relations in an encoder-decoder manner and generates the
representation of a local region enhanced based on the attention over input
features called \textit{slot
}. Specifically, upon the input snippet-level
features, PRSlot module takes the target snippet as \textit{query
}, its
surrounding region as \textit{key
} and then generates slot representations for
each \textit{query-key
} slot by aggregating the local snippet context with a
parallel pyramid strategy. Based on PRSlot modules, we present a novel Pyramid
Region-based Slot Attention Network termed PRSA-Net to learn a unified visual
representation with rich temporal and semantic context for better proposal
generation. Extensive experiments are conducted on two widely adopted THUMOS14
and ActivityNet-1.3 benchmarks. Our PRSA-Net outperforms other state-of-the-art
methods. In particular, we improve the AR@100 from the previous best 50.67% to
56.12% for proposal generation and raise the mAP under 0.5 tIoU from 51.9\% to
58.7\% for action detection on THUMOS14. \textit{Code is available at
}
\url{https: //github.com/handhand123/PRSA-Net}
</summary>
    <author>
      <name>Shuaicheng Li</name>
    </author>
    <author>
      <name>Feng Zhang</name>
    </author>
    <author>
      <name>Rui-Wei Zhao</name>
    </author>
    <author>
      <name>Rui Feng</name>
    </author>
    <author>
      <name>Kunlin Yang</name>
    </author>
    <author>
      <name>Lingbo Liu</name>
    </author>
    <author>
      <name>Jun Hou</name>
    </author>
    <link href="http://arxiv.org/abs/2206.10095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.10095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2504.14553v1</id>
    <updated>2025-04-20T09: 54: 25Z</updated>
    <published>2025-04-20T09: 54: 25Z</published>
    <title>Grounding-MD: Grounded Video-language Pre-training for Open-World Moment
  Detection</title>
    <summary>  Temporal Action Detection and Moment Retrieval constitute two pivotal tasks
in video understanding, focusing on precisely localizing temporal segments
corresponding to specific actions or events. Recent advancements introduced
Moment Detection to unify these two tasks, yet existing approaches remain
confined to closed-set scenarios, limiting their applicability in open-world
contexts. To bridge this gap, we present Grounding-MD, an innovative, grounded
video-language pre-training framework tailored for open-world moment detection.
Our framework incorporates an arbitrary number of open-ended natural language
queries through a structured prompt mechanism, enabling flexible and scalable
moment detection. Grounding-MD leverages a Cross-Modality Fusion Encoder and a
Text-Guided Fusion Decoder to facilitate comprehensive video-text alignment and
enable effective cross-task collaboration. Through large-scale pre-training on
temporal action detection and moment retrieval datasets, Grounding-MD
demonstrates exceptional semantic representation learning capabilities,
effectively handling diverse and complex query conditions. Comprehensive
evaluations across four benchmark datasets including ActivityNet, THUMOS14,
ActivityNet-Captions, and Charades-STA demonstrate that Grounding-MD
establishes new state-of-the-art performance in zero-shot and supervised
settings in open-world moment detection scenarios. All source code and trained
models will be released.
</summary>
    <author>
      <name>Weijun Zhuang</name>
    </author>
    <author>
      <name>Qizhang Li</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Ming Liu</name>
    </author>
    <author>
      <name>Xiaopeng Hong</name>
    </author>
    <author>
      <name>Feng Gao</name>
    </author>
    <author>
      <name>Fan Yang</name>
    </author>
    <author>
      <name>Wangmeng Zuo</name>
    </author>
    <link href="http://arxiv.org/abs/2504.14553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.14553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2408.09354v1</id>
    <updated>2024-08-18T04: 34: 49Z</updated>
    <published>2024-08-18T04: 34: 49Z</published>
    <title>Boundary-Recovering Network for Temporal Action Detection</title>
    <summary>  Temporal action detection (TAD) is challenging, yet fundamental for
real-world video applications. Large temporal scale variation of actions is one
of the most primary difficulties in TAD. Naturally, multi-scale features have
potential in localizing actions of diverse lengths as widely used in object
detection. Nevertheless, unlike objects in images, actions have more ambiguity
in their boundaries. That is, small neighboring objects are not considered as a
large one while short adjoining actions can be misunderstood as a long one. In
the coarse-to-fine feature pyramid via pooling, these vague action boundaries
can fade out, which we call 'vanishing boundary problem'. To this end, we
propose Boundary-Recovering Network (BRN) to address the vanishing boundary
problem. BRN constructs scale-time features by introducing a new axis called
scale dimension by interpolating multi-scale features to the same temporal
length. On top of scale-time features, scale-time blocks learn to exchange
features across scale levels, which can effectively settle down the issue. Our
extensive experiments demonstrate that our model outperforms the
state-of-the-art on the two challenging benchmarks, ActivityNet-v1.3 and
THUMOS14, with remarkably reduced degree of the vanishing boundary problem.
</summary>
    <author>
      <name>Jihwan Kim</name>
    </author>
    <author>
      <name>Jaehyun Choi</name>
    </author>
    <author>
      <name>Yerim Jeon</name>
    </author>
    <author>
      <name>Jae-Pil Heo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Pattern Recognition Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.09354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.09354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2204.11695v2</id>
    <updated>2022-11-21T09: 09: 17Z</updated>
    <published>2022-04-25T14: 33: 49Z</published>
    <title>Estimation of Reliable Proposal Quality for Temporal Action Detection</title>
    <summary>  Temporal action detection (TAD) aims to locate and recognize the actions in
an untrimmed video. Anchor-free methods have made remarkable progress which
mainly formulate TAD into two tasks: classification and localization using two
separate branches. This paper reveals the temporal misalignment between the two
tasks hindering further progress. To address this, we propose a new method that
gives insights into moment and region perspectives simultaneously to align the
two tasks by acquiring reliable proposal quality. For the moment perspective,
Boundary Evaluate Module (BEM) is designed which focuses on local appearance
and motion evolvement to estimate boundary quality and adopts a multi-scale
manner to deal with varied action durations. For the region perspective, we
introduce Region Evaluate Module (REM) which uses a new and efficient sampling
method for proposal feature representation containing more contextual
information compared with point feature to refine category score and proposal
boundary. The proposed Boundary Evaluate Module and Region Evaluate Module
(BREM) are generic, and they can be easily integrated with other anchor-free
TAD methods to achieve superior performance. In our experiments, BREM is
combined with two different frameworks and improves the performance on THUMOS14
by 3.6% and 1.0% respectively, reaching a new state-of-the-art (63.6% average
mAP). Meanwhile, a competitive result of 36.2% average mAP is achieved on
ActivityNet-1.3 with the consistent improvement of BREM. The codes are released
at https: //github.com/Junshan233/BREM.
</summary>
    <author>
      <name>Junshan Hu</name>
    </author>
    <author>
      <name>Chaoxu guo</name>
    </author>
    <author>
      <name>Liansheng Zhuang</name>
    </author>
    <author>
      <name>Biao Wang</name>
    </author>
    <author>
      <name>Tiezheng Ge</name>
    </author>
    <author>
      <name>Yuning Jiang</name>
    </author>
    <author>
      <name>Houqiang Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACM Multimedia 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.11695v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.11695v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2412.21205v1</id>
    <updated>2024-12-30T18: 59: 55Z</updated>
    <published>2024-12-30T18: 59: 55Z</published>
    <title>Action-Agnostic Point-Level Supervision for Temporal Action Detection</title>
    <summary>  We propose action-agnostic point-level (AAPL) supervision for temporal action
detection to achieve accurate action instance detection with a lightly
annotated dataset. In the proposed scheme, a small portion of video frames is
sampled in an unsupervised manner and presented to human annotators, who then
label the frames with action categories. Unlike point-level supervision, which
requires annotators to search for every action instance in an untrimmed video,
frames to annotate are selected without human intervention in AAPL supervision.
We also propose a detection model and learning method to effectively utilize
the AAPL labels. Extensive experiments on the variety of datasets (THUMOS '14,
FineAction, GTEA, BEOID, and ActivityNet 1.3) demonstrate that the proposed
approach is competitive with or outperforms prior methods for video-level and
point-level supervision in terms of the trade-off between the annotation cost
and detection performance.
</summary>
    <author>
      <name>Shuhei M. Yoshida</name>
    </author>
    <author>
      <name>Takashi Shibata</name>
    </author>
    <author>
      <name>Makoto Terao</name>
    </author>
    <author>
      <name>Takayuki Okatani</name>
    </author>
    <author>
      <name>Masashi Sugiyama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI-25. Technical appendices included. 15 pages,
    3 figures,
    11
  tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.21205v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.21205v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2104.14135v1</id>
    <updated>2021-04-29T06: 19: 44Z</updated>
    <published>2021-04-29T06: 19: 44Z</published>
    <title>Action Unit Memory Network for Weakly Supervised Temporal Action
  Localization</title>
    <summary>  Weakly supervised temporal action localization aims to detect and localize
actions in untrimmed videos with only video-level labels during training.
However, without frame-level annotations, it is challenging to achieve
localization completeness and relieve background interference. In this paper,
we present an Action Unit Memory Network (AUMN) for weakly supervised temporal
action localization, which can mitigate the above two challenges by learning an
action unit memory bank. In the proposed AUMN, two attention modules are
designed to update the memory bank adaptively and learn action units specific
classifiers. Furthermore, three effective mechanisms (diversity, homogeneity
and sparsity) are designed to guide the updating of the memory network. To the
best of our knowledge, this is the first work to explicitly model the action
units with a memory network. Extensive experimental results on two standard
benchmarks (THUMOS14 and ActivityNet) demonstrate that our AUMN performs
favorably against state-of-the-art methods. Specifically, the average mAP of
IoU thresholds from 0.1 to 0.5 on the THUMOS14 dataset is significantly
improved from 47.0% to 52.1%.
</summary>
    <author>
      <name>Wang Luo</name>
    </author>
    <author>
      <name>Tianzhu Zhang</name>
    </author>
    <author>
      <name>Wenfei Yang</name>
    </author>
    <author>
      <name>Jingen Liu</name>
    </author>
    <author>
      <name>Tao Mei</name>
    </author>
    <author>
      <name>Feng Wu</name>
    </author>
    <author>
      <name>Yongdong Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVPR2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.14135v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.14135v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1910.01286v1</id>
    <updated>2019-10-03T02: 54: 04Z</updated>
    <published>2019-10-03T02: 54: 04Z</published>
    <title>Learning Temporal Action Proposals With Fewer Labels</title>
    <summary>  Temporal action proposals are a common module in action detection pipelines
today. Most current methods for training action proposal modules rely on fully
supervised approaches that require large amounts of annotated temporal action
intervals in long video sequences. The large cost and effort in annotation that
this entails motivate us to study the problem of training proposal modules with
less supervision. In this work, we propose a semi-supervised learning algorithm
specifically designed for training temporal action proposal networks. When only
a small number of labels are available, our semi-supervised method generates
significantly better proposals than the fully-supervised counterpart and other
strong semi-supervised baselines. We validate our method on two challenging
action detection video datasets, ActivityNet v1.3 and THUMOS14. We show that
our semi-supervised approach consistently matches or outperforms the fully
supervised state-of-the-art approaches.
</summary>
    <author>
      <name>Jingwei Ji</name>
    </author>
    <author>
      <name>Kaidi Cao</name>
    </author>
    <author>
      <name>Juan Carlos Niebles</name>
    </author>
    <link href="http://arxiv.org/abs/1910.01286v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.01286v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2311.15916v1</id>
    <updated>2023-11-27T15: 24: 54Z</updated>
    <published>2023-11-27T15: 24: 54Z</published>
    <title>ADM-Loc: Actionness Distribution Modeling for Point-supervised Temporal
  Action Localization</title>
    <summary>  This paper addresses the challenge of point-supervised temporal action
detection, in which only one frame per action instance is annotated in the
training set. Self-training aims to provide supplementary supervision for the
training process by generating pseudo-labels (action proposals) from a base
model. However, most current methods generate action proposals by applying
manually designed thresholds to action classification probabilities and
treating adjacent snippets as independent entities. As a result, these methods
struggle to generate complete action proposals, exhibit sensitivity to
fluctuations in action classification scores, and generate redundant and
overlapping action proposals. This paper proposes a novel framework termed
ADM-Loc, which stands for Actionness Distribution Modeling for point-supervised
action Localization. ADM-Loc generates action proposals by fitting a composite
distribution, comprising both Gaussian and uniform distributions, to the action
classification signals. This fitting process is tailored to each action class
present in the video and is applied separately for each action instance,
ensuring the distinctiveness of their distributions. ADM-Loc significantly
enhances the alignment between the generated action proposals and ground-truth
action instances and offers high-quality pseudo-labels for self-training.
Moreover, to model action boundary snippets, it enforces consistency in action
classification scores during training by employing Gaussian kernels, supervised
with the proposed loss functions. ADM-Loc outperforms the state-of-the-art
point-supervised methods on THUMOS14 and ActivityNet-v1.2 datasets.
</summary>
    <author>
      <name>Elahe Vahdani</name>
    </author>
    <author>
      <name>Yingli Tian</name>
    </author>
    <link href="http://arxiv.org/abs/2311.15916v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.15916v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1807.10706v1</id>
    <updated>2018-07-27T16: 13: 22Z</updated>
    <published>2018-07-27T16: 13: 22Z</published>
    <title>Diagnosing Error in Temporal Action Detectors</title>
    <summary>  Despite the recent progress in video understanding and the continuous rate of
improvement in temporal action localization throughout the years, it is still
unclear how far (or close?) we are to solving the problem. To this end, we
introduce a new diagnostic tool to analyze the performance of temporal action
detectors in videos and compare different methods beyond a single scalar
metric. We exemplify the use of our tool by analyzing the performance of the
top rewarded entries in the latest ActivityNet action localization challenge.
Our analysis shows that the most impactful areas to work on are: strategies to
better handle temporal context around the instances, improving the robustness
w.r.t. the instance absolute and relative size, and strategies to reduce the
localization errors. Moreover, our experimental analysis finds the lack of
agreement among annotator is not a major roadblock to attain progress in the
field. Our diagnostic tool is publicly available to keep fueling the minds of
other researchers with additional insights about their algorithms.
</summary>
    <author>
      <name>Humam Alwassel</name>
    </author>
    <author>
      <name>Fabian Caba Heilbron</name>
    </author>
    <author>
      <name>Victor Escorcia</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2003.04145v1</id>
    <updated>2020-03-09T13: 47: 36Z</updated>
    <published>2020-03-09T13: 47: 36Z</published>
    <title>Accurate Temporal Action Proposal Generation with Relation-Aware Pyramid
  Network</title>
    <summary>  Accurate temporal action proposals play an important role in detecting
actions from untrimmed videos. The existing approaches have difficulties in
capturing global contextual information and simultaneously localizing actions
with different durations. To this end, we propose a Relation-aware pyramid
Network (RapNet) to generate highly accurate temporal action proposals. In
RapNet, a novel relation-aware module is introduced to exploit bi-directional
long-range relations between local features for context distilling. This
embedded module enhances the RapNet in terms of its multi-granularity temporal
proposal generation ability, given predefined anchor boxes. We further
introduce a two-stage adjustment scheme to refine the proposal boundaries and
measure their confidence in containing an action with snippet-level actionness.
Extensive experiments on the challenging ActivityNet and THUMOS14 benchmarks
demonstrate our RapNet generates superior accurate proposals over the existing
state-of-the-art methods.
</summary>
    <author>
      <name>Jialin Gao</name>
    </author>
    <author>
      <name>Zhixiang Shi</name>
    </author>
    <author>
      <name>Jiani Li</name>
    </author>
    <author>
      <name>Guanshuo Wang</name>
    </author>
    <author>
      <name>Yufeng Yuan</name>
    </author>
    <author>
      <name>Shiming Ge</name>
    </author>
    <author>
      <name>Xi Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by AAAI-20</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.04145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.04145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1909.03877v1</id>
    <updated>2019-09-09T14: 13: 48Z</updated>
    <published>2019-09-09T14: 13: 48Z</published>
    <title>Gaussian Temporal Awareness Networks for Action Localization</title>
    <summary>  Temporally localizing actions in a video is a fundamental challenge in video
understanding. Most existing approaches have often drawn inspiration from image
object detection and extended the advances, e.g., SSD and Faster R-CNN, to
produce temporal locations of an action in a 1D sequence. Nevertheless, the
results can suffer from robustness problem due to the design of predetermined
temporal scales, which overlooks the temporal structure of an action and limits
the utility on detecting actions with complex variations. In this paper, we
propose to address the problem by introducing Gaussian kernels to dynamically
optimize temporal scale of each action proposal. Specifically, we present
Gaussian Temporal Awareness Networks (GTAN) --- a new architecture that novelly
integrates the exploitation of temporal structure into an one-stage action
localization framework. Technically, GTAN models the temporal structure through
learning a set of Gaussian kernels, each for a cell in the feature maps. Each
Gaussian kernel corresponds to a particular interval of an action proposal and
a mixture of Gaussian kernels could further characterize action proposals with
various length. Moreover, the values in each Gaussian curve reflect the
contextual contributions to the localization of an action proposal. Extensive
experiments are conducted on both THUMOS14 and ActivityNet v1.3 datasets, and
superior results are reported when comparing to state-of-the-art approaches.
More remarkably, GTAN achieves 1.9% and 1.1% improvements in mAP on testing set
of the two datasets.
</summary>
    <author>
      <name>Fuchen Long</name>
    </author>
    <author>
      <name>Ting Yao</name>
    </author>
    <author>
      <name>Zhaofan Qiu</name>
    </author>
    <author>
      <name>Xinmei Tian</name>
    </author>
    <author>
      <name>Jiebo Luo</name>
    </author>
    <author>
      <name>Tao Mei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2019 Oral</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.03877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.03877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2011.14478v3</id>
    <updated>2021-11-01T03: 27: 54Z</updated>
    <published>2020-11-30T00: 26: 58Z</published>
    <title>Annotation-Efficient Untrimmed Video Action Recognition</title>
    <summary>  Deep learning has achieved great success in recognizing video actions, but
the collection and annotation of training data are still quite laborious, which
mainly lies in two aspects: (1) the amount of required annotated data is large;
(2) temporally annotating the location of each action is time-consuming. Works
such as few-shot learning or untrimmed video recognition have been proposed to
handle either one aspect or the other. However, very few existing works can
handle both issues simultaneously. In this paper, we target a new problem,
Annotation-Efficient Video Recognition, to reduce the requirement of
annotations for both large amount of samples and the action location. Such
problem is challenging due to two aspects: (1) the untrimmed videos only have
weak supervision; (2) video segments not relevant to current actions of
interests (background, BG) could contain actions of interests (foreground, FG)
in novel classes, which is a widely existing phenomenon but has rarely been
studied in few-shot untrimmed video recognition. To achieve this goal, by
analyzing the property of BG, we categorize BG into informative BG (IBG) and
non-informative BG (NBG), and we propose (1) an open-set detection based method
to find the NBG and FG, (2) a contrastive learning method to learn IBG and
distinguish NBG in a self-supervised way, and (3) a self-weighting mechanism
for the better distinguishing of IBG and FG. Extensive experiments on
ActivityNet v1.2 and ActivityNet v1.3 verify the rationale and effectiveness of
the proposed methods.
</summary>
    <author>
      <name>Yixiong Zou</name>
    </author>
    <author>
      <name>Shanghang Zhang</name>
    </author>
    <author>
      <name>Guangyao Chen</name>
    </author>
    <author>
      <name>Yonghong Tian</name>
    </author>
    <author>
      <name>Kurt Keutzer</name>
    </author>
    <author>
      <name>José M. F. Moura</name>
    </author>
    <link href="http://arxiv.org/abs/2011.14478v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.14478v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2004.10141v2</id>
    <updated>2021-07-17T10: 54: 57Z</updated>
    <published>2020-04-21T16: 32: 10Z</published>
    <title>TAEN: Temporal Aware Embedding Network for Few-Shot Action Recognition</title>
    <summary>  Classification of new class entities requires collecting and annotating
hundreds or thousands of samples that is often prohibitively costly. Few-shot
learning suggests learning to classify new classes using just a few examples.
Only a small number of studies address the challenge of few-shot learning on
spatio-temporal patterns such as videos. In this paper, we present the Temporal
Aware Embedding Network (TAEN) for few-shot action recognition, that learns to
represent actions, in a metric space as a trajectory, conveying both short term
semantics and longer term connectivity between action parts. We demonstrate the
effectiveness of TAEN on two few shot tasks, video classification and temporal
action detection and evaluate our method on the Kinetics-400 and on ActivityNet
1.2 few-shot benchmarks. With training of just a few fully connected layers we
reach comparable results to prior art on both few shot video classification and
temporal detection tasks, while reaching state-of-the-art in certain scenarios.
</summary>
    <author>
      <name>Rami Ben-Ari</name>
    </author>
    <author>
      <name>Mor Shpigel</name>
    </author>
    <author>
      <name>Ophir Azulai</name>
    </author>
    <author>
      <name>Udi Barzelay</name>
    </author>
    <author>
      <name>Daniel Rotman</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Learning from Limited and Imperfect Data (L2ID)
  Workshop - CVPR 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2004.10141v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.10141v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.04933v2</id>
    <updated>2024-07-11T12: 03: 35Z</updated>
    <published>2024-04-07T12: 14: 42Z</published>
    <title>UniMD: Towards Unifying Moment Retrieval and Temporal Action Detection</title>
    <summary>  Temporal Action Detection (TAD) focuses on detecting pre-defined actions,
while Moment Retrieval (MR) aims to identify the events described by open-ended
natural language within untrimmed videos. Despite that they focus on different
events, we observe they have a significant connection. For instance, most
descriptions in MR involve multiple actions from TAD. In this paper, we aim to
investigate the potential synergy between TAD and MR. Firstly, we propose a
unified architecture, termed Unified Moment Detection (UniMD), for both TAD and
MR. It transforms the inputs of the two tasks, namely actions for TAD or events
for MR, into a common embedding space, and utilizes two novel query-dependent
decoders to generate a uniform output of classification score and temporal
segments. Secondly, we explore the efficacy of two task fusion learning
approaches, pre-training and co-training, in order to enhance the mutual
benefits between TAD and MR. Extensive experiments demonstrate that the
proposed task fusion learning scheme enables the two tasks to help each other
and outperform the separately trained counterparts. Impressively, UniMD
achieves state-of-the-art results on three paired datasets Ego4D, Charades-STA,
and ActivityNet. Our code is available at https: //github.com/yingsen1/UniMD.
</summary>
    <author>
      <name>Yingsen Zeng</name>
    </author>
    <author>
      <name>Yujie Zhong</name>
    </author>
    <author>
      <name>Chengjian Feng</name>
    </author>
    <author>
      <name>Lin Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.04933v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.04933v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1908.02422v1</id>
    <updated>2019-08-07T02: 33: 18Z</updated>
    <published>2019-08-07T02: 33: 18Z</published>
    <title>Adversarial Seeded Sequence Growing for Weakly-Supervised Temporal
  Action Localization</title>
    <summary>  Temporal action localization is an important yet challenging research topic
due to its various applications. Since the frame-level or segment-level
annotations of untrimmed videos require amounts of labor expenditure, studies
on the weakly-supervised action detection have been springing up. However, most
of existing frameworks rely on Class Activation Sequence (CAS) to localize
actions by minimizing the video-level classification loss, which exploits the
most discriminative parts of actions but ignores the minor regions. In this
paper, we propose a novel weakly-supervised framework by adversarial learning
of two modules for eliminating such demerits. Specifically, the first module is
designed as a well-designed Seeded Sequence Growing (SSG) Network for
progressively extending seed regions (namely the highly reliable regions
initialized by a CAS-based framework) to their expected boundaries. The second
module is a specific classifier for mining trivial or incomplete action
regions, which is trained on the shared features after erasing the seeded
regions activated by SSG. In this way, a whole network composed of these two
modules can be trained in an adversarial manner. The goal of the adversary is
to mine features that are difficult for the action classifier. That is, erasion
from SSG will force the classifier to discover minor or even new action regions
on the input feature sequence, and the classifier will drive the seeds to grow,
alternately. At last, we could obtain the action locations and categories from
the well-trained SSG and the classifier. Extensive experiments on two public
benchmarks THUMOS'14 and ActivityNet1.3 demonstrate the impressive performance
of our proposed method compared with the state-of-the-arts.
</summary>
    <author>
      <name>Chengwei Zhang</name>
    </author>
    <author>
      <name>Yunlu Xu</name>
    </author>
    <author>
      <name>Zhanzhan Cheng</name>
    </author>
    <author>
      <name>Yi Niu</name>
    </author>
    <author>
      <name>Shiliang Pu</name>
    </author>
    <author>
      <name>Fei Wu</name>
    </author>
    <author>
      <name>Futai Zou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be appeared in ACM MM2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.02422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.02422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.16916v1</id>
    <updated>2025-03-21T07: 26: 55Z</updated>
    <published>2025-03-21T07: 26: 55Z</published>
    <title>Temporal Action Detection Model Compression by Progressive Block Drop</title>
    <summary>  Temporal action detection (TAD) aims to identify and localize action
instances in untrimmed videos, which is essential for various video
understanding tasks. However, recent improvements in model performance, driven
by larger feature extractors and datasets, have led to increased computational
demands. This presents a challenge for applications like autonomous driving and
robotics, which rely on limited computational resources. While existing channel
pruning methods can compress these models, reducing the number of channels
often hinders the parallelization efficiency of GPU, due to the inefficient
multiplication between small matrices. Instead of pruning channels, we propose
a Progressive Block Drop method that reduces model depth while retaining layer
width. In this way, we still use large matrices for computation but reduce the
number of multiplications. Our approach iteratively removes redundant blocks in
two steps: first, we drop blocks with minimal impact on model performance; and
second, we employ a parameter-efficient cross-depth alignment technique,
fine-tuning the pruned model to restore model accuracy. Our method achieves a
25% reduction in computational overhead on two TAD benchmarks (THUMOS14 and
ActivityNet-1.3) to achieve lossless compression. More critically, we
empirically show that our method is orthogonal to channel pruning methods and
can be combined with it to yield further efficiency gains.
</summary>
    <author>
      <name>Xiaoyong Chen</name>
    </author>
    <author>
      <name>Yong Guo</name>
    </author>
    <author>
      <name>Jiaming Liang</name>
    </author>
    <author>
      <name>Sitong Zhuang</name>
    </author>
    <author>
      <name>Runhao Zeng</name>
    </author>
    <author>
      <name>Xiping Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CVPR 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.16916v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16916v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1703.03329v2</id>
    <updated>2017-05-22T12: 38: 02Z</updated>
    <published>2017-03-09T16: 29: 39Z</published>
    <title>UntrimmedNets for Weakly Supervised Action Recognition and Detection</title>
    <summary>  Current action recognition methods heavily rely on trimmed videos for model
training. However, it is expensive and time-consuming to acquire a large-scale
trimmed video dataset. This paper presents a new weakly supervised
architecture, called UntrimmedNet, which is able to directly learn action
recognition models from untrimmed videos without the requirement of temporal
annotations of action instances. Our UntrimmedNet couples two important
components, the classification module and the selection module, to learn the
action models and reason about the temporal duration of action instances,
respectively. These two components are implemented with feed-forward networks,
and UntrimmedNet is therefore an end-to-end trainable architecture. We exploit
the learned models for action recognition (WSR) and detection (WSD) on the
untrimmed video datasets of THUMOS14 and ActivityNet. Although our UntrimmedNet
only employs weak supervision, our method achieves performance superior or
comparable to that of those strongly supervised approaches on these two
datasets.
</summary>
    <author>
      <name>Limin Wang</name>
    </author>
    <author>
      <name>Yuanjun Xiong</name>
    </author>
    <author>
      <name>Dahua Lin</name>
    </author>
    <author>
      <name>Luc Van Gool</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">camera-ready version to appear in CVPR2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.03329v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.03329v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1804.07667v1</id>
    <updated>2018-04-20T15: 22: 07Z</updated>
    <published>2018-04-20T15: 22: 07Z</published>
    <title>Rethinking the Faster R-CNN Architecture for Temporal Action
  Localization</title>
    <summary>  We propose TAL-Net, an improved approach to temporal action localization in
video that is inspired by the Faster R-CNN object detection framework. TAL-Net
addresses three key shortcomings of existing approaches: (1) we improve
receptive field alignment using a multi-scale architecture that can accommodate
extreme variation in action durations; (2) we better exploit the temporal
context of actions for both proposal generation and action classification by
appropriately extending receptive fields; and (3) we explicitly consider
multi-stream feature fusion and demonstrate that fusing motion late is
important. We achieve state-of-the-art performance for both action proposal and
localization on THUMOS'14 detection benchmark and competitive performance on
ActivityNet challenge.
</summary>
    <author>
      <name>Yu-Wei Chao</name>
    </author>
    <author>
      <name>Sudheendra Vijayanarasimhan</name>
    </author>
    <author>
      <name>Bryan Seybold</name>
    </author>
    <author>
      <name>David A. Ross</name>
    </author>
    <author>
      <name>Jia Deng</name>
    </author>
    <author>
      <name>Rahul Sukthankar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in CVPR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.07667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.07667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1811.02189v6</id>
    <updated>2019-12-16T03: 09: 43Z</updated>
    <published>2018-11-06T06: 54: 58Z</published>
    <title>BLP -- Boundary Likelihood Pinpointing Networks for Accurate Temporal
  Action Localization</title>
    <summary>  Despite tremendous progress achieved in temporal action detection,
state-of-the-art methods still suffer from the sharp performance deterioration
when localizing the starting and ending temporal action boundaries. Although
most methods apply boundary regression paradigm to tackle this problem, we
argue that the direct regression lacks detailed enough information to yield
accurate temporal boundaries. In this paper, we propose a novel Boundary
Likelihood Pinpointing (BLP) network to alleviate this deficiency of boundary
regression and improve the localization accuracy. Given a loosely localized
search interval that contains an action instance, BLP casts the problem of
localizing temporal boundaries as that of assigning probabilities on each
equally divided unit of this interval. These generated probabilities provide
useful information regarding the boundary location of the action inside this
search interval. Based on these probabilities, we introduce a boundary
pinpointing paradigm to pinpoint the accurate boundaries under a simple
probabilistic framework. Compared with other C3D feature based detectors,
extensive experiments demonstrate that BLP significantly improves the
localization performance of recent state-of-the-art detectors, and achieves
competitive detection mAP on both THUMOS' 14 and ActivityNet datasets,
particularly when the evaluation tIoU is high.
</summary>
    <author>
      <name>Weijie Kong</name>
    </author>
    <author>
      <name>Nannan Li</name>
    </author>
    <author>
      <name>Shan Liu</name>
    </author>
    <author>
      <name>Thomas Li</name>
    </author>
    <author>
      <name>Ge Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP),
    2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.02189v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.02189v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2310.13585v2</id>
    <updated>2024-06-05T18: 31: 34Z</updated>
    <published>2023-10-20T15: 28: 06Z</published>
    <title>POTLoc: Pseudo-Label Oriented Transformer for Point-Supervised Temporal
  Action Localization</title>
    <summary>  This paper tackles the challenge of point-supervised temporal action
detection, wherein only a single frame is annotated for each action instance in
the training set. Most of the current methods, hindered by the sparse nature of
annotated points, struggle to effectively represent the continuous structure of
actions or the inherent temporal and semantic dependencies within action
instances. Consequently, these methods frequently learn merely the most
distinctive segments of actions, leading to the creation of incomplete action
proposals. This paper proposes POTLoc, a Pseudo-label Oriented Transformer for
weakly-supervised Action Localization utilizing only point-level annotation.
POTLoc is designed to identify and track continuous action structures via a
self-training strategy. The base model begins by generating action proposals
solely with point-level supervision. These proposals undergo refinement and
regression to enhance the precision of the estimated action boundaries, which
subsequently results in the production of `pseudo-labels' to serve as
supplementary supervisory signals. The architecture of the model integrates a
transformer with a temporal feature pyramid to capture video snippet
dependencies and model actions of varying duration. The pseudo-labels,
providing information about the coarse locations and boundaries of actions,
assist in guiding the transformer for enhanced learning of action dynamics.
POTLoc outperforms the state-of-the-art point-supervised methods on THUMOS'14
and ActivityNet-v1.2 datasets.
</summary>
    <author>
      <name>Elahe Vahdani</name>
    </author>
    <author>
      <name>Yingli Tian</name>
    </author>
    <link href="http://arxiv.org/abs/2310.13585v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.13585v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1802.06822v3</id>
    <updated>2018-07-23T05: 15: 15Z</updated>
    <published>2018-02-19T19: 39: 05Z</published>
    <title>Online Detection of Action Start in Untrimmed, Streaming Videos</title>
    <summary>  We aim to tackle a novel task in action detection - Online Detection of
Action Start (ODAS) in untrimmed, streaming videos. The goal of ODAS is to
detect the start of an action instance, with high categorization accuracy and
low detection latency. ODAS is important in many applications such as early
alert generation to allow timely security or emergency response. We propose
three novel methods to specifically address the challenges in training ODAS
models: (1) hard negative samples generation based on Generative Adversarial
Network (GAN) to distinguish ambiguous background, (2) explicitly modeling the
temporal consistency between data around action start and data succeeding
action start, and (3) adaptive sampling strategy to handle the scarcity of
training data. We conduct extensive experiments using THUMOS'14 and
ActivityNet. We show that our proposed methods lead to significant performance
gains and improve the state-of-the-art methods. An ablation study confirms the
effectiveness of each proposed method.
</summary>
    <author>
      <name>Zheng Shou</name>
    </author>
    <author>
      <name>Junting Pan</name>
    </author>
    <author>
      <name>Jonathan Chan</name>
    </author>
    <author>
      <name>Kazuyuki Miyazawa</name>
    </author>
    <author>
      <name>Hassan Mansour</name>
    </author>
    <author>
      <name>Anthony Vetro</name>
    </author>
    <author>
      <name>Xavier Giro-i-Nieto</name>
    </author>
    <author>
      <name>Shih-Fu Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ECCV'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.06822v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06822v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2307.07493v1</id>
    <updated>2023-07-14T17: 24: 39Z</updated>
    <published>2023-07-14T17: 24: 39Z</published>
    <title>BehAVExplor: Behavior Diversity Guided Testing for Autonomous Driving
  Systems</title>
    <summary>  Testing Autonomous Driving Systems (ADSs) is a critical task for ensuring the
reliability and safety of autonomous vehicles. Existing methods mainly focus on
searching for safety violations while the diversity of the generated test cases
is ignored, which may generate many redundant test cases and failures. Such
redundant failures can reduce testing performance and increase failure analysis
costs. In this paper, we present a novel behavior-guided fuzzing technique
(BehAVExplor) to explore the different behaviors of the ego vehicle (i.e., the
vehicle controlled by the ADS under test) and detect diverse violations.
Specifically, we design an efficient unsupervised model, called BehaviorMiner,
to characterize the behavior of the ego vehicle. BehaviorMiner extracts the
temporal features from the given scenarios and performs a clustering-based
abstraction to group behaviors with similar features into abstract states. A
new test case will be added to the seed corpus if it triggers new behaviors
(e.g., cover new abstract states). Due to the potential conflict between the
behavior diversity and the general violation feedback, we further propose an
energy mechanism to guide the seed selection and the mutation. The energy of a
seed quantifies how good it is. We evaluated BehAVExplor on Apollo, an
industrial-level ADS, and LGSVL simulation environment. Empirical evaluation
results show that BehAVExplor can effectively find more diverse violations than
the state-of-the-art.
</summary>
    <author>
      <name>Mingfei Cheng</name>
    </author>
    <author>
      <name>Yuan Zhou</name>
    </author>
    <author>
      <name>Xiaofei Xie</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3597926.3598072</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3597926.3598072" rel="related"/>
    <link href="http://arxiv.org/abs/2307.07493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.07493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2210.11035v3</id>
    <updated>2023-03-21T16: 03: 50Z</updated>
    <published>2022-10-20T06: 08: 03Z</published>
    <title>PointTAD: Multi-Label Temporal Action Detection with Learnable Query
  Points</title>
    <summary>  Traditional temporal action detection (TAD) usually handles untrimmed videos
with small number of action instances from a single label (e.g., ActivityNet,
THUMOS). However, this setting might be unrealistic as different classes of
actions often co-occur in practice. In this paper, we focus on the task of
multi-label temporal action detection that aims to localize all action
instances from a multi-label untrimmed video. Multi-label TAD is more
challenging as it requires for fine-grained class discrimination within a
single video and precise localization of the co-occurring instances. To
mitigate this issue, we extend the sparse query-based detection paradigm from
the traditional TAD and propose the multi-label TAD framework of PointTAD.
Specifically, our PointTAD introduces a small set of learnable query points to
represent the important frames of each action instance. This point-based
representation provides a flexible mechanism to localize the discriminative
frames at boundaries and as well the important frames inside the action.
Moreover, we perform the action decoding process with the Multi-level
Interactive Module to capture both point-level and instance-level action
semantics. Finally, our PointTAD employs an end-to-end trainable framework
simply based on RGB input for easy deployment. We evaluate our proposed method
on two popular benchmarks and introduce the new metric of detection-mAP for
multi-label TAD. Our model outperforms all previous methods by a large margin
under the detection-mAP metric, and also achieves promising results under the
segmentation-mAP metric. Code is available at
https: //github.com/MCG-NJU/PointTAD.
</summary>
    <author>
      <name>Jing Tan</name>
    </author>
    <author>
      <name>Xiaotong Zhao</name>
    </author>
    <author>
      <name>Xintian Shi</name>
    </author>
    <author>
      <name>Bin Kang</name>
    </author>
    <author>
      <name>Limin Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS 2022 camera ready version</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.11035v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.11035v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2103.13141v1</id>
    <updated>2021-03-24T12: 34: 49Z</updated>
    <published>2021-03-24T12: 34: 49Z</published>
    <title>Temporal Context Aggregation Network for Temporal Action Proposal
  Refinement</title>
    <summary>  Temporal action proposal generation aims to estimate temporal intervals of
actions in untrimmed videos, which is a challenging yet important task in the
video understanding field. The proposals generated by current methods still
suffer from inaccurate temporal boundaries and inferior confidence used for
retrieval owing to the lack of efficient temporal modeling and effective
boundary context utilization. In this paper, we propose Temporal Context
Aggregation Network (TCANet) to generate high-quality action proposals through
"local and global" temporal context aggregation and complementary as well as
progressive boundary refinement. Specifically, we first design a Local-Global
Temporal Encoder (LGTE), which adopts the channel grouping strategy to
efficiently encode both "local and global" temporal inter-dependencies.
Furthermore, both the boundary and internal context of proposals are adopted
for frame-level and segment-level boundary regressions, respectively. Temporal
Boundary Regressor (TBR) is designed to combine these two regression
granularities in an end-to-end fashion, which achieves the precise boundaries
and reliable confidence of proposals through progressive refinement. Extensive
experiments are conducted on three challenging datasets: HACS,
ActivityNet-v1.3, and THUMOS-14, where TCANet can generate proposals with high
precision and recall. By combining with the existing action classifier, TCANet
can obtain remarkable temporal action detection performance compared with other
methods. Not surprisingly, the proposed TCANet won the 1$^{st
    }$ place in the
CVPR 2020 - HACS challenge leaderboard on temporal action localization task.
</summary>
    <author>
      <name>Zhiwu Qing</name>
    </author>
    <author>
      <name>Haisheng Su</name>
    </author>
    <author>
      <name>Weihao Gan</name>
    </author>
    <author>
      <name>Dongliang Wang</name>
    </author>
    <author>
      <name>Wei Wu</name>
    </author>
    <author>
      <name>Xiang Wang</name>
    </author>
    <author>
      <name>Yu Qiao</name>
    </author>
    <author>
      <name>Junjie Yan</name>
    </author>
    <author>
      <name>Changxin Gao</name>
    </author>
    <author>
      <name>Nong Sang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVPR2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.13141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.13141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1904.00227v3</id>
    <updated>2020-11-08T22: 22: 59Z</updated>
    <published>2019-03-30T14: 51: 44Z</published>
    <title>RefineLoc: Iterative Refinement for Weakly-Supervised Action
  Localization</title>
    <summary>  Video action detectors are usually trained using datasets with
fully-supervised temporal annotations. Building such datasets is an expensive
task. To alleviate this problem, recent methods have tried to leverage weak
labeling, where videos are untrimmed and only a video-level label is available.
In this paper, we propose RefineLoc, a novel weakly-supervised temporal action
localization method. RefineLoc uses an iterative refinement approach by
estimating and training on snippet-level pseudo ground truth at every
iteration. We show the benefit of this iterative approach and present an
extensive analysis of five different pseudo ground truth generators. We show
the effectiveness of our model on two standard action datasets, ActivityNet
v1.2 and THUMOS14. RefineLoc shows competitive results with the
state-of-the-art in weakly-supervised temporal localization. Additionally, our
iterative refinement process is able to significantly improve the performance
of two state-of-the-art methods, setting a new state-of-the-art on THUMOS14.
</summary>
    <author>
      <name>Alejandro Pardo</name>
    </author>
    <author>
      <name>Humam Alwassel</name>
    </author>
    <author>
      <name>Fabian Caba Heilbron</name>
    </author>
    <author>
      <name>Ali Thabet</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to WACV 2021. Project website:
  http: //humamalwassel.com/publication/refineloc</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.00227v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.00227v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2110.09819v1</id>
    <updated>2021-10-19T10: 09: 09Z</updated>
    <published>2021-10-19T10: 09: 09Z</published>
    <title>LSTC: Boosting Atomic Action Detection with Long-Short-Term Context</title>
    <summary>  In this paper, we place the atomic action detection problem into a Long-Short
Term Context (LSTC) to analyze how the temporal reliance among video signals
affect the action detection results. To do this, we decompose the action
recognition pipeline into short-term and long-term reliance, in terms of the
hypothesis that the two kinds of context are conditionally independent given
the objective action instance. Within our design, a local aggregation branch is
utilized to gather dense and informative short-term cues, while a high order
long-term inference branch is designed to reason the objective action class
from high-order interaction between actor and other person or person pairs.
Both branches independently predict the context-specific actions and the
results are merged in the end. We demonstrate that both temporal grains are
beneficial to atomic action recognition. On the mainstream benchmarks of atomic
action detection, our design can bring significant performance gain from the
existing state-of-the-art pipeline. The code of this project can be found at
[this url
    ](https: //github.com/TencentYoutuResearch/ActionDetection-LSTC)
</summary>
    <author>
      <name>Yuxi Li</name>
    </author>
    <author>
      <name>Boshen Zhang</name>
    </author>
    <author>
      <name>Jian Li</name>
    </author>
    <author>
      <name>Yabiao Wang</name>
    </author>
    <author>
      <name>Weiyao Lin</name>
    </author>
    <author>
      <name>Chengjie Wang</name>
    </author>
    <author>
      <name>Jilin Li</name>
    </author>
    <author>
      <name>Feiyue Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Multimedia 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.09819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.09819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2001.04627v2</id>
    <updated>2021-08-05T15: 25: 12Z</updated>
    <published>2020-01-14T05: 03: 54Z</published>
    <title>Self-supervising Action Recognition by Statistical Moment and Subspace
  Descriptors</title>
    <summary>  In this paper, we build on a concept of self-supervision by taking RGB frames
as input to learn to predict both action concepts and auxiliary descriptors
e.g., object descriptors. So-called hallucination streams are trained to
predict auxiliary cues, simultaneously fed into classification layers, and then
hallucinated at the testing stage to aid network. We design and hallucinate two
descriptors, one leveraging four popular object detectors applied to training
videos, and the other leveraging image- and video-level saliency detectors. The
first descriptor encodes the detector- and ImageNet-wise class prediction
scores, confidence scores, and spatial locations of bounding boxes and frame
indexes to capture the spatio-temporal distribution of features per video.
Another descriptor encodes spatio-angular gradient distributions of saliency
maps and intensity patterns. Inspired by the characteristic function of the
probability distribution, we capture four statistical moments on the above
intermediate descriptors. As numbers of coefficients in the mean, covariance,
coskewness and cokurtotsis grow linearly, quadratically, cubically and
quartically w.r.t. the dimension of feature vectors, we describe the covariance
matrix by its leading n' eigenvectors (so-called subspace) and we capture
skewness/kurtosis rather than costly coskewness/cokurtosis. We obtain state of
the art on five popular datasets such as Charades and EPIC-Kitchens.
</summary>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Piotr Koniusz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3474085.3475572</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3474085.3475572" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM MM'21</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.04627v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.04627v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2203.08942v1</id>
    <updated>2022-03-16T21: 06: 34Z</updated>
    <published>2022-03-16T21: 06: 34Z</published>
    <title>ABN: Agent-Aware Boundary Networks for Temporal Action Proposal
  Generation</title>
    <summary>  Temporal action proposal generation (TAPG) aims to estimate temporal
intervals of actions in untrimmed videos, which is a challenging yet plays an
important role in many tasks of video analysis and understanding. Despite the
great achievement in TAPG, most existing works ignore the human perception of
interaction between agents and the surrounding environment by applying a deep
learning model as a black-box to the untrimmed videos to extract video visual
representation. Therefore, it is beneficial and potentially improve the
performance of TAPG if we can capture these interactions between agents and the
environment. In this paper, we propose a novel framework named Agent-Aware
Boundary Network (ABN), which consists of two sub-networks (i) an Agent-Aware
Representation Network to obtain both agent-agent and agents-environment
relationships in the video representation, and (ii) a Boundary Generation
Network to estimate the confidence score of temporal intervals. In the
Agent-Aware Representation Network, the interactions between agents are
expressed through local pathway, which operates at a local level to focus on
the motions of agents whereas the overall perception of the surroundings are
expressed through global pathway, which operates at a global level to perceive
the effects of agents-environment. Comprehensive evaluations on 20-action
THUMOS-14 and 200-action ActivityNet-1.3 datasets with different backbone
networks (i.e C3D, SlowFast and Two-Stream) show that our proposed ABN robustly
outperforms state-of-the-art methods regardless of the employed backbone
network on TAPG. We further examine the proposal quality by leveraging
proposals generated by our method onto temporal action detection (TAD)
frameworks and evaluate their detection performances. The source code can be
found in this URL https: //github.com/vhvkhoa/TAPG-AgentEnvNetwork.git.
</summary>
    <author>
      <name>Khoa Vo</name>
    </author>
    <author>
      <name>Kashu Yamazaki</name>
    </author>
    <author>
      <name>Sang Truong</name>
    </author>
    <author>
      <name>Minh-Triet Tran</name>
    </author>
    <author>
      <name>Akihiro Sugimoto</name>
    </author>
    <author>
      <name>Ngan Le</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2021.3110973</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCESS.2021.3110973" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in the journal of IEEE Access Vol. 9</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.08942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.08942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1912.01326v3</id>
    <updated>2020-03-30T13: 53: 26Z</updated>
    <published>2019-12-03T11: 59: 55Z</published>
    <title>A Context-Aware Loss Function for Action Spotting in Soccer Videos</title>
    <summary>  In video understanding, action spotting consists in temporally localizing
human-induced events annotated with single timestamps. In this paper, we
propose a novel loss function that specifically considers the temporal context
naturally present around each action, rather than focusing on the single
annotated frame to spot. We benchmark our loss on a large dataset of soccer
videos, SoccerNet, and achieve an improvement of 12.8% over the baseline. We
show the generalization capability of our loss for generic activity proposals
and detection on ActivityNet, by spotting the beginning and the end of each
activity. Furthermore, we provide an extended ablation study and display
challenging cases for action spotting in soccer videos. Finally, we
qualitatively illustrate how our loss induces a precise temporal understanding
of actions and show how such semantic knowledge can be used for automatic
highlights generation.
</summary>
    <author>
      <name>Anthony Cioppa</name>
    </author>
    <author>
      <name>Adrien Deliège</name>
    </author>
    <author>
      <name>Silvio Giancola</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <author>
      <name>Marc Van Droogenbroeck</name>
    </author>
    <author>
      <name>Rikke Gade</name>
    </author>
    <author>
      <name>Thomas B. Moeslund</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for CVPR2020 main conference. This document contains 8 pages
  + references + supplementary material</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.01326v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.01326v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1511.06984v2</id>
    <updated>2017-03-13T07: 33: 15Z</updated>
    <published>2015-11-22T09: 41: 50Z</published>
    <title>End-to-end Learning of Action Detection from Frame Glimpses in Videos</title>
    <summary>  In this work we introduce a fully end-to-end approach for action detection in
videos that learns to directly predict the temporal bounds of actions. Our
intuition is that the process of detecting actions is naturally one of
observation and refinement: observing moments in video, and refining hypotheses
about when an action is occurring. Based on this insight, we formulate our
model as a recurrent neural network-based agent that interacts with a video
over time. The agent observes video frames and decides both where to look next
and when to emit a prediction. Since backpropagation is not adequate in this
non-differentiable setting, we use REINFORCE to learn the agent's decision
policy. Our model achieves state-of-the-art results on the THUMOS'14 and
ActivityNet datasets while observing only a fraction (2% or less) of the video
frames.
</summary>
    <author>
      <name>Serena Yeung</name>
    </author>
    <author>
      <name>Olga Russakovsky</name>
    </author>
    <author>
      <name>Greg Mori</name>
    </author>
    <author>
      <name>Li Fei-Fei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Update to version in CVPR 2016 proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.06984v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06984v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2204.09456v1</id>
    <updated>2022-04-20T13: 42: 51Z</updated>
    <published>2022-04-20T13: 42: 51Z</published>
    <title>STAU: A SpatioTemporal-Aware Unit for Video Prediction and Beyond</title>
    <summary>  Video prediction aims to predict future frames by modeling the complex
spatiotemporal dynamics in videos. However, most of the existing methods only
model the temporal information and the spatial information for videos in an
independent manner but haven't fully explored the correlations between both
terms. In this paper, we propose a SpatioTemporal-Aware Unit (STAU) for video
prediction and beyond by exploring the significant spatiotemporal correlations
in videos. On the one hand, the motion-aware attention weights are learned from
the spatial states to help aggregate the temporal states in the temporal
domain. On the other hand, the appearance-aware attention weights are learned
from the temporal states to help aggregate the spatial states in the spatial
domain. In this way, the temporal information and the spatial information can
be greatly aware of each other in both domains, during which, the
spatiotemporal receptive field can also be greatly broadened for more reliable
spatiotemporal modeling. Experiments are not only conducted on traditional
video prediction tasks but also other tasks beyond video prediction, including
the early action recognition and object detection tasks. Experimental results
show that our STAU can outperform other methods on all tasks in terms of
performance and computation efficiency.
</summary>
    <author>
      <name>Zheng Chang</name>
    </author>
    <author>
      <name>Xinfeng Zhang</name>
    </author>
    <author>
      <name>Shanshe Wang</name>
    </author>
    <author>
      <name>Siwei Ma</name>
    </author>
    <author>
      <name>Wen Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to TPAMI</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.09456v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.09456v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2303.14863v2</id>
    <updated>2023-07-14T12: 06: 13Z</updated>
    <published>2023-03-27T00: 40: 52Z</published>
    <title>DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion</title>
    <summary>  We propose a new formulation of temporal action detection (TAD) with
denoising diffusion, DiffTAD in short. Taking as input random temporal
proposals, it can yield action proposals accurately given an untrimmed long
video. This presents a generative modeling perspective, against previous
discriminative learning manners. This capability is achieved by first diffusing
the ground-truth proposals to random ones (i.e., the forward/noising process)
and then learning to reverse the noising process (i.e., the backward/denoising
process). Concretely, we establish the denoising process in the Transformer
decoder (e.g., DETR) by introducing a temporal location query design with
faster convergence in training. We further propose a cross-step selective
conditioning algorithm for inference acceleration. Extensive evaluations on
ActivityNet and THUMOS show that our DiffTAD achieves top performance compared
to previous art alternatives. The code will be made available at
https: //github.com/sauradip/DiffusionTAD.
</summary>
    <author>
      <name>Sauradip Nag</name>
    </author>
    <author>
      <name>Xiatian Zhu</name>
    </author>
    <author>
      <name>Jiankang Deng</name>
    </author>
    <author>
      <name>Yi-Zhe Song</name>
    </author>
    <author>
      <name>Tao Xiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV 2023; Code available at https: //github.com/sauradip/DiffusionTAD</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.14863v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.14863v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2206.09082v1</id>
    <updated>2022-06-18T01: 43: 43Z</updated>
    <published>2022-06-18T01: 43: 43Z</published>
    <title>Context-aware Proposal Network for Temporal Action Detection</title>
    <summary>  This technical report presents our first place winning solution for temporal
action detection task in CVPR-2022 AcitivityNet Challenge. The task aims to
localize temporal boundaries of action instances with specific classes in long
untrimmed videos. Recent mainstream attempts are based on dense boundary
matchings and enumerate all possible combinations to produce proposals. We
argue that the generated proposals contain rich contextual information, which
may benefits detection confidence prediction. To this end, our method mainly
consists of the following three steps: 1) action classification and feature
extraction by Slowfast, CSN, TimeSformer, TSP, I3D-flow, VGGish-audio, TPN and
ViViT; 2) proposal generation. Our proposed Context-aware Proposal Network
(CPN) builds on top of BMN, GTAD and PRN to aggregate contextual information by
randomly masking some proposal features. 3) action detection. The final
detection prediction is calculated by assigning the proposals with
corresponding video-level classifcation results. Finally, we ensemble the
results under different feature combination settings and achieve 45.8%
performance on the test set, which improves the champion result in CVPR-2021
ActivityNet Challenge by 1.1% in terms of average mAP.
</summary>
    <author>
      <name>Xiang Wang</name>
    </author>
    <author>
      <name>Huaxin Zhang</name>
    </author>
    <author>
      <name>Shiwei Zhang</name>
    </author>
    <author>
      <name>Changxin Gao</name>
    </author>
    <author>
      <name>Yuanjie Shao</name>
    </author>
    <author>
      <name>Nong Sang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First place winning solution for temporal action detection task in
  CVPR-2022 AcitivityNet Challenge. arXiv admin note: substantial text overlap
  with arXiv: 2106.11812</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.09082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.09082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2403.20254v1</id>
    <updated>2024-03-29T16: 01: 00Z</updated>
    <published>2024-03-29T16: 01: 00Z</published>
    <title>Benchmarking the Robustness of Temporal Action Detection Models Against
  Temporal Corruptions</title>
    <summary>  Temporal action detection (TAD) aims to locate action positions and recognize
action categories in long-term untrimmed videos. Although many methods have
achieved promising results, their robustness has not been thoroughly studied.
In practice, we observe that temporal information in videos can be occasionally
corrupted, such as missing or blurred frames. Interestingly, existing methods
often incur a significant performance drop even if only one frame is affected.
To formally evaluate the robustness, we establish two temporal corruption
robustness benchmarks, namely THUMOS14-C and ActivityNet-v1.3-C. In this paper,
we extensively analyze the robustness of seven leading TAD methods and obtain
some interesting findings: 1) Existing methods are particularly vulnerable to
temporal corruptions, and end-to-end methods are often more susceptible than
those with a pre-trained feature extractor; 2) Vulnerability mainly comes from
localization error rather than classification error; 3) When corruptions occur
in the middle of an action instance, TAD models tend to yield the largest
performance drop. Besides building a benchmark, we further develop a simple but
effective robust training method to defend against temporal corruptions,
through the FrameDrop augmentation and Temporal-Robust Consistency loss.
Remarkably, our approach not only improves robustness but also yields promising
improvements on clean data. We believe that this study will serve as a
benchmark for future research in robust video analysis. Source code and models
are available at https: //github.com/Alvin-Zeng/temporal-robustness-benchmark.
</summary>
    <author>
      <name>Runhao Zeng</name>
    </author>
    <author>
      <name>Xiaoyong Chen</name>
    </author>
    <author>
      <name>Jiaming Liang</name>
    </author>
    <author>
      <name>Huisi Wu</name>
    </author>
    <author>
      <name>Guangzhong Cao</name>
    </author>
    <author>
      <name>Yong Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVPR2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.20254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.20254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1910.11285v3</id>
    <updated>2020-03-23T02: 56: 39Z</updated>
    <published>2019-10-24T17: 00: 14Z</published>
    <title>Towards Train-Test Consistency for Semi-supervised Temporal Action
  Localization</title>
    <summary>  Recently, Weakly-supervised Temporal Action Localization (WTAL) has been
densely studied but there is still a large gap between weakly-supervised models
and fully-supervised models. It is practical and intuitive to annotate temporal
boundaries of a few examples and utilize them to help WTAL models better detect
actions. However, the train-test discrepancy of action localization strategy
prevents WTAL models from leveraging semi-supervision for further improvement.
At training time, attention or multiple instance learning is used to aggregate
predictions of each snippet for video-level classification; at test time, they
first obtain action score sequences over time, then truncate segments of scores
higher than a fixed threshold, and post-process action segments. The
inconsistent strategy makes it hard to explicitly supervise the action
localization model with temporal boundary annotations at training time. In this
paper, we propose a Train-Test Consistent framework, TTC-Loc. In both training
and testing time, our TTC-Loc localizes actions by comparing scores of action
classes and predicted threshold, which enables it to be trained with
semi-supervision. By fixing the train-test discrepancy, our TTC-Loc
significantly outperforms the state-of-the-art performance on THUMOS'14,
ActivityNet 1.2 and 1.3 when only video-level labels are provided for training.
With full annotations of only one video per class and video-level labels for
the other videos, our TTC-Loc further boosts the performance and achieves
33.4\% mAP (IoU threshold 0.5) on THUMOS's 14.
</summary>
    <author>
      <name>Xudong Lin</name>
    </author>
    <author>
      <name>Zheng Shou</name>
    </author>
    <author>
      <name>Shih-Fu Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.11285v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.11285v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2212.10596v2</id>
    <updated>2023-01-10T19: 44: 37Z</updated>
    <published>2022-12-20T19: 12: 58Z</published>
    <title>Open-Vocabulary Temporal Action Detection with Off-the-Shelf Image-Text
  Features</title>
    <summary>  Detecting actions in untrimmed videos should not be limited to a small,
closed set of classes. We present a simple, yet effective strategy for
open-vocabulary temporal action detection utilizing pretrained image-text
co-embeddings. Despite being trained on static images rather than videos, we
show that image-text co-embeddings enable openvocabulary performance
competitive with fully-supervised models. We show that the performance can be
further improved by ensembling the image-text features with features encoding
local motion, like optical flow based features, or other modalities, like
audio. In addition, we propose a more reasonable open-vocabulary evaluation
setting for the ActivityNet data set, where the category splits are based on
similarity rather than random assignment.
</summary>
    <author>
      <name>Vivek Rathod</name>
    </author>
    <author>
      <name>Bryan Seybold</name>
    </author>
    <author>
      <name>Sudheendra Vijayanarasimhan</name>
    </author>
    <author>
      <name>Austin Myers</name>
    </author>
    <author>
      <name>Xiuye Gu</name>
    </author>
    <author>
      <name>Vighnesh Birodkar</name>
    </author>
    <author>
      <name>David A. Ross</name>
    </author>
    <link href="http://arxiv.org/abs/2212.10596v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.10596v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2211.14924v2</id>
    <updated>2023-03-03T07: 23: 20Z</updated>
    <published>2022-11-27T19: 50: 37Z</published>
    <title>Post-Processing Temporal Action Detection</title>
    <summary>  Existing Temporal Action Detection (TAD) methods typically take a
pre-processing step in converting an input varying-length video into a
fixed-length snippet representation sequence, before temporal boundary
estimation and action classification. This pre-processing step would temporally
downsample the video, reducing the inference resolution and hampering the
detection performance in the original temporal resolution. In essence, this is
due to a temporal quantization error introduced during the resolution
downsampling and recovery. This could negatively impact the TAD performance,
but is largely ignored by existing methods. To address this problem, in this
work we introduce a novel model-agnostic post-processing method without model
redesign and retraining. Specifically, we model the start and end points of
action instances with a Gaussian distribution for enabling temporal boundary
inference at a sub-snippet level. We further introduce an efficient
Taylor-expansion based approximation, dubbed as Gaussian Approximated
Post-processing (GAP). Extensive experiments demonstrate that our GAP can
consistently improve a wide variety of pre-trained off-the-shelf TAD models on
the challenging ActivityNet (+0.2% -0.7% in average mAP) and THUMOS (+0.2%
-0.5% in average mAP) benchmarks. Such performance gains are already
significant and highly comparable to those achieved by novel model designs.
Also, GAP can be integrated with model training for further performance gain.
Importantly, GAP enables lower temporal resolutions for more efficient
inference, facilitating low-resource applications. The code will be available
in https: //github.com/sauradip/GAP
</summary>
    <author>
      <name>Sauradip Nag</name>
    </author>
    <author>
      <name>Xiatian Zhu</name>
    </author>
    <author>
      <name>Yi-Zhe Song</name>
    </author>
    <author>
      <name>Tao Xiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2023; Code available at https: //github.com/sauradip/GAP</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.14924v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.14924v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1807.10066v1</id>
    <updated>2018-07-26T11: 11: 25Z</updated>
    <published>2018-07-26T11: 11: 25Z</published>
    <title>A Better Baseline for AVA</title>
    <summary>  We introduce a simple baseline for action localization on the AVA dataset.
The model builds upon the Faster R-CNN bounding box detection framework,
adapted to operate on pure spatiotemporal features - in our case produced
exclusively by an I3D model pretrained on Kinetics. This model obtains 21.9%
average AP on the validation set of AVA v2.1, up from 14.5% for the best RGB
spatiotemporal model used in the original AVA paper (which was pretrained on
Kinetics and ImageNet), and up from 11.3 of the publicly available baseline
using a ResNet101 image feature extractor, that was pretrained on ImageNet. Our
final model obtains 22.8%/21.9% mAP on the val/test sets and outperforms all
submissions to the AVA challenge at CVPR 2018.
</summary>
    <author>
      <name>Rohit Girdhar</name>
    </author>
    <author>
      <name>João Carreira</name>
    </author>
    <author>
      <name>Carl Doersch</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ActivityNet Workshop (AVA Challenge), CVPR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10066v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10066v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2203.11111v1</id>
    <updated>2022-03-21T16: 40: 55Z</updated>
    <published>2022-03-21T16: 40: 55Z</published>
    <title>Facial Expression Analysis Using Decomposed Multiscale Spatiotemporal
  Networks</title>
    <summary>  Video-based analysis of facial expressions has been increasingly applied to
infer health states of individuals, such as depression and pain. Among the
existing approaches, deep learning models composed of structures for multiscale
spatiotemporal processing have shown strong potential for encoding facial
dynamics. However, such models have high computational complexity, making for a
difficult deployment of these solutions. To address this issue, we introduce a
new technique to decompose the extraction of multiscale spatiotemporal
features. Particularly, a building block structure called Decomposed Multiscale
Spatiotemporal Network (DMSN) is presented along with three variants: DMSN-A,
DMSN-B, and DMSN-C blocks. The DMSN-A block generates multiscale
representations by analyzing spatiotemporal features at multiple temporal
ranges, while the DMSN-B block analyzes spatiotemporal features at multiple
ranges, and the DMSN-C block analyzes spatiotemporal features at multiple
spatial sizes. Using these variants, we design our DMSN architecture which has
the ability to explore a variety of multiscale spatiotemporal features,
favoring the adaptation to different facial behaviors. Our extensive
experiments on challenging datasets show that the DMSN-C block is effective for
depression detection, whereas the DMSN-A block is efficient for pain
estimation. Results also indicate that our DMSN architecture provides a
cost-effective solution for expressions that range from fewer facial variations
over time, as in depression detection, to greater variations, as in pain
estimation.
</summary>
    <author>
      <name>Wheidima Carneiro de Melo</name>
    </author>
    <author>
      <name>Eric Granger</name>
    </author>
    <author>
      <name>Miguel Bordallo Lopez</name>
    </author>
    <link href="http://arxiv.org/abs/2203.11111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.11111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2408.13152v2</id>
    <updated>2024-09-09T16: 16: 19Z</updated>
    <published>2024-08-23T15: 20: 53Z</published>
    <title>Long-term Pre-training for Temporal Action Detection with Transformers</title>
    <summary>  Temporal action detection (TAD) is challenging, yet fundamental for
real-world video applications. Recently, DETR-based models for TAD have been
prevailing thanks to their unique benefits. However, transformers demand a huge
dataset, and unfortunately data scarcity in TAD causes a severe degeneration.
In this paper, we identify two crucial problems from data scarcity: attention
collapse and imbalanced performance. To this end, we propose a new pre-training
strategy, Long-Term Pre-training (LTP), tailored for transformers. LTP has two
main components: 1) class-wise synthesis,
    2) long-term pretext tasks. Firstly,
we synthesize long-form video features by merging video snippets of a target
class and non-target classes. They are analogous to untrimmed data used in TAD,
despite being created from trimmed data. In addition, we devise two types of
long-term pretext tasks to learn long-term dependency. They impose long-term
conditions such as finding second-to-fourth or short-duration actions. Our
extensive experiments show state-of-the-art performances in DETR-based methods
on ActivityNet-v1.3 and THUMOS14 by a large margin. Moreover, we demonstrate
that LTP significantly relieves the data scarcity issues in TAD.
</summary>
    <author>
      <name>Jihwan Kim</name>
    </author>
    <author>
      <name>Miso Lee</name>
    </author>
    <author>
      <name>Jae-Pil Heo</name>
    </author>
    <link href="http://arxiv.org/abs/2408.13152v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.13152v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1801.01415v1</id>
    <updated>2018-01-04T15: 47: 47Z</updated>
    <published>2018-01-04T15: 47: 47Z</published>
    <title>What have we learned from deep representations for action recognition?</title>
    <summary>  As the success of deep models has led to their deployment in all areas of
computer vision, it is increasingly important to understand how these
representations work and what they are capturing. In this paper, we shed light
on deep spatiotemporal representations by visualizing what two-stream models
have learned in order to recognize actions in video. We show that local
detectors for appearance and motion objects arise to form distributed
representations for recognizing human actions. Key observations include the
following. First, cross-stream fusion enables the learning of true
spatiotemporal features rather than simply separate appearance and motion
features. Second, the networks can learn local representations that are highly
class specific, but also generic representations that can serve a range of
classes. Third, throughout the hierarchy of the network, features become more
abstract and show increasing invariance to aspects of the data that are
unimportant to desired distinctions (e.g. motion patterns across various
speeds). Fourth, visualizations can be used not only to shed light on learned
representations, but also to reveal idiosyncracies of training data and to
explain failure cases of the system.
</summary>
    <author>
      <name>Christoph Feichtenhofer</name>
    </author>
    <author>
      <name>Axel Pinz</name>
    </author>
    <author>
      <name>Richard P. Wildes</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This document is best viewed in Adobe Reader where figures play on
  click. Supplementary material can be downloaded at
  http: //feichtenhofer.github.io/action_vis.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.01415v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01415v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2006.07006v3</id>
    <updated>2020-12-17T07: 12: 38Z</updated>
    <published>2020-06-12T08: 54: 35Z</published>
    <title>Weakly-supervised Temporal Action Localization by Uncertainty Modeling</title>
    <summary>  Weakly-supervised temporal action localization aims to learn detecting
temporal intervals of action classes with only video-level labels. To this end,
it is crucial to separate frames of action classes from the background frames
(i.e., frames not belonging to any action classes). In this paper, we present a
new perspective on background frames where they are modeled as
out-of-distribution samples regarding their inconsistency. Then, background
frames can be detected by estimating the probability of each frame being
out-of-distribution, known as uncertainty, but it is infeasible to directly
learn uncertainty without frame-level labels. To realize the uncertainty
learning in the weakly-supervised setting, we leverage the multiple instance
learning formulation. Moreover, we further introduce a background entropy loss
to better discriminate background frames by encouraging their in-distribution
(action) probabilities to be uniformly distributed over all action classes.
Experimental results show that our uncertainty modeling is effective at
alleviating the interference of background frames and brings a large
performance gain without bells and whistles. We demonstrate that our model
significantly outperforms state-of-the-art methods on the benchmarks, THUMOS'14
and ActivityNet (1.2 &amp; 1.3). Our code is available at
https: //github.com/Pilhyeon/WTAL-Uncertainty-Modeling.
</summary>
    <author>
      <name>Pilhyeon Lee</name>
    </author>
    <author>
      <name>Jinglu Wang</name>
    </author>
    <author>
      <name>Yan Lu</name>
    </author>
    <author>
      <name>Hyeran Byun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by the 35th AAAI Conference on Artificial Intelligence (AAAI
  2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.07006v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.07006v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2008.08332v1</id>
    <updated>2020-08-19T08: 47: 50Z</updated>
    <published>2020-08-19T08: 47: 50Z</published>
    <title>CFAD: Coarse-to-Fine Action Detector for Spatiotemporal Action
  Localization</title>
    <summary>  Most current pipelines for spatio-temporal action localization connect
frame-wise or clip-wise detection results to generate action proposals, where
only local information is exploited and the efficiency is hindered by dense
per-frame localization. In this paper, we propose Coarse-to-Fine Action
Detector (CFAD),an original end-to-end trainable framework for efficient
spatio-temporal action localization. The CFAD introduces a new paradigm that
first estimates coarse spatio-temporal action tubes from video streams, and
then refines the tubes' location based on key timestamps. This concept is
implemented by two key components, the Coarse and Refine Modules in our
framework. The parameterized modeling of long temporal information in the
Coarse Module helps obtain accurate initial tube estimation, while the Refine
Module selectively adjusts the tube location under the guidance of key
timestamps. Against other methods, theproposed CFAD achieves competitive
results on action detection benchmarks of UCF101-24, UCFSports and JHMDB-21
with inference speed that is 3.3x faster than the nearest competitors.
</summary>
    <author>
      <name>Yuxi Li</name>
    </author>
    <author>
      <name>Weiyao Lin</name>
    </author>
    <author>
      <name>John See</name>
    </author>
    <author>
      <name>Ning Xu</name>
    </author>
    <author>
      <name>Shugong Xu</name>
    </author>
    <author>
      <name>Ke Yan</name>
    </author>
    <author>
      <name>Cong Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 figures,
    3 tables; ECCV2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.08332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.08332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2208.12940v2</id>
    <updated>2022-09-07T12: 37: 05Z</updated>
    <published>2022-08-27T06: 51: 12Z</published>
    <title>Actor-identified Spatiotemporal Action Detection -- Detecting Who Is
  Doing What in Videos</title>
    <summary>  The success of deep learning on video Action Recognition (AR) has motivated
researchers to progressively promote related tasks from the coarse level to the
fine-grained level. Compared with conventional AR which only predicts an action
label for the entire video, Temporal Action Detection (TAD) has been
investigated for estimating the start and end time for each action in videos.
Taking TAD a step further, Spatiotemporal Action Detection (SAD) has been
studied for localizing the action both spatially and temporally in videos.
However, who performs the action, is generally ignored in SAD, while
identifying the actor could also be important. To this end, we propose a novel
task, Actor-identified Spatiotemporal Action Detection (ASAD), to bridge the
gap between SAD and actor identification.
  In ASAD, we not only detect the spatiotemporal boundary for instance-level
action but also assign the unique ID to each actor. To approach ASAD, Multiple
Object Tracking (MOT) and Action Classification (AC) are two fundamental
elements. By using MOT, the spatiotemporal boundary of each actor is obtained
and assigned to a unique actor identity. By using AC, the action class is
estimated within the corresponding spatiotemporal boundary. Since ASAD is a new
task, it poses many new challenges that cannot be addressed by existing
methods: i) no dataset is specifically created for ASAD, ii) no evaluation
metrics are designed for ASAD, iii) current MOT performance is the bottleneck
to obtain satisfactory ASAD results. To address those problems, we contribute
to i) annotate a new ASAD dataset, ii) propose ASAD evaluation metrics by
considering multi-label actions and actor identification, iii) improve the data
association strategies in MOT to boost the MOT performance, which leads to
better ASAD results. The code is available at https: //github.com/fandulu/ASAD.
</summary>
    <author>
      <name>Fan Yang</name>
    </author>
    <author>
      <name>Norimichi Ukita</name>
    </author>
    <author>
      <name>Sakriani Sakti</name>
    </author>
    <author>
      <name>Satoshi Nakamura</name>
    </author>
    <link href="http://arxiv.org/abs/2208.12940v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.12940v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1703.02716v1</id>
    <updated>2017-03-08T05: 52: 52Z</updated>
    <published>2017-03-08T05: 52: 52Z</published>
    <title>A Pursuit of Temporal Accuracy in General Activity Detection</title>
    <summary>  Detecting activities in untrimmed videos is an important but challenging
task. The performance of existing methods remains unsatisfactory, e.g., they
often meet difficulties in locating the beginning and end of a long complex
action. In this paper, we propose a generic framework that can accurately
detect a wide variety of activities from untrimmed videos. Our first
contribution is a novel proposal scheme that can efficiently generate
candidates with accurate temporal boundaries. The other contribution is a
cascaded classification pipeline that explicitly distinguishes between
relevance and completeness of a candidate instance. On two challenging temporal
activity detection datasets, THUMOS14 and ActivityNet, the proposed framework
significantly outperforms the existing state-of-the-art methods, demonstrating
superior accuracy and strong adaptivity in handling activities with various
temporal structures.
</summary>
    <author>
      <name>Yuanjun Xiong</name>
    </author>
    <author>
      <name>Yue Zhao</name>
    </author>
    <author>
      <name>Limin Wang</name>
    </author>
    <author>
      <name>Dahua Lin</name>
    </author>
    <author>
      <name>Xiaoou Tang</name>
    </author>
    <link href="http://arxiv.org/abs/1703.02716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.02716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2104.11403v1</id>
    <updated>2021-04-23T03: 57: 34Z</updated>
    <published>2021-04-23T03: 57: 34Z</published>
    <title>Low Pass Filter for Anti-aliasing in Temporal Action Localization</title>
    <summary>  In temporal action localization methods, temporal downsampling operations are
widely used to extract proposal features, but they often lead to the aliasing
problem, due to lacking consideration of sampling rates. This paper aims to
verify the existence of aliasing in TAL methods and investigate utilizing low
pass filters to solve this problem by inhibiting the high-frequency band.
However, the high-frequency band usually contains large amounts of specific
information, which is important for model inference. Therefore, it is necessary
to make a tradeoff between anti-aliasing and reserving high-frequency
information. To acquire optimal performance, this paper learns different cutoff
frequencies for different instances dynamically. This design can be plugged
into most existing temporal modeling programs requiring only one additional
cutoff frequency parameter. Integrating low pass filters to the downsampling
operations significantly improves the detection performance and achieves
comparable results on THUMOS'14, ActivityNet~1.3, and Charades datasets.
Experiments demonstrate that anti-aliasing with low pass filters in TAL is
advantageous and efficient.
</summary>
    <author>
      <name>Cece Jin</name>
    </author>
    <author>
      <name>Yuanqi Chen</name>
    </author>
    <author>
      <name>Ge Li</name>
    </author>
    <author>
      <name>Tao Zhang</name>
    </author>
    <author>
      <name>Thomas Li</name>
    </author>
    <link href="http://arxiv.org/abs/2104.11403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.11403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2106.14447v1</id>
    <updated>2021-06-28T08: 00: 21Z</updated>
    <published>2021-06-28T08: 00: 21Z</published>
    <title>Feature Combination Meets Attention: Baidu Soccer Embeddings and
  Transformer based Temporal Detection</title>
    <summary>  With rapidly evolving internet technologies and emerging tools, sports
related videos generated online are increasing at an unprecedentedly fast pace.
To automate sports video editing/highlight generation process, a key task is to
precisely recognize and locate the events in the long untrimmed videos. In this
tech report, we present a two-stage paradigm to detect what and when events
happen in soccer broadcast videos. Specifically, we fine-tune multiple action
recognition models on soccer data to extract high-level semantic features, and
design a transformer based temporal detection module to locate the target
events. This approach achieved the state-of-the-art performance in both two
tasks, i.e., action spotting and replay grounding, in the SoccerNet-v2
Challenge, under CVPR 2021 ActivityNet workshop. Our soccer embedding features
are released at https: //github.com/baidu-research/vidpress-sports. By sharing
these features with the broader community, we hope to accelerate the research
into soccer video understanding.
</summary>
    <author>
      <name>Xin Zhou</name>
    </author>
    <author>
      <name>Le Kang</name>
    </author>
    <author>
      <name>Zhiyu Cheng</name>
    </author>
    <author>
      <name>Bo He</name>
    </author>
    <author>
      <name>Jingyu Xin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Tech Report. Authors Xin Zhou, Le Kang, and Zhiyu Cheng made equal
  contributions</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.14447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.14447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2412.07072v2</id>
    <updated>2024-12-23T01: 39: 48Z</updated>
    <published>2024-12-10T00: 25: 33Z</published>
    <title>Stable Mean Teacher for Semi-supervised Video Action Detection</title>
    <summary>  In this work, we focus on semi-supervised learning for video action
detection. Video action detection requires spatiotemporal localization in
addition to classification, and a limited amount of labels makes the model
prone to unreliable predictions. We present Stable Mean Teacher, a simple
end-to-end teacher-based framework that benefits from improved and temporally
consistent pseudo labels. It relies on a novel Error Recovery (EoR) module,
which learns from students' mistakes on labeled samples and transfers this
knowledge to the teacher to improve pseudo labels for unlabeled samples.
Moreover, existing spatiotemporal losses do not take temporal coherency into
account and are prone to temporal inconsistencies. To address this, we present
Difference of Pixels (DoP), a simple and novel constraint focused on temporal
consistency, leading to coherent temporal detections. We evaluate our approach
on four different spatiotemporal detection benchmarks: UCF101-24, JHMDB21, AVA,
and YouTube-VOS. Our approach outperforms the supervised baselines for action
detection by an average margin of 23.5% on UCF101-24,
    16% on JHMDB21, and 3.3%
on AVA. Using merely 10% and 20% of data, it provides competitive performance
compared to the supervised baseline trained on 100% annotations on UCF101-24
and JHMDB21, respectively. We further evaluate its effectiveness on AVA for
scaling to large-scale datasets and YouTube-VOS for video object segmentation,
demonstrating its generalization capability to other tasks in the video domain.
Code and models are publicly available.
</summary>
    <author>
      <name>Akash Kumar</name>
    </author>
    <author>
      <name>Sirshapan Mitra</name>
    </author>
    <author>
      <name>Yogesh Singh Rawat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI Conference on Artificial Intelligence, Main Technical Track
  (AAAI),
    2025, Code: https: //github.com/AKASH2907/stable_mean_teacher</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.07072v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.07072v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2305.17861v1</id>
    <updated>2023-05-29T02: 48: 04Z</updated>
    <published>2023-05-29T02: 48: 04Z</published>
    <title>Proposal-Based Multiple Instance Learning for Weakly-Supervised Temporal
  Action Localization</title>
    <summary>  Weakly-supervised temporal action localization aims to localize and recognize
actions in untrimmed videos with only video-level category labels during
training. Without instance-level annotations, most existing methods follow the
Segment-based Multiple Instance Learning (S-MIL) framework, where the
predictions of segments are supervised by the labels of videos. However, the
objective for acquiring segment-level scores during training is not consistent
with the target for acquiring proposal-level scores during testing, leading to
suboptimal results. To deal with this problem, we propose a novel
Proposal-based Multiple Instance Learning (P-MIL) framework that directly
classifies the candidate proposals in both the training and testing stages,
which includes three key designs: 1) a surrounding contrastive feature
extraction module to suppress the discriminative short proposals by considering
the surrounding contrastive information,
    2) a proposal completeness evaluation
module to inhibit the low-quality proposals with the guidance of the
completeness pseudo labels, and 3) an instance-level rank consistency loss to
achieve robust detection by leveraging the complementarity of RGB and FLOW
modalities. Extensive experimental results on two challenging benchmarks
including THUMOS14 and ActivityNet demonstrate the superior performance of our
method.
</summary>
    <author>
      <name>Huan Ren</name>
    </author>
    <author>
      <name>Wenfei Yang</name>
    </author>
    <author>
      <name>Tianzhu Zhang</name>
    </author>
    <author>
      <name>Yongdong Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVPR 2023. Code is available at
  https: //github.com/RenHuan1999/CVPR2023_P-MIL</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.17861v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.17861v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2008.13759v1</id>
    <updated>2020-08-31T17: 28: 51Z</updated>
    <published>2020-08-31T17: 28: 51Z</published>
    <title>Online Spatiotemporal Action Detection and Prediction via Causal
  Representations</title>
    <summary>  In this thesis, we focus on video action understanding problems from an
online and real-time processing point of view. We start with the conversion of
the traditional offline spatiotemporal action detection pipeline into an online
spatiotemporal action tube detection system. An action tube is a set of
bounding connected over time, which bounds an action instance in space and
time. Next, we explore the future prediction capabilities of such detection
methods by extending an existing action tube into the future by regression.
Later, we seek to establish that online/causal representations can achieve
similar performance to that of offline three dimensional (3D) convolutional
neural networks (CNNs) on various tasks, including action recognition, temporal
action segmentation and early prediction.
</summary>
    <author>
      <name>Gurkirt Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD thesis, Oxford Brookes University, Examiners: Dr. Andrea Vedaldi
  and Dr. Fridolin Wild,
    172 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.13759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.13759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1801.09184v1</id>
    <updated>2018-01-28T05: 46: 01Z</updated>
    <published>2018-01-28T05: 46: 01Z</published>
    <title>Contextual Multi-Scale Region Convolutional 3D Network for Activity
  Detection</title>
    <summary>  Activity detection is a fundamental problem in computer vision. Detecting
activities of different temporal scales is particularly challenging. In this
paper, we propose the contextual multi-scale region convolutional 3D network
(CMS-RC3D) for activity detection. To deal with the inherent temporal scale
variability of activity instances, the temporal feature pyramid is used to
represent activities of different temporal scales. On each level of the
temporal feature pyramid, an activity proposal detector and an activity
classifier are learned to detect activities of specific temporal scales.
Temporal contextual information is fused into activity classifiers for better
recognition. More importantly, the entire model at all levels can be trained
end-to-end. Our CMS-RC3D detector can deal with activities at all temporal
scale ranges with only a single pass through the backbone network. We test our
detector on two public activity detection benchmarks, THUMOS14 and ActivityNet.
Extensive experiments show that the proposed CMS-RC3D detector outperforms
state-of-the-art methods on THUMOS14 by a substantial margin and achieves
comparable results on ActivityNet despite using a shallow feature extractor.
</summary>
    <author>
      <name>Yancheng Bai</name>
    </author>
    <author>
      <name>Huijuan Xu</name>
    </author>
    <author>
      <name>Kate Saenko</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,
    3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.09184v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09184v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2408.16729v3</id>
    <updated>2024-12-19T10: 53: 48Z</updated>
    <published>2024-08-29T17: 20: 59Z</published>
    <title>Prediction-Feedback DETR for Temporal Action Detection</title>
    <summary>  Temporal Action Detection (TAD) is fundamental yet challenging for real-world
video applications. Leveraging the unique benefits of transformers, various
DETR-based approaches have been adopted in TAD. However, it has recently been
identified that the attention collapse in self-attention causes the performance
degradation of DETR for TAD. Building upon previous research, this paper newly
addresses the attention collapse problem in cross-attention within DETR-based
TAD methods. Moreover, our findings reveal that cross-attention exhibits
patterns distinct from predictions, indicating a short-cut phenomenon. To
resolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR),
which utilizes predictions to restore the collapse and align the cross- and
self-attention with predictions. Specifically, we devise novel
prediction-feedback objectives using guidance from the relations of the
predictions. As a result, Pred-DETR significantly alleviates the collapse and
achieves state-of-the-art performance among DETR-based methods on various
challenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, and
FineAction.
</summary>
    <author>
      <name>Jihwan Kim</name>
    </author>
    <author>
      <name>Miso Lee</name>
    </author>
    <author>
      <name>Cheol-Ho Cho</name>
    </author>
    <author>
      <name>Jihyun Lee</name>
    </author>
    <author>
      <name>Jae-Pil Heo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to AAAI 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.16729v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.16729v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1204.2169v3</id>
    <updated>2012-09-26T12: 16: 40Z</updated>
    <published>2012-04-10T14: 42: 56Z</published>
    <title>Spatiotemporal correlations of handset-based service usages</title>
    <summary>  We study spatiotemporal correlations and temporal diversities of
handset-based service usages by analyzing a dataset that includes detailed
information about locations and service usages of 124 users over 16 months. By
constructing the spatiotemporal trajectories of the users we detect several
meaningful places or contexts for each one of them and show how the context
affects the service usage patterns. We find that temporal patterns of service
usages are bound to the typical weekly cycles of humans, yet they show maximal
activities at different times. We first discuss their temporal correlations and
then investigate the time-ordering behavior of communication services like
calls being followed by the non-communication services like applications. We
also find that the behavioral overlap network based on the clustering of
temporal patterns is comparable to the communication network of users. Our
approach provides a useful framework for handset-based data analysis and helps
us to understand the complexities of information and communications technology
enabled human behavior.
</summary>
    <author>
      <name>Hang-Hyun Jo</name>
    </author>
    <author>
      <name>Márton Karsai</name>
    </author>
    <author>
      <name>Juuso Karikoski</name>
    </author>
    <author>
      <name>Kimmo Kaski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1140/epjds10</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1140/epjds10" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages,
    15 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPJ Data Science 1,
    10 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1204.2169v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2169v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2212.02875v1</id>
    <updated>2022-12-06T10: 41: 00Z</updated>
    <published>2022-12-06T10: 41: 00Z</published>
    <title>Multi-Task Edge Prediction in Temporally-Dynamic Video Graphs</title>
    <summary>  Graph neural networks have shown to learn effective node representations,
enabling node-, link-, and graph-level inference. Conventional graph networks
assume static relations between nodes, while relations between entities in a
video often evolve over time, with nodes entering and exiting dynamically. In
such temporally-dynamic graphs, a core problem is inferring the future state of
spatio-temporal edges, which can constitute multiple types of relations. To
address this problem, we propose MTD-GNN, a graph network for predicting
temporally-dynamic edges for multiple types of relations. We propose a
factorized spatio-temporal graph attention layer to learn dynamic node
representations and present a multi-task edge prediction loss that models
multiple relations simultaneously. The proposed architecture operates on top of
scene graphs that we obtain from videos through object detection and
spatio-temporal linking. Experimental evaluations on ActionGenome and CLEVRER
show that modeling multiple relations in our temporally-dynamic graph network
can be mutually beneficial, outperforming existing static and spatio-temporal
graph neural networks, as well as state-of-the-art predicate classification
methods.
</summary>
    <author>
      <name>Osman Ülger</name>
    </author>
    <author>
      <name>Julian Wiederer</name>
    </author>
    <author>
      <name>Mohsen Ghafoorian</name>
    </author>
    <author>
      <name>Vasileios Belagiannis</name>
    </author>
    <author>
      <name>Pascal Mettes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BMVC2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.02875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.02875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2501.11124v1</id>
    <updated>2025-01-19T17: 31: 40Z</updated>
    <published>2025-01-19T17: 31: 40Z</published>
    <title>Rethinking Pseudo-Label Guided Learning for Weakly Supervised Temporal
  Action Localization from the Perspective of Noise Correction</title>
    <summary>  Pseudo-label learning methods have been widely applied in weakly-supervised
temporal action localization. Existing works directly utilize weakly-supervised
base model to generate instance-level pseudo-labels for training the
fully-supervised detection head. We argue that the noise in pseudo-labels would
interfere with the learning of fully-supervised detection head, leading to
significant performance leakage. Issues with noisy labels include:(1)
inaccurate boundary localization; (2) undetected short action clips; (3)
multiple adjacent segments incorrectly detected as one segment. To target these
issues, we introduce a two-stage noisy label learning strategy to harness every
potential useful signal in noisy labels. First, we propose a frame-level
pseudo-label generation model with a context-aware denoising algorithm to
refine the boundaries. Second, we introduce an online-revised teacher-student
framework with a missing instance compensation module and an ambiguous instance
correction module to solve the short-action-missing and many-to-one problems.
Besides, we apply a high-quality pseudo-label mining loss in our online-revised
teacher-student framework to add different weights to the noisy labels to train
more effectively. Our model outperforms the previous state-of-the-art method in
detection accuracy and inference speed greatly upon the THUMOS14 and
ActivityNet v1.2 benchmarks.
</summary>
    <author>
      <name>Quan Zhang</name>
    </author>
    <author>
      <name>Yuxin Qi</name>
    </author>
    <author>
      <name>Xi Tang</name>
    </author>
    <author>
      <name>Rui Yuan</name>
    </author>
    <author>
      <name>Xi Lin</name>
    </author>
    <author>
      <name>Ke Zhang</name>
    </author>
    <author>
      <name>Chun Yuan</name>
    </author>
    <link href="http://arxiv.org/abs/2501.11124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.11124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2004.03481v1</id>
    <updated>2020-04-05T14: 28: 26Z</updated>
    <published>2020-04-05T14: 28: 26Z</published>
    <title>Routine pattern discovery and anomaly detection in individual travel
  behavior</title>
    <summary>  Discovering patterns and detecting anomalies in individual travel behavior is
a crucial problem in both research and practice. In this paper, we address this
problem by building a probabilistic framework to model individual
spatiotemporal travel behavior data (e.g., trip records and trajectory data).
We develop a two-dimensional latent Dirichlet allocation (LDA) model to
characterize the generative mechanism of spatiotemporal trip records of each
traveler. This model introduces two separate factor matrices for the spatial
dimension and the temporal dimension, respectively, and use a two-dimensional
core structure at the individual level to effectively model the joint
interactions and complex dependencies. This model can efficiently summarize
travel behavior patterns on both spatial and temporal dimensions from very
sparse trip sequences in an unsupervised way. In this way, complex travel
behavior can be modeled as a mixture of representative and interpretable
spatiotemporal patterns. By applying the trained model on future/unseen
spatiotemporal records of a traveler, we can detect her behavior anomalies by
scoring those observations using perplexity. We demonstrate the effectiveness
of the proposed modeling framework on a real-world license plate recognition
(LPR) data set. The results confirm the advantage of statistical learning
methods in modeling sparse individual travel behavior data. This type of
pattern discovery and anomaly detection applications can provide useful
insights for traffic monitoring, law enforcement, and individual travel
behavior profiling.
</summary>
    <author>
      <name>Lijun Sun</name>
    </author>
    <author>
      <name>Xinyu Chen</name>
    </author>
    <author>
      <name>Zhaocheng He</name>
    </author>
    <author>
      <name>Luis F. Miranda-Moreno</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11067-021-09542-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11067-021-09542-9" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Networks and Spatial Economics (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2004.03481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.03481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2206.07846v2</id>
    <updated>2022-08-03T16: 41: 54Z</updated>
    <published>2022-06-15T23: 22: 36Z</published>
    <title>Action Spotting using Dense Detection Anchors Revisited: Submission to
  the SoccerNet Challenge 2022</title>
    <summary>  This brief technical report describes our submission to the Action Spotting
SoccerNet Challenge 2022. The challenge was part of the CVPR 2022 ActivityNet
Workshop. Our submission was based on a recently proposed method which focuses
on increasing temporal precision via a densely sampled set of detection
anchors. Due to its emphasis on temporal precision, this approach had shown
significant improvements in the tight average-mAP metric. Tight average-mAP was
used as the evaluation criterion for the challenge, and is defined using small
temporal evaluation tolerances, thus being more sensitive to small temporal
errors. In order to further improve results, here we introduce small changes in
the pre- and post-processing steps, and also combine different input feature
types via late fusion. These changes brought improvements that helped us
achieve the first place in the challenge and also led to a new state-of-the-art
on SoccerNet's test set when using the dataset's standard experimental
protocol. This report briefly reviews the action spotting method based on dense
detection anchors, then focuses on the modifications introduced for the
challenge. We also describe the experimental protocols and training procedures
we used, and finally present our results.
</summary>
    <author>
      <name>João V. B. Soares</name>
    </author>
    <author>
      <name>Avijit Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v2: a few more experiments, more detailed method description</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.07846v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.07846v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2208.00548v1</id>
    <updated>2022-08-01T00: 22: 40Z</updated>
    <published>2022-08-01T00: 22: 40Z</published>
    <title>Safer Traffic Recovery from the Pandemic in London -- Spatiotemporal
  Data Mining of Car Crashes</title>
    <summary>  In the aim to support London's safer recovery from the pandemic by improving
road safety intelligently, this study investigated the spatiotemporal patterns
of age-involved car crashes and affecting factors, upon answering two main
research questions: (1)"What are the spatial and temporal patterns of car
crashes as well as their changes in two typical years,
    2019 and 2020, in
London, and how the influential factors work?"; (2)"What are the spatiotemporal
patterns of casualty by age groups, and how people's daily activities affect
the patterns pre- and para- the pandemic"? Three approaches, i.e., spatial
analysis (network Kernel Density Estimation, NetKDE), factor analysis, and
spatiotemporal data mining (tensor decomposition), had been implemented to
identify the temporal patterns of car crashes on weekly and daily basis
respectively, detect the crashes' hot spots, and to gain better understanding
the effect from citizens' daily activity on crashes' patterns pre- and para-
the pandemic. It had been found from the study that car crashes mainly
clustered in the central part of London, especially busier areas around denser
hubs of point-of-interest (POIs); the POIs, as a reflector for citizens' daily
activities and travel behaviours, can be of help to gain a better understanding
of the crashes' patterns, upon further assessment on interactions through the
geographical detector; the crashes' casualty patterns varied by age group, with
distinctive relationships between POIs and crashes' pattern for corresponding
age group categorised. In all, the paper provided an in-depth exploratory
analysis of car crashes and their casualty patterns in London to facilitate
deployment policies towards post-pandemic safer recovery upon COVID-19.
</summary>
    <author>
      <name>Kejiang Qian</name>
    </author>
    <author>
      <name>Yijing Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages,
    8 figures,
    6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.00548v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.00548v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1611.05267v1</id>
    <updated>2016-11-16T13: 19: 19Z</updated>
    <published>2016-11-16T13: 19: 19Z</published>
    <title>Temporal Convolutional Networks for Action Segmentation and Detection</title>
    <summary>  The ability to identify and temporally segment fine-grained human actions
throughout a video is crucial for robotics, surveillance, education, and
beyond. Typical approaches decouple this problem by first extracting local
spatiotemporal features from video frames and then feeding them into a temporal
classifier that captures high-level temporal patterns. We introduce a new class
of temporal models, which we call Temporal Convolutional Networks (TCNs), that
use a hierarchy of temporal convolutions to perform fine-grained action
segmentation or detection. Our Encoder-Decoder TCN uses pooling and upsampling
to efficiently capture long-range temporal patterns whereas our Dilated TCN
uses dilated convolutions. We show that TCNs are capable of capturing action
compositions, segment durations, and long-range dependencies, and are over a
magnitude faster to train than competing LSTM-based Recurrent Neural Networks.
We apply these models to three challenging fine-grained datasets and show large
improvements over the state of the art.
</summary>
    <author>
      <name>Colin Lea</name>
    </author>
    <author>
      <name>Michael D. Flynn</name>
    </author>
    <author>
      <name>Rene Vidal</name>
    </author>
    <author>
      <name>Austin Reiter</name>
    </author>
    <author>
      <name>Gregory D. Hager</name>
    </author>
    <link href="http://arxiv.org/abs/1611.05267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.05267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2308.09946v2</id>
    <updated>2023-09-26T03: 37: 42Z</updated>
    <published>2023-08-19T08: 45: 49Z</published>
    <title>Weakly-Supervised Action Localization by Hierarchically-structured
  Latent Attention Modeling</title>
    <summary>  Weakly-supervised action localization aims to recognize and localize action
instancese in untrimmed videos with only video-level labels. Most existing
models rely on multiple instance learning(MIL), where the predictions of
unlabeled instances are supervised by classifying labeled bags. The MIL-based
methods are relatively well studied with cogent performance achieved on
classification but not on localization. Generally, they locate temporal regions
by the video-level classification but overlook the temporal variations of
feature semantics. To address this problem, we propose a novel attention-based
hierarchically-structured latent model to learn the temporal variations of
feature semantics. Specifically, our model entails two components, the first is
an unsupervised change-points detection module that detects change-points by
learning the latent representations of video features in a temporal hierarchy
based on their rates of change, and the second is an attention-based
classification model that selects the change-points of the foreground as the
boundaries. To evaluate the effectiveness of our model, we conduct extensive
experiments on two benchmark datasets, THUMOS-14 and ActivityNet-v1.3. The
experiments show that our method outperforms current state-of-the-art methods,
and even achieves comparable performance with fully-supervised methods.
</summary>
    <author>
      <name>Guiqin Wang</name>
    </author>
    <author>
      <name>Peng Zhao</name>
    </author>
    <author>
      <name>Cong Zhao</name>
    </author>
    <author>
      <name>Shusen Yang</name>
    </author>
    <author>
      <name>Jie Cheng</name>
    </author>
    <author>
      <name>Luziwei Leng</name>
    </author>
    <author>
      <name>Jianxing Liao</name>
    </author>
    <author>
      <name>Qinghai Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICCV 2023. arXiv admin note: text overlap with
  arXiv: 2203.15187, arXiv: 2003.12424, arXiv: 2104.02967 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.09946v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.09946v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2304.08451v3</id>
    <updated>2023-08-28T10: 22: 23Z</updated>
    <published>2023-04-17T17: 21: 21Z</published>
    <title>Efficient Video Action Detection with Token Dropout and Context
  Refinement</title>
    <summary>  Streaming video clips with large-scale video tokens impede vision
transformers (ViTs) for efficient recognition, especially in video action
detection where sufficient spatiotemporal representations are required for
precise actor identification. In this work, we propose an end-to-end framework
for efficient video action detection (EVAD) based on vanilla ViTs. Our EVAD
consists of two specialized designs for video action detection. First, we
propose a spatiotemporal token dropout from a keyframe-centric perspective. In
a video clip, we maintain all tokens from its keyframe, preserve tokens
relevant to actor motions from other frames, and drop out the remaining tokens
in this clip. Second, we refine scene context by leveraging remaining tokens
for better recognizing actor identities. The region of interest (RoI) in our
action detector is expanded into temporal domain. The captured spatiotemporal
actor identity representations are refined via scene context in a decoder with
the attention mechanism. These two designs make our EVAD efficient while
maintaining accuracy, which is validated on three benchmark datasets (i.e.,
AVA, UCF101-24, JHMDB). Compared to the vanilla ViT backbone, our EVAD reduces
the overall GFLOPs by 43% and improves real-time inference speed by 40% with no
performance degradation. Moreover, even at similar computational costs, our
EVAD can improve the performance by 1.1 mAP with higher resolution inputs. Code
is available at https: //github.com/MCG-NJU/EVAD.
</summary>
    <author>
      <name>Lei Chen</name>
    </author>
    <author>
      <name>Zhan Tong</name>
    </author>
    <author>
      <name>Yibing Song</name>
    </author>
    <author>
      <name>Gangshan Wu</name>
    </author>
    <author>
      <name>Limin Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.08451v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.08451v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1911.06644v5</id>
    <updated>2021-10-18T12: 53: 47Z</updated>
    <published>2019-11-15T14: 09: 47Z</published>
    <title>You Only Watch Once: A Unified CNN Architecture for Real-Time
  Spatiotemporal Action Localization</title>
    <summary>  Spatiotemporal action localization requires the incorporation of two sources
of information into the designed architecture: (1) temporal information from
the previous frames and (2) spatial information from the key frame. Current
state-of-the-art approaches usually extract these information with separate
networks and use an extra mechanism for fusion to get detections. In this work,
we present YOWO, a unified CNN architecture for real-time spatiotemporal action
localization in video streams. YOWO is a single-stage architecture with two
branches to extract temporal and spatial information concurrently and predict
bounding boxes and action probabilities directly from video clips in one
evaluation. Since the whole architecture is unified, it can be optimized
end-to-end. The YOWO architecture is fast providing 34 frames-per-second on
16-frames input clips and 62 frames-per-second on 8-frames input clips, which
is currently the fastest state-of-the-art architecture on spatiotemporal action
localization task. Remarkably, YOWO outperforms the previous state-of-the art
results on J-HMDB-21 and UCF101-24 with an impressive improvement of ~3% and
~12%, respectively. Moreover, YOWO is the first and only single-stage
architecture that provides competitive results on AVA dataset. We make our code
and pretrained models publicly available.
</summary>
    <author>
      <name>Okan Köpüklü</name>
    </author>
    <author>
      <name>Xiangyu Wei</name>
    </author>
    <author>
      <name>Gerhard Rigoll</name>
    </author>
    <link href="http://arxiv.org/abs/1911.06644v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.06644v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2006.07976v3</id>
    <updated>2021-04-20T20: 30: 27Z</updated>
    <published>2020-06-14T18: 51: 49Z</published>
    <title>Actor-Context-Actor Relation Network for Spatio-Temporal Action
  Localization</title>
    <summary>  Localizing persons and recognizing their actions from videos is a challenging
task towards high-level video understanding. Recent advances have been achieved
by modeling direct pairwise relations between entities. In this paper, we take
one step further, not only model direct relations between pairs but also take
into account indirect higher-order relations established upon multiple
elements. We propose to explicitly model the Actor-Context-Actor Relation,
which is the relation between two actors based on their interactions with the
context. To this end, we design an Actor-Context-Actor Relation Network
(ACAR-Net) which builds upon a novel High-order Relation Reasoning Operator and
an Actor-Context Feature Bank to enable indirect relation reasoning for
spatio-temporal action localization. Experiments on AVA and UCF101-24 datasets
show the advantages of modeling actor-context-actor relations, and
visualization of attention maps further verifies that our model is capable of
finding relevant higher-order relations to support action detection. Notably,
our method ranks first in the AVA-Kineticsaction localization task of
ActivityNet Challenge 2020, out-performing other entries by a significant
margin (+6.71mAP). Training code and models will be available at
https: //github.com/Siyu-C/ACAR-Net.
</summary>
    <author>
      <name>Junting Pan</name>
    </author>
    <author>
      <name>Siyu Chen</name>
    </author>
    <author>
      <name>Mike Zheng Shou</name>
    </author>
    <author>
      <name>Yu Liu</name>
    </author>
    <author>
      <name>Jing Shao</name>
    </author>
    <author>
      <name>Hongsheng Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in CVPR 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.07976v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.07976v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2312.14138v1</id>
    <updated>2023-12-21T18: 57: 12Z</updated>
    <published>2023-12-21T18: 57: 12Z</published>
    <title>Revisiting Foreground and Background Separation in Weakly-supervised
  Temporal Action Localization: A Clustering-based Approach</title>
    <summary>  Weakly-supervised temporal action localization aims to localize action
instances in videos with only video-level action labels. Existing methods
mainly embrace a localization-by-classification pipeline that optimizes the
snippet-level prediction with a video classification loss. However, this
formulation suffers from the discrepancy between classification and detection,
resulting in inaccurate separation of foreground and background (F\&amp;B)
snippets. To alleviate this problem, we propose to explore the underlying
structure among the snippets by resorting to unsupervised snippet clustering,
rather than heavily relying on the video classification loss. Specifically, we
propose a novel clustering-based F\&amp;B separation algorithm. It comprises two
core components: a snippet clustering component that groups the snippets into
multiple latent clusters and a cluster classification component that further
classifies the cluster as foreground or background. As there are no
ground-truth labels to train these two components, we introduce a unified
self-labeling mechanism based on optimal transport to produce high-quality
pseudo-labels that match several plausible prior distributions. This ensures
that the cluster assignments of the snippets can be accurately associated with
their F\&amp;B labels, thereby boosting the F\&amp;B separation. We evaluate our method
on three benchmarks: THUMOS14, ActivityNet v1.2 and v1.3. Our method achieves
promising performance on all three benchmarks while being significantly more
lightweight than previous methods. Code is available at
https: //github.com/Qinying-Liu/CASE
</summary>
    <author>
      <name>Qinying Liu</name>
    </author>
    <author>
      <name>Zilei Wang</name>
    </author>
    <author>
      <name>Shenghai Rong</name>
    </author>
    <author>
      <name>Junjie Li</name>
    </author>
    <author>
      <name>Yixin Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.14138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.14138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2004.01494v1</id>
    <updated>2020-04-03T12: 16: 45Z</updated>
    <published>2020-04-03T12: 16: 45Z</published>
    <title>Two-Stream AMTnet for Action Detection</title>
    <summary>  In this paper, we propose Two-Stream AMTnet, which leverages recent advances
in video-based action representation[
        1
    ] and incremental action tube
generation[
        2
    ]. Majority of the present action detectors follow a frame-based
representation, a late-fusion followed by an offline action tube building
steps. These are sub-optimal as: frame-based features barely encode the
temporal relations; late-fusion restricts the network to learn robust
spatiotemporal features; and finally, an offline action tube generation is not
suitable for many real-world problems such as autonomous driving, human-robot
interaction to name a few. The key contributions of this work are: (1)
combining AMTnet's 3D proposal architecture with an online action tube
generation technique which allows the model to learn stronger temporal features
needed for accurate action detection and facilitates running inference online;
(2) an efficient fusion technique allowing the deep network to learn strong
spatiotemporal action representations. This is achieved by augmenting the
previous Action Micro-Tube (AMTnet) action detection framework in three
distinct ways: by adding a parallel motion stIn this paper, we propose a new
deep neural network architecture for online action detection, termed ream to
the original appearance one in AMTnet; (2) in opposition to state-of-the-art
action detectors which train appearance and motion streams separately, and use
a test time late fusion scheme to fuse RGB and flow cues, by jointly training
both streams in an end-to-end fashion and merging RGB and optical flow features
at training time; (3) by introducing an online action tube generation algorithm
which works at video-level, and in real-time (when exploiting only appearance
features). Two-Stream AMTnet exhibits superior action detection performance
over state-of-the-art approaches on the standard action detection benchmarks.
</summary>
    <author>
      <name>Suman Saha</name>
    </author>
    <author>
      <name>Gurkirt Singh</name>
    </author>
    <author>
      <name>Fabio Cuzzolin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.01494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.01494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2101.10511v5</id>
    <updated>2021-08-19T07: 46: 55Z</updated>
    <published>2021-01-26T01: 31: 30Z</published>
    <title>Generic Event Boundary Detection: A Benchmark for Event Segmentation</title>
    <summary>  This paper presents a novel task together with a new benchmark for detecting
generic, taxonomy-free event boundaries that segment a whole video into chunks.
Conventional work in temporal video segmentation and action detection focuses
on localizing pre-defined action categories and thus does not scale to generic
videos. Cognitive Science has known since last century that humans consistently
segment videos into meaningful temporal chunks. This segmentation happens
naturally, without pre-defined event categories and without being explicitly
asked to do so. Here, we repeat these cognitive experiments on mainstream CV
datasets; with our novel annotation guideline which addresses the complexities
of taxonomy-free event boundary annotation, we introduce the task of Generic
Event Boundary Detection (GEBD) and the new benchmark Kinetics-GEBD. Our
Kinetics-GEBD has the largest number of boundaries (e.g. 32 of ActivityNet,
    8
of EPIC-Kitchens-100) which are in-the-wild, taxonomy-free, cover generic event
change, and respect human perception diversity. We view GEBD as an important
stepping stone towards understanding the video as a whole, and believe it has
been previously neglected due to a lack of proper task definition and
annotations. Through experiment and human study we demonstrate the value of the
annotations. Further, we benchmark supervised and un-supervised GEBD approaches
on the TAPOS dataset and our Kinetics-GEBD. We release our annotations and
baseline codes at CVPR'21 LOVEU Challenge:
https: //sites.google.com/view/loveucvpr21.
</summary>
    <author>
      <name>Mike Zheng Shou</name>
    </author>
    <author>
      <name>Stan Weixian Lei</name>
    </author>
    <author>
      <name>Weiyao Wang</name>
    </author>
    <author>
      <name>Deepti Ghadiyaram</name>
    </author>
    <author>
      <name>Matt Feiszli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.10511v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.10511v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2309.09611v1</id>
    <updated>2023-09-18T09: 33: 25Z</updated>
    <published>2023-09-18T09: 33: 25Z</published>
    <title>Collaborative Three-Stream Transformers for Video Captioning</title>
    <summary>  As the most critical components in a sentence, subject, predicate and object
require special attention in the video captioning task. To implement this idea,
we design a novel framework, named COllaborative three-Stream Transformers
(COST), to model the three parts separately and complement each other for
better representation. Specifically, COST is formed by three branches of
transformers to exploit the visual-linguistic interactions of different
granularities in spatial-temporal domain between videos and text, detected
objects and text, and actions and text. Meanwhile, we propose a
cross-granularity attention module to align the interactions modeled by the
three branches of transformers, then the three branches of transformers can
support each other to exploit the most discriminative semantic information of
different granularities for accurate predictions of captions. The whole model
is trained in an end-to-end fashion. Extensive experiments conducted on three
large-scale challenging datasets, i.e., YouCookII, ActivityNet Captions and
MSVD, demonstrate that the proposed method performs favorably against the
state-of-the-art methods.
</summary>
    <author>
      <name>Hao Wang</name>
    </author>
    <author>
      <name>Libo Zhang</name>
    </author>
    <author>
      <name>Heng Fan</name>
    </author>
    <author>
      <name>Tiejian Luo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cviu.2023.103799</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cviu.2023.103799" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVIU</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.09611v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.09611v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1907.06968v1</id>
    <updated>2019-07-16T12: 50: 42Z</updated>
    <published>2019-07-16T12: 50: 42Z</published>
    <title>A Unified Deep Framework for Joint 3D Pose Estimation and Action
  Recognition from a Single RGB Camera</title>
    <summary>  We present a deep learning-based multitask framework for joint 3D human pose
estimation and action recognition from RGB video sequences. Our approach
proceeds along two stages. In the first, we run a real-time 2D pose detector to
determine the precise pixel location of important keypoints of the body. A
two-stream neural network is then designed and trained to map detected 2D
keypoints into 3D poses. In the second, we deploy the Efficient Neural
Architecture Search (ENAS) algorithm to find an optimal network architecture
that is used for modeling the spatio-temporal evolution of the estimated 3D
poses via an image-based intermediate representation and performing action
recognition. Experiments on Human3.6M, MSR Action3D and SBU Kinect Interaction
datasets verify the effectiveness of the proposed method on the targeted tasks.
Moreover, we show that our method requires a low computational budget for
training and inference.
</summary>
    <author>
      <name>Huy Hieu Pham</name>
    </author>
    <author>
      <name>Houssam Salmane</name>
    </author>
    <author>
      <name>Louahdi Khoudour</name>
    </author>
    <author>
      <name>Alain Crouzil</name>
    </author>
    <author>
      <name>Pablo Zegers</name>
    </author>
    <author>
      <name>Sergio A Velastin</name>
    </author>
    <link href="http://arxiv.org/abs/1907.06968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.06968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1906.10555v1</id>
    <updated>2019-06-25T14: 11: 14Z</updated>
    <published>2019-06-25T14: 11: 14Z</published>
    <title>Naver at ActivityNet Challenge 2019 -- Task B Active Speaker Detection
  (AVA)</title>
    <summary>  This report describes our submission to the ActivityNet Challenge at CVPR
2019. We use a 3D convolutional neural network (CNN) based front-end and an
ensemble of temporal convolution and LSTM classifiers to predict whether a
visible person is speaking or not. Our results show significant improvements
over the baseline on the AVA-ActiveSpeaker dataset.
</summary>
    <author>
      <name>Joon Son Chung</name>
    </author>
    <link href="http://arxiv.org/abs/1906.10555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.10555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2110.03562v1</id>
    <updated>2021-10-07T15: 30: 18Z</updated>
    <published>2021-10-07T15: 30: 18Z</published>
    <title>Weakly Supervised Human-Object Interaction Detection in Video via
  Contrastive Spatiotemporal Regions</title>
    <summary>  We introduce the task of weakly supervised learning for detecting human and
object interactions in videos. Our task poses unique challenges as a system
does not know what types of human-object interactions are present in a video or
the actual spatiotemporal location of the human and the object. To address
these challenges, we introduce a contrastive weakly supervised training loss
that aims to jointly associate spatiotemporal regions in a video with an action
and object vocabulary and encourage temporal continuity of the visual
appearance of moving objects as a form of self-supervision. To train our model,
we introduce a dataset comprising over 6.5k videos with human-object
interaction annotations that have been semi-automatically curated from sentence
captions associated with the videos. We demonstrate improved performance over
weakly supervised baselines adapted to our task on our video dataset.
</summary>
    <author>
      <name>Shuang Li</name>
    </author>
    <author>
      <name>Yilun Du</name>
    </author>
    <author>
      <name>Antonio Torralba</name>
    </author>
    <author>
      <name>Josef Sivic</name>
    </author>
    <author>
      <name>Bryan Russell</name>
    </author>
    <link href="http://arxiv.org/abs/2110.03562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.03562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2104.08194v2</id>
    <updated>2022-10-30T08: 26: 22Z</updated>
    <published>2021-04-16T16: 05: 34Z</published>
    <title>Spatiotemporal Deformable Scene Graphs for Complex Activity Detection</title>
    <summary>  Long-term complex activity recognition and localisation can be crucial for
decision making in autonomous systems such as smart cars and surgical robots.
Here we address the problem via a novel deformable, spatiotemporal scene graph
approach, consisting of three main building blocks: (i) action tube detection,
(ii) the modelling of the deformable geometry of parts, and (iii) a graph
convolutional network. Firstly, action tubes are detected in a series of
snippets. Next, a new 3D deformable RoI pooling layer is designed for learning
the flexible, deformable geometry of the constituent action tubes. Finally, a
scene graph is constructed by considering all parts as nodes and connecting
them based on different semantics such as order of appearance, sharing the same
action label and feature similarity. We also contribute fresh temporal complex
activity annotation for the recently released ROAD autonomous driving and
SARAS-ESAD surgical action datasets and show the adaptability of our framework
to different domains. Our method is shown to significantly outperform
graph-based competitors on both augmented datasets.
</summary>
    <author>
      <name>Salman Khan</name>
    </author>
    <author>
      <name>Fabio Cuzzolin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is published at BMVC 2021</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">https: //www.bmvc2021-virtualconference.com/assets/papers/0706.pdf</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.08194v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.08194v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2110.00111v1</id>
    <updated>2021-09-30T22: 42: 25Z</updated>
    <published>2021-09-30T22: 42: 25Z</published>
    <title>Deep Learning-based Action Detection in Untrimmed Videos: A Survey</title>
    <summary>  Understanding human behavior and activity facilitates advancement of numerous
real-world applications, and is critical for video analysis. Despite the
progress of action recognition algorithms in trimmed videos, the majority of
real-world videos are lengthy and untrimmed with sparse segments of interest.
The task of temporal activity detection in untrimmed videos aims to localize
the temporal boundary of actions and classify the action categories. Temporal
activity detection task has been investigated in full and limited supervision
settings depending on the availability of action annotations. This paper
provides an extensive overview of deep learning-based algorithms to tackle
temporal action detection in untrimmed videos with different supervision levels
including fully-supervised, weakly-supervised, unsupervised, self-supervised,
and semi-supervised. In addition, this paper also reviews advances in
spatio-temporal action detection where actions are localized in both temporal
and spatial dimensions. Moreover, the commonly used action detection benchmark
datasets and evaluation metrics are described, and the performance of the
state-of-the-art methods are compared. Finally, real-world applications of
temporal action detection in untrimmed videos and a set of future directions
are discussed.
</summary>
    <author>
      <name>Elahe Vahdani</name>
    </author>
    <author>
      <name>Yingli Tian</name>
    </author>
    <link href="http://arxiv.org/abs/2110.00111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.00111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/0911.1165v2</id>
    <updated>2010-01-25T02: 11: 34Z</updated>
    <published>2009-11-06T02: 41: 20Z</published>
    <title>Solar cycle dependence of the diurnal anisotropy of 0.6 TeV cosmic ray
  intensity observed with the Matsushiro underground muon detector</title>
    <summary>  We analyze the temporal variation of the diurnal anisotropy of sub-TeV cosmic
ray intensity observed with the Matsushiro (Japan) underground muon detector
over two full solar activity cycles in 1985-2008. The average sidereal
amplitude over the entire period is 0.034+-0.003 %, which is roughly one third
of the amplitude reported from AS and deep-underground muon experiments
monitoring multi-TeV GCR intensity suggesting a significant attenuation of the
anisotropy due to the solar modulation. We find, on the other hand, only weak
correlations either with the solar activity- or magnetic-cycles. We examine the
temporal variation of the "single-band valley depth" (SBVD) quoted by the
Milagro experiment and, by contrast with recent Milagro reports, we find no
steady increase in the Matsushiro observations in a 7-year period between 2000
and 2007. We suggest, therefore, that the steady increase of the SBVD reported
by the Milagro experiment is not caused by the decreasing solar modulation in
the declining phase of the 23rd solar activity cycle.
</summary>
    <author>
      <name>K. Munakata</name>
    </author>
    <author>
      <name>Y. Mizoguchi</name>
    </author>
    <author>
      <name>C. Kato</name>
    </author>
    <author>
      <name>S. Yasue</name>
    </author>
    <author>
      <name>S. Mori</name>
    </author>
    <author>
      <name>M. Takita</name>
    </author>
    <author>
      <name>J. Kota</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0004-637X/712/2/1100</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0004-637X/712/2/1100" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to the Astrophysical Journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Astrophys.J.712: 1100-1106,
    2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.1165v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.1165v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2207.11805v1</id>
    <updated>2022-07-24T20: 32: 24Z</updated>
    <published>2022-07-24T20: 32: 24Z</published>
    <title>Weakly-Supervised Temporal Action Detection for Fine-Grained Videos with
  Hierarchical Atomic Actions</title>
    <summary>  Action understanding has evolved into the era of fine granularity, as most
human behaviors in real life have only minor differences. To detect these
fine-grained actions accurately in a label-efficient way, we tackle the problem
of weakly-supervised fine-grained temporal action detection in videos for the
first time. Without the careful design to capture subtle differences between
fine-grained actions, previous weakly-supervised models for general action
detection cannot perform well in the fine-grained setting. We propose to model
actions as the combinations of reusable atomic actions which are automatically
discovered from data through self-supervised clustering, in order to capture
the commonality and individuality of fine-grained actions. The learnt atomic
actions, represented by visual concepts, are further mapped to fine and coarse
action labels leveraging the semantic label hierarchy. Our approach constructs
a visual representation hierarchy of four levels: clip level, atomic action
level, fine action class level and coarse action class level, with supervision
at each level. Extensive experiments on two large-scale fine-grained video
datasets, FineAction and FineGym, show the benefit of our proposed
weakly-supervised model for fine-grained action detection, and it achieves
state-of-the-art results.
</summary>
    <author>
      <name>Zhi Li</name>
    </author>
    <author>
      <name>Lu He</name>
    </author>
    <author>
      <name>Huijuan Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.11805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.11805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.03179v2</id>
    <updated>2024-08-12T03: 31: 57Z</updated>
    <published>2024-04-04T03: 28: 57Z</published>
    <title>UniAV: Unified Audio-Visual Perception for Multi-Task Video Event
  Localization</title>
    <summary>  Video localization tasks aim to temporally locate specific instances in
videos, including temporal action localization (TAL), sound event detection
(SED) and audio-visual event localization (AVEL). Existing methods
over-specialize on each task, overlooking the fact that these instances often
occur in the same video to form the complete video content. In this work, we
present UniAV, a Unified Audio-Visual perception network, to achieve joint
learning of TAL, SED and AVEL tasks for the first time. UniAV can leverage
diverse data available in task-specific datasets, allowing the model to learn
and share mutually beneficial knowledge across tasks and modalities. To tackle
the challenges posed by substantial variations in datasets
(size/domain/duration) and distinct task characteristics, we propose to
uniformly encode visual and audio modalities of all videos to derive generic
representations, while also designing task-specific experts to capture unique
knowledge for each task. Besides, we develop a unified language-aware
classifier by utilizing a pre-trained text encoder, enabling the model to
flexibly detect various types of instances and previously unseen ones by simply
changing prompts during inference. UniAV outperforms its single-task
counterparts by a large margin with fewer parameters, achieving on-par or
superior performances compared to state-of-the-art task-specific methods across
ActivityNet 1.3, DESED and UnAV-100 benchmarks.
</summary>
    <author>
      <name>Tiantian Geng</name>
    </author>
    <author>
      <name>Teng Wang</name>
    </author>
    <author>
      <name>Yanfu Zhang</name>
    </author>
    <author>
      <name>Jinming Duan</name>
    </author>
    <author>
      <name>Weili Guan</name>
    </author>
    <author>
      <name>Feng Zheng</name>
    </author>
    <author>
      <name>Ling shao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to the IEEE for possible publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.03179v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.03179v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2303.15879v1</id>
    <updated>2023-03-28T10: 47: 06Z</updated>
    <published>2023-03-28T10: 47: 06Z</published>
    <title>STMixer: A One-Stage Sparse Action Detector</title>
    <summary>  Traditional video action detectors typically adopt the two-stage pipeline,
where a person detector is first employed to generate actor boxes and then 3D
RoIAlign is used to extract actor-specific features for classification. This
detection paradigm requires multi-stage training and inference, and cannot
capture context information outside the bounding box. Recently, a few
query-based action detectors are proposed to predict action instances in an
end-to-end manner. However, they still lack adaptability in feature sampling
and decoding, thus suffering from the issues of inferior performance or slower
convergence. In this paper, we propose a new one-stage sparse action detector,
termed STMixer. STMixer is based on two core designs. First, we present a
query-based adaptive feature sampling module, which endows our STMixer with the
flexibility of mining a set of discriminative features from the entire
spatiotemporal domain. Second, we devise a dual-branch feature mixing module,
which allows our STMixer to dynamically attend to and mix video features along
the spatial and the temporal dimension respectively for better feature
decoding. Coupling these two designs with a video backbone yields an efficient
end-to-end action detector. Without bells and whistles, our STMixer obtains the
state-of-the-art results on the datasets of AVA, UCF101-24, and JHMDB.
</summary>
    <author>
      <name>Tao Wu</name>
    </author>
    <author>
      <name>Mengqi Cao</name>
    </author>
    <author>
      <name>Ziteng Gao</name>
    </author>
    <author>
      <name>Gangshan Wu</name>
    </author>
    <author>
      <name>Limin Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVPR 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.15879v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.15879v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2302.13748v1</id>
    <updated>2023-02-27T13: 24: 08Z</updated>
    <published>2023-02-27T13: 24: 08Z</published>
    <title>Unsupervised Video Anomaly Detection for Stereotypical Behaviours in
  Autism</title>
    <summary>  Monitoring and analyzing stereotypical behaviours is important for early
intervention and care taking in Autism Spectrum Disorder (ASD). This paper
focuses on automatically detecting stereotypical behaviours with computer
vision techniques. Off-the-shelf methods tackle this task by supervised
classification and activity recognition techniques. However, the unbounded
types of stereotypical behaviours and the difficulty in collecting video
recordings of ASD patients largely limit the feasibility of the existing
supervised detection methods. As a result, we tackle these challenges from a
new perspective, i.e. unsupervised video anomaly detection for stereotypical
behaviours detection. The models can be trained among unlabeled videos
containing only normal behaviours and unknown types of abnormal behaviours can
be detected during inference. Correspondingly, we propose a Dual Stream deep
model for Stereotypical Behaviours Detection, DS-SBD, based on the temporal
trajectory of human poses and the repetition patterns of human actions.
Extensive experiments are conducted to verify the effectiveness of our proposed
method and suggest that it serves as a potential benchmark for future research.
</summary>
    <author>
      <name>Jiaqi Gao</name>
    </author>
    <author>
      <name>Xinyang Jiang</name>
    </author>
    <author>
      <name>Yuqing Yang</name>
    </author>
    <author>
      <name>Dongsheng Li</name>
    </author>
    <author>
      <name>Lili Qiu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP49357.2023.10094676</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP49357.2023.10094676" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2023 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2302.13748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.13748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2504.05541v2</id>
    <updated>2025-04-09T02: 30: 44Z</updated>
    <published>2025-04-07T22: 35: 36Z</published>
    <title>Caption Anything in Video: Fine-grained Object-centric Captioning via
  Spatiotemporal Multimodal Prompting</title>
    <summary>  We present CAT-V (Caption AnyThing in Video), a training-free framework for
fine-grained object-centric video captioning that enables detailed descriptions
of user-selected objects through time. CAT-V integrates three key components: a
Segmenter based on SAMURAI for precise object segmentation across frames, a
Temporal Analyzer powered by TRACE-Uni for accurate event boundary detection
and temporal analysis, and a Captioner using InternVL-2.5 for generating
detailed object-centric descriptions. Through spatiotemporal visual prompts and
chain-of-thought reasoning, our framework generates detailed, temporally-aware
descriptions of objects' attributes, actions, statuses, interactions, and
environmental contexts without requiring additional training data. CAT-V
supports flexible user interactions through various visual prompts (points,
bounding boxes, and irregular regions) and maintains temporal sensitivity by
tracking object states and interactions across different time segments. Our
approach addresses limitations of existing video captioning methods, which
either produce overly abstract descriptions or lack object-level precision,
enabling fine-grained, object-specific descriptions while maintaining temporal
coherence and spatial accuracy. The GitHub repository for this project is
available at https: //github.com/yunlong10/CAT-V
</summary>
    <author>
      <name>Yunlong Tang</name>
    </author>
    <author>
      <name>Jing Bi</name>
    </author>
    <author>
      <name>Chao Huang</name>
    </author>
    <author>
      <name>Susan Liang</name>
    </author>
    <author>
      <name>Daiki Shimada</name>
    </author>
    <author>
      <name>Hang Hua</name>
    </author>
    <author>
      <name>Yunzhong Xiao</name>
    </author>
    <author>
      <name>Yizhi Song</name>
    </author>
    <author>
      <name>Pinxin Liu</name>
    </author>
    <author>
      <name>Mingqian Feng</name>
    </author>
    <author>
      <name>Junjia Guo</name>
    </author>
    <author>
      <name>Zhuo Liu</name>
    </author>
    <author>
      <name>Luchuan Song</name>
    </author>
    <author>
      <name>Ali Vosoughi</name>
    </author>
    <author>
      <name>Jinxi He</name>
    </author>
    <author>
      <name>Liu He</name>
    </author>
    <author>
      <name>Zeliang Zhang</name>
    </author>
    <author>
      <name>Jiebo Luo</name>
    </author>
    <author>
      <name>Chenliang Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2504.05541v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.05541v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1905.04430v2</id>
    <updated>2019-06-26T04: 44: 58Z</updated>
    <published>2019-05-11T02: 54: 01Z</published>
    <title>Follow the Attention: Combining Partial Pose and Object Motion for
  Fine-Grained Action Detection</title>
    <summary>  Retailers have long been searching for ways to effectively understand their
customers' behaviour in order to provide a smooth and pleasant shopping
experience that attracts more customers everyday and maximises their revenue,
consequently. Humans can flawlessly understand others' behaviour by combining
different visual cues from activity to gestures and facial expressions.
Empowering the computer vision systems to do so, however, is still an open
problem due to its intrinsic challenges as well as extrinsic enforced
difficulties like lack of publicly available data and unique environment
conditions (wild). In this work, We emphasise on detecting the first and by far
the most crucial cue in behaviour analysis; that is human activity detection in
computer vision. To do so, we introduce a framework for integrating human pose
and object motion to both temporally detect and classify the activities in a
fine-grained manner (very short and similar activities). We incorporate partial
human pose and interaction with the objects in a multi-stream neural network
architecture to guide the spatiotemporal attention mechanism for more efficient
activity recognition. To this end, in the absence of pose supervision, we
propose to use the Generative Adversarial Network (GAN) to generate exact joint
locations from noisy probability heat maps. Additionally, based on the
intuition that complex actions demand more than one source of information to be
identified even by humans, we integrate the second stream of object motion to
our network as a prior knowledge that we quantitatively show improves the
recognition results. We empirically show the capability of our approach by
achieving state-of-the-art results on MERL shopping dataset. We further
investigate the effectiveness of this approach on a new shopping dataset that
we have collected to address existing shortcomings.
</summary>
    <author>
      <name>Mohammad Mahdi Kazemi Moghaddam</name>
    </author>
    <author>
      <name>Ehsan Abbasnejad</name>
    </author>
    <author>
      <name>Javen Shi</name>
    </author>
    <link href="http://arxiv.org/abs/1905.04430v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.04430v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2412.13845v3</id>
    <updated>2025-02-24T03: 37: 59Z</updated>
    <published>2024-12-18T13: 38: 06Z</published>
    <title>Do Language Models Understand Time?</title>
    <summary>  Large language models (LLMs) have revolutionized video-based computer vision
applications, including action recognition, anomaly detection, and video
summarization. Videos inherently pose unique challenges, combining spatial
complexity with temporal dynamics that are absent in static images or textual
data. Current approaches to video understanding with LLMs often rely on
pretrained video encoders to extract spatiotemporal features and text encoders
to capture semantic meaning. These representations are integrated within LLM
frameworks, enabling multimodal reasoning across diverse video tasks. However,
the critical question persists: Can LLMs truly understand the concept of time,
and how effectively can they reason about temporal relationships in videos?
This work critically examines the role of LLMs in video processing, with a
specific focus on their temporal reasoning capabilities. We identify key
limitations in the interaction between LLMs and pretrained encoders, revealing
gaps in their ability to model long-term dependencies and abstract temporal
concepts such as causality and event progression. Furthermore, we analyze
challenges posed by existing video datasets, including biases, lack of temporal
annotations, and domain-specific limitations that constrain the temporal
understanding of LLMs. To address these gaps, we explore promising future
directions, including the co-evolution of LLMs and encoders, the development of
enriched datasets with explicit temporal labels, and innovative architectures
for integrating spatial, temporal, and semantic reasoning. By addressing these
challenges, we aim to advance the temporal comprehension of LLMs, unlocking
their full potential in video analysis and beyond. Our paper's GitHub
repository can be found at https: //github.com/Darcyddx/Video-LLM.
</summary>
    <author>
      <name>Xi Ding</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3701716.3717744</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3701716.3717744" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in the Companion Proceedings of the ACM Web
  Conference (WWW Companion 2025)</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.13845v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.13845v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1608.01529v1</id>
    <updated>2016-08-04T13: 38: 38Z</updated>
    <published>2016-08-04T13: 38: 38Z</published>
    <title>Deep Learning for Detecting Multiple Space-Time Action Tubes in Videos</title>
    <summary>  In this work, we propose an approach to the spatiotemporal localisation
(detection) and classification of multiple concurrent actions within temporally
untrimmed videos. Our framework is composed of three stages. In stage 1,
appearance and motion detection networks are employed to localise and score
actions from colour images and optical flow. In stage 2, the appearance network
detections are boosted by combining them with the motion detection scores, in
proportion to their respective spatial overlap. In stage 3, sequences of
detection boxes most likely to be associated with a single action instance,
called action tubes, are constructed by solving two energy maximisation
problems via dynamic programming. While in the first pass, action paths
spanning the whole video are built by linking detection boxes over time using
their class-specific scores and their spatial overlap, in the second pass,
temporal trimming is performed by ensuring label consistency for all
constituting detection boxes. We demonstrate the performance of our algorithm
on the challenging UCF101, J-HMDB-21 and LIRIS-HARL datasets, achieving new
state-of-the-art results across the board and significantly increasing
detection speed at test time. We achieve a huge leap forward in action
detection performance and report a 20% and 11% gain in mAP (mean average
precision) on UCF-101 and J-HMDB-21 datasets respectively when compared to the
state-of-the-art.
</summary>
    <author>
      <name>Suman Saha</name>
    </author>
    <author>
      <name>Gurkirt Singh</name>
    </author>
    <author>
      <name>Michael Sapienza</name>
    </author>
    <author>
      <name>Philip H. S. Torr</name>
    </author>
    <author>
      <name>Fabio Cuzzolin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by British Machine Vision Conference 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2411.12525v1</id>
    <updated>2024-11-19T14: 18: 02Z</updated>
    <published>2024-11-19T14: 18: 02Z</published>
    <title>Rethinking Top Probability from Multi-view for Distracted Driver
  Behaviour Localization</title>
    <summary>  Naturalistic driving action localization task aims to recognize and
comprehend human behaviors and actions from video data captured during
real-world driving scenarios. Previous studies have shown great action
localization performance by applying a recognition model followed by
probability-based post-processing. Nevertheless, the probabilities provided by
the recognition model frequently contain confused information causing challenge
for post-processing. In this work, we adopt an action recognition model based
on self-supervise learning to detect distracted activities and give potential
action probabilities. Subsequently, a constraint ensemble strategy takes
advantages of multi-camera views to provide robust predictions. Finally, we
introduce a conditional post-processing operation to locate distracted
behaviours and action temporal boundaries precisely. Experimenting on test set
A2, our method obtains the sixth position on the public leaderboard of track 3
of the 2024 AI City Challenge.
</summary>
    <author>
      <name>Quang Vinh Nguyen</name>
    </author>
    <author>
      <name>Vo Hoang Thanh Son</name>
    </author>
    <author>
      <name>Chau Truong Vinh Hoang</name>
    </author>
    <author>
      <name>Duc Duy Nguyen</name>
    </author>
    <author>
      <name>Nhat Huy Nguyen Minh</name>
    </author>
    <author>
      <name>Soo-Hyung Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Vision and Pattern Recognition Workshop 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.12525v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.12525v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1912.11014v2</id>
    <updated>2020-11-03T15: 33: 23Z</updated>
    <published>2019-12-23T18: 15: 36Z</published>
    <title>Complete spatiotemporal and polarization characterization of ultrafast
  vector beams</title>
    <summary>  The use of structured ultrashort pulses with coupled spatiotemporal
properties is emerging as a key tool for ultrafast manipulation. Ultrafast
vector beams are opening exciting opportunities in different fields such as
microscopy, time-resolved imaging, nonlinear optics, particle acceleration or
attosecond science. Here, we implement a technique for the full
characterization of structured time-dependent polarization light waveforms with
spatiotemporal resolution, using a compact twofold spectral interferometer,
based on in-line bulk interferometry and fibre-optic coupler assisted
interferometry. We measure structured infrared femtosecond vector beams,
including radially polarized beams and complex-shaped beams exhibiting both
temporal and spatial evolving polarization. Our measurements confirm that light
waveforms with polarization evolving at the micrometer and femtosecond scales
can be achieved through the use of structured waveplates and polarization
gates. This new scale of measurement achieved will open the way to predict,
check and optimize applications of structured vector beams at the femtosecond
and micrometer scales.
</summary>
    <author>
      <name>Benjamín Alonso</name>
    </author>
    <author>
      <name>Ignacio Lopez-Quintas</name>
    </author>
    <author>
      <name>Warein Holgado</name>
    </author>
    <author>
      <name>Rokas Drevinskas</name>
    </author>
    <author>
      <name>Peter G. Kazansky</name>
    </author>
    <author>
      <name>Carlos Hernández-García</name>
    </author>
    <author>
      <name>Íñigo J. Sola</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s42005-020-00419-w</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s42005-020-00419-w" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Information: 37 pages,
    15 figures, final version. This work was
  partially supported by the European Union's Horizon 2020 research and
  innovation programme under the Marie Sklodowska-Curie Action with grant
  agreement No. 798264, project FastMeasure entitled 'Development and industry
  transfer of new techniques: full characterization of vector ultrashort pulsed
  laser beams'</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Commun. Phys. 3,
    151 (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1912.11014v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11014v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1611.08563v6</id>
    <updated>2017-08-24T09: 15: 13Z</updated>
    <published>2016-11-25T19: 21: 36Z</published>
    <title>Online Real-time Multiple Spatiotemporal Action Localisation and
  Prediction</title>
    <summary>  We present a deep-learning framework for real-time multiple spatio-temporal
(S/T) action localisation, classification and early prediction. Current
state-of-the-art approaches work offline and are too slow to be useful in real-
world settings. To overcome their limitations we introduce two major
developments. Firstly, we adopt real-time SSD (Single Shot MultiBox Detector)
convolutional neural networks to regress and classify detection boxes in each
video frame potentially containing an action of interest. Secondly, we design
an original and efficient online algorithm to incrementally construct and label
`action tubes' from the SSD frame level detections. As a result, our system is
not only capable of performing S/T detection in real time, but can also perform
early action prediction in an online fashion. We achieve new state-of-the-art
results in both S/T action localisation and early action prediction on the
challenging UCF101-24 and J-HMDB-21 benchmarks, even when compared to the top
offline competitors. To the best of our knowledge, ours is the first real-time
(up to 40fps) system able to perform online S/T action localisation and early
action prediction on the untrimmed videos of UCF101-24.
</summary>
    <author>
      <name>Gurkirt Singh</name>
    </author>
    <author>
      <name>Suman Saha</name>
    </author>
    <author>
      <name>Michael Sapienza</name>
    </author>
    <author>
      <name>Philip Torr</name>
    </author>
    <author>
      <name>Fabio Cuzzolin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages 3 figures, ICCV 2017, Added link to new annotations of
  ucf101-24</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.08563v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.08563v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1411.6179v1</id>
    <updated>2014-11-22T23: 42: 04Z</updated>
    <published>2014-11-22T23: 42: 04Z</published>
    <title>Spatiotemporal Detection of Unusual Human Population Behavior Using
  Mobile Phone Data</title>
    <summary>  With the aim to contribute to humanitarian response to disasters and violent
events, scientists have proposed the development of analytical tools that could
identify emergency events in real-time, using mobile phone data. The assumption
is that dramatic and discrete changes in behavior, measured with mobile phone
data, will indicate extreme events. In this study, we propose an efficient
system for spatiotemporal detection of behavioral anomalies from mobile phone
data and compare sites with behavioral anomalies to an extensive database of
emergency and non-emergency events in Rwanda. Our methodology successfully
captures anomalous behavioral patterns associated with a broad range of events,
from religious and official holidays to earthquakes, floods, violence against
civilians and protests. Our results suggest that human behavioral responses to
extreme events are complex and multi-dimensional, including extreme increases
and decreases in both calling and movement behaviors. We also find significant
temporal and spatial variance in responses to extreme events. Our behavioral
anomaly detection system and extensive discussion of results are a significant
contribution to the long-term project of creating an effective real-time event
detection system with mobile phone data and we discuss the implications of our
findings for future research to this end.
  KEYWORDS: Big data, call detail record, emergency events, human mobility
</summary>
    <author>
      <name>Adrian Dobra</name>
    </author>
    <author>
      <name>Nathalie E. Williams</name>
    </author>
    <author>
      <name>Nathan Eagle</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0120449</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0120449" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages,
    32 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.6179v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.6179v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2104.14280v1</id>
    <updated>2021-04-29T12: 01: 08Z</updated>
    <published>2021-04-29T12: 01: 08Z</published>
    <title>Relevance Detection in Cataract Surgery Videos by Spatio-Temporal Action
  Localization</title>
    <summary>  In cataract surgery, the operation is performed with the help of a
microscope. Since the microscope enables watching real-time surgery by up to
two people only, a major part of surgical training is conducted using the
recorded videos. To optimize the training procedure with the video content, the
surgeons require an automatic relevance detection approach. In addition to
relevance-based retrieval, these results can be further used for skill
assessment and irregularity detection in cataract surgery videos. In this
paper, a three-module framework is proposed to detect and classify the relevant
phase segments in cataract videos. Taking advantage of an idle frame
recognition network, the video is divided into idle and action segments. To
boost the performance in relevance detection, the cornea where the relevant
surgical actions are conducted is detected in all frames using Mask R-CNN. The
spatiotemporally localized segments containing higher-resolution information
about the pupil texture and actions, and complementary temporal information
from the same phase are fed into the relevance detection module. This module
consists of four parallel recurrent CNNs being responsible to detect four
relevant phases that have been defined with medical experts. The results will
then be integrated to classify the action phases as irrelevant or one of four
relevant phases. Experimental results reveal that the proposed approach
outperforms static CNNs and different configurations of feature-based and
end-to-end recurrent networks.
</summary>
    <author>
      <name>Negin Ghamsarian</name>
    </author>
    <author>
      <name>Mario Taschwer</name>
    </author>
    <author>
      <name>Doris Putzgruber-Adamitsch</name>
    </author>
    <author>
      <name>Stephanie Sarny</name>
    </author>
    <author>
      <name>Klaus Schoeffmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICPR48806.2021.9412525</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICPR48806.2021.9412525" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,
    4 figures, accepted at 5th International Conference on
  Pattern Recognition (ICPR), Milan, Italy,
    2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.14280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.14280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2502.14197v1</id>
    <updated>2025-02-20T02: 01: 40Z</updated>
    <published>2025-02-20T02: 01: 40Z</published>
    <title>Adaptive Sparsified Graph Learning Framework for Vessel Behavior
  Anomalies</title>
    <summary>  Graph neural networks have emerged as a powerful tool for learning
spatiotemporal interactions. However, conventional approaches often rely on
predefined graphs, which may obscure the precise relationships being modeled.
Additionally, existing methods typically define nodes based on fixed spatial
locations, a strategy that is ill-suited for dynamic environments like maritime
environments. Our method introduces an innovative graph representation where
timestamps are modeled as distinct nodes, allowing temporal dependencies to be
explicitly captured through graph edges. This setup is extended to construct a
multi-ship graph that effectively captures spatial interactions while
preserving graph sparsity. The graph is processed using Graph Convolutional
Network layers to capture spatiotemporal patterns, with a forecasting layer for
feature prediction and a Variational Graph Autoencoder for reconstruction,
enabling robust anomaly detection.
</summary>
    <author>
      <name>Jeehong Kim</name>
    </author>
    <author>
      <name>Minchan Kim</name>
    </author>
    <author>
      <name>Jaeseong Ju</name>
    </author>
    <author>
      <name>Youngseok Hwang</name>
    </author>
    <author>
      <name>Wonhee Lee</name>
    </author>
    <author>
      <name>Hyunwoo Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Anomaly Detection in Scientific Domains AAAI Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.14197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.14197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1811.07157v2</id>
    <updated>2019-08-31T09: 28: 25Z</updated>
    <published>2018-11-17T13: 07: 30Z</published>
    <title>Recurrent Convolutions for Causal 3D CNNs</title>
    <summary>  Recently, three dimensional (3D) convolutional neural networks (CNNs) have
emerged as dominant methods to capture spatiotemporal representations in
videos, by adding to pre-existing 2D CNNs a third, temporal dimension. Such 3D
CNNs, however, are anti-causal (i.e., they exploit information from both the
past and the future frames to produce feature representations, thus preventing
their use in online settings), constrain the temporal reasoning horizon to the
size of the temporal convolution kernel, and are not temporal
resolution-preserving for video sequence-to-sequence modelling, as, for
instance, in action detection. To address these serious limitations, here we
present a new 3D CNN architecture for the causal/online processing of videos.
  Namely, we propose a novel Recurrent Convolutional Network (RCN), which
relies on recurrence to capture the temporal context across frames at each
network level. Our network decomposes 3D convolutions into (1) a 2D spatial
convolution component, and (2) an additional hidden state $1\times 1$
convolution, applied across time. The hidden state at any time $t$ is assumed
to depend on the hidden state at $t-1$ and on the current output of the spatial
convolution component. As a result, the proposed network: (i) produces causal
outputs, (ii) provides flexible temporal reasoning, (iii) preserves temporal
resolution. Our experiments on the large-scale large Kinetics and MultiThumos
datasets show that the proposed method performs comparably to anti-causal 3D
CNNs, while being causal and using fewer parameters.
</summary>
    <author>
      <name>Gurkirt Singh</name>
    </author>
    <author>
      <name>Fabio Cuzzolin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop on Large Scale Holistic Video Understanding, ICCVW,
    2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.07157v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.07157v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2309.01270v2</id>
    <updated>2023-10-26T09: 58: 37Z</updated>
    <published>2023-09-03T20: 50: 53Z</published>
    <title>COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action
  Spotting using Transformers</title>
    <summary>  We present COMEDIAN, a novel pipeline to initialize spatiotemporal
transformers for action spotting, which involves self-supervised learning and
knowledge distillation. Action spotting is a timestamp-level temporal action
detection task. Our pipeline consists of three steps, with two initialization
stages. First, we perform self-supervised initialization of a spatial
transformer using short videos as input. Additionally, we initialize a temporal
transformer that enhances the spatial transformer's outputs with global context
through knowledge distillation from a pre-computed feature bank aligned with
each short video segment. In the final step, we fine-tune the transformers to
the action spotting task. The experiments, conducted on the SoccerNet-v2
dataset, demonstrate state-of-the-art performance and validate the
effectiveness of COMEDIAN's pretraining paradigm. Our results highlight several
advantages of our pretraining pipeline, including improved performance and
faster convergence compared to non-pretrained models.
</summary>
    <author>
      <name>Julien Denize</name>
    </author>
    <author>
      <name>Mykola Liashuha</name>
    </author>
    <author>
      <name>Jaonary Rabarisoa</name>
    </author>
    <author>
      <name>Astrid Orcesi</name>
    </author>
    <author>
      <name>Romain Hérault</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Source code is available here:
  https: //github.com/juliendenize/eztorch</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.01270v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.01270v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2103.14123v1</id>
    <updated>2021-03-25T20: 29: 58Z</updated>
    <published>2021-03-25T20: 29: 58Z</published>
    <title>Preliminary Experimental Results of Context-Aware Teams of Multiple
  Autonomous Agents Operating under Constrained Communications</title>
    <summary>  This work presents and experimentally test the framework used by our
context-aware, distributed team of small Unmanned Aerial Systems (SUAS) capable
of operating in real-time, in an autonomous fashion, and under constrained
communications. Our framework relies on three layered approach: (1) Operational
layer, where fast temporal and narrow spatial decisions are made; (2) Tactical
Layer, where temporal and spatial decisions are made for a team of agents; and
(3) Strategical Layer, where slow temporal and wide spatial decisions are made
for the team of agents. These three layers are coordinated by an ad-hoc,
software-defined communications network, which ensures sparse, but timely
delivery of messages amongst groups and teams of agents at each layer even
under constrained communications. Experimental results are presented for a team
of 10 small unmanned aerial systems tasked with searching and monitoring a
person in an open area. At the operational layer, our use case presents an
agent autonomously performing searching, detection, localization,
classification, identification, tracking, and following of the person, while
avoiding malicious collisions. At the tactical layer, our experimental use case
presents the cooperative interaction of a group of multiple agents that enable
the monitoring of the targeted person over a wider spatial and temporal
regions. At the strategic layer, our use case involves the detection of complex
behaviors-i.e. the person being followed enters a car and runs away, or the
person being followed exits the car and runs away-that requires strategic
responses to successfully accomplish the mission.
</summary>
    <author>
      <name>Jose Martinez-Lorenzo</name>
    </author>
    <author>
      <name>Jeff Hudack</name>
    </author>
    <author>
      <name>Yutao Jing</name>
    </author>
    <author>
      <name>Michael Shaham</name>
    </author>
    <author>
      <name>Zixuan Liang</name>
    </author>
    <author>
      <name>Abdullah Al Bashit</name>
    </author>
    <author>
      <name>Yushu Wu</name>
    </author>
    <author>
      <name>Weite Zhang</name>
    </author>
    <author>
      <name>Matthew Skopin</name>
    </author>
    <author>
      <name>Juan Heredia-Juesas</name>
    </author>
    <author>
      <name>Yuntao Ma</name>
    </author>
    <author>
      <name>Tristan Sweeney</name>
    </author>
    <author>
      <name>Nicolas Ares</name>
    </author>
    <author>
      <name>Ari Fox</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages,
    6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.14123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.14123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2309.15231v1</id>
    <updated>2023-09-26T19: 48: 03Z</updated>
    <published>2023-09-26T19: 48: 03Z</published>
    <title>Spatiotemporal patterns of Io's bright transient eruptions,
    1978-2022</title>
    <summary>  This study analyzes Io's thermally detected volcanic outbursts and
mini-outbursts, generally called bright transient eruptions. We examine their
evolving characteristics over the history of outburst observations between the
Voyager flybys in 1978 and 2022. We catalog, compare, and interpret the data of
these bright transient eruptions from several spacecraft flybys and numerous
ground-based observation campaigns. To test the spatiotemporal behavior of
these events, we compare them to a population of randomly spaced, stochastic
events with an equal likelihood of occurrence anywhere on Io's surface. We find
that the aggregate of all outbursts is consistent with a random distribution
across Io, whereas mini-outbursts strongly prefer the trailing hemisphere (180
to 360 W). On shorter timescales, however, outbursts show a significant change
in spatiotemporal behavior before and after the year 2012. Outbursts from 1995
to 2007 favor the northern leading hemisphere, while outbursts from 2013 to
2021 favor the southern trailing hemisphere. These temporally separated
clusters of outbursts are remarkably similar to Io's two primary mountainous
regions, indicating that outbursts may be related to mountain-forming activity.
These trends show how bright transient eruptions are distinct from Io's other
forms of volcanism. These could be essential constraints to assess models of
Io's interior heat transport between tidal generation and volcanic
distribution.
</summary>
    <author>
      <name>Christian D. Tate</name>
    </author>
    <author>
      <name>Julie A. Rathbun</name>
    </author>
    <author>
      <name>Alexander G. Hayes</name>
    </author>
    <author>
      <name>Rosaly M. C. Lopes</name>
    </author>
    <author>
      <name>Madeline Pettine</name>
    </author>
    <link href="http://arxiv.org/abs/2309.15231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.15231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2211.10052v2</id>
    <updated>2022-11-28T13: 02: 01Z</updated>
    <published>2022-11-18T06: 41: 02Z</published>
    <title>Pedestrian Spatio-Temporal Information Fusion For Video Anomaly
  Detection</title>
    <summary>  Aiming at the problem that the current video anomaly detection cannot fully
use the temporal information and ignore the diversity of normal behavior, an
anomaly detection method is proposed to integrate the spatiotemporal
information of pedestrians. Based on the convolutional autoencoder, the input
frame is compressed and restored through the encoder and decoder. Anomaly
detection is realized according to the difference between the output frame and
the true value. In order to strengthen the characteristic information
connection between continuous video frames, the residual temporal shift module
and the residual channel attention module are introduced to improve the
modeling ability of the network on temporal information and channel
information, respectively. Due to the excessive generalization of convolutional
neural networks, in the memory enhancement modules, the hopping connections of
each codec layer are added to limit autoencoders' ability to represent abnormal
frames too vigorously and improve the anomaly detection accuracy of the
network. In addition, the objective function is modified by a feature
discretization loss, which effectively distinguishes different normal behavior
patterns. The experimental results on the CUHK Avenue and ShanghaiTech datasets
show that the proposed method is superior to the current mainstream video
anomaly detection methods while meeting the real-time requirements.
</summary>
    <author>
      <name>Chao Hu</name>
    </author>
    <author>
      <name>Liqiang Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Intelligent Media, Big Data and Knowledge
  Mining</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IMBDKM 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2211.10052v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.10052v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2305.15649v2</id>
    <updated>2023-05-26T09: 54: 16Z</updated>
    <published>2023-05-25T01: 55: 47Z</published>
    <title>The spatiotemporal doubled density operator: a unified framework for
  analyzing spatial and temporal quantum processes</title>
    <summary>  The measurement statistics for spatial and temporal quantum processes are
produced through distinct mechanisms. Measurements that are space-like
separated exhibit non-signaling behavior. However, time-like separated
measurements can only result in one-way non-signaling, as the past is
independent of the future, but the opposite is not true. This work presents the
doubled density operator as a comprehensive framework for studying quantum
processes in space-time. It effectively captures all the physical information
of the process, with the measurement and Born rule showing uniformity for both
spatial and temporal cases. We demonstrate that the equal-time density operator
can be derived by performing a partial trace operation on the doubled density
operator. Furthermore, the temporality of the quantum process can be detected
by conducting a partial trace operation on either the left or right half of the
doubled density operator.
</summary>
    <author>
      <name>Zhian Jia</name>
    </author>
    <author>
      <name>Dagomir Kaszlikowski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/qute.202400102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/qute.202400102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v1: 7+7 pages, comments welcome</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Adv Quantum Technol. 2024,
    2400102</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2305.15649v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.15649v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2410.04640v2</id>
    <updated>2024-10-10T17: 09: 24Z</updated>
    <published>2024-10-06T22: 13: 30Z</published>
    <title>Unpacking Failure Modes of Generative Policies: Runtime Monitoring of
  Consistency and Progress</title>
    <summary>  Robot behavior policies trained via imitation learning are prone to failure
under conditions that deviate from their training data. Thus, algorithms that
monitor learned policies at test time and provide early warnings of failure are
necessary to facilitate scalable deployment. We propose Sentinel, a runtime
monitoring framework that splits the detection of failures into two
complementary categories: 1) Erratic failures, which we detect using
statistical measures of temporal action consistency, and 2) task progression
failures, where we use Vision Language Models (VLMs) to detect when the policy
confidently and consistently takes actions that do not solve the task. Our
approach has two key strengths. First, because learned policies exhibit diverse
failure modes, combining complementary detectors leads to significantly higher
accuracy at failure detection. Second, using a statistical temporal action
consistency measure ensures that we quickly detect when multimodal, generative
policies exhibit erratic behavior at negligible computational cost. In
contrast, we only use VLMs to detect failure modes that are less
time-sensitive. We demonstrate our approach in the context of diffusion
policies trained on robotic mobile manipulation domains in both simulation and
the real world. By unifying temporal consistency detection and VLM runtime
monitoring, Sentinel detects 18% more failures than using either of the two
detectors alone and significantly outperforms baselines, thus highlighting the
importance of assigning specialized detectors to complementary categories of
failure. Qualitative results are made available at
https: //sites.google.com/stanford.edu/sentinel.
</summary>
    <author>
      <name>Christopher Agia</name>
    </author>
    <author>
      <name>Rohan Sinha</name>
    </author>
    <author>
      <name>Jingyun Yang</name>
    </author>
    <author>
      <name>Zi-ang Cao</name>
    </author>
    <author>
      <name>Rika Antonova</name>
    </author>
    <author>
      <name>Marco Pavone</name>
    </author>
    <author>
      <name>Jeannette Bohg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https: //sites.google.com/stanford.edu/sentinel. 35
  pages,
    9 figures. Accepted to the Conference on Robot Learning (CoRL) 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.04640v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.04640v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7; I.2.9; I.2.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2312.07621v1</id>
    <updated>2023-12-11T22: 30: 13Z</updated>
    <published>2023-12-11T22: 30: 13Z</published>
    <title>Spatiotemporal Event Graphs for Dynamic Scene Understanding</title>
    <summary>  Dynamic scene understanding is the ability of a computer system to interpret
and make sense of the visual information present in a video of a real-world
scene. In this thesis, we present a series of frameworks for dynamic scene
understanding starting from road event detection from an autonomous driving
perspective to complex video activity detection, followed by continual learning
approaches for the life-long learning of the models. Firstly, we introduce the
ROad event Awareness Dataset (ROAD) for Autonomous Driving, to our knowledge
the first of its kind. Due to the lack of datasets equipped with formally
specified logical requirements, we also introduce the ROad event Awareness
Dataset with logical Requirements (ROAD-R), the first publicly available
dataset for autonomous driving with requirements expressed as logical
constraints, as a tool for driving neurosymbolic research in the area. Next, we
extend event detection to holistic scene understanding by proposing two complex
activity detection methods. In the first method, we present a deformable,
spatiotemporal scene graph approach, consisting of three main building blocks:
action tube detection, a 3D deformable RoI pooling layer designed for learning
the flexible, deformable geometry of the constituent action tubes, and a scene
graph constructed by considering all parts as nodes and connecting them based
on different semantics. In a second approach evolving from the first, we
propose a hybrid graph neural network that combines attention applied to a
graph encoding of the local (short-term) dynamic scene with a temporal graph
modelling the overall long-duration activity. Finally, the last part of the
thesis is about presenting a new continual semi-supervised learning (CSSL)
paradigm.
</summary>
    <author>
      <name>Salman Khan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD thesis, Oxford Brookes University, Examiners: Prof. Dima Damen
  and Dr. Matthias Rolf,
    183 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.07621v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.07621v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2312.14161v1</id>
    <updated>2023-11-29T03: 30: 16Z</updated>
    <published>2023-11-29T03: 30: 16Z</published>
    <title>Statistical Machine Learning Meets High-Dimensional Spatiotemporal
  Challenges -- A Case Study of COVID-19 Modeling</title>
    <summary>  Diverse non-pharmacological interventions (NPIs), serving as the primary
approach for COVID-19 control prior to pharmaceutical interventions, showed
heterogeneous spatiotemporal effects on pandemic management. Investigating the
dynamic compounding impacts of NPIs on pandemic spread is imperative. However,
the challenges posed by data availability of high-dimensional human behaviors
and the complexity of modeling changing and interrelated factors are
substantial. To address these challenges, this study analyzed social media
data, COVID-19 case rates, Apple mobility data, and the stringency of
stay-at-home policies in the United States throughout the year 2020, aiming to
(1) uncover the spatiotemporal variations in NPIs during the COVID-19 pandemic
utilizing geospatial big data; (2) develop a statistical machine learning model
that incorporates spatiotemporal dependencies and temporal lag effects for the
detection of relationships; (3) dissect the impacts of NPIs on the pandemic
across space and time. Three indices were computed based on Twitter (currently
known as X) data: the Negative and Positive Sentiments Adjusted by Demographics
(N-SAD and P-SAD) and the Ratio Adjusted by Demographics (RAD), representing
negative sentiment, positive sentiment, and public awareness of COVID-19,
respectively. The Multivariate Bayesian Structural Time Series Time Lagged
model (MBSTS-TL) was proposed to investigate the effects of NPIs, accounting
for spatial dependencies and temporal lag effects. The developed MBSTS-TL model
exhibited a high degree of accuracy. Determinants of COVID-19 health impacts
transitioned from an emphasis on human mobility during the initial outbreak
period to a combination of human mobility and stay-at-home policies during the
rapid spread phase, and ultimately to the compound of human mobility,
stay-at-home policies, and public awareness of COVID-19.
</summary>
    <author>
      <name>Binbin Lin</name>
    </author>
    <author>
      <name>Yimin Dai</name>
    </author>
    <author>
      <name>Lei Zou</name>
    </author>
    <author>
      <name>Ning Ning</name>
    </author>
    <link href="http://arxiv.org/abs/2312.14161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.14161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1807.10418v3</id>
    <updated>2018-12-15T17: 02: 27Z</updated>
    <published>2018-07-27T02: 31: 49Z</published>
    <title>W-TALC: Weakly-supervised Temporal Activity Localization and
  Classification</title>
    <summary>  Most activity localization methods in the literature suffer from the burden
of frame-wise annotation requirement. Learning from weak labels may be a
potential solution towards reducing such manual labeling effort. Recent years
have witnessed a substantial influx of tagged videos on the Internet, which can
serve as a rich source of weakly-supervised training data. Specifically, the
correlations between videos with similar tags can be utilized to temporally
localize the activities. Towards this goal, we present W-TALC, a
Weakly-supervised Temporal Activity Localization and Classification framework
using only video-level labels. The proposed network can be divided into two
sub-networks, namely the Two-Stream based feature extractor network and a
weakly-supervised module, which we learn by optimizing two complimentary loss
functions. Qualitative and quantitative results on two challenging datasets -
Thumos14 and ActivityNet1.2, demonstrate that the proposed method is able to
detect activities at a fine granularity and achieve better performance than
current state-of-the-art methods.
</summary>
    <author>
      <name>Sujoy Paul</name>
    </author>
    <author>
      <name>Sourya Roy</name>
    </author>
    <author>
      <name>Amit K Roy-Chowdhury</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at European Conference on Computer Vision (ECCV),
    2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10418v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10418v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1901.09403v1</id>
    <updated>2019-01-27T16: 53: 55Z</updated>
    <published>2019-01-27T16: 53: 55Z</published>
    <title>Spatio-temporal Action Recognition: A Survey</title>
    <summary>  The task of action recognition or action detection involves analyzing videos
and determining what action or motion is being performed. The primary subject
of these videos are predominantly humans performing some action. However, this
requirement can be relaxed to generalize over other subjects such as animals or
robots. The applications can range from anywhere between human-computer
inter-action to automated video editing proposals. When we consider
spatiotemporal action recognition, we deal with action localization. This task
not only involves determining what action is being performed but also when and
where itis being performed in said video. This paper aims to survey the
plethora of approaches and algorithms attempted to solve this task, give a
comprehensive comparison between them, explore various datasets available for
the problem, and determine the most promising approaches.
</summary>
    <author>
      <name>Amlaan Bhoi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages,
    7 figures,
    6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.09403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.09403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1907.12919v1</id>
    <updated>2019-07-21T16: 53: 43Z</updated>
    <published>2019-07-21T16: 53: 43Z</published>
    <title>Attention Filtering for Multi-person Spatiotemporal Action Detection on
  Deep Two-Stream CNN Architectures</title>
    <summary>  Action detection and recognition tasks have been the target of much focus in
the computer vision community due to their many applications, namely, security,
robotics and recommendation systems. Recently, datasets like AVA, provide
multi-person, multi-label, spatiotemporal action detection and recognition
challenges. Being unable to discern which portions of the input to use for
classification is a limitation of two-stream CNN approaches, once the vision
task involves several people with several labels. We address this limitation
and improve the state-of-the-art performance of two-stream CNNs. In this paper
we present four contributions: our fovea attention filtering that highlights
targets for classification without discarding background; a generalized binary
loss function designed for the AVA dataset; miniAVA, a partition of AVA that
maintains temporal continuity and class distribution with only one tenth of the
dataset size; and ablation studies on alternative attention filters. Our
method, using fovea attention filtering and our generalized binary loss,
achieves a relative video mAP improvement of 20% over the two-stream baseline
in AVA, and is competitive with the state-of-the-art in the UCF101-24. We also
show a relative video mAP improvement of 12.6% when using our generalized
binary loss over the standard sum-of-sigmoids.
</summary>
    <author>
      <name>João Antunes</name>
    </author>
    <author>
      <name>Pedro Abreu</name>
    </author>
    <author>
      <name>Alexandre Bernardino</name>
    </author>
    <author>
      <name>Asim Smailagic</name>
    </author>
    <author>
      <name>Daniel Siewiorek</name>
    </author>
    <link href="http://arxiv.org/abs/1907.12919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.12919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1808.07712v1</id>
    <updated>2018-08-23T12: 11: 06Z</updated>
    <published>2018-08-23T12: 11: 06Z</published>
    <title>Predicting Action Tubes</title>
    <summary>  In this work, we present a method to predict an entire `action tube' (a set
of temporally linked bounding boxes) in a trimmed video just by observing a
smaller subset of it. Predicting where an action is going to take place in the
near future is essential to many computer vision based applications such as
autonomous driving or surgical robotics. Importantly, it has to be done in
real-time and in an online fashion. We propose a Tube Prediction network
(TPnet) which jointly predicts the past, present and future bounding boxes
along with their action classification scores. At test time TPnet is used in a
(temporal) sliding window setting, and its predictions are put into a tube
estimation framework to construct/predict the video long action tubes not only
for the observed part of the video but also for the unobserved part.
Additionally, the proposed action tube predictor helps in completing action
tubes for unobserved segments of the video. We quantitatively demonstrate the
latter ability, and the fact that TPnet improves state-of-the-art detection
performance, on one of the standard action detection benchmarks - J-HMDB-21
dataset.
</summary>
    <author>
      <name>Gurkirt Singh</name>
    </author>
    <author>
      <name>Suman Saha</name>
    </author>
    <author>
      <name>Fabio Cuzzolin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV workshop; Anticipating Human Behaviour 2018; 16 page 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07712v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07712v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1706.04269v2</id>
    <updated>2018-07-27T16: 27: 25Z</updated>
    <published>2017-06-13T22: 15: 09Z</published>
    <title>Action Search: Spotting Actions in Videos and Its Application to
  Temporal Action Localization</title>
    <summary>  State-of-the-art temporal action detectors inefficiently search the entire
video for specific actions. Despite the encouraging progress these methods
achieve, it is crucial to design automated approaches that only explore parts
of the video which are the most relevant to the actions being searched for. To
address this need, we propose the new problem of action spotting in video,
which we define as finding a specific action in a video while observing a small
portion of that video. Inspired by the observation that humans are extremely
efficient and accurate in spotting and finding action instances in video, we
propose Action Search, a novel Recurrent Neural Network approach that mimics
the way humans spot actions. Moreover, to address the absence of data recording
the behavior of human annotators, we put forward the Human Searches dataset,
which compiles the search sequences employed by human annotators spotting
actions in the AVA and THUMOS14 datasets. We consider temporal action
localization as an application of the action spotting problem. Experiments on
the THUMOS14 dataset reveal that our model is not only able to explore the
video efficiently (observing on average 17.3% of the video) but it also
accurately finds human activities with 30.8% mAP.
</summary>
    <author>
      <name>Humam Alwassel</name>
    </author>
    <author>
      <name>Fabian Caba Heilbron</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ECCV 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.04269v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04269v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1910.08250v1</id>
    <updated>2019-10-18T03: 57: 05Z</updated>
    <published>2019-10-18T03: 57: 05Z</published>
    <title>AFO-TAD: Anchor-free One-Stage Detector for Temporal Action Detection</title>
    <summary>  Temporal action detection is a fundamental yet challenging task in video
understanding. Many of the state-of-the-art methods predict the boundaries of
action instances based on predetermined anchors akin to the two-dimensional
object detection detectors. However, it is hard to detect all the action
instances with predetermined temporal scales because the durations of instances
in untrimmed videos can vary from few seconds to several minutes. In this
paper, we propose a novel action detection architecture named anchor-free
one-stage temporal action detector (AFO-TAD). AFO-TAD achieves better
performance for detecting action instances with arbitrary lengths and high
temporal resolution, which can be attributed to two aspects. First, we design a
receptive field adaption module which dynamically adjusts the receptive field
for precise action detection. Second, AFO-TAD directly predicts the categories
and boundaries at every temporal locations without predetermined anchors.
Extensive experiments show that AFO-TAD improves the state-of-the-art
performance on THUMOS'14.
</summary>
    <author>
      <name>Yiping Tang</name>
    </author>
    <author>
      <name>Chuang Niu</name>
    </author>
    <author>
      <name>Minghao Dong</name>
    </author>
    <author>
      <name>Shenghan Ren</name>
    </author>
    <author>
      <name>Jimin Liang</name>
    </author>
    <link href="http://arxiv.org/abs/1910.08250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.08250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2407.12622v1</id>
    <updated>2024-07-17T14: 49: 54Z</updated>
    <published>2024-07-17T14: 49: 54Z</published>
    <title>Rethinking the Architecture Design for Efficient Generic Event Boundary
  Detection</title>
    <summary>  Generic event boundary detection (GEBD), inspired by human visual cognitive
behaviors of consistently segmenting videos into meaningful temporal chunks,
finds utility in various applications such as video editing and. In this paper,
we demonstrate that SOTA GEBD models often prioritize final performance over
model complexity, resulting in low inference speed and hindering efficient
deployment in real-world scenarios. We contribute to addressing this challenge
by experimentally reexamining the architecture of GEBD models and uncovering
several surprising findings. Firstly, we reveal that a concise GEBD baseline
model already achieves promising performance without any sophisticated design.
Secondly, we find that the widely applied image-domain backbones in GEBD models
can contain plenty of architecture redundancy, motivating us to gradually
``modernize'' each component to enhance efficiency. Thirdly, we show that the
GEBD models using image-domain backbones conducting the spatiotemporal learning
in a spatial-then-temporal greedy manner can suffer from a distraction issue,
which might be the inefficient villain for GEBD. Using a video-domain backbone
to jointly conduct spatiotemporal modeling is an effective solution for this
issue. The outcome of our exploration is a family of GEBD models, named
EfficientGEBD, significantly outperforms the previous SOTA methods by up to
1.7\% performance gain and 280\% speedup under the same backbone. Our research
prompts the community to design modern GEBD methods with the consideration of
model complexity, particularly in resource-aware applications. The code is
available at \url{https: //github.com/Ziwei-Zheng/EfficientGEBD}.
</summary>
    <author>
      <name>Ziwei Zheng</name>
    </author>
    <author>
      <name>Zechuan Zhang</name>
    </author>
    <author>
      <name>Yulin Wang</name>
    </author>
    <author>
      <name>Shiji Song</name>
    </author>
    <author>
      <name>Gao Huang</name>
    </author>
    <author>
      <name>Le Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM MM 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.12622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.12622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1910.09920v1</id>
    <updated>2019-10-22T12: 31: 07Z</updated>
    <published>2019-10-22T12: 31: 07Z</published>
    <title>Weakly-Supervised Completion Moment Detection using Temporal Attention</title>
    <summary>  Monitoring the progression of an action towards completion offers fine
grained insight into the actor's behaviour. In this work, we target detecting
the completion moment of actions, that is the moment when the action's goal has
been successfully accomplished. This has potential applications from
surveillance to assistive living and human-robot interactions. Previous effort
required human annotations of the completion moment for training (i.e. full
supervision). In this work, we present an approach for moment detection from
weak video-level labels. Given both complete and incomplete sequences, of the
same action, we learn temporal attention, along with accumulated completion
prediction from all frames in the sequence. We also demonstrate how the
approach can be used when completion moment supervision is available. We
evaluate and compare our approach on actions from three datasets, namely HMDB,
UCF101 and RGBD-AC, and show that temporal attention improves detection in both
weakly-supervised and fully-supervised settings.
</summary>
    <author>
      <name>Farnoosh Heidarivincheh</name>
    </author>
    <author>
      <name>Majid Mirmehdi</name>
    </author>
    <author>
      <name>Dima Damen</name>
    </author>
    <link href="http://arxiv.org/abs/1910.09920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.09920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1804.08274v1</id>
    <updated>2018-04-23T08: 18: 35Z</updated>
    <published>2018-04-23T08: 18: 35Z</published>
    <title>Jointly Localizing and Describing Events for Dense Video Captioning</title>
    <summary>  Automatically describing a video with natural language is regarded as a
fundamental challenge in computer vision. The problem nevertheless is not
trivial especially when a video contains multiple events to be worthy of
mention, which often happens in real videos. A valid question is how to
temporally localize and then describe events, which is known as "dense video
captioning." In this paper, we present a novel framework for dense video
captioning that unifies the localization of temporal event proposals and
sentence generation of each proposal, by jointly training them in an end-to-end
manner. To combine these two worlds, we integrate a new design, namely
descriptiveness regression, into a single shot detection structure to infer the
descriptive complexity of each detected proposal via sentence generation. This
in turn adjusts the temporal locations of each event proposal. Our model
differs from existing dense video captioning methods since we propose a joint
and global optimization of detection and captioning, and the framework uniquely
capitalizes on an attribute-augmented video captioning architecture. Extensive
experiments are conducted on ActivityNet Captions dataset and our framework
shows clear improvements when compared to the state-of-the-art techniques. More
remarkably, we obtain a new record: METEOR of 12.96% on ActivityNet Captions
official test set.
</summary>
    <author>
      <name>Yehao Li</name>
    </author>
    <author>
      <name>Ting Yao</name>
    </author>
    <author>
      <name>Yingwei Pan</name>
    </author>
    <author>
      <name>Hongyang Chao</name>
    </author>
    <author>
      <name>Tao Mei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2018 Spotlight, Rank 1 in ActivityNet Captions Challenge 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.08274v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08274v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2405.13206v2</id>
    <updated>2025-02-05T19: 06: 38Z</updated>
    <published>2024-05-21T21: 16: 55Z</published>
    <title>Identity-free Artificial Emotional Intelligence via Micro-Gesture
  Understanding</title>
    <summary>  In this work, we focus on a special group of human body language -- the
micro-gesture (MG), which differs from the range of ordinary illustrative
gestures in that they are not intentional behaviors performed to convey
information to others, but rather unintentional behaviors driven by inner
feelings. This characteristic introduces two novel challenges regarding
micro-gestures that are worth rethinking. The first is whether strategies
designed for other action recognition are entirely applicable to
micro-gestures. The second is whether micro-gestures, as supplementary data,
can provide additional insights for emotional understanding. In recognizing
micro-gestures, we explored various augmentation strategies that take into
account the subtle spatial and brief temporal characteristics of
micro-gestures, often accompanied by repetitiveness, to determine more suitable
augmentation methods. Considering the significance of temporal domain
information for micro-gestures, we introduce a simple and efficient
plug-and-play spatiotemporal balancing fusion method. We not only studied our
method on the considered micro-gesture dataset but also conducted experiments
on mainstream action datasets. The results show that our approach performs well
in micro-gesture recognition and on other datasets, achieving state-of-the-art
performance compared to previous micro-gesture recognition methods. For
emotional understanding based on micro-gestures, we construct complex emotional
reasoning scenarios. Our evaluation, conducted with large language models,
shows that micro-gestures play a significant and positive role in enhancing
comprehensive emotional understanding. The scenarios we developed can be
extended to other micro-gesture-based tasks such as deception detection and
interviews. We confirm that our new insights contribute to advancing research
in micro-gesture and emotional artificial intelligence.
</summary>
    <author>
      <name>Rong Gao</name>
    </author>
    <author>
      <name>Xin Liu</name>
    </author>
    <author>
      <name>Bohao Xing</name>
    </author>
    <author>
      <name>Zitong Yu</name>
    </author>
    <author>
      <name>Bjorn W. Schuller</name>
    </author>
    <author>
      <name>Heikki Kälviäinen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">We provide a link to the public release of the code and data in this
  new version</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.13206v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.13206v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1404.7048v2</id>
    <updated>2015-02-06T00: 15: 42Z</updated>
    <published>2014-04-25T13: 28: 37Z</published>
    <title>Multiscale Event Detection in Social Media</title>
    <summary>  Event detection has been one of the most important research topics in social
media analysis. Most of the traditional approaches detect events based on fixed
temporal and spatial resolutions, while in reality events of different scales
usually occur simultaneously, namely, they span different intervals in time and
space. In this paper, we propose a novel approach towards multiscale event
detection using social media data, which takes into account different temporal
and spatial scales of events in the data. Specifically, we explore the
properties of the wavelet transform, which is a well-developed multiscale
transform in signal processing, to enable automatic handling of the interaction
between temporal and spatial scales. We then propose a novel algorithm to
compute a data similarity graph at appropriate scales and detect events of
different scales simultaneously by a single graph-based clustering process.
Furthermore, we present spatiotemporal statistical analysis of the noisy
information present in the data stream, which allows us to define a novel
term-filtering procedure for the proposed event detection algorithm and helps
us study its behavior using simulated noisy data. Experimental results on both
synthetically generated data and real world data collected from Twitter
demonstrate the meaningfulness and effectiveness of the proposed approach. Our
framework further extends to numerous application domains that involve
multiscale and multiresolution data analysis.
</summary>
    <author>
      <name>Xiaowen Dong</name>
    </author>
    <author>
      <name>Dimitrios Mavroeidis</name>
    </author>
    <author>
      <name>Francesco Calabrese</name>
    </author>
    <author>
      <name>Pascal Frossard</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10618-015-0421-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10618-015-0421-2" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Data Mining and Knowledge Discovery, vol. 29, no. 5, pp.
  1374-1405, September 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.7048v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.7048v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1902.01601v1</id>
    <updated>2019-02-05T09: 24: 50Z</updated>
    <published>2019-02-05T09: 24: 50Z</published>
    <title>Detecting Permanent and Intermittent Purchase Hotspots via Computational
  Stigmergy</title>
    <summary>  The analysis of credit card transactions allows gaining new insights into the
spending occurrences and mobility behavior of large numbers of individuals at
an unprecedented scale. However, unfolding such spatiotemporal patterns at a
community level implies a non-trivial system modeling and parametrization, as
well as, a proper representation of the temporal dynamic. In this work we
address both those issues by means of a novel computational technique, i.e.
computational stigmergy. By using computational stigmergy each sample position
is associated with a digital pheromone deposit, which aggregates with other
deposits according to their spatiotemporal proximity. By processing
transactions data with computational stigmergy, it is possible to identify
high-density areas (hotspots) occurring in different time and days, as well as,
analyze their consistency over time. Indeed, a hotspot can be permanent, i.e.
present throughout the period of observation, or intermittent, i.e. present
only in certain time and days due to community level occurrences (e.g.
nightlife). Such difference is not only spatial (where the hotspot occurs) and
temporal (when the hotspot occurs) but affects also which people visit the
hotspot. The proposed approach is tested on a real-world dataset containing the
credit card transaction of 60k users between 2014 and 2015.
</summary>
    <author>
      <name>Antonio L. Alfeo</name>
    </author>
    <author>
      <name>Mario G. C. A. Cimino</name>
    </author>
    <author>
      <name>Bruno Lepri</name>
    </author>
    <author>
      <name>Alex "Sandy" Pentland</name>
    </author>
    <author>
      <name>Gigliola Vaglini</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">in Proc. ICPRAM The 8th International Conference on Pattern
  Recognition Applications and Methods (ICPRAM 2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1902.01601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.01601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1905.12708v1</id>
    <updated>2019-05-29T20: 28: 49Z</updated>
    <published>2019-05-29T20: 28: 49Z</published>
    <title>Dynamic Traffic Scene Classification with Space-Time Coherence</title>
    <summary>  This paper examines the problem of dynamic traffic scene classification under
space-time variations in viewpoint that arise from video captured on-board a
moving vehicle. Solutions to this problem are important for realization of
effective driving assistance technologies required to interpret or predict road
user behavior. Currently, dynamic traffic scene classification has not been
adequately addressed due to a lack of benchmark datasets that consider
spatiotemporal evolution of traffic scenes resulting from a vehicle's
ego-motion. This paper has three main contributions. First, an annotated
dataset is released to enable dynamic scene classification that includes 80
hours of diverse high quality driving video data clips collected in the San
Francisco Bay area. The dataset includes temporal annotations for road places,
road types, weather, and road surface conditions. Second, we introduce novel
and baseline algorithms that utilize semantic context and temporal nature of
the dataset for dynamic classification of road scenes. Finally, we showcase
algorithms and experimental results that highlight how extracted features from
scene classification serve as strong priors and help with tactical driver
behavior understanding. The results show significant improvement from
previously reported driving behavior detection baselines in the literature.
</summary>
    <author>
      <name>Athma Narayanan</name>
    </author>
    <author>
      <name>Isht Dwivedi</name>
    </author>
    <author>
      <name>Behzad Dariush</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accpeted in (International Conference on Robotics and Automation)ICRA
  2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.12708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.12708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1712.04851v2</id>
    <updated>2018-07-27T03: 20: 56Z</updated>
    <published>2017-12-13T16: 40: 55Z</published>
    <title>Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in
  Video Classification</title>
    <summary>  Despite the steady progress in video analysis led by the adoption of
convolutional neural networks (CNNs), the relative improvement has been less
drastic as that in 2D static image classification. Three main challenges exist
including spatial (image) feature representation, temporal information
representation, and model/computation complexity. It was recently shown by
Carreira and Zisserman that 3D CNNs, inflated from 2D networks and pretrained
on ImageNet, could be a promising way for spatial and temporal representation
learning. However, as for model/computation complexity,
        3D CNNs are much more
expensive than 2D CNNs and prone to overfit. We seek a balance between speed
and accuracy by building an effective and efficient video classification system
through systematic exploration of critical network design choices. In
particular, we show that it is possible to replace many of the 3D convolutions
by low-cost 2D convolutions. Rather surprisingly, best result (in both speed
and accuracy) is achieved when replacing the 3D convolutions at the bottom of
the network, suggesting that temporal representation learning on high-level
semantic features is more useful. Our conclusion generalizes to datasets with
very different properties. When combined with several other cost-effective
designs including separable spatial/temporal convolution and feature gating,
our system results in an effective video classification system that that
produces very competitive results on several action classification benchmarks
(Kinetics, Something-something, UCF101 and HMDB), as well as two action
detection (localization) benchmarks (JHMDB and UCF101-24).
</summary>
    <author>
      <name>Saining Xie</name>
    </author>
    <author>
      <name>Chen Sun</name>
    </author>
    <author>
      <name>Jonathan Huang</name>
    </author>
    <author>
      <name>Zhuowen Tu</name>
    </author>
    <author>
      <name>Kevin Murphy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2018 camera ready</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.04851v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04851v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.09842v1</id>
    <updated>2024-04-15T14: 52: 02Z</updated>
    <published>2024-04-15T14: 52: 02Z</published>
    <title>STMixer: A One-Stage Sparse Action Detector</title>
    <summary>  Traditional video action detectors typically adopt the two-stage pipeline,
where a person detector is first employed to generate actor boxes and then 3D
RoIAlign is used to extract actor-specific features for classification. This
detection paradigm requires multi-stage training and inference, and the feature
sampling is constrained inside the box, failing to effectively leverage richer
context information outside. Recently, a few query-based action detectors have
been proposed to predict action instances in an end-to-end manner. However,
they still lack adaptability in feature sampling and decoding, thus suffering
from the issues of inferior performance or slower convergence. In this paper,
we propose two core designs for a more flexible one-stage sparse action
detector. First, we present a query-based adaptive feature sampling module,
which endows the detector with the flexibility of mining a group of
discriminative features from the entire spatio-temporal domain. Second, we
devise a decoupled feature mixing module, which dynamically attends to and
mixes video features along the spatial and temporal dimensions respectively for
better feature decoding. Based on these designs, we instantiate two detection
pipelines, that is, STMixer-K for keyframe action detection and STMixer-T for
action tubelet detection. Without bells and whistles, our STMixer detectors
obtain state-of-the-art results on five challenging spatio-temporal action
detection benchmarks for keyframe action detection or action tube detection.
</summary>
    <author>
      <name>Tao Wu</name>
    </author>
    <author>
      <name>Mengqi Cao</name>
    </author>
    <author>
      <name>Ziteng Gao</name>
    </author>
    <author>
      <name>Gangshan Wu</name>
    </author>
    <author>
      <name>Limin Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of the paper arXiv: 2303.15879 presented at CVPR
  2023. Accepted by TPAMI 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.09842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.09842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2403.12574v2</id>
    <updated>2024-08-24T16: 04: 52Z</updated>
    <published>2024-03-19T09: 34: 11Z</published>
    <title>EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based
  Detection with Recurrent Spiking Neural Networks</title>
    <summary>  Event cameras, with their high dynamic range and temporal resolution, are
ideally suited for object detection, especially under scenarios with motion
blur and challenging lighting conditions. However, while most existing
approaches prioritize optimizing spatiotemporal representations with advanced
detection backbones and early aggregation functions, the crucial issue of
adaptive event sampling remains largely unaddressed. Spiking Neural Networks
(SNNs), which operate on an event-driven paradigm through sparse spike
communication, emerge as a natural fit for addressing this challenge. In this
study, we discover that the neural dynamics of spiking neurons align closely
with the behavior of an ideal temporal event sampler. Motivated by this
insight, we propose a novel adaptive sampling module that leverages recurrent
convolutional SNNs enhanced with temporal memory, facilitating a fully
end-to-end learnable framework for event-based detection. Additionally, we
introduce Residual Potential Dropout (RPD) and Spike-Aware Training (SAT) to
regulate potential distribution and address performance degradation encountered
in spike-based sampling modules. Empirical evaluation on neuromorphic detection
datasets demonstrates that our approach outperforms existing state-of-the-art
spike-based methods with significantly fewer parameters and time steps. For
instance, our method yields a 4.4\% mAP improvement on the Gen1 dataset, while
requiring 38\% fewer parameters and only three time steps. Moreover, the
applicability and effectiveness of our adaptive sampling methodology extend
beyond SNNs, as demonstrated through further validation on conventional
non-spiking models. Code is available at https: //github.com/Windere/EAS-SNN.
</summary>
    <author>
      <name>Ziming Wang</name>
    </author>
    <author>
      <name>Ziling Wang</name>
    </author>
    <author>
      <name>Huaning Li</name>
    </author>
    <author>
      <name>Lang Qin</name>
    </author>
    <author>
      <name>Runhao Jiang</name>
    </author>
    <author>
      <name>De Ma</name>
    </author>
    <author>
      <name>Huajin Tang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ECCV2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.12574v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.12574v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2008.13196v1</id>
    <updated>2020-08-30T15: 38: 44Z</updated>
    <published>2020-08-30T15: 38: 44Z</published>
    <title>Finding Action Tubes with a Sparse-to-Dense Framework</title>
    <summary>  The task of spatial-temporal action detection has attracted increasing
attention among researchers. Existing dominant methods solve this problem by
relying on short-term information and dense serial-wise detection on each
individual frames or clips. Despite their effectiveness, these methods showed
inadequate use of long-term information and are prone to inefficiency. In this
paper, we propose for the first time, an efficient framework that generates
action tube proposals from video streams with a single forward pass in a
sparse-to-dense manner. There are two key characteristics in this framework:
(1) Both long-term and short-term sampled information are explicitly utilized
in our spatiotemporal network, (2) A new dynamic feature sampling module (DTS)
is designed to effectively approximate the tube output while keeping the system
tractable. We evaluate the efficacy of our model on the UCF101-24, JHMDB-21 and
UCFSports benchmark datasets, achieving promising results that are competitive
to state-of-the-art methods. The proposed sparse-to-dense strategy rendered our
framework about 7.6 times more efficient than the nearest competitor.
</summary>
    <author>
      <name>Yuxi Li</name>
    </author>
    <author>
      <name>Weiyao Lin</name>
    </author>
    <author>
      <name>Tao Wang</name>
    </author>
    <author>
      <name>John See</name>
    </author>
    <author>
      <name>Rui Qian</name>
    </author>
    <author>
      <name>Ning Xu</name>
    </author>
    <author>
      <name>Limin Wang</name>
    </author>
    <author>
      <name>Shugong Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 figures; AAAI 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.13196v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.13196v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2004.12531v2</id>
    <updated>2020-06-01T13: 59: 38Z</updated>
    <published>2020-04-27T01: 12: 48Z</published>
    <title>Spatial-Temporal Mitosis Detection in Phase-Contrast Microscopy via
  Likelihood Map Estimation by 3DCNN</title>
    <summary>  Automated mitotic detection in time-lapse phasecontrast microscopy provides
us much information for cell behavior analysis, and thus several mitosis
detection methods have been proposed. However, these methods still have two
problems; 1) they cannot detect multiple mitosis events when there are closely
placed. 2) they do not consider the annotation gaps, which may occur since the
appearances of mitosis cells are very similar before and after the annotated
frame. In this paper, we propose a novel mitosis detection method that can
detect multiple mitosis events in a candidate sequence and mitigate the human
annotation gap via estimating a spatiotemporal likelihood map by 3DCNN. In this
training, the loss gradually decreases with the gap size between ground truth
and estimation. This mitigates the annotation gaps. Our method outperformed the
compared methods in terms of F1- score using a challenging dataset that
contains the data under four different conditions.
</summary>
    <author>
      <name>Kazuya Nishimura</name>
    </author>
    <author>
      <name>Ryoma Bise</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages,
        6 figures, Accepted in EMBC 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.12531v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.12531v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1410.7743v1</id>
    <updated>2014-10-28T19: 17: 17Z</updated>
    <published>2014-10-28T19: 17: 17Z</published>
    <title>Data Driven Authentication: On the Effectiveness of User Behaviour
  Modelling with Mobile Device Sensors</title>
    <summary>  We propose a lightweight, and temporally and spatially aware user behaviour
modelling technique for sensor-based authentication. Operating in the
background, our data driven technique compares current behaviour with a user
profile. If the behaviour deviates sufficiently from the established norm,
actions such as explicit authentication can be triggered. To support a quick
and lightweight deployment, our solution automatically switches from training
mode to deployment mode when the user's behaviour is sufficiently learned.
Furthermore, it allows the device to automatically determine a suitable
detection threshold. We use our model to investigate practical aspects of
sensor-based authentication by applying it to three publicly available data
sets, computing expected times for training duration and behaviour drift. We
also test our model with scenarios involving an attacker with varying knowledge
and capabilities.
</summary>
    <author>
      <name>Hilmi Gunes Kayacik</name>
    </author>
    <author>
      <name>Mike Just</name>
    </author>
    <author>
      <name>Lynne Baillie</name>
    </author>
    <author>
      <name>David Aspinall</name>
    </author>
    <author>
      <name>Nicholas Micallef</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the Third Workshop on Mobile Security Technologies
  (MoST) 2014 (http: //arxiv.org/abs/1410.6674)</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.7743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.7743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2402.04567v1</id>
    <updated>2024-02-07T04: 06: 53Z</updated>
    <published>2024-02-07T04: 06: 53Z</published>
    <title>OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences</title>
    <summary>  Anomaly detection in decision-making sequences is a challenging problem due
to the complexity of normality representation learning and the sequential
nature of the task. Most existing methods based on Reinforcement Learning (RL)
are difficult to implement in the real world due to unrealistic assumptions,
such as having access to environment dynamics, reward signals, and online
interactions with the environment. To address these limitations, we propose an
unsupervised method named Offline Imitation Learning based Anomaly Detection
(OIL-AD), which detects anomalies in decision-making sequences using two
extracted behaviour features: action optimality and sequential association. Our
offline learning model is an adaptation of behavioural cloning with a
transformer policy network, where we modify the training process to learn a Q
function and a state value function from normal trajectories. We propose that
the Q function and the state value function can provide sufficient information
about agents' behavioural data, from which we derive two features for anomaly
detection. The intuition behind our method is that the action optimality
feature derived from the Q function can differentiate the optimal action from
others at each local state, and the sequential association feature derived from
the state value function has the potential to maintain the temporal
correlations between decisions (state-action pairs). Our experiments show that
OIL-AD can achieve outstanding online anomaly detection performance with up to
34.8% improvement in F1 score over comparable baselines.
</summary>
    <author>
      <name>Chen Wang</name>
    </author>
    <author>
      <name>Sarah Erfani</name>
    </author>
    <author>
      <name>Tansu Alpcan</name>
    </author>
    <author>
      <name>Christopher Leckie</name>
    </author>
    <link href="http://arxiv.org/abs/2402.04567v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.04567v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1608.08128v3</id>
    <updated>2017-03-02T23: 07: 00Z</updated>
    <published>2016-08-29T16: 14: 52Z</published>
    <title>Temporal Activity Detection in Untrimmed Videos with Recurrent Neural
  Networks</title>
    <summary>  This thesis explore different approaches using Convolutional and Recurrent
Neural Networks to classify and temporally localize activities on videos,
furthermore an implementation to achieve it has been proposed. As the first
step, features have been extracted from video frames using an state of the art
3D Convolutional Neural Network. This features are fed in a recurrent neural
network that solves the activity classification and temporally location tasks
in a simple and flexible way. Different architectures and configurations have
been tested in order to achieve the best performance and learning of the video
dataset provided. In addition it has been studied different kind of post
processing over the trained network's output to achieve a better results on the
temporally localization of activities on the videos. The results provided by
the neural network developed in this thesis have been submitted to the
ActivityNet Challenge 2016 of the CVPR, achieving competitive results using a
simple and flexible architecture.
</summary>
    <author>
      <name>Alberto Montes</name>
    </author>
    <author>
      <name>Amaia Salvador</name>
    </author>
    <author>
      <name>Santiago Pascual</name>
    </author>
    <author>
      <name>Xavier Giro-i-Nieto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Best Poster Award at the 1st NIPS Workshop on Large Scale Computer
  Vision Systems (Barcelona, December 2016). Source code available at
  https: //imatge-upc.github.io/activitynet-2016-cvprw/</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08128v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08128v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.8; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1705.03634v2</id>
    <updated>2017-06-23T07: 18: 54Z</updated>
    <published>2017-05-10T07: 12: 31Z</published>
    <title>4d isip: 4d implicit surface interest point detection</title>
    <summary>  In this paper, we propose a new method to detect 4D spatiotemporal interest
points though an implicit surface, we refer to as the 4D-ISIP. We use a 3D
volume which has a truncated signed distance function(TSDF) for every voxel to
represent our 3D object model. The TSDF represents the distance between the
spatial points and object surface points which is an implicit surface
representation. Our novelty is to detect the points where the local
neighborhood has significant variations along both spatial and temporal
directions. We established a system to acquire 3D human motion dataset using
only one Kinect. Experimental results show that our method can detect 4D-ISIP
for different human actions.
</summary>
    <author>
      <name>Shirui Li</name>
    </author>
    <author>
      <name>Alper Yilmaz</name>
    </author>
    <author>
      <name>Changlin Xiao</name>
    </author>
    <author>
      <name>Hua Li</name>
    </author>
    <link href="http://arxiv.org/abs/1705.03634v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03634v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.04078v1</id>
    <updated>2025-03-06T04: 28: 11Z</updated>
    <published>2025-03-06T04: 28: 11Z</published>
    <title>Spatial-Temporal Perception with Causal Inference for Naturalistic
  Driving Action Recognition</title>
    <summary>  Naturalistic driving action recognition is essential for vehicle cabin
monitoring systems. However, the complexity of real-world backgrounds presents
significant challenges for this task, and previous approaches have struggled
with practical implementation due to their limited ability to observe subtle
behavioral differences and effectively learn inter-frame features from video.
In this paper, we propose a novel Spatial-Temporal Perception (STP)
architecture that emphasizes both temporal information and spatial
relationships between key objects, incorporating a causal decoder to perform
behavior recognition and temporal action localization. Without requiring
multimodal input, STP directly extracts temporal and spatial distance features
from RGB video clips. Subsequently, these dual features are jointly encoded by
maximizing the expected likelihood across all possible permutations of the
factorization order. By integrating temporal and spatial features at different
scales, STP can perceive subtle behavioral changes in challenging scenarios.
Additionally, we introduce a causal-aware module to explore relationships
between video frame features, significantly enhancing detection efficiency and
performance. We validate the effectiveness of our approach using two publicly
available driver distraction detection benchmarks. The results demonstrate that
our framework achieves state-of-the-art performance.
</summary>
    <author>
      <name>Qing Chang</name>
    </author>
    <author>
      <name>Wei Dai</name>
    </author>
    <author>
      <name>Zhihao Shuai</name>
    </author>
    <author>
      <name>Limin Yu</name>
    </author>
    <author>
      <name>Yutao Yue</name>
    </author>
    <link href="http://arxiv.org/abs/2503.04078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.04078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.17381v1</id>
    <updated>2024-04-26T12: 56: 16Z</updated>
    <published>2024-04-26T12: 56: 16Z</published>
    <title>Frequency-Guided Multi-Level Human Action Anomaly Detection with
  Normalizing Flows</title>
    <summary>  We introduce the task of human action anomaly detection (HAAD), which aims to
identify anomalous motions in an unsupervised manner given only the
pre-determined normal category of training action samples. Compared to prior
human-related anomaly detection tasks which primarily focus on unusual events
from videos, HAAD involves the learning of specific action labels to recognize
semantically anomalous human behaviors. To address this task, we propose a
normalizing flow (NF)-based detection framework where the sample likelihood is
effectively leveraged to indicate anomalies. As action anomalies often occur in
some specific body parts, in addition to the full-body action feature learning,
we incorporate extra encoding streams into our framework for a finer modeling
of body subsets. Our framework is thus multi-level to jointly discover global
and local motion anomalies. Furthermore, to show awareness of the potentially
jittery data during recording, we resort to discrete cosine transformation by
converting the action samples from the temporal to the frequency domain to
mitigate the issue of data instability. Extensive experimental results on two
human action datasets demonstrate that our method outperforms the baselines
formed by adapting state-of-the-art human activity AD approaches to our task of
HAAD.
</summary>
    <author>
      <name>Shun Maeda</name>
    </author>
    <author>
      <name>Chunzhi Gu</name>
    </author>
    <author>
      <name>Jun Yu</name>
    </author>
    <author>
      <name>Shogo Tokai</name>
    </author>
    <author>
      <name>Shangce Gao</name>
    </author>
    <author>
      <name>Chao Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2404.17381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.17381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2006.07896v1</id>
    <updated>2020-06-14T13: 21: 37Z</updated>
    <published>2020-06-14T13: 21: 37Z</published>
    <title>Team RUC_AIM3 Technical Report at Activitynet 2020 Task 2: Exploring
  Sequential Events Detection for Dense Video Captioning</title>
    <summary>  Detecting meaningful events in an untrimmed video is essential for dense
video captioning. In this work, we propose a novel and simple model for event
sequence generation and explore temporal relationships of the event sequence in
the video. The proposed model omits inefficient two-stage proposal generation
and directly generates event boundaries conditioned on bi-directional temporal
dependency in one pass. Experimental results show that the proposed event
sequence generation model can generate more accurate and diverse events within
a small number of proposals. For the event captioning, we follow our previous
work to employ the intra-event captioning models into our pipeline system. The
overall system achieves state-of-the-art performance on the dense-captioning
events in video task with 9.894 METEOR score on the challenge testing set.
</summary>
    <author>
      <name>Yuqing Song</name>
    </author>
    <author>
      <name>Shizhe Chen</name>
    </author>
    <author>
      <name>Yida Zhao</name>
    </author>
    <author>
      <name>Qin Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Winner solution in CVPR 2020 Activitynet Dense Video Captioning
  challenge</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.07896v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.07896v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1705.01861v3</id>
    <updated>2017-08-21T13: 54: 34Z</updated>
    <published>2017-05-04T14: 41: 56Z</published>
    <title>Action Tubelet Detector for Spatio-Temporal Action Localization</title>
    <summary>  Current state-of-the-art approaches for spatio-temporal action localization
rely on detections at the frame level that are then linked or tracked across
time. In this paper, we leverage the temporal continuity of videos instead of
operating at the frame level. We propose the ACtion Tubelet detector
(ACT-detector) that takes as input a sequence of frames and outputs tubelets,
i.e., sequences of bounding boxes with associated scores. The same way
state-of-the-art object detectors rely on anchor boxes, our ACT-detector is
based on anchor cuboids. We build upon the SSD framework. Convolutional
features are extracted for each frame, while scores and regressions are based
on the temporal stacking of these features, thus exploiting information from a
sequence. Our experimental results show that leveraging sequences of frames
significantly improves detection performance over using individual frames. The
gain of our tubelet detector can be explained by both more accurate scores and
more precise localization. Our ACT-detector outperforms the state-of-the-art
methods for frame-mAP and video-mAP on the J-HMDB and UCF-101 datasets, in
particular at high overlap thresholds.
</summary>
    <author>
      <name>Vicky Kalogeiton</name>
    </author>
    <author>
      <name>Philippe Weinzaepfel</name>
    </author>
    <author>
      <name>Vittorio Ferrari</name>
    </author>
    <author>
      <name>Cordelia Schmid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.01861v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01861v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2310.16447v1</id>
    <updated>2023-10-25T08: 11: 02Z</updated>
    <published>2023-10-25T08: 11: 02Z</published>
    <title>ChimpACT: A Longitudinal Dataset for Understanding Chimpanzee Behaviors</title>
    <summary>  Understanding the behavior of non-human primates is crucial for improving
animal welfare, modeling social behavior, and gaining insights into
distinctively human and phylogenetically shared behaviors. However, the lack of
datasets on non-human primate behavior hinders in-depth exploration of primate
social interactions, posing challenges to research on our closest living
relatives. To address these limitations, we present ChimpACT, a comprehensive
dataset for quantifying the longitudinal behavior and social relations of
chimpanzees within a social group. Spanning from 2015 to 2018, ChimpACT
features videos of a group of over 20 chimpanzees residing at the Leipzig Zoo,
Germany, with a particular focus on documenting the developmental trajectory of
one young male, Azibo. ChimpACT is both comprehensive and challenging,
consisting of 163 videos with a cumulative 160,
        500 frames, each richly
annotated with detection, identification, pose estimation, and fine-grained
spatiotemporal behavior labels. We benchmark representative methods of three
tracks on ChimpACT: (i) tracking and identification, (ii) pose estimation, and
(iii) spatiotemporal action detection of the chimpanzees. Our experiments
reveal that ChimpACT offers ample opportunities for both devising new methods
and adapting existing ones to solve fundamental computer vision tasks applied
to chimpanzee groups, such as detection, pose estimation, and behavior
analysis, ultimately deepening our comprehension of communication and sociality
in non-human primates.
</summary>
    <author>
      <name>Xiaoxuan Ma</name>
    </author>
    <author>
      <name>Stephan P. Kaufhold</name>
    </author>
    <author>
      <name>Jiajun Su</name>
    </author>
    <author>
      <name>Wentao Zhu</name>
    </author>
    <author>
      <name>Jack Terwilliger</name>
    </author>
    <author>
      <name>Andres Meza</name>
    </author>
    <author>
      <name>Yixin Zhu</name>
    </author>
    <author>
      <name>Federico Rossano</name>
    </author>
    <author>
      <name>Yizhou Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.16447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.16447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2102.03391v1</id>
    <updated>2021-02-05T19: 27: 38Z</updated>
    <published>2021-02-05T19: 27: 38Z</published>
    <title>Single Run Action Detector over Video Stream -- A Privacy Preserving
  Approach</title>
    <summary>  This paper takes initial strides at designing and evaluating a vision-based
system for privacy ensured activity monitoring. The proposed technology
utilizing Artificial Intelligence (AI)-empowered proactive systems offering
continuous monitoring, behavioral analysis, and modeling of human activities.
To this end, this paper presents Single Run Action Detector (S-RAD) which is a
real-time privacy-preserving action detector that performs end-to-end action
localization and classification. It is based on Faster-RCNN combined with
temporal shift modeling and segment based sampling to capture the human
actions. Results on UCF-Sports and UR Fall dataset present comparable accuracy
to State-of-the-Art approaches with significantly lower model size and
computation demand and the ability for real-time execution on edge embedded
device (e.g. Nvidia Jetson Xavier).
</summary>
    <author>
      <name>Anbumalar Saravanan</name>
    </author>
    <author>
      <name>Justin Sanchez</name>
    </author>
    <author>
      <name>Hassan Ghasemzadeh</name>
    </author>
    <author>
      <name>Aurelia Macabasco-O'Connell</name>
    </author>
    <author>
      <name>Hamed Tabkhi</name>
    </author>
    <link href="http://arxiv.org/abs/2102.03391v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.03391v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2410.09954v1</id>
    <updated>2024-10-13T18: 21: 15Z</updated>
    <published>2024-10-13T18: 21: 15Z</published>
    <title>EITNet: An IoT-Enhanced Framework for Real-Time Basketball Action
  Recognition</title>
    <summary>  Integrating IoT technology into basketball action recognition enhances sports
analytics, providing crucial insights into player performance and game
strategy. However, existing methods often fall short in terms of accuracy and
efficiency, particularly in complex, real-time environments where player
movements are frequently occluded or involve intricate interactions. To
overcome these challenges, we propose the EITNet model, a deep learning
framework that combines EfficientDet for object detection, I3D for
spatiotemporal feature extraction, and TimeSformer for temporal analysis, all
integrated with IoT technology for seamless real-time data collection and
processing. Our contributions include developing a robust architecture that
improves recognition accuracy to 92\%, surpassing the baseline EfficientDet
model's 87\%, and reducing loss to below 5.0 compared to EfficientDet's 9.0
over 50 epochs. Furthermore, the integration of IoT technology enhances
real-time data processing, providing adaptive insights into player performance
and strategy. The paper details the design and implementation of EITNet,
experimental validation, and a comprehensive evaluation against existing
models. The results demonstrate EITNet's potential to significantly advance
automated sports analysis and optimize data utilization for player performance
and strategy improvement.
</summary>
    <author>
      <name>Jingyu Liu</name>
    </author>
    <author>
      <name>Xinyu Liu</name>
    </author>
    <author>
      <name>Mingzhe Qu</name>
    </author>
    <author>
      <name>Tianyi Lyu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.09954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.09954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1811.08496v2</id>
    <updated>2018-11-23T03: 06: 34Z</updated>
    <published>2018-11-20T21: 35: 07Z</published>
    <title>A Proposal-Based Solution to Spatio-Temporal Action Detection in
  Untrimmed Videos</title>
    <summary>  Existing approaches for spatio-temporal action detection in videos are
limited by the spatial extent and temporal duration of the actions. In this
paper, we present a modular system for spatio-temporal action detection in
untrimmed security videos. We propose a two stage approach. The first stage
generates dense spatio-temporal proposals using hierarchical clustering and
temporal jittering techniques on frame-wise object detections. The second stage
is a Temporal Refinement I3D (TRI-3D) network that performs action
classification and temporal refinement on the generated proposals. The object
detection-based proposal generation step helps in detecting actions occurring
in a small spatial region of a video frame, while temporal jittering and
refinement helps in detecting actions of variable lengths. Experimental results
on the spatio-temporal action detection dataset - DIVA - show the effectiveness
of our system. For comparison, the performance of our system is also evaluated
on the THUMOS14 temporal action detection dataset.
</summary>
    <author>
      <name>Joshua Gleason</name>
    </author>
    <author>
      <name>Rajeev Ranjan</name>
    </author>
    <author>
      <name>Steven Schwarcz</name>
    </author>
    <author>
      <name>Carlos D. Castillo</name>
    </author>
    <author>
      <name>Jun-Chen Cheng</name>
    </author>
    <author>
      <name>Rama Chellappa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IEEE Winter Conference on Applications of Computer
  Vision (WACV) 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.08496v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.08496v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2009.01617v1</id>
    <updated>2020-09-03T12: 38: 55Z</updated>
    <published>2020-09-03T12: 38: 55Z</published>
    <title>Modification method for single-stage object detectors that allows to
  exploit the temporal behaviour of a scene to improve detection accuracy</title>
    <summary>  A simple modification method for single-stage generic object detection neural
networks, such as YOLO and SSD, is proposed, which allows for improving the
detection accuracy on video data by exploiting the temporal behavior of the
scene in the detection pipeline. It is shown that, using this method, the
detection accuracy of the base network can be considerably improved, especially
for occluded and hidden objects. It is shown that a modified network is more
prone to detect hidden objects with more confidence than an unmodified one. A
weakly supervised training method is proposed, which allows for training a
modified network without requiring any additional annotated data.
</summary>
    <author>
      <name>Menua Gevorgyan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages,
        5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.01617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.01617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T45, 68T07" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.8; I.4.9; I.2.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1605.05106v2</id>
    <updated>2017-04-03T10: 39: 02Z</updated>
    <published>2016-05-17T10: 53: 07Z</published>
    <title>Detecting Violent and Abnormal Crowd activity using Temporal Analysis of
  Grey Level Co-occurrence Matrix (GLCM) Based Texture Measures</title>
    <summary>  The severity of sustained injury resulting from assault-related violence can
be minimised by reducing detection time. However, it has been shown that human
operators perform poorly at detecting events found in video footage when
presented with simultaneous feeds. We utilise computer vision techniques to
develop an automated method of abnormal crowd detection that can aid a human
operator in the detection of violent behaviour. We observed that behaviour in
city centre environments often occur in crowded areas, resulting in individual
actions being occluded by other crowd members. We propose a real-time
descriptor that models crowd dynamics by encoding changes in crowd texture
using temporal summaries of Grey Level Co-Occurrence Matrix (GLCM) features. We
introduce a measure of inter-frame uniformity (IFU) and demonstrate that the
appearance of violent behaviour changes in a less uniform manner when compared
to other types of crowd behaviour. Our proposed method is computationally cheap
and offers real-time description. Evaluating our method using a privately held
CCTV dataset and the publicly available Violent Flows, UCF Web Abnormality, and
UMN Abnormal Crowd datasets, we report a receiver operating characteristic
score of 0.9782,
        0.9403,
        0.8218 and 0.9956 respectively.
</summary>
    <author>
      <name>Kaelon Lloyd</name>
    </author>
    <author>
      <name>David Marshall</name>
    </author>
    <author>
      <name>Simon C. Moore</name>
    </author>
    <author>
      <name>Paul L. Rosin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00138-017-0830-x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00138-017-0830-x" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published under open access,
        9 pages,
        12 Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Vision and Applications (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1605.05106v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05106v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10; I.4.7; I.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2004.00180v1</id>
    <updated>2020-04-01T00: 54: 56Z</updated>
    <published>2020-04-01T00: 54: 56Z</published>
    <title>Spatio-Temporal Action Detection with Multi-Object Interaction</title>
    <summary>  Spatio-temporal action detection in videos requires localizing the action
both spatially and temporally in the form of an "action tube". Nowadays, most
spatio-temporal action detection datasets (e.g. UCF101-24, AVA, DALY) are
annotated with action tubes that contain a single person performing the action,
thus the predominant action detection models simply employ a person detection
and tracking pipeline for localization. However, when the action is defined as
an interaction between multiple objects, such methods may fail since each
bounding box in the action tube contains multiple objects instead of one
person. In this paper, we study the spatio-temporal action detection problem
with multi-object interaction. We introduce a new dataset that is annotated
with action tubes containing multi-object interactions. Moreover, we propose an
end-to-end spatio-temporal action detection model that performs both spatial
and temporal regression simultaneously. Our spatial regression may enclose
multiple objects participating in the action. During test time, we simply
connect the regressed bounding boxes within the predicted temporal duration
using a simple heuristic. We report the baseline results of our proposed model
on this new dataset, and also show competitive results on the standard
benchmark UCF101-24 using only RGB input.
</summary>
    <author>
      <name>Huijuan Xu</name>
    </author>
    <author>
      <name>Lizhi Yang</name>
    </author>
    <author>
      <name>Stan Sclaroff</name>
    </author>
    <author>
      <name>Kate Saenko</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <link href="http://arxiv.org/abs/2004.00180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.00180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1905.13417v1</id>
    <updated>2019-05-31T05: 14: 39Z</updated>
    <published>2019-05-31T05: 14: 39Z</published>
    <title>TACNet: Transition-Aware Context Network for Spatio-Temporal Action
  Detection</title>
    <summary>  Current state-of-the-art approaches for spatio-temporal action detection have
achieved impressive results but remain unsatisfactory for temporal extent
detection. The main reason comes from that, there are some ambiguous states
similar to the real actions which may be treated as target actions even by a
well-trained network. In this paper, we define these ambiguous samples as
"transitional states", and propose a Transition-Aware Context Network (TACNet)
to distinguish transitional states. The proposed TACNet includes two main
components, i.e., temporal context detector and transition-aware classifier.
The temporal context detector can extract long-term context information with
constant time complexity by constructing a recurrent network. The
transition-aware classifier can further distinguish transitional states by
classifying action and transitional states simultaneously. Therefore, the
proposed TACNet can substantially improve the performance of spatio-temporal
action detection. We extensively evaluate the proposed TACNet on UCF101-24 and
J-HMDB datasets. The experimental results demonstrate that TACNet obtains
competitive performance on JHMDB and significantly outperforms the
state-of-the-art methods on the untrimmed UCF101-24 in terms of both frame-mAP
and video-mAP.
</summary>
    <author>
      <name>Lin Song</name>
    </author>
    <author>
      <name>Shiwei Zhang</name>
    </author>
    <author>
      <name>Gang Yu</name>
    </author>
    <author>
      <name>Hongbin Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR-2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.13417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.13417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T06" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2410.21958v1</id>
    <updated>2024-10-29T11: 23: 09Z</updated>
    <published>2024-10-29T11: 23: 09Z</published>
    <title>Spatio-temporal Transformers for Action Unit Classification with Event
  Cameras</title>
    <summary>  Face analysis has been studied from different angles to infer emotion, poses,
shapes, and landmarks. Traditionally RGB cameras are used, yet for fine-grained
tasks standard sensors might not be up to the task due to their latency, making
it impossible to record and detect micro-movements that carry a highly
informative signal, which is necessary for inferring the true emotions of a
subject. Event cameras have been increasingly gaining interest as a possible
solution to this and similar high-frame rate tasks. We propose a novel
spatiotemporal Vision Transformer model that uses Shifted Patch Tokenization
(SPT) and Locality Self-Attention (LSA) to enhance the accuracy of Action Unit
classification from event streams. We also address the lack of labeled event
data in the literature, which can be considered one of the main causes of an
existing gap between the maturity of RGB and neuromorphic vision models.
Gathering data is harder in the event domain since it cannot be crawled from
the web and labeling frames should take into account event aggregation rates
and the fact that static parts might not be visible in certain frames. To this
end, we present FACEMORPHIC, a temporally synchronized multimodal face dataset
composed of RGB videos and event streams. The dataset is annotated at a video
level with facial Action Units and contains streams collected with various
possible applications, ranging from 3D shape estimation to lip-reading. We then
show how temporal synchronization can allow effective neuromorphic face
analysis without the need to manually annotate videos: we instead leverage
cross-modal supervision bridging the domain gap by representing face shapes in
a 3D space. Our proposed model outperforms baseline methods by effectively
capturing spatial and temporal information, crucial for recognizing subtle
facial micro-expressions.
</summary>
    <author>
      <name>Luca Cultrera</name>
    </author>
    <author>
      <name>Federico Becattini</name>
    </author>
    <author>
      <name>Lorenzo Berlincioni</name>
    </author>
    <author>
      <name>Claudio Ferrari</name>
    </author>
    <author>
      <name>Alberto Del Bimbo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review at CVIU. arXiv admin note: substantial text overlap with
  arXiv: 2409.10213</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.21958v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.21958v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.12258v1</id>
    <updated>2024-04-18T15: 25: 59Z</updated>
    <published>2024-04-18T15: 25: 59Z</published>
    <title>DeepLocalization: Using change point detection for Temporal Action
  Localization</title>
    <summary>  In this study, we introduce DeepLocalization, an innovative framework devised
for the real-time localization of actions tailored explicitly for monitoring
driver behavior. Utilizing the power of advanced deep learning methodologies,
our objective is to tackle the critical issue of distracted driving-a
significant factor contributing to road accidents. Our strategy employs a dual
approach: leveraging Graph-Based Change-Point Detection for pinpointing actions
in time alongside a Video Large Language Model (Video-LLM) for precisely
categorizing activities. Through careful prompt engineering, we customize the
Video-LLM to adeptly handle driving activities' nuances, ensuring its
classification efficacy even with sparse data. Engineered to be lightweight,
our framework is optimized for consumer-grade GPUs, making it vastly applicable
in practical scenarios. We subjected our method to rigorous testing on the
SynDD2 dataset, a complex benchmark for distracted driving behaviors, where it
demonstrated commendable performance-achieving 57.5% accuracy in event
classification and 51% in event detection. These outcomes underscore the
substantial promise of DeepLocalization in accurately identifying diverse
driver behaviors and their temporal occurrences, all within the bounds of
limited computational resources.
</summary>
    <author>
      <name>Mohammed Shaiqur Rahman</name>
    </author>
    <author>
      <name>Ibne Farabi Shihab</name>
    </author>
    <author>
      <name>Lynna Chu</name>
    </author>
    <author>
      <name>Anuj Sharma</name>
    </author>
    <link href="http://arxiv.org/abs/2404.12258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.12258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2206.03064v2</id>
    <updated>2022-10-27T13: 11: 48Z</updated>
    <published>2022-06-07T07: 31: 56Z</published>
    <title>A Simple and Efficient Pipeline to Build an End-to-End Spatial-Temporal
  Action Detector</title>
    <summary>  Spatial-temporal action detection is a vital part of video understanding.
Current spatial-temporal action detection methods mostly use an object detector
to obtain person candidates and classify these person candidates into different
action categories. So-called two-stage methods are heavy and hard to apply in
real-world applications. Some existing methods build one-stage pipelines, But a
large performance drop exists with the vanilla one-stage pipeline and extra
classification modules are needed to achieve comparable performance. In this
paper, we explore a simple and effective pipeline to build a strong one-stage
spatial-temporal action detector. The pipeline is composed by two parts: one is
a simple end-to-end spatial-temporal action detector. The proposed end-to-end
detector has minor architecture changes to current proposal-based detectors and
does not add extra action classification modules. The other part is a novel
labeling strategy to utilize unlabeled frames in sparse annotated data. We
named our model as SE-STAD. The proposed SE-STAD achieves around 2% mAP boost
and around 80% FLOPs reduction. Our code will be released at
https: //github.com/4paradigm-CV/SE-STAD.
</summary>
    <author>
      <name>Lin Sui</name>
    </author>
    <author>
      <name>Chen-Lin Zhang</name>
    </author>
    <author>
      <name>Lixin Gu</name>
    </author>
    <author>
      <name>Feng Han</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted By WACV 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.03064v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.03064v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2409.05587v2</id>
    <updated>2024-09-12T15: 24: 44Z</updated>
    <published>2024-09-09T13: 16: 15Z</published>
    <title>DSDFormer: An Innovative Transformer-Mamba Framework for Robust
  High-Precision Driver Distraction Identification</title>
    <summary>  Driver distraction remains a leading cause of traffic accidents, posing a
critical threat to road safety globally. As intelligent transportation systems
evolve, accurate and real-time identification of driver distraction has become
essential. However, existing methods struggle to capture both global contextual
and fine-grained local features while contending with noisy labels in training
datasets. To address these challenges, we propose DSDFormer, a novel framework
that integrates the strengths of Transformer and Mamba architectures through a
Dual State Domain Attention (DSDA) mechanism, enabling a balance between
long-range dependencies and detailed feature extraction for robust driver
behavior recognition. Additionally, we introduce Temporal Reasoning Confident
Learning (TRCL), an unsupervised approach that refines noisy labels by
leveraging spatiotemporal correlations in video sequences. Our model achieves
state-of-the-art performance on the AUC-V1, AUC-V2, and 100-Driver datasets and
demonstrates real-time processing efficiency on the NVIDIA Jetson AGX Orin
platform. Extensive experimental results confirm that DSDFormer and TRCL
significantly improve both the accuracy and robustness of driver distraction
detection, offering a scalable solution to enhance road safety.
</summary>
    <author>
      <name>Junzhou Chen</name>
    </author>
    <author>
      <name>Zirui Zhang</name>
    </author>
    <author>
      <name>Jing Yu</name>
    </author>
    <author>
      <name>Heqiang Huang</name>
    </author>
    <author>
      <name>Ronghui Zhang</name>
    </author>
    <author>
      <name>Xuemiao Xu</name>
    </author>
    <author>
      <name>Bin Sheng</name>
    </author>
    <author>
      <name>Hong Yan</name>
    </author>
    <link href="http://arxiv.org/abs/2409.05587v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.05587v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2309.02722v1</id>
    <updated>2023-09-06T05: 12: 07Z</updated>
    <published>2023-09-06T05: 12: 07Z</published>
    <title>Reinforcement Learning of Action and Query Policies with LTL
  Instructions under Uncertain Event Detector</title>
    <summary>  Reinforcement learning (RL) with linear temporal logic (LTL) objectives can
allow robots to carry out symbolic event plans in unknown environments. Most
existing methods assume that the event detector can accurately map
environmental states to symbolic events; however, uncertainty is inevitable for
real-world event detectors. Such uncertainty in an event detector generates
multiple branching possibilities on LTL instructions, confusing action
decisions. Moreover, the queries to the uncertain event detector, necessary for
the task's progress, may increase the uncertainty further. To cope with those
issues, we propose an RL framework, Learning Action and Query over Belief LTL
(LAQBL), to learn an agent that can consider the diversity of LTL instructions
due to uncertain event detection while avoiding task failure due to the
unnecessary event-detection query. Our framework simultaneously learns 1) an
embedding of belief LTL, which is multiple branching possibilities on LTL
instructions using a graph neural network,
        2) an action policy, and 3) a query
policy which decides whether or not to query for the event detector.
Simulations in a 2D grid world and image-input robotic inspection environments
show that our method successfully learns actions to follow LTL instructions
even with uncertain event detectors.
</summary>
    <author>
      <name>Wataru Hatanaka</name>
    </author>
    <author>
      <name>Ryota Yamashina</name>
    </author>
    <author>
      <name>Takamitsu Matsubara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, Accepted by Robotics and Automation Letters (RA-L)</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.02722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.02722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.16609v2</id>
    <updated>2024-08-13T03: 13: 50Z</updated>
    <published>2024-04-25T13: 49: 42Z</published>
    <title>SFMViT: SlowFast Meet ViT in Chaotic World</title>
    <summary>  The task of spatiotemporal action localization in chaotic scenes is a
challenging task toward advanced video understanding. Paving the way with
high-quality video feature extraction and enhancing the precision of
detector-predicted anchors can effectively improve model performance. To this
end, we propose a high-performance dual-stream spatiotemporal feature
extraction network SFMViT with an anchor pruning strategy. The backbone of our
SFMViT is composed of ViT and SlowFast with prior knowledge of spatiotemporal
action localization, which fully utilizes ViT's excellent global feature
extraction capabilities and SlowFast's spatiotemporal sequence modeling
capabilities. Secondly, we introduce the confidence maximum heap to prune the
anchors detected in each frame of the picture to filter out the effective
anchors. These designs enable our SFMViT to achieve a mAP of 26.62% in the
Chaotic World dataset, far exceeding existing models. Code is available at
https: //github.com/jfightyr/SlowFast-Meet-ViT.
</summary>
    <author>
      <name>Jiaying Lin</name>
    </author>
    <author>
      <name>Jiajun Wen</name>
    </author>
    <author>
      <name>Mengyuan Liu</name>
    </author>
    <author>
      <name>Jinfu Liu</name>
    </author>
    <author>
      <name>Baiqiao Yin</name>
    </author>
    <author>
      <name>Yue Li</name>
    </author>
    <link href="http://arxiv.org/abs/2404.16609v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.16609v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2304.05930v3</id>
    <updated>2024-09-17T02: 10: 16Z</updated>
    <published>2023-04-12T15: 50: 19Z</published>
    <title>MED-VT++: Unifying Multimodal Learning with a Multiscale Encoder-Decoder
  Video Transformer</title>
    <summary>  In this paper, we present an end-to-end trainable unified multiscale
encoder-decoder transformer that is focused on dense prediction tasks in video.
The presented Multiscale Encoder-Decoder Video Transformer (MED-VT) uses
multiscale representation throughout and employs an optional input beyond video
(e.g., audio), when available, for multimodal processing (MED-VT++). Multiscale
representation at both encoder and decoder yields three key benefits: (i)
implicit extraction of spatiotemporal features at different levels of
abstraction for capturing dynamics without reliance on input optical flow, (ii)
temporal consistency at encoding and (iii) coarse-to-fine detection for
high-level (e.g., object) semantics to guide precise localization at decoding.
Moreover, we present a transductive learning scheme through many-to-many label
propagation to provide temporally consistent video predictions. We showcase
MED-VT/MED-VT++ on three unimodal video segmentation tasks (Automatic Video
Object Segmentation (AVOS), actor-action segmentation and Video Semantic
Segmentation (VSS)) as well as a multimodal segmentation task (Audio-Visual
Segmentation (AVS)). Results show that the proposed architecture outperforms
alternative state-of-the-art approaches on multiple benchmarks using only video
(and optional audio) as input, without reliance on optical flow. Finally, to
document details of the model's internal learned representations, we present a
detailed interpretability study, encompassing both quantitative and qualitative
analyses.
</summary>
    <author>
      <name>Rezaul Karim</name>
    </author>
    <author>
      <name>He Zhao</name>
    </author>
    <author>
      <name>Richard P. Wildes</name>
    </author>
    <author>
      <name>Mennatullah Siam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extension of CVPR'23 paper for journal submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.05930v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.05930v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1809.07075v1</id>
    <updated>2018-09-19T09: 04: 21Z</updated>
    <published>2018-09-19T09: 04: 21Z</published>
    <title>Detect, anticipate and generate: Semi-supervised recurrent latent
  variable models for human activity modeling</title>
    <summary>  Successful Human-Robot collaboration requires a predictive model of human
behavior. The robot needs to be able to recognize current goals and actions and
to predict future activities in a given context. However, the spatio-temporal
sequence of human actions is difficult to model since latent factors such as
intention, task, knowledge, intuition and preference determine the action
choices of each individual. In this work we introduce semi-supervised
variational recurrent neural networks which are able to a) model temporal
distributions over latent factors and the observable feature space, b)
incorporate discrete labels such as activity type when available, and c)
generate possible future action sequences on both feature and label level. We
evaluate our model on the Cornell Activity Dataset CAD-120 dataset. Our model
outperforms state-of-the-art approaches in both activity and affordance
detection and anticipation. Additionally, we show how samples of possible
future action sequences are in line with past observations.
</summary>
    <author>
      <name>Judith Bütepage</name>
    </author>
    <author>
      <name>Danica Kragic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted at the IROS 2018 workshop "Human-Robot
  Cooperation and Collaboration in Manipulation: Advancements and Challenges"</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.07075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.07075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.02405v2</id>
    <updated>2024-04-04T02: 56: 00Z</updated>
    <published>2024-04-03T02: 16: 30Z</published>
    <title>TE-TAD: Towards Full End-to-End Temporal Action Detection via
  Time-Aligned Coordinate Expression</title>
    <summary>  In this paper, we investigate that the normalized coordinate expression is a
key factor as reliance on hand-crafted components in query-based detectors for
temporal action detection (TAD). Despite significant advancements towards an
end-to-end framework in object detection, query-based detectors have been
limited in achieving full end-to-end modeling in TAD. To address this issue, we
propose \modelname{}, a full end-to-end temporal action detection transformer
that integrates time-aligned coordinate expression. We reformulate coordinate
expression utilizing actual timeline values, ensuring length-invariant
representations from the extremely diverse video duration environment.
Furthermore, our proposed adaptive query selection dynamically adjusts the
number of queries based on video length, providing a suitable solution for
varying video durations compared to a fixed query set. Our approach not only
simplifies the TAD process by eliminating the need for hand-crafted components
but also significantly improves the performance of query-based detectors. Our
TE-TAD outperforms the previous query-based detectors and achieves competitive
performance compared to state-of-the-art methods on popular benchmark datasets.
Code is available at: https: //github.com/Dotori-HJ/TE-TAD
</summary>
    <author>
      <name>Ho-Joong Kim</name>
    </author>
    <author>
      <name>Jung-Ho Hong</name>
    </author>
    <author>
      <name>Heejo Kong</name>
    </author>
    <author>
      <name>Seong-Whan Lee</name>
    </author>
    <link href="http://arxiv.org/abs/2404.02405v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.02405v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1810.08375v1</id>
    <updated>2018-10-19T07: 22: 40Z</updated>
    <published>2018-10-19T07: 22: 40Z</published>
    <title>Temporal Action Detection by Joint Identification-Verification</title>
    <summary>  Temporal action detection aims at not only recognizing action category but
also detecting start time and end time for each action instance in an untrimmed
video. The key challenge of this task is to accurately classify the action and
determine the temporal boundaries of each action instance. In temporal action
detection benchmark: THUMOS 2014, large variations exist in the same action
category while many similarities exist in different action categories, which
always limit the performance of temporal action detection. To address this
problem, we propose to use joint Identification-Verification network to reduce
the intra-action variations and enlarge inter-action differences. The joint
Identification-Verification network is a siamese network based on 3D ConvNets,
which can simultaneously predict the action categories and the similarity
scores for the input pairs of video proposal segments. Extensive experimental
results on the challenging THUMOS 2014 dataset demonstrate the effectiveness of
our proposed method compared to the existing state-of-art methods for temporal
action detection in untrimmed videos.
</summary>
    <author>
      <name>Wen Wang</name>
    </author>
    <author>
      <name>Yongjian Wu</name>
    </author>
    <author>
      <name>Haijun Liu</name>
    </author>
    <author>
      <name>Shiguang Wang</name>
    </author>
    <author>
      <name>Jian Cheng</name>
    </author>
    <link href="http://arxiv.org/abs/1810.08375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.08375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1906.02182v1</id>
    <updated>2019-06-05T02: 48: 37Z</updated>
    <published>2019-06-05T02: 48: 37Z</published>
    <title>Two-Stream Region Convolutional 3D Network for Temporal Activity
  Detection</title>
    <summary>  We address the problem of temporal activity detection in continuous,
untrimmed video streams. This is a difficult task that requires extracting
meaningful spatio-temporal features to capture activities, accurately
localizing the start and end times of each activity. We introduce a new model,
Region Convolutional 3D Network (R-C3D), which encodes the video streams using
a three-dimensional fully convolutional network, then generates candidate
temporal regions containing activities and finally classifies selected regions
into specific activities. Computation is saved due to the sharing of
convolutional features between the proposal and the classification pipelines.
We further improve the detection performance by efficiently integrating an
optical flow based motion stream with the original RGB stream. The two-stream
network is jointly optimized by fusing the flow and RGB feature maps at
different levels. Additionally, the training stage incorporates an online hard
example mining strategy to address the extreme foreground-background imbalance
typically observed in any detection pipeline. Instead of heuristically sampling
the candidate segments for the final activity classification stage, we rank
them according to their performance and only select the worst performers to
update the model. This improves the model without heavy hyper-parameter tuning.
Extensive experiments on three benchmark datasets are carried out to show
superior performance over existing temporal activity detection methods. Our
model achieves state-of-the-art results on the THUMOS'14 and Charades datasets.
We further demonstrate that our model is a general temporal activity detection
framework that does not rely on assumptions about particular dataset properties
by evaluating our approach on the ActivityNet dataset.
</summary>
    <author>
      <name>Huijuan Xu</name>
    </author>
    <author>
      <name>Abir Das</name>
    </author>
    <author>
      <name>Kate Saenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in TPAMI. arXiv admin note: substantial text overlap with
  arXiv: 1703.07814,
        1710.08011</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.02182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.02182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2311.12015v4</id>
    <updated>2024-09-26T18: 35: 52Z</updated>
    <published>2023-11-20T18: 54: 39Z</published>
    <title>GPT-4V(ision) for Robotics: Multimodal Task Planning from Human
  Demonstration</title>
    <summary>  We introduce a pipeline that enhances a general-purpose Vision Language
Model, GPT-4V(ision), to facilitate one-shot visual teaching for robotic
manipulation. This system analyzes videos of humans performing tasks and
outputs executable robot programs that incorporate insights into affordances.
The process begins with GPT-4V analyzing the videos to obtain textual
explanations of environmental and action details. A GPT-4-based task planner
then encodes these details into a symbolic task plan. Subsequently, vision
systems spatially and temporally ground the task plan in the videos. Objects
are identified using an open-vocabulary object detector, and hand-object
interactions are analyzed to pinpoint moments of grasping and releasing. This
spatiotemporal grounding allows for the gathering of affordance information
(e.g., grasp types, waypoints, and body postures) critical for robot execution.
Experiments across various scenarios demonstrate the method's efficacy in
enabling real robots to operate from one-shot human demonstrations. Meanwhile,
quantitative tests have revealed instances of hallucination in GPT-4V,
highlighting the importance of incorporating human supervision within the
pipeline. The prompts of GPT-4V/GPT-4 are available at this project page:
https: //microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/
</summary>
    <author>
      <name>Naoki Wake</name>
    </author>
    <author>
      <name>Atsushi Kanehira</name>
    </author>
    <author>
      <name>Kazuhiro Sasabuchi</name>
    </author>
    <author>
      <name>Jun Takamatsu</name>
    </author>
    <author>
      <name>Katsushi Ikeuchi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LRA.2024.3477090</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LRA.2024.3477090" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,
        10 figures,
        3 tables. Published in IEEE Robotics and
  Automation Letters (RA-L) (in press). Last updated on September 26th,
        2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.12015v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.12015v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1612.04854v1</id>
    <updated>2016-12-14T21: 46: 09Z</updated>
    <published>2016-12-14T21: 46: 09Z</published>
    <title>Temporal-Needle: A view and appearance invariant video descriptor</title>
    <summary>  The ability to detect similar actions across videos can be very useful for
real-world applications in many fields. However, this task is still challenging
for existing systems, since videos that present the same action, can be taken
from significantly different viewing directions, performed by different actors
and backgrounds and under various video qualities. Video descriptors play a
significant role in these systems. In this work we propose the
"temporal-needle" descriptor which captures the dynamic behavior, while being
invariant to viewpoint and appearance. The descriptor is computed using multi
temporal scales of the video and by computing self-similarity for every patch
through time in every temporal scale. The descriptor is computed for every
pixel in the video. However, to find similar actions across videos, we consider
only a small subset of the descriptors - the statistical significant
descriptors. This allow us to find good correspondences across videos more
efficiently. Using the descriptor, we were able to detect the same behavior
across videos in a variety of scenarios. We demonstrate the use of the
descriptor in tasks such as temporal and spatial alignment, action detection
and even show its potential in unsupervised video clustering into categories.
In this work we handled only videos taken with stationary cameras, but the
descriptor can be extended to handle moving camera as well.
</summary>
    <author>
      <name>Michal Yarom</name>
    </author>
    <author>
      <name>Michal Irani</name>
    </author>
    <link href="http://arxiv.org/abs/1612.04854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1708.02349v1</id>
    <updated>2017-08-08T01: 46: 03Z</updated>
    <published>2017-08-08T01: 46: 03Z</published>
    <title>Temporal Context Network for Activity Localization in Videos</title>
    <summary>  We present a Temporal Context Network (TCN) for precise temporal localization
of human activities. Similar to the Faster-RCNN architecture, proposals are
placed at equal intervals in a video which span multiple temporal scales. We
propose a novel representation for ranking these proposals. Since pooling
features only inside a segment is not sufficient to predict activity
boundaries, we construct a representation which explicitly captures context
around a proposal for ranking it. For each temporal segment inside a proposal,
features are uniformly sampled at a pair of scales and are input to a temporal
convolutional neural network for classification. After ranking proposals,
non-maximum suppression is applied and classification is performed to obtain
final detections. TCN outperforms state-of-the-art methods on the ActivityNet
dataset and the THUMOS14 dataset.
</summary>
    <author>
      <name>Xiyang Dai</name>
    </author>
    <author>
      <name>Bharat Singh</name>
    </author>
    <author>
      <name>Guyue Zhang</name>
    </author>
    <author>
      <name>Larry S. Davis</name>
    </author>
    <author>
      <name>Yan Qiu Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ICCV 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.02349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.02349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2409.18408v1</id>
    <updated>2024-09-27T02: 54: 24Z</updated>
    <published>2024-09-27T02: 54: 24Z</published>
    <title>Query matching for spatio-temporal action detection with query-based
  object detector</title>
    <summary>  In this paper, we propose a method that extends the query-based object
detection model, DETR, to spatio-temporal action detection, which requires
maintaining temporal consistency in videos. Our proposed method applies DETR to
each frame and uses feature shift to incorporate temporal information. However,
DETR's object queries in each frame may correspond to different objects, making
a simple feature shift ineffective. To overcome this issue, we propose query
matching across different frames, ensuring that queries for the same object are
matched and used for the feature shift. Experimental results show that
performance on the JHMDB21 dataset improves significantly when query features
are shifted using the proposed query matching.
</summary>
    <author>
      <name>Shimon Hori</name>
    </author>
    <author>
      <name>Kazuki Omi</name>
    </author>
    <author>
      <name>Toru Tamaki</name>
    </author>
    <link href="http://arxiv.org/abs/2409.18408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.18408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1612.05038v1</id>
    <updated>2016-12-15T12: 15: 36Z</updated>
    <published>2016-12-15T12: 15: 36Z</published>
    <title>Objective Micro-Facial Movement Detection Using FACS-Based Regions and
  Baseline Evaluation</title>
    <summary>  Micro-facial expressions are regarded as an important human behavioural event
that can highlight emotional deception. Spotting these movements is difficult
for humans and machines, however research into using computer vision to detect
subtle facial expressions is growing in popularity. This paper proposes an
individualised baseline micro-movement detection method using 3D Histogram of
Oriented Gradients (3D HOG) temporal difference method. We define a face
template consisting of 26 regions based on the Facial Action Coding System
(FACS). We extract the temporal features of each region using 3D HOG. Then, we
use Chi-square distance to find subtle facial motion in the local regions.
Finally, an automatic peak detector is used to detect micro-movements above the
newly proposed adaptive baseline threshold. The performance is validated on two
FACS coded datasets: SAMM and CASME II. This objective method focuses on the
movement of the 26 face regions. When comparing with the ground truth, the best
result was an AUC of 0.7512 and 0.7261 on SAMM and CASME II, respectively. The
results show that 3D HOG outperformed for micro-movement detection, compared to
state-of-the-art feature representations: Local Binary Patterns in Three
Orthogonal Planes and Histograms of Oriented Optical Flow.
</summary>
    <author>
      <name>Adrian K. Davison</name>
    </author>
    <author>
      <name>Cliff Lansley</name>
    </author>
    <author>
      <name>Choon Ching Ng</name>
    </author>
    <author>
      <name>Kevin Tan</name>
    </author>
    <author>
      <name>Moi Hoon Yap</name>
    </author>
    <link href="http://arxiv.org/abs/1612.05038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1907.01847v1</id>
    <updated>2019-07-03T10: 55: 35Z</updated>
    <published>2019-07-03T10: 55: 35Z</published>
    <title>Deformable Tube Network for Action Detection in Videos</title>
    <summary>  We address the problem of spatio-temporal action detection in videos.
Existing methods commonly either ignore temporal context in action recognition
and localization, or lack the modelling of flexible shapes of action tubes. In
this paper, we propose a two-stage action detector called Deformable Tube
Network (DTN), which is composed of a Deformation Tube Proposal Network (DTPN)
and a Deformable Tube Recognition Network (DTRN) similar to the Faster R-CNN
architecture. In DTPN, a fast proposal linking algorithm (FTL) is introduced to
connect region proposals across frames to generate multiple deformable action
tube proposals. To perform action detection, we design a 3D convolution network
with skip connections for tube classification and regression. Modelling action
proposals as deformable tubes explicitly considers the shape of action tubes
compared to 3D cuboids. Moreover,
        3D convolution based recognition network can
learn temporal dynamics sufficiently for action detection. Our experimental
results show that we significantly outperform the methods with 3D cuboids and
obtain the state-of-the-art results on both UCF-Sports and AVA datasets.
</summary>
    <author>
      <name>Wei Li</name>
    </author>
    <author>
      <name>Zehuan Yuan</name>
    </author>
    <author>
      <name>Dashan Guo</name>
    </author>
    <author>
      <name>Lei Huang</name>
    </author>
    <author>
      <name>Xiangzhong Fang</name>
    </author>
    <author>
      <name>Changhu Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1907.01847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.01847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1909.09566v1</id>
    <updated>2019-09-20T15: 40: 05Z</updated>
    <published>2019-09-20T15: 40: 05Z</published>
    <title>Target-Specific Action Classification for Automated Assessment of Human
  Motor Behavior from Video</title>
    <summary>  Objective monitoring and assessment of human motor behavior can improve the
diagnosis and management of several medical conditions. Over the past decade,
significant advances have been made in the use of wearable technology for
continuously monitoring human motor behavior in free-living conditions.
However, wearable technology remains ill-suited for applications which require
monitoring and interpretation of complex motor behaviors (e.g. involving
interactions with the environment). Recent advances in computer vision and deep
learning have opened up new possibilities for extracting information from video
recordings. In this paper, we present a hierarchical vision-based behavior
phenotyping method for classification of basic human actions in video
recordings performed using a single RGB camera. Our method addresses challenges
associated with tracking multiple human actors and classification of actions in
videos recorded in changing environments with different fields of view. We
implement a cascaded pose tracker that uses temporal relationships between
detections for short-term tracking and appearance-based tracklet fusion for
long-term tracking. Furthermore, for action classification, we use pose
evolution maps derived from the cascaded pose tracker as low-dimensional and
interpretable representations of the movement sequences for training a
convolutional neural network. The cascaded pose tracker achieves an average
accuracy of 88\% in tracking the target human actor in our video recordings,
and overall system achieves average test accuracy of 84\% for target-specific
action classification in untrimmed video recordings.
</summary>
    <author>
      <name>Behnaz Rezaei</name>
    </author>
    <author>
      <name>Yiorgos Christakis</name>
    </author>
    <author>
      <name>Bryan Ho</name>
    </author>
    <author>
      <name>Kevin Thomas</name>
    </author>
    <author>
      <name>Kelley Erb</name>
    </author>
    <author>
      <name>Sarah Ostadabbas</name>
    </author>
    <author>
      <name>Shyamal Patel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This manuscript is under submission to the Sensors journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.09566v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.09566v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1909.01083v1</id>
    <updated>2019-08-20T14: 00: 10Z</updated>
    <published>2019-08-20T14: 00: 10Z</published>
    <title>General Relativity, Mental Causation, and Energy Conservation</title>
    <summary>  The conservation of energy and momentum have been viewed as undermining
Cartesian mental causation since the 1690s. Modern discussions of the topic
tend to use mid-19th century physics, neglecting both locality and Noether's
theorem and its converse. The relevance of General Relativity (GR) has rarely
been considered. But a few authors have proposed that the non-localizability of
gravitational energy and consequent lack of physically meaningful local
conservation laws answers the conservation objection to mental causation:
conservation already fails in GR, so there is nothing for minds to violate.
  This paper is motivated by two ideas. First, one might take seriously the
fact that GR formally has an infinity of rigid symmetries of the action and
hence, by Noether's first theorem, an infinity of conserved energies-momenta
(thus answering Schr\"{o}dinger's 1918 false-negative objection). Second, Sean
Carroll has asked (rhetorically) how one should modify the
Dirac-Maxwell-Einstein equations to describe mental causation. This paper uses
the generalized Bianchi identities to show that General Relativity tends to
exclude, not facilitate, such Cartesian mental causation. In the simplest case,
Cartesian mental influence must be spatio-temporally constant, and hence 0. The
difficulty may diminish for more complicated models. Its persuasiveness is also
affected by larger world-view considerations.
  The new general relativistic objection provides some support for realism
about gravitational energy-momentum in GR (taking pseudotensor laws seriously).
Such realism also answers an objection to theories of causation involving
conserved quantities, because energies-momenta would be conserved even in GR.
</summary>
    <author>
      <name>J. Brian Pitts</name>
    </author>
    <link href="http://arxiv.org/abs/1909.01083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.01083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.hist-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.hist-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.pop-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1703.07814v2</id>
    <updated>2017-08-04T22: 37: 54Z</updated>
    <published>2017-03-22T18: 49: 05Z</published>
    <title>R-C3D: Region Convolutional 3D Network for Temporal Activity Detection</title>
    <summary>  We address the problem of activity detection in continuous, untrimmed video
streams. This is a difficult task that requires extracting meaningful
spatio-temporal features to capture activities, accurately localizing the start
and end times of each activity. We introduce a new model, Region Convolutional
3D Network (R-C3D), which encodes the video streams using a three-dimensional
fully convolutional network, then generates candidate temporal regions
containing activities, and finally classifies selected regions into specific
activities. Computation is saved due to the sharing of convolutional features
between the proposal and the classification pipelines. The entire model is
trained end-to-end with jointly optimized localization and classification
losses. R-C3D is faster than existing methods (569 frames per second on a
single Titan X Maxwell GPU) and achieves state-of-the-art results on THUMOS'14.
We further demonstrate that our model is a general activity detection framework
that does not rely on assumptions about particular dataset properties by
evaluating our approach on ActivityNet and Charades. Our code is available at
http: //ai.bu.edu/r-c3d/.
</summary>
    <author>
      <name>Huijuan Xu</name>
    </author>
    <author>
      <name>Abir Das</name>
    </author>
    <author>
      <name>Kate Saenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV 2017 Camera Ready Version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on Computer Vision
  (ICCV),
        2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1703.07814v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07814v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2207.08625v1</id>
    <updated>2022-07-18T14: 18: 13Z</updated>
    <published>2022-07-18T14: 18: 13Z</published>
    <title>Unifying Event Detection and Captioning as Sequence Generation via
  Pre-Training</title>
    <summary>  Dense video captioning aims to generate corresponding text descriptions for a
series of events in the untrimmed video, which can be divided into two
sub-tasks, event detection and event captioning. Unlike previous works that
tackle the two sub-tasks separately, recent works have focused on enhancing the
inter-task association between the two sub-tasks. However, designing inter-task
interactions for event detection and captioning is not trivial due to the large
differences in their task specific solutions. Besides, previous event detection
methods normally ignore temporal dependencies between events, leading to event
redundancy or inconsistency problems. To tackle above the two defects, in this
paper, we define event detection as a sequence generation task and propose a
unified pre-training and fine-tuning framework to naturally enhance the
inter-task association between event detection and captioning. Since the model
predicts each event with previous events as context, the inter-dependency
between events is fully exploited and thus our model can detect more diverse
and consistent events in the video. Experiments on the ActivityNet dataset show
that our model outperforms the state-of-the-art methods, and can be further
boosted when pre-trained on extra large-scale video-text data. Code is
available at \url{https: //github.com/QiQAng/UEDVC}.
</summary>
    <author>
      <name>Qi Zhang</name>
    </author>
    <author>
      <name>Yuqing Song</name>
    </author>
    <author>
      <name>Qin Jin</name>
    </author>
    <link href="http://arxiv.org/abs/2207.08625v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.08625v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1905.11824v2</id>
    <updated>2021-06-06T19: 56: 25Z</updated>
    <published>2019-05-28T13: 56: 44Z</published>
    <title>Attacker Behaviour Profiling using Stochastic Ensemble of Hidden Markov
  Models</title>
    <summary>  Cyber threat intelligence is one of the emerging areas of focus in
information security. Much of the recent work has focused on rule-based methods
and detection of network attacks using Intrusion Detection algorithms. In this
paper we propose a framework for inspecting and modelling the behavioural
aspect of an attacker to obtain better insight predictive power on his future
actions. For modelling we propose a novel semi-supervised algorithm called
Fusion Hidden Markov Model (FHMM) which is more robust to noise, requires
comparatively less training time, and utilizes the benefits of ensemble
learning to better model temporal relationships in data. This paper evaluates
the performances of FHMM and compares it with both traditional algorithms like
Markov Chain, Hidden Markov Model (HMM) and recently developed Deep Recurrent
Neural Network (Deep RNN) architectures. We conduct the experiments on dataset
consisting of real data attacks on a Cowrie honeypot system. FHMM provides
accuracy comparable to deep RNN architectures at significant lower training
time. Given these experimental results, we recommend using FHMM for modelling
discrete temporal data for significantly faster training and better performance
than existing methods.
</summary>
    <author>
      <name>Soham Deshmukh</name>
    </author>
    <author>
      <name>Rahul Rade</name>
    </author>
    <author>
      <name>Faruk Kazi</name>
    </author>
    <link href="http://arxiv.org/abs/1905.11824v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.11824v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2012.08097v1</id>
    <updated>2020-12-15T05: 21: 50Z</updated>
    <published>2020-12-15T05: 21: 50Z</published>
    <title>Towards Improving Spatiotemporal Action Recognition in Videos</title>
    <summary>  Spatiotemporal action recognition deals with locating and classifying actions
in videos. Motivated by the latest state-of-the-art real-time object detector
You Only Watch Once (YOWO), we aim to modify its structure to increase action
detection precision and reduce computational time. Specifically, we propose
four novel approaches in attempts to improve YOWO and address the imbalanced
class issue in videos by modifying the loss function. We consider two
moderate-sized datasets to apply our modification of YOWO - the popular
Joint-annotated Human Motion Data Base (J-HMDB-21) and a private dataset of
restaurant video footage provided by a Carnegie Mellon University-based
startup, Agot.AI. The latter involves fast-moving actions with small objects as
well as unbalanced data classes, making the task of action localization more
challenging. We implement our proposed methods in the GitHub repository
https: //github.com/stoneMo/YOWOv2.
</summary>
    <author>
      <name>Shentong Mo</name>
    </author>
    <author>
      <name>Xiaoqing Tan</name>
    </author>
    <author>
      <name>Jingfei Xia</name>
    </author>
    <author>
      <name>Pinxu Ren</name>
    </author>
    <link href="http://arxiv.org/abs/2012.08097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.08097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2502.15462v1</id>
    <updated>2025-02-21T13: 41: 38Z</updated>
    <published>2025-02-21T13: 41: 38Z</published>
    <title>Game State and Spatio-temporal Action Detection in Soccer using Graph
  Neural Networks and 3D Convolutional Networks</title>
    <summary>  Soccer analytics rely on two data sources: the player positions on the pitch
and the sequences of events they perform. With around 2000 ball events per
game, their precise and exhaustive annotation based on a monocular video stream
remains a tedious and costly manual task. While state-of-the-art
spatio-temporal action detection methods show promise for automating this task,
they lack contextual understanding of the game. Assuming professional players'
behaviors are interdependent, we hypothesize that incorporating surrounding
players' information such as positions, velocity and team membership can
enhance purely visual predictions. We propose a spatio-temporal action
detection approach that combines visual and game state information via Graph
Neural Networks trained end-to-end with state-of-the-art 3D CNNs, demonstrating
improved metrics through game state integration.
</summary>
    <author>
      <name>Jeremie Ochin</name>
    </author>
    <author>
      <name>Guillaume Devineau</name>
    </author>
    <author>
      <name>Bogdan Stanciulescu</name>
    </author>
    <author>
      <name>Sotiris Manitsaris</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 14th International Conference on Pattern
  Recognition Applications and Methods (2025), ISBN 978-989-758-730-6, ISSN
  2184-4313, pages 636-646</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2502.15462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.15462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2411.19714v2</id>
    <updated>2025-01-13T02: 43: 47Z</updated>
    <published>2024-11-29T14: 02: 00Z</published>
    <title>The Streetscape Application Services Stack (SASS): Towards a Distributed
  Sensing Architecture for Urban Applications</title>
    <summary>  As urban populations grow, cities are becoming more complex, driving the
deployment of interconnected sensing systems to realize the vision of smart
cities. These systems aim to improve safety, mobility, and quality of life
through applications that integrate diverse sensors with real-time
decision-making. Streetscape applications-focusing on challenges like
pedestrian safety and adaptive traffic management-depend on managing
distributed, heterogeneous sensor data, aligning information across time and
space, and enabling real-time processing. These tasks are inherently complex
and often difficult to scale. The Streetscape Application Services Stack (SASS)
addresses these challenges with three core services: multimodal data
synchronization, spatiotemporal data fusion, and distributed edge computing. By
structuring these capabilities as clear, composable abstractions with clear
semantics, SASS allows developers to scale streetscape applications efficiently
while minimizing the complexity of multimodal integration.
  We evaluated SASS in two real-world testbed environments: a controlled
parking lot and an urban intersection in a major U.S. city. These testbeds
allowed us to test SASS under diverse conditions, demonstrating its practical
applicability. The Multimodal Data Synchronization service reduced temporal
misalignment errors by 88%, achieving synchronization accuracy within 50
milliseconds. Spatiotemporal Data Fusion service improved detection accuracy
for pedestrians and vehicles by over 10%, leveraging multicamera integration.
The Distributed Edge Computing service increased system throughput by more than
an order of magnitude. Together, these results show how SASS provides the
abstractions and performance needed to support real-time, scalable urban
applications, bridging the gap between sensing infrastructure and actionable
streetscape intelligence.
</summary>
    <author>
      <name>Navid Salami Pargoo</name>
    </author>
    <author>
      <name>Mahshid Ghasemi</name>
    </author>
    <author>
      <name>Shuren Xia</name>
    </author>
    <author>
      <name>Mehmet Kerem Turkcan</name>
    </author>
    <author>
      <name>Taqiya Ehsan</name>
    </author>
    <author>
      <name>Chengbo Zang</name>
    </author>
    <author>
      <name>Yuan Sun</name>
    </author>
    <author>
      <name>Javad Ghaderi</name>
    </author>
    <author>
      <name>Gil Zussman</name>
    </author>
    <author>
      <name>Zoran Kostic</name>
    </author>
    <author>
      <name>Jorge Ortiz</name>
    </author>
    <link href="http://arxiv.org/abs/2411.19714v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.19714v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2412.11467v1</id>
    <updated>2024-12-16T05: 48: 44Z</updated>
    <published>2024-12-16T05: 48: 44Z</published>
    <title>Exploring Temporal Event Cues for Dense Video Captioning in Cyclic
  Co-learning</title>
    <summary>  Dense video captioning aims to detect and describe all events in untrimmed
videos. This paper presents a dense video captioning network called
Multi-Concept Cyclic Learning (MCCL), which aims to: (1) detect multiple
concepts at the frame level, using these concepts to enhance video features and
provide temporal event cues; and (2) design cyclic co-learning between the
generator and the localizer within the captioning network to promote semantic
perception and event localization. Specifically, we perform weakly supervised
concept detection for each frame, and the detected concept embeddings are
integrated into the video features to provide event cues. Additionally,
video-level concept contrastive learning is introduced to obtain more
discriminative concept embeddings. In the captioning network, we establish a
cyclic co-learning strategy where the generator guides the localizer for event
localization through semantic matching, while the localizer enhances the
generator's event semantic perception through location matching, making
semantic perception and event localization mutually beneficial. MCCL achieves
state-of-the-art performance on the ActivityNet Captions and YouCook2 datasets.
Extensive experiments demonstrate its effectiveness and interpretability.
</summary>
    <author>
      <name>Zhuyang Xie</name>
    </author>
    <author>
      <name>Yan Yang</name>
    </author>
    <author>
      <name>Yankai Yu</name>
    </author>
    <author>
      <name>Jie Wang</name>
    </author>
    <author>
      <name>Yongquan Jiang</name>
    </author>
    <author>
      <name>Xiao Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at AAAI 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.11467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.11467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2107.04362v1</id>
    <updated>2021-07-09T11: 10: 11Z</updated>
    <published>2021-07-09T11: 10: 11Z</published>
    <title>RGB Stream Is Enough for Temporal Action Detection</title>
    <summary>  State-of-the-art temporal action detectors to date are based on two-stream
input including RGB frames and optical flow. Although combining RGB frames and
optical flow boosts performance significantly, optical flow is a hand-designed
representation which not only requires heavy computation, but also makes it
methodologically unsatisfactory that two-stream methods are often not learned
end-to-end jointly with the flow. In this paper, we argue that optical flow is
dispensable in high-accuracy temporal action detection and image level data
augmentation (ILDA) is the key solution to avoid performance degradation when
optical flow is removed. To evaluate the effectiveness of ILDA, we design a
simple yet efficient one-stage temporal action detector based on single RGB
stream named DaoTAD. Our results show that when trained with ILDA, DaoTAD has
comparable accuracy with all existing state-of-the-art two-stream detectors
while surpassing the inference speed of previous methods by a large margin and
the inference speed is astounding 6668 fps on GeForce GTX 1080 Ti. Code is
available at \url{https: //github.com/Media-Smart/vedatad}.
</summary>
    <author>
      <name>Chenhao Wang</name>
    </author>
    <author>
      <name>Hongxiang Cai</name>
    </author>
    <author>
      <name>Yuxin Zou</name>
    </author>
    <author>
      <name>Yichao Xiong</name>
    </author>
    <link href="http://arxiv.org/abs/2107.04362v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.04362v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2201.05290v1</id>
    <updated>2022-01-14T03: 35: 22Z</updated>
    <published>2022-01-14T03: 35: 22Z</published>
    <title>Argus++: Robust Real-time Activity Detection for Unconstrained Video
  Streams with Overlapping Cube Proposals</title>
    <summary>  Activity detection is one of the attractive computer vision tasks to exploit
the video streams captured by widely installed cameras. Although achieving
impressive performance, conventional activity detection algorithms are usually
designed under certain constraints, such as using trimmed and/or
object-centered video clips as inputs. Therefore, they failed to deal with the
multi-scale multi-instance cases in real-world unconstrained video streams,
which are untrimmed and have large field-of-views. Real-time requirements for
streaming analysis also mark brute force expansion of them unfeasible.
  To overcome these issues, we propose Argus++, a robust real-time activity
detection system for analyzing unconstrained video streams. The design of
Argus++ introduces overlapping spatio-temporal cubes as an intermediate concept
of activity proposals to ensure coverage and completeness of activity detection
through over-sampling. The overall system is optimized for real-time processing
on standalone consumer-level hardware. Extensive experiments on different
surveillance and driving scenarios demonstrated its superior performance in a
series of activity detection benchmarks, including CVPR ActivityNet ActEV 2021,
NIST ActEV SDL UF/KF, TRECVID ActEV 2020/2021, and ICCV ROAD 2021.
</summary>
    <author>
      <name>Lijun Yu</name>
    </author>
    <author>
      <name>Yijun Qian</name>
    </author>
    <author>
      <name>Wenhe Liu</name>
    </author>
    <author>
      <name>Alexander G. Hauptmann</name>
    </author>
    <link href="http://arxiv.org/abs/2201.05290v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.05290v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2405.02346v1</id>
    <updated>2024-05-02T15: 15: 05Z</updated>
    <published>2024-05-02T15: 15: 05Z</published>
    <title>Temporal assessment of malicious behaviors: application to turnout field
  data monitoring</title>
    <summary>  Monitored data collected from railway turnouts are vulnerable to
cyberattacks: attackers may either conceal failures or trigger unnecessary
maintenance actions. To address this issue, a cyberattack investigation method
is proposed based on predictions made from the temporal evolution of the
turnout behavior. These predictions are then compared to the field acquired
data to detect any discrepancy. This method is illustrated on a collection of
real-life data.
</summary>
    <author>
      <name>Sara Abdellaoui</name>
    </author>
    <author>
      <name>Emil Dumitrescu</name>
    </author>
    <author>
      <name>Cédric Escudero</name>
    </author>
    <author>
      <name>Eric Zamaï</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in the International Conference on Control,
  Automation and Diagnosis (ICCAD24)</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.02346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.02346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2412.20042v1</id>
    <updated>2024-12-28T06: 13: 44Z</updated>
    <published>2024-12-28T06: 13: 44Z</published>
    <title>DAVE: Diverse Atomic Visual Elements Dataset with High Representation of
  Vulnerable Road Users in Complex and Unpredictable Environments</title>
    <summary>  Most existing traffic video datasets including Waymo are structured, focusing
predominantly on Western traffic, which hinders global applicability.
Specifically, most Asian scenarios are far more complex, involving numerous
objects with distinct motions and behaviors. Addressing this gap, we present a
new dataset, DAVE, designed for evaluating perception methods with high
representation of Vulnerable Road Users (VRUs: e.g. pedestrians, animals,
motorbikes, and bicycles) in complex and unpredictable environments. DAVE is a
manually annotated dataset encompassing 16 diverse actor categories (spanning
animals, humans, vehicles, etc.) and 16 action types (complex and rare cases
like cut-ins, zigzag movement, U-turn, etc.), which require high reasoning
ability. DAVE densely annotates over 13 million bounding boxes (bboxes) actors
with identification, and more than 1.6 million boxes are annotated with both
actor identification and action/behavior details. The videos within DAVE are
collected based on a broad spectrum of factors, such as weather conditions, the
time of day, road scenarios, and traffic density. DAVE can benchmark video
tasks like Tracking, Detection, Spatiotemporal Action Localization,
Language-Visual Moment retrieval, and Multi-label Video Action Recognition.
Given the critical importance of accurately identifying VRUs to prevent
accidents and ensure road safety, in DAVE, vulnerable road users constitute
41.13% of instances, compared to 23.71% in Waymo. DAVE provides an invaluable
resource for the development of more sensitive and accurate visual perception
algorithms in the complex real world. Our experiments show that existing
methods suffer degradation in performance when evaluated on DAVE, highlighting
its benefit for future video recognition research.
</summary>
    <author>
      <name>Xijun Wang</name>
    </author>
    <author>
      <name>Pedro Sandoval-Segura</name>
    </author>
    <author>
      <name>Chengyuan Zhang</name>
    </author>
    <author>
      <name>Junyun Huang</name>
    </author>
    <author>
      <name>Tianrui Guan</name>
    </author>
    <author>
      <name>Ruiqi Xian</name>
    </author>
    <author>
      <name>Fuxiao Liu</name>
    </author>
    <author>
      <name>Rohan Chandra</name>
    </author>
    <author>
      <name>Boqing Gong</name>
    </author>
    <author>
      <name>Dinesh Manocha</name>
    </author>
    <link href="http://arxiv.org/abs/2412.20042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.20042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2004.04383v2</id>
    <updated>2021-02-01T15: 19: 40Z</updated>
    <published>2020-04-09T06: 49: 29Z</published>
    <title>Security proof of practical quantum key distribution with
  detection-efficiency mismatch</title>
    <summary>  Quantum key distribution (QKD) protocols with threshold detectors are driving
high-performance QKD demonstrations. The corresponding security proofs usually
assume that all physical detectors have the same detection efficiency. However,
the efficiencies of the detectors used in practice might show a mismatch
depending on the manufacturing and setup of these detectors. A mismatch can
also be induced as the different spatial-temporal modes of an incoming signal
might couple differently to a detector. Here we develop a method that allows to
provide security proofs without the usual assumption. Our method can take the
detection-efficiency mismatch into account without having to restrict the
attack strategy of the adversary. Especially, we do not rely on any
photon-number cut-off of incoming signals such that our security proof is
directly applicable to practical situations. We illustrate our method for a
receiver that is designed for polarization encoding and is sensitive to a
number of spatial-temporal modes. In our detector model, the absence of quantum
interference between any pair of spatial-temporal modes is assumed. For a QKD
protocol with this detector model, we can perform a security proof with
characterized efficiency mismatch and without photon-number cut-off assumption.
Our method also shows that in the absence of efficiency mismatch in our
detector model, the key rate increases if the loss due to detection
inefficiency is assumed to be outside of the adversary's control, as compared
to the view where for a security proof this loss is attributed to the action of
the adversary.
</summary>
    <author>
      <name>Yanbao Zhang</name>
    </author>
    <author>
      <name>Patrick J. Coles</name>
    </author>
    <author>
      <name>Adam Winick</name>
    </author>
    <author>
      <name>Jie Lin</name>
    </author>
    <author>
      <name>Norbert Lutkenhaus</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevResearch.3.013076</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevResearch.3.013076" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages,
                10 figures (the version accepted to Phys. Rev. Research)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. Research 3,
                013076 (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2004.04383v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.04383v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1812.00622v1</id>
    <updated>2018-12-03T09: 29: 03Z</updated>
    <published>2018-12-03T09: 29: 03Z</published>
    <title>Protection of an information system by artificial intelligence: a
  three-phase approach based on behaviour analysis to detect a hostile scenario</title>
    <summary>  The analysis of the behaviour of individuals and entities (UEBA) is an area
of artificial intelligence that detects hostile actions (e.g. attacks, fraud,
influence, poisoning) due to the unusual nature of observed events, by affixing
to a signature-based operation. A UEBA process usually involves two phases,
learning and inference. Intrusion detection systems (IDS) available still
suffer from bias, including over-simplification of problems, underexploitation
of the AI potential, insufficient consideration of the temporality of events,
and perfectible management of the memory cycle of behaviours. In addition,
while an alert generated by a signature-based IDS can refer to the signature on
which the detection is based, the IDS in the UEBA domain produce results, often
associated with a score, whose explainable character is less obvious. Our
unsupervised approach is to enrich this process by adding a third phase to
correlate events (incongruities, weak signals) that are presumed to be linked
together, with the benefit of a reduction of false positives and negatives. We
also seek to avoid a so-called "boiled frog" bias inherent in continuous
learning. Our first results are interesting and have an explainable character,
both on synthetic and real data.
</summary>
    <author>
      <name>Jean-Philippe Fauvelle</name>
    </author>
    <author>
      <name>Alexandre Dey</name>
    </author>
    <author>
      <name>Sylvain Navers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French. European Cyber Week - C\&amp;ESAR Conference - Artificial
  Intelligence and Cybersecurity, Nov 2018, Rennes, France. 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.00622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.00622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2407.00099v1</id>
    <updated>2024-06-27T04: 29: 21Z</updated>
    <published>2024-06-27T04: 29: 21Z</published>
    <title>Optimal Transport for Latent Integration with An Application to
  Heterogeneous Neuronal Activity Data</title>
    <summary>  Detecting dynamic patterns of task-specific responses shared across
heterogeneous datasets is an essential and challenging problem in many
scientific applications in medical science and neuroscience. In our motivating
example of rodent electrophysiological data, identifying the dynamical patterns
in neuronal activity associated with ongoing cognitive demands and behavior is
key to uncovering the neural mechanisms of memory. One of the greatest
challenges in investigating a cross-subject biological process is that the
systematic heterogeneity across individuals could significantly undermine the
power of existing machine learning methods to identify the underlying
biological dynamics. In addition, many technically challenging neurobiological
experiments are conducted on only a handful of subjects where rich longitudinal
data are available for each subject. The low sample sizes of such experiments
could further reduce the power to detect common dynamic patterns among
subjects. In this paper, we propose a novel heterogeneous data integration
framework based on optimal transport to extract shared patterns in complex
biological processes. The key advantages of the proposed method are that it can
increase discriminating power in identifying common patterns by reducing
heterogeneity unrelated to the signal by aligning the extracted latent
spatiotemporal information across subjects. Our approach is effective even with
a small number of subjects, and does not require auxiliary matching information
for the alignment. In particular, our method can align longitudinal data across
heterogeneous subjects in a common latent space to capture the dynamics of
shared patterns while utilizing temporal dependency within subjects.
</summary>
    <author>
      <name>Yubai Yuan</name>
    </author>
    <author>
      <name>Babak Shahbaba</name>
    </author>
    <author>
      <name>Norbert Fortin</name>
    </author>
    <author>
      <name>Keiland Cooper</name>
    </author>
    <author>
      <name>Qing Nie</name>
    </author>
    <author>
      <name>Annie Qu</name>
    </author>
    <link href="http://arxiv.org/abs/2407.00099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.00099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2403.13678v1</id>
    <updated>2024-03-20T15: 37: 19Z</updated>
    <published>2024-03-20T15: 37: 19Z</published>
    <title>AUD-TGN: Advancing Action Unit Detection with Temporal Convolution and
  GPT-2 in Wild Audiovisual Contexts</title>
    <summary>  Leveraging the synergy of both audio data and visual data is essential for
understanding human emotions and behaviors, especially in in-the-wild setting.
Traditional methods for integrating such multimodal information often stumble,
leading to less-than-ideal outcomes in the task of facial action unit
detection. To overcome these shortcomings, we propose a novel approach
utilizing audio-visual multimodal data. This method enhances audio feature
extraction by leveraging Mel Frequency Cepstral Coefficients (MFCC) and Log-Mel
spectrogram features alongside a pre-trained VGGish network. Moreover, this
paper adaptively captures fusion features across modalities by modeling the
temporal relationships, and ultilizes a pre-trained GPT-2 model for
sophisticated context-aware fusion of multimodal information. Our method
notably improves the accuracy of AU detection by understanding the temporal and
contextual nuances of the data, showcasing significant advancements in the
comprehension of intricate scenarios. These findings underscore the potential
of integrating temporal dynamics and contextual interpretation, paving the way
for future research endeavors.
</summary>
    <author>
      <name>Jun Yu</name>
    </author>
    <author>
      <name>Zerui Zhang</name>
    </author>
    <author>
      <name>Zhihong Wei</name>
    </author>
    <author>
      <name>Gongpeng Zhao</name>
    </author>
    <author>
      <name>Zhongpeng Cai</name>
    </author>
    <author>
      <name>Yongqi Wang</name>
    </author>
    <author>
      <name>Guochen Xie</name>
    </author>
    <author>
      <name>Jichao Zhu</name>
    </author>
    <author>
      <name>Wangyuan Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2403.13678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.13678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1710.06236v1</id>
    <updated>2017-10-17T12: 41: 17Z</updated>
    <published>2017-10-17T12: 41: 17Z</published>
    <title>Single Shot Temporal Action Detection</title>
    <summary>  Temporal action detection is a very important yet challenging problem, since
videos in real applications are usually long, untrimmed and contain multiple
action instances. This problem requires not only recognizing action categories
but also detecting start time and end time of each action instance. Many
state-of-the-art methods adopt the "detection by classification" framework:
first do proposal, and then classify proposals. The main drawback of this
framework is that the boundaries of action instance proposals have been fixed
during the classification step. To address this issue, we propose a novel
Single Shot Action Detector (SSAD) network based on 1D temporal convolutional
layers to skip the proposal generation step via directly detecting action
instances in untrimmed video. On pursuit of designing a particular SSAD network
that can work effectively for temporal action detection, we empirically search
for the best network architecture of SSAD due to lacking existing models that
can be directly adopted. Moreover, we investigate into input feature types and
fusion strategies to further improve detection accuracy. We conduct extensive
experiments on two challenging datasets: THUMOS 2014 and MEXaction2. When
setting Intersection-over-Union threshold to 0.5 during evaluation, SSAD
significantly outperforms other state-of-the-art systems by increasing mAP from
19.0% to 24.6% on THUMOS 2014 and from 7.4% to 11.0% on MEXaction2.
</summary>
    <author>
      <name>Tianwei Lin</name>
    </author>
    <author>
      <name>Xu Zhao</name>
    </author>
    <author>
      <name>Zheng Shou</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3123266.3123343</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3123266.3123343" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Multimedia 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.06236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1611.09078v1</id>
    <updated>2016-11-28T11: 34: 20Z</updated>
    <published>2016-11-28T11: 34: 20Z</published>
    <title>Social Scene Understanding: End-to-End Multi-Person Action Localization
  and Collective Activity Recognition</title>
    <summary>  We present a unified framework for understanding human social behaviors in
raw image sequences. Our model jointly detects multiple individuals, infers
their social actions, and estimates the collective actions with a single
feed-forward pass through a neural network. We propose a single architecture
that does not rely on external detection algorithms but rather is trained
end-to-end to generate dense proposal maps that are refined via a novel
inference scheme. The temporal consistency is handled via a person-level
matching Recurrent Neural Network. The complete model takes as input a sequence
of frames and outputs detections along with the estimates of individual actions
and collective activities. We demonstrate state-of-the-art performance of our
algorithm on multiple publicly available benchmarks.
</summary>
    <author>
      <name>Timur Bagautdinov</name>
    </author>
    <author>
      <name>Alexandre Alahi</name>
    </author>
    <author>
      <name>François Fleuret</name>
    </author>
    <author>
      <name>Pascal Fua</name>
    </author>
    <author>
      <name>Silvio Savarese</name>
    </author>
    <link href="http://arxiv.org/abs/1611.09078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2303.09917v2</id>
    <updated>2023-03-20T14: 39: 32Z</updated>
    <published>2023-03-16T13: 43: 02Z</published>
    <title>Vision Transformer for Action Units Detection</title>
    <summary>  Facial Action Units detection (FAUs) represents a fine-grained classification
problem that involves identifying different units on the human face, as defined
by the Facial Action Coding System. In this paper, we present a simple yet
efficient Vision Transformer-based approach for addressing the task of Action
Units (AU) detection in the context of Affective Behavior Analysis in-the-wild
(ABAW) competition. We employ the Video Vision Transformer(ViViT) Network to
capture the temporal facial change in the video. Besides, to reduce massive
size of the Vision Transformers model, we replace the ViViT feature extraction
layers with the CNN backbone (Regnet). Our model outperform the baseline model
of ABAW 2023 challenge, with a notable 14% difference in result. Furthermore,
the achieved results are comparable to those of the top three teams in the
previous ABAW 2022 challenge.
</summary>
    <author>
      <name>Tu Vu</name>
    </author>
    <author>
      <name>Van Thong Huynh</name>
    </author>
    <author>
      <name>Soo Hyung Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Will be updated</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.09917v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.09917v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2303.10644v3</id>
    <updated>2023-06-05T04: 49: 34Z</updated>
    <published>2023-03-19T12: 28: 59Z</published>
    <title>Spatio-Temporal AU Relational Graph Representation Learning For Facial
  Action Units Detection</title>
    <summary>  This paper presents our Facial Action Units (AUs) detection submission to the
fifth Affective Behavior Analysis in-the-wild Competition (ABAW). Our approach
consists of three main modules: (i) a pre-trained facial representation encoder
which produce a strong facial representation from each input face image in the
input sequence; (ii) an AU-specific feature generator that specifically learns
a set of AU features from each facial representation; and (iii) a
spatio-temporal graph learning module that constructs a spatio-temporal graph
representation. This graph representation describes AUs contained in all frames
and predicts the occurrence of each AU based on both the modeled spatial
information within the corresponding face and the learned temporal dynamics
among frames. The experimental results show that our approach outperformed the
baseline and the spatio-temporal graph representation learning allows our model
to generate the best results among all ablated systems. Our model ranks at the
4th place in the AU recognition track at the 5th ABAW Competition. Our code is
publicly available at https: //github.com/wzh125/ABAW-5.
</summary>
    <author>
      <name>Zihan Wang</name>
    </author>
    <author>
      <name>Siyang Song</name>
    </author>
    <author>
      <name>Cheng Luo</name>
    </author>
    <author>
      <name>Yuzhi Zhou</name>
    </author>
    <author>
      <name>Shiling Wu</name>
    </author>
    <author>
      <name>Weicheng Xie</name>
    </author>
    <author>
      <name>Linlin Shen</name>
    </author>
    <link href="http://arxiv.org/abs/2303.10644v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.10644v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T40" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2008.11149v1</id>
    <updated>2020-08-25T16: 30: 01Z</updated>
    <published>2020-08-25T16: 30: 01Z</published>
    <title>Spatiotemporal Action Recognition in Restaurant Videos</title>
    <summary>  Spatiotemporal action recognition is the task of locating and classifying
actions in videos. Our project applies this task to analyzing video footage of
restaurant workers preparing food, for which potential applications include
automated checkout and inventory management. Such videos are quite different
from the standardized datasets that researchers are used to, as they involve
small objects, rapid actions, and notoriously unbalanced data classes. We
explore two approaches. The first approach involves the familiar object
detector You Only Look Once, and another applying a recently proposed analogue
for action recognition, You Only Watch Once. In the first, we design and
implement a novel, recurrent modification of YOLO using convolutional LSTMs and
explore the various subtleties in the training of such a network. In the
second, we study the ability of YOWOs three dimensional convolutions to capture
the spatiotemporal features of our unique dataset
</summary>
    <author>
      <name>Akshat Gupta</name>
    </author>
    <author>
      <name>Milan Desai</name>
    </author>
    <author>
      <name>Wusheng Liang</name>
    </author>
    <author>
      <name>Magesh Kannan</name>
    </author>
    <link href="http://arxiv.org/abs/2008.11149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1605.05197v2</id>
    <updated>2017-05-23T19: 19: 23Z</updated>
    <published>2016-05-17T14: 55: 03Z</published>
    <title>Human Action Localization with Sparse Spatial Supervision</title>
    <summary>  We introduce an approach for spatio-temporal human action localization using
sparse spatial supervision. Our method leverages the large amount of annotated
humans available today and extracts human tubes by combining a state-of-the-art
human detector with a tracking-by-detection approach. Given these high-quality
human tubes and temporal supervision, we select positive and negative tubes
with very sparse spatial supervision, i.e., only one spatially annotated frame
per instance. The selected tubes allow us to effectively learn a
spatio-temporal action detector based on dense trajectories or CNNs. We conduct
experiments on existing action localization benchmarks: UCF-Sports, J-HMDB and
UCF-101. Our results show that our approach, despite using sparse spatial
supervision, performs on par with methods using full supervision, i.e., one
bounding box annotation per frame. To further validate our method, we introduce
DALY (Daily Action Localization in YouTube), a dataset for realistic action
localization in space and time. It contains high quality temporal and spatial
annotations for 3.6k instances of 10 actions in 31 hours of videos (3.3M
frames). It is an order of magnitude larger than existing datasets, with more
diversity in appearance and long untrimmed videos.
</summary>
    <author>
      <name>Philippe Weinzaepfel</name>
    </author>
    <author>
      <name>Xavier Martin</name>
    </author>
    <author>
      <name>Cordelia Schmid</name>
    </author>
    <link href="http://arxiv.org/abs/1605.05197v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05197v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1908.08178v1</id>
    <updated>2019-08-22T03: 13: 22Z</updated>
    <published>2019-08-22T03: 13: 22Z</published>
    <title>Multi-Stream Single Shot Spatial-Temporal Action Detection</title>
    <summary>  We present a 3D Convolutional Neural Networks (CNNs) based single shot
detector for spatial-temporal action detection tasks. Our model includes: (1)
two short-term appearance and motion streams, with single RGB and optical flow
image input separately, in order to capture the spatial and temporal
information for the current frame; (2) two long-term 3D ConvNet based stream,
working on sequences of continuous RGB and optical flow images to capture the
context from past frames. Our model achieves strong performance for action
detection in video and can be easily integrated into any current two-stream
action detection methods. We report a frame-mAP of 71.30% on the challenging
UCF101-24 actions dataset, achieving the state-of-the-art result of the
one-stage methods. To the best of our knowledge, our work is the first system
that combined 3D CNN and SSD in action detection tasks.
</summary>
    <author>
      <name>Pengfei Zhang</name>
    </author>
    <author>
      <name>Yu Cao</name>
    </author>
    <author>
      <name>Benyuan Liu</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">26th IEEE International Conference on Image Processing (ICIP 2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1908.08178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.08178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2308.09268v1</id>
    <updated>2023-08-18T03: 14: 05Z</updated>
    <published>2023-08-18T03: 14: 05Z</published>
    <title>Progression-Guided Temporal Action Detection in Videos</title>
    <summary>  We present a novel framework, Action Progression Network (APN), for temporal
action detection (TAD) in videos. The framework locates actions in videos by
detecting the action evolution process. To encode the action evolution, we
quantify a complete action process into 101 ordered stages (0\%,
                1\%, ...,
                100\%), referred to as action progressions. We then train a neural network to
recognize the action progressions. The framework detects action boundaries by
detecting complete action processes in the videos, e.g., a video segment with
detected action progressions closely follow the sequence 0\%,
                1\%, ...,
                100\%.
The framework offers three major advantages: (1) Our neural networks are
trained end-to-end, contrasting conventional methods that optimize modules
separately; (2) The APN is trained using action frames exclusively, enabling
models to be trained on action classification datasets and robust to videos
with temporal background styles differing from those in training; (3) Our
framework effectively avoids detecting incomplete actions and excels in
detecting long-lasting actions due to the fine-grained and explicit encoding of
the temporal structure of actions. Leveraging these advantages, the APN
achieves competitive performance and significantly surpasses its counterparts
in detecting long-lasting actions. With an IoU threshold of 0.5, the APN
achieves a mean Average Precision (mAP) of 58.3\% on the THUMOS14 dataset and
98.9\% mAP on the DFMAD70 dataset.
</summary>
    <author>
      <name>Chongkai Lu</name>
    </author>
    <author>
      <name>Man-Wai Mak</name>
    </author>
    <author>
      <name>Ruimin Li</name>
    </author>
    <author>
      <name>Zheru Chi</name>
    </author>
    <author>
      <name>Hong Fu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under Review. Code available at https: //github.com/makecent/APN</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.09268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.09268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.05849v1</id>
    <updated>2024-04-08T20: 31: 27Z</updated>
    <published>2024-04-08T20: 31: 27Z</published>
    <title>Localizing Moments of Actions in Untrimmed Videos of Infants with Autism
  Spectrum Disorder</title>
    <summary>  Autism Spectrum Disorder (ASD) presents significant challenges in early
diagnosis and intervention, impacting children and their families. With
prevalence rates rising, there is a critical need for accessible and efficient
screening tools. Leveraging machine learning (ML) techniques, in particular
Temporal Action Localization (TAL), holds promise for automating ASD screening.
This paper introduces a self-attention based TAL model designed to identify
ASD-related behaviors in infant videos. Unlike existing methods, our approach
simplifies complex modeling and emphasizes efficiency, which is essential for
practical deployment in real-world scenarios. Importantly, this work
underscores the importance of developing computer vision methods capable of
operating in naturilistic environments with little equipment control,
addressing key challenges in ASD screening. This study is the first to conduct
end-to-end temporal action localization in untrimmed videos of infants with
ASD, offering promising avenues for early intervention and support. We report
baseline results of behavior detection using our TAL model. We achieve 70%
accuracy for look face,
                79% accuracy for look object,
                72% for smile and 65% for
vocalization.
</summary>
    <author>
      <name>Halil Ismail Helvaci</name>
    </author>
    <author>
      <name>Sen-ching Samson Cheung</name>
    </author>
    <author>
      <name>Chen-Nee Chuah</name>
    </author>
    <author>
      <name>Sally Ozonoff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages,
                2 figures,
                3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.05849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.05849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2207.12817v2</id>
    <updated>2022-12-07T22: 02: 35Z</updated>
    <published>2022-07-26T11: 24: 00Z</published>
    <title>Bodily Behaviors in Social Interaction: Novel Annotations and
  State-of-the-Art Evaluation</title>
    <summary>  Body language is an eye-catching social signal and its automatic analysis can
significantly advance artificial intelligence systems to understand and
actively participate in social interactions. While computer vision has made
impressive progress in low-level tasks like head and body pose estimation, the
detection of more subtle behaviors such as gesturing, grooming, or fumbling is
not well explored. In this paper we present BBSI, the first set of annotations
of complex Bodily Behaviors embedded in continuous Social Interactions in a
group setting. Based on previous work in psychology, we manually annotated 26
hours of spontaneous human behavior in the MPIIGroupInteraction dataset with 15
distinct body language classes. We present comprehensive descriptive statistics
on the resulting dataset as well as results of annotation quality evaluations.
For automatic detection of these behaviors, we adapt the Pyramid Dilated
Attention Network (PDAN), a state-of-the-art approach for human action
detection. We perform experiments using four variants of spatial-temporal
features as input to PDAN: Two-Stream Inflated 3D CNN, Temporal Segment
Networks, Temporal Shift Module and Swin Transformer. Results are promising and
indicate a great room for improvement in this difficult task. Representing a
key piece in the puzzle towards automatic understanding of social behavior,
BBSI is fully available to the research community.
</summary>
    <author>
      <name>Michal Balazia</name>
    </author>
    <author>
      <name>Philipp Müller</name>
    </author>
    <author>
      <name>Ákos Levente Tánczos</name>
    </author>
    <author>
      <name>August von Liechtenstein</name>
    </author>
    <author>
      <name>François Brémond</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3503161.3548363</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3503161.3548363" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint. Full paper accepted at the ACM International Conference on
  Multimedia (ACMMM), Lisbon, Portugal, October 2022. 10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.12817v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.12817v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05, 68T10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2207.13552v2</id>
    <updated>2022-12-21T10: 29: 37Z</updated>
    <published>2022-07-27T14: 43: 29Z</published>
    <title>iCub Knows Where You Look: Exploiting Social Cues for Interactive Object
  Detection Learning</title>
    <summary>  Performing joint interaction requires constant mutual monitoring of own
actions and their effects on the other's behaviour. Such an action-effect
monitoring is boosted by social cues and might result in an increasing sense of
agency. Joint actions and joint attention are strictly correlated and both of
them contribute to the formation of a precise temporal coordination. In
human-robot interaction, the robot's ability to establish joint attention with
a human partner and exploit various social cues to react accordingly is a
crucial step in creating communicative robots. Along the social component, an
effective human-robot interaction can be seen as a new method to improve and
make the robot's learning process more natural and robust for a given task. In
this work we use different social skills, such as mutual gaze, gaze following,
speech and human face recognition, to develop an effective teacher-learner
scenario tailored to visual object learning in dynamic environments.
Experiments on the iCub robot demonstrate that the system allows the robot to
learn new objects through a natural interaction with a human teacher in
presence of distractors.
</summary>
    <author>
      <name>Maria Lombardi</name>
    </author>
    <author>
      <name>Elisa Maiettini</name>
    </author>
    <author>
      <name>Vadim Tikhanoff</name>
    </author>
    <author>
      <name>Lorenzo Natale</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Humanoid Robots,
                2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2207.13552v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.13552v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2203.10719v1</id>
    <updated>2022-03-21T03: 35: 32Z</updated>
    <published>2022-03-21T03: 35: 32Z</published>
    <title>LocATe: End-to-end Localization of Actions in 3D with Transformers</title>
    <summary>  Understanding a person's behavior from their 3D motion is a fundamental
problem in computer vision with many applications. An important component of
this problem is 3D Temporal Action Localization (3D-TAL), which involves
recognizing what actions a person is performing, and when. State-of-the-art
3D-TAL methods employ a two-stage approach in which the action span detection
task and the action recognition task are implemented as a cascade. This
approach, however, limits the possibility of error-correction. In contrast, we
propose LocATe, an end-to-end approach that jointly localizes and recognizes
actions in a 3D sequence. Further, unlike existing autoregressive models that
focus on modeling the local context in a sequence, LocATe's transformer model
is capable of capturing long-term correlations between actions in a sequence.
Unlike transformer-based object-detection and classification models which
consider image or patch features as input, the input in 3D-TAL is a long
sequence of highly correlated frames. To handle the high-dimensional input, we
implement an effective input representation, and overcome the diffuse attention
across long time horizons by introducing sparse attention in the model. LocATe
outperforms previous approaches on the existing PKU-MMD 3D-TAL benchmark
(mAP=93.2%). Finally, we argue that benchmark datasets are most useful where
there is clear room for performance improvement. To that end, we introduce a
new, challenging, and more realistic benchmark dataset, BABEL-TAL-20 (BT20),
where the performance of state-of-the-art methods is significantly worse. The
dataset and code for the method will be available for research purposes.
</summary>
    <author>
      <name>Jiankai Sun</name>
    </author>
    <author>
      <name>Bolei Zhou</name>
    </author>
    <author>
      <name>Michael J. Black</name>
    </author>
    <author>
      <name>Arjun Chandrasekaran</name>
    </author>
    <link href="http://arxiv.org/abs/2203.10719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.10719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2203.02966v1</id>
    <updated>2022-03-06T13: 57: 09Z</updated>
    <published>2022-03-06T13: 57: 09Z</published>
    <title>Exploring Optical-Flow-Guided Motion and Detection-Based Appearance for
  Temporal Sentence Grounding</title>
    <summary>  Temporal sentence grounding aims to localize a target segment in an untrimmed
video semantically according to a given sentence query. Most previous works
focus on learning frame-level features of each whole frame in the entire video,
and directly match them with the textual information. Such frame-level feature
extraction leads to the obstacles of these methods in distinguishing ambiguous
video frames with complicated contents and subtle appearance differences, thus
limiting their performance. In order to differentiate fine-grained appearance
similarities among consecutive frames, some state-of-the-art methods
additionally employ a detection model like Faster R-CNN to obtain detailed
object-level features in each frame for filtering out the redundant background
contents. However, these methods suffer from missing motion analysis since the
object detection module in Faster R-CNN lacks temporal modeling. To alleviate
the above limitations, in this paper, we propose a novel Motion- and
Appearance-guided 3D Semantic Reasoning Network (MA3SRN), which incorporates
optical-flow-guided motion-aware, detection-based appearance-aware, and
3D-aware object-level features to better reason the spatial-temporal object
relations for accurately modelling the activity among consecutive frames.
Specifically, we first develop three individual branches for motion,
appearance, and 3D encoding separately to learn fine-grained motion-guided,
appearance-guided, and 3D-aware object features, respectively. Then, both
motion and appearance information from corresponding branches are associated to
enhance the 3D-aware features for the final precise grounding. Extensive
experiments on three challenging datasets (ActivityNet Caption, Charades-STA
and TACoS) demonstrate that the proposed MA3SRN model achieves a new
state-of-the-art.
</summary>
    <author>
      <name>Daizong Liu</name>
    </author>
    <author>
      <name>Xiang Fang</name>
    </author>
    <author>
      <name>Wei Hu</name>
    </author>
    <author>
      <name>Pan Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv: 2201.00457</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.02966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.02966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2303.17285v1</id>
    <updated>2023-03-30T10: 47: 26Z</updated>
    <published>2023-03-30T10: 47: 26Z</published>
    <title>Decomposed Cross-modal Distillation for RGB-based Temporal Action
  Detection</title>
    <summary>  Temporal action detection aims to predict the time intervals and the classes
of action instances in the video. Despite the promising performance, existing
two-stream models exhibit slow inference speed due to their reliance on
computationally expensive optical flow. In this paper, we introduce a
decomposed cross-modal distillation framework to build a strong RGB-based
detector by transferring knowledge of the motion modality. Specifically,
instead of direct distillation, we propose to separately learn RGB and motion
representations, which are in turn combined to perform action localization. The
dual-branch design and the asymmetric training objectives enable effective
motion knowledge transfer while preserving RGB information intact. In addition,
we introduce a local attentive fusion to better exploit the multimodal
complementarity. It is designed to preserve the local discriminability of the
features that is important for action localization. Extensive experiments on
the benchmarks verify the effectiveness of the proposed method in enhancing
RGB-based action detectors. Notably, our framework is agnostic to backbones and
detection heads, bringing consistent gains across different model combinations.
</summary>
    <author>
      <name>Pilhyeon Lee</name>
    </author>
    <author>
      <name>Taeoh Kim</name>
    </author>
    <author>
      <name>Minho Shim</name>
    </author>
    <author>
      <name>Dongyoon Wee</name>
    </author>
    <author>
      <name>Hyeran Byun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVPR 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.17285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.17285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1804.01824v1</id>
    <updated>2018-04-05T13: 08: 25Z</updated>
    <published>2018-04-05T13: 08: 25Z</published>
    <title>Guess Where? Actor-Supervision for Spatiotemporal Action Localization</title>
    <summary>  This paper addresses the problem of spatiotemporal localization of actions in
videos. Compared to leading approaches, which all learn to localize based on
carefully annotated boxes on training video frames, we adhere to a
weakly-supervised solution that only requires a video class label. We introduce
an actor-supervised architecture that exploits the inherent compositionality of
actions in terms of actor transformations, to localize actions. We make two
contributions. First, we propose actor proposals derived from a detector for
human and non-human actors intended for images, which is linked over time by
Siamese similarity matching to account for actor deformations. Second, we
propose an actor-based attention mechanism that enables the localization of the
actions from action class labels and actor proposals and is end-to-end
trainable. Experiments on three human and non-human action datasets show actor
supervision is state-of-the-art for weakly-supervised action localization and
is even competitive to some fully-supervised alternatives.
</summary>
    <author>
      <name>Victor Escorcia</name>
    </author>
    <author>
      <name>Cuong D. Dao</name>
    </author>
    <author>
      <name>Mihir Jain</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <author>
      <name>Cees Snoek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">cvpr version</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.01824v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01824v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1708.00042v1</id>
    <updated>2017-07-31T19: 03: 19Z</updated>
    <published>2017-07-31T19: 03: 19Z</published>
    <title>Spatio-Temporal Action Detection with Cascade Proposal and Location
  Anticipation</title>
    <summary>  In this work, we address the problem of spatio-temporal action detection in
temporally untrimmed videos. It is an important and challenging task as finding
accurate human actions in both temporal and spatial space is important for
analyzing large-scale video data. To tackle this problem, we propose a cascade
proposal and location anticipation (CPLA) model for frame-level action
detection. There are several salient points of our model: (1) a cascade region
proposal network (casRPN) is adopted for action proposal generation and shows
better localization accuracy compared with single region proposal network
(RPN); (2) action spatio-temporal consistencies are exploited via a location
anticipation network (LAN) and thus frame-level action detection is not
conducted independently. Frame-level detections are then linked by solving an
linking score maximization problem, and temporally trimmed into spatio-temporal
action tubes. We demonstrate the effectiveness of our model on the challenging
UCF101 and LIRIS-HARL datasets, both achieving state-of-the-art performance.
</summary>
    <author>
      <name>Zhenheng Yang</name>
    </author>
    <author>
      <name>Jiyang Gao</name>
    </author>
    <author>
      <name>Ram Nevatia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at BMVC 2017 (oral)</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.00042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2107.10492v3</id>
    <updated>2023-06-13T05: 39: 46Z</updated>
    <published>2021-07-22T07: 25: 35Z</published>
    <title>Bandit Quickest Changepoint Detection</title>
    <summary>  Many industrial and security applications employ a suite of sensors for
detecting abrupt changes in temporal behavior patterns. These abrupt changes
typically manifest locally, rendering only a small subset of sensors
informative. Continuous monitoring of every sensor can be expensive due to
resource constraints, and serves as a motivation for the bandit quickest
changepoint detection problem, where sensing actions (or sensors) are
sequentially chosen, and only measurements corresponding to chosen actions are
observed. We derive an information-theoretic lower bound on the detection delay
for a general class of finitely parameterized probability distributions. We
then propose a computationally efficient online sensing scheme, which
seamlessly balances the need for exploration of different sensing options with
exploitation of querying informative actions. We derive expected delay bounds
for the proposed scheme and show that these bounds match our
information-theoretic lower bounds at low false alarm rates, establishing
optimality of the proposed method. We then perform a number of experiments on
synthetic and real datasets demonstrating the effectiveness of our proposed
method.
</summary>
    <author>
      <name>Aditya Gopalan</name>
    </author>
    <author>
      <name>Venkatesh Saligrama</name>
    </author>
    <author>
      <name>Braghadeesh Lakshminarayanan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Some typos fixed in the NeurIPS 2021 version</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.10492v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.10492v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/0810.5315v1</id>
    <updated>2008-10-29T16: 30: 09Z</updated>
    <published>2008-10-29T16: 30: 09Z</published>
    <title>Levy stable noise induced transitions: stochastic resonance, resonant
  activation and dynamic hysteresis</title>
    <summary>  A standard approach to analysis of noise-induced effects in stochastic
dynamics assumes a Gaussian character of the noise term describing interaction
of the analyzed system with its complex surroundings. An additional assumption
about the existence of timescale separation between the dynamics of the
measured observable and the typical timescale of the noise allows external
fluctuations to be modeled as temporally uncorrelated and therefore white.
However, in many natural phenomena the assumptions concerning the
abovementioned properties of "Gaussianity" and "whiteness" of the noise can be
violated. In this context, in contrast to the spatiotemporal coupling
characterizing general forms of non-Markovian or semi-Markovian L\'evy walks,
so called L\'evy flights correspond to the class of Markov processes which
still can be interpreted as white, but distributed according to a more general,
infinitely divisible, stable and non-Gaussian law. L\'evy noise-driven
non-equilibrium systems are known to manifest interesting physical properties
and have been addressed in various scenarios of physical transport exhibiting a
superdiffusive behavior. Here we present a brief overview of our recent
investigations aimed to understand features of stochastic dynamics under the
influence of L\'evy white noise perturbations. We find that the archetypal
phenomena of noise-induced ordering are robust and can be detected also in
systems driven by non-Gaussian, heavy-tailed fluctuations with infinite
variance.
</summary>
    <author>
      <name>Bartlomiej Dybiec</name>
    </author>
    <author>
      <name>Ewa Gudowska-Nowak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-5468/2009/05/P05004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-5468/2009/05/P05004" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages,
                8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Stat. Mech. P05004 (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0810.5315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.5315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2004.12340v5</id>
    <updated>2021-02-11T18: 07: 06Z</updated>
    <published>2020-04-26T10: 14: 14Z</published>
    <title>Analysis of Spatial-temporal Behavior Pattern of the Share Bike Usage
  during COVID-19 Pandemic in Beijing</title>
    <summary>  During the epidemics of COVID-19, the whole world is experiencing a serious
crisis on public health and economy. Understanding human mobility during the
pandemic helps one to design intervention strategies and resilience measures.
The widely used Bike Sharing System (BSS) can characterize the activities of
urban dwellers over time &amp; space in big cities but is rarely reported in
epidemiological research. In this paper, we present a human mobility analyzing
framework
            } based on BSS data, which examines the spatiotemporal characteristics
of share bike users, detects the key time nodes of different pandemic stages,
and demonstrats the evolution of human mobility due to the onset of the
COVID-19 threat and administrative restrictions. We assessed the net impact of
the pandemic by using the result of co-location analysis between share bike
usage and POIs (Point Of Interest). Our results show the pandemic reduced the
overall bike usage by 64.8%, then an average increase (15.9%) in share bike
usage appeared afterwards, suggesting that productive and residential
activities have partially recovered but far from the ordinary days. These
findings could be a reference for epidemiological researches and inform
policymaking in the context of the current COVID-19 outbreak and other epidemic
events at city-scale.
</summary>
    <author>
      <name>Xinwei Chai</name>
    </author>
    <author>
      <name>Xian Guo</name>
    </author>
    <author>
      <name>Jihua Xiao</name>
    </author>
    <author>
      <name>Jie Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages,
            9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.12340v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.12340v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2101.04183v2</id>
    <updated>2022-02-15T21: 09: 01Z</updated>
    <published>2021-01-11T20: 46: 35Z</published>
    <title>Detecting seasonal episodic-like spatiotemporal memory patterns using
  animal movement modelling</title>
    <summary>  1. Spatial memory plays a role in the way animals perceive their
environments, resulting in memory-informed movement patterns that are
observable to ecologists. Developing mathematical techniques to understand how
animals use memory in their environments allows for an increased understanding
of animal cognition. 2. Here we describe a model that accounts for the memory
of seasonal or ephemeral qualities of an animal's environment. The model
captures multiple behaviors at once by allowing for resource selection in the
present time as well as long-distance navigations to previously visited
locations within an animal's home range. 3. We performed a set of analyses on
simulated data to test our model, determining that it can provide informative
results from as little as one year of discrete-time location data. We also show
that the accuracy of model selection and parameter estimation increases with
more location data. 4. This model has potential to identify a specific
mechanism in which animals use memory to optimize their foraging, by revisiting
temporally and predictably variable resources at consistent time lags.
</summary>
    <author>
      <name>Peter R. Thompson</name>
    </author>
    <author>
      <name>Andrew E. Derocher</name>
    </author>
    <author>
      <name>Mark A. Edwards</name>
    </author>
    <author>
      <name>Mark A. Lewis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/2041-210X.13743</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/2041-210X.13743" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, including title and abstract; 4 tables; 3 figures; one
  Appendix containing 2 additional tables and 1 additional figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.04183v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.04183v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1802.10250v3</id>
    <updated>2018-12-25T05: 58: 28Z</updated>
    <published>2018-02-28T03: 40: 05Z</published>
    <title>Joint Event Detection and Description in Continuous Video Streams</title>
    <summary>  Dense video captioning is a fine-grained video understanding task that
involves two sub-problems: localizing distinct events in a long video stream,
and generating captions for the localized events. We propose the Joint Event
Detection and Description Network (JEDDi-Net), which solves the dense video
captioning task in an end-to-end fashion. Our model continuously encodes the
input video stream with three-dimensional convolutional layers, proposes
variable-length temporal events based on pooled features, and generates their
captions. Proposal features are extracted within each proposal segment through
3D Segment-of-Interest pooling from shared video feature encoding. In order to
explicitly model temporal relationships between visual events and their
captions in a single video, we also propose a two-level hierarchical captioning
module that keeps track of context. On the large-scale ActivityNet Captions
dataset, JEDDi-Net demonstrates improved results as measured by standard
metrics. We also present the first dense captioning results on the
TACoS-MultiLevel dataset.
</summary>
    <author>
      <name>Huijuan Xu</name>
    </author>
    <author>
      <name>Boyang Li</name>
    </author>
    <author>
      <name>Vasili Ramanishka</name>
    </author>
    <author>
      <name>Leonid Sigal</name>
    </author>
    <author>
      <name>Kate Saenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">WACV2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.10250v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.10250v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2104.00969v5</id>
    <updated>2022-05-10T07: 39: 03Z</updated>
    <published>2021-04-02T10: 21: 22Z</published>
    <title>TubeR: Tubelet Transformer for Video Action Detection</title>
    <summary>  We propose TubeR: a simple solution for spatio-temporal video action
detection. Different from existing methods that depend on either an off-line
actor detector or hand-designed actor-positional hypotheses like proposals or
anchors, we propose to directly detect an action tubelet in a video by
simultaneously performing action localization and recognition from a single
representation. TubeR learns a set of tubelet-queries and utilizes a
tubelet-attention module to model the dynamic spatio-temporal nature of a video
clip, which effectively reinforces the model capacity compared to using
actor-positional hypotheses in the spatio-temporal space. For videos containing
transitional states or scene changes, we propose a context aware classification
head to utilize short-term and long-term context to strengthen action
classification, and an action switch regression head for detecting the precise
temporal action extent. TubeR directly produces action tubelets with variable
lengths and even maintains good results for long video clips. TubeR outperforms
the previous state-of-the-art on commonly used action detection datasets AVA,
UCF101-24 and JHMDB51-21.
</summary>
    <author>
      <name>Jiaojiao Zhao</name>
    </author>
    <author>
      <name>Yanyi Zhang</name>
    </author>
    <author>
      <name>Xinyu Li</name>
    </author>
    <author>
      <name>Hao Chen</name>
    </author>
    <author>
      <name>Shuai Bing</name>
    </author>
    <author>
      <name>Mingze Xu</name>
    </author>
    <author>
      <name>Chunhui Liu</name>
    </author>
    <author>
      <name>Kaustav Kundu</name>
    </author>
    <author>
      <name>Yuanjun Xiong</name>
    </author>
    <author>
      <name>Davide Modolo</name>
    </author>
    <author>
      <name>Ivan Marsic</name>
    </author>
    <author>
      <name>Cees G. M. Snoek</name>
    </author>
    <author>
      <name>Joseph Tighe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CVPR 2022 (Oral)</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.00969v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.00969v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1904.09862v1</id>
    <updated>2019-04-22T13: 30: 34Z</updated>
    <published>2019-04-22T13: 30: 34Z</published>
    <title>Real-time Intent Prediction of Pedestrians for Autonomous Ground
  Vehicles via Spatio-Temporal DenseNet</title>
    <summary>  Understanding the behaviors and intentions of humans are one of the main
challenges autonomous ground vehicles still faced with. More specifically, when
it comes to complex environments such as urban traffic scenes, inferring the
intentions and actions of vulnerable road users such as pedestrians become even
harder. In this paper, we address the problem of intent action prediction of
pedestrians in urban traffic environments using only image sequences from a
monocular RGB camera. We propose a real-time framework that can accurately
detect, track and predict the intended actions of pedestrians based on a
tracking-by-detection technique in conjunction with a novel spatio-temporal
DenseNet model. We trained and evaluated our framework based on real data
collected from urban traffic environments. Our framework has shown resilient
and competitive results in comparison to other baseline approaches. Overall, we
achieved an average precision score of 84.76% with a real-time performance at
20 FPS.
</summary>
    <author>
      <name>Khaled Saleh</name>
    </author>
    <author>
      <name>Mohammed Hossny</name>
    </author>
    <author>
      <name>Saeid Nahavandi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICRA 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.09862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.09862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2401.04351v1</id>
    <updated>2024-01-09T04: 35: 17Z</updated>
    <published>2024-01-09T04: 35: 17Z</published>
    <title>A Change Point Detection Integrated Remaining Useful Life Estimation
  Model under Variable Operating Conditions</title>
    <summary>  By informing the onset of the degradation process, health status evaluation
serves as a significant preliminary step for reliable remaining useful life
(RUL) estimation of complex equipment. This paper proposes a novel temporal
dynamics learning-based model for detecting change points of individual
devices, even under variable operating conditions, and utilises the learnt
change points to improve the RUL estimation accuracy. During offline model
development, the multivariate sensor data are decomposed to learn fused
temporal correlation features that are generalisable and representative of
normal operation dynamics across multiple operating conditions. Monitoring
statistics and control limit thresholds for normal behaviour are dynamically
constructed from these learnt temporal features for the unsupervised detection
of device-level change points. The detected change points then inform the
degradation data labelling for training a long short-term memory (LSTM)-based
RUL estimation model. During online monitoring, the temporal correlation
dynamics of a query device is monitored for breach of the control limit derived
in offline training. If a change point is detected, the device's RUL is
estimated with the well-trained offline model for early preventive action.
Using C-MAPSS turbofan engines as the case study, the proposed method improved
the accuracy by 5.6\% and 7.5\% for two scenarios with six operating
conditions, when compared to existing LSTM-based RUL estimation models that do
not consider heterogeneous change points.
</summary>
    <author>
      <name>Anushiya Arunan</name>
    </author>
    <author>
      <name>Yan Qin</name>
    </author>
    <author>
      <name>Xiaoli Li</name>
    </author>
    <author>
      <name>Chau Yuen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.conengprac.2023.105840</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.conengprac.2023.105840" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in Control Engineering Practice Journal with DOI:
  https: //doi.org/10.1016/j.conengprac.2023.105840</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.04351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.04351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2111.15446v1</id>
    <updated>2021-11-26T02: 54: 47Z</updated>
    <published>2021-11-26T02: 54: 47Z</published>
    <title>TEGDetector: A Phishing Detector that Knows Evolving Transaction
  Behaviors</title>
    <summary>  Recently, phishing scams have posed a significant threat to blockchains.
Phishing detectors direct their efforts in hunting phishing addresses. Most of
the detectors extract target addresses' transaction behavior features by random
walking or constructing static subgraphs. The random walking
methods,unfortunately, usually miss structural information due to limited
sampling sequence length, while the static subgraph methods tend to ignore
temporal features lying in the evolving transaction behaviors. More
importantly, their performance undergoes severe degradation when the malicious
users intentionally hide phishing behaviors. To address these challenges, we
propose TEGDetector, a dynamic graph classifier that learns the evolving
behavior features from transaction evolution graphs (TEGs). First, we cast the
transaction series into multiple time slices, capturing the target address's
transaction behaviors in different periods. Then, we provide a fast
non-parametric phishing detector to narrow down the search space of suspicious
addresses. Finally, TEGDetector considers both the spatial and temporal
evolutions towards a complete characterization of the evolving transaction
behaviors. Moreover, TEGDetector utilizes adaptively learnt time coefficient to
pay distinct attention to different periods, which provides several novel
insights. Extensive experiments on the large-scale Ethereum transaction dataset
demonstrate that the proposed method achieves state-of-the-art detection
performance.
</summary>
    <author>
      <name>Jinyin Chen</name>
    </author>
    <author>
      <name>Haiyang Xiong</name>
    </author>
    <author>
      <name>Dunjie Zhang</name>
    </author>
    <author>
      <name>Zhenguang Liu</name>
    </author>
    <author>
      <name>Jiajing Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2111.15446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.15446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2307.05121v1</id>
    <updated>2023-07-11T08: 56: 53Z</updated>
    <published>2023-07-11T08: 56: 53Z</published>
    <title>Transaction Fraud Detection via Spatial-Temporal-Aware Graph Transformer</title>
    <summary>  How to obtain informative representations of transactions and then perform
the identification of fraudulent transactions is a crucial part of ensuring
financial security. Recent studies apply Graph Neural Networks (GNNs) to the
transaction fraud detection problem. Nevertheless, they encounter challenges in
effectively learning spatial-temporal information due to structural
limitations. Moreover, few prior GNN-based detectors have recognized the
significance of incorporating global information, which encompasses similar
behavioral patterns and offers valuable insights for discriminative
representation learning. Therefore, we propose a novel heterogeneous graph
neural network called Spatial-Temporal-Aware Graph Transformer (STA-GT) for
transaction fraud detection problems. Specifically, we design a temporal
encoding strategy to capture temporal dependencies and incorporate it into the
graph neural network framework, enhancing spatial-temporal information modeling
and improving expressive ability. Furthermore, we introduce a transformer
module to learn local and global information. Pairwise node-node interactions
overcome the limitation of the GNN structure and build up the interactions with
the target node and long-distance ones. Experimental results on two financial
datasets compared to general GNN models and GNN-based fraud detectors
demonstrate that our proposed method STA-GT is effective on the transaction
fraud detection task.
</summary>
    <author>
      <name>Yue Tian</name>
    </author>
    <author>
      <name>Guanjun Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2307.05121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.05121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2204.04416v4</id>
    <updated>2022-10-30T03: 45: 04Z</updated>
    <published>2022-04-09T07: 52: 11Z</published>
    <title>E^2TAD: An Energy-Efficient Tracking-based Action Detector</title>
    <summary>  Video action detection (spatio-temporal action localization) is usually the
starting point for human-centric intelligent analysis of videos nowadays. It
has high practical impacts for many applications across robotics, security,
healthcare, etc. The two-stage paradigm of Faster R-CNN inspires a standard
paradigm of video action detection in object detection, i.e., firstly
generating person proposals and then classifying their actions. However, none
of the existing solutions could provide fine-grained action detection to the
"who-when-where-what" level. This paper presents a tracking-based solution to
accurately and efficiently localize predefined key actions spatially (by
predicting the associated target IDs and locations) and temporally (by
predicting the time in exact frame indices). This solution won first place in
the UAV-Video Track of 2021 Low-Power Computer Vision Challenge (LPCVC).
</summary>
    <author>
      <name>Xin Hu</name>
    </author>
    <author>
      <name>Zhenyu Wu</name>
    </author>
    <author>
      <name>Hao-Yu Miao</name>
    </author>
    <author>
      <name>Siqi Fan</name>
    </author>
    <author>
      <name>Taiyu Long</name>
    </author>
    <author>
      <name>Zhenyu Hu</name>
    </author>
    <author>
      <name>Pengcheng Pi</name>
    </author>
    <author>
      <name>Yi Wu</name>
    </author>
    <author>
      <name>Zhou Ren</name>
    </author>
    <author>
      <name>Zhangyang Wang</name>
    </author>
    <author>
      <name>Gang Hua</name>
    </author>
    <link href="http://arxiv.org/abs/2204.04416v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.04416v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2110.00428v1</id>
    <updated>2021-08-29T13: 21: 50Z</updated>
    <published>2021-08-29T13: 21: 50Z</published>
    <title>Zero-shot Natural Language Video Localization</title>
    <summary>  Understanding videos to localize moments with natural language often requires
large expensive annotated video regions paired with language queries. To
eliminate the annotation costs, we make a first attempt to train a natural
language video localization model in zero-shot manner. Inspired by unsupervised
image captioning setup, we merely require random text corpora, unlabeled video
collections, and an off-the-shelf object detector to train a model. With the
unpaired data, we propose to generate pseudo-supervision of candidate temporal
regions and corresponding query sentences, and develop a simple NLVL model to
train with the pseudo-supervision. Our empirical validations show that the
proposed pseudo-supervised method outperforms several baseline approaches and a
number of methods using stronger supervision on Charades-STA and
ActivityNet-Captions.
</summary>
    <author>
      <name>Jinwoo Nam</name>
    </author>
    <author>
      <name>Daechul Ahn</name>
    </author>
    <author>
      <name>Dongyeop Kang</name>
    </author>
    <author>
      <name>Seong Jong Ha</name>
    </author>
    <author>
      <name>Jonghyun Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,
            7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.00428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.00428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2207.09927v2</id>
    <updated>2022-10-31T12: 44: 11Z</updated>
    <published>2022-07-20T14: 12: 05Z</published>
    <title>ViGAT: Bottom-up event recognition and explanation in video using
  factorized graph attention network</title>
    <summary>  In this paper a pure-attention bottom-up approach, called ViGAT, that
utilizes an object detector together with a Vision Transformer (ViT) backbone
network to derive object and frame features, and a head network to process
these features for the task of event recognition and explanation in video, is
proposed. The ViGAT head consists of graph attention network (GAT) blocks
factorized along the spatial and temporal dimensions in order to capture
effectively both local and long-term dependencies between objects or frames.
Moreover, using the weighted in-degrees (WiDs) derived from the adjacency
matrices at the various GAT blocks, we show that the proposed architecture can
identify the most salient objects and frames that explain the decision of the
network. A comprehensive evaluation study is performed, demonstrating that the
proposed approach provides state-of-the-art results on three large, publicly
available video datasets (FCVID, Mini-Kinetics, ActivityNet).
</summary>
    <author>
      <name>Nikolaos Gkalelis</name>
    </author>
    <author>
      <name>Dimitrios Daskalakis</name>
    </author>
    <author>
      <name>Vasileios Mezaris</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2022.3213652</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCESS.2022.3213652" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Access, vol. 10, pp. 108797-108816,
            2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2207.09927v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.09927v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2208.04206v1</id>
    <updated>2022-08-08T15: 12: 27Z</updated>
    <published>2022-08-08T15: 12: 27Z</published>
    <title>Vision-Based Activity Recognition in Children with Autism-Related
  Behaviors</title>
    <summary>  Advances in machine learning and contactless sensors have enabled the
understanding complex human behaviors in a healthcare setting. In particular,
several deep learning systems have been introduced to enable comprehensive
analysis of neuro-developmental conditions such as Autism Spectrum Disorder
(ASD). This condition affects children from their early developmental stages
onwards, and diagnosis relies entirely on observing the child's behavior and
detecting behavioral cues. However, the diagnosis process is time-consuming as
it requires long-term behavior observation, and the scarce availability of
specialists. We demonstrate the effect of a region-based computer vision system
to help clinicians and parents analyze a child's behavior. For this purpose, we
adopt and enhance a dataset for analyzing autism-related actions using videos
of children captured in uncontrolled environments (e.g. videos collected with
consumer-grade cameras, in varied environments). The data is pre-processed by
detecting the target child in the video to reduce the impact of background
noise. Motivated by the effectiveness of temporal convolutional models, we
propose both light-weight and conventional models capable of extracting action
features from video frames and classifying autism-related behaviors by
analyzing the relationships between frames in a video. Through extensive
evaluations on the feature extraction and learning strategies, we demonstrate
that the best performance is achieved with an Inflated 3D Convnet and
Multi-Stage Temporal Convolutional Networks, achieving a 0.83 Weighted F1-score
for classification of the three autism-related actions, outperforming existing
methods. We also propose a light-weight solution by employing the ESNet
backbone within the same system, achieving competitive results of 0.71 Weighted
F1-score, and enabling potential deployment on embedded systems.
</summary>
    <author>
      <name>Pengbo Wei</name>
    </author>
    <author>
      <name>David Ahmedt-Aristizabal</name>
    </author>
    <author>
      <name>Harshala Gammulle</name>
    </author>
    <author>
      <name>Simon Denman</name>
    </author>
    <author>
      <name>Mohammad Ali Armin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.heliyon.2023.e16763</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.heliyon.2023.e16763" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Heliyon, Volume 9, Issue 6, June 2023, e16763</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2208.04206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.04206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1612.00558v4</id>
    <updated>2017-05-16T00: 56: 24Z</updated>
    <published>2016-12-02T03: 39: 38Z</published>
    <title>Unsupervised Human Action Detection by Action Matching</title>
    <summary>  We propose a new task of unsupervised action detection by action matching.
Given two long videos, the objective is to temporally detect all pairs of
matching video segments. A pair of video segments are matched if they share the
same human action. The task is category independent---it does not matter what
action is being performed---and no supervision is used to discover such video
segments. Unsupervised action detection by action matching allows us to align
videos in a meaningful manner. As such, it can be used to discover new action
categories or as an action proposal technique within, say, an action detection
pipeline. Moreover, it is a useful pre-processing step for generating video
highlights, e.g., from sports videos.
  We present an effective and efficient method for unsupervised action
detection. We use an unsupervised temporal encoding method and exploit the
temporal consistency in human actions to obtain candidate action segments. We
evaluate our method on this challenging task using three activity recognition
benchmarks, namely, the MPII Cooking activities dataset, the THUMOS15 action
detection benchmark and a new dataset called the IKEA dataset. On the MPII
Cooking dataset we detect action segments with a precision of 21.6% and recall
of 11.7% over 946 long video pairs and over 5000 ground truth action segments.
Similarly, on THUMOS dataset we obtain 18.4% precision and 25.1% recall over
5094 ground truth action segment pairs.
</summary>
    <author>
      <name>Basura Fernando</name>
    </author>
    <author>
      <name>Sareh Shirazi</name>
    </author>
    <author>
      <name>Stephen Gould</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Computer Vision and Pattern
  Recognition CVPR 2017 Workshops</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.00558v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.00558v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2105.14477v1</id>
    <updated>2021-05-30T09: 28: 43Z</updated>
    <published>2021-05-30T09: 28: 43Z</published>
    <title>Towards Diverse Paragraph Captioning for Untrimmed Videos</title>
    <summary>  Video paragraph captioning aims to describe multiple events in untrimmed
videos with descriptive paragraphs. Existing approaches mainly solve the
problem in two steps: event detection and then event captioning. Such two-step
manner makes the quality of generated paragraphs highly dependent on the
accuracy of event proposal detection which is already a challenging task. In
this paper, we propose a paragraph captioning model which eschews the
problematic event detection stage and directly generates paragraphs for
untrimmed videos. To describe coherent and diverse events, we propose to
enhance the conventional temporal attention with dynamic video memories, which
progressively exposes new video features and suppresses over-accessed video
contents to control visual focuses of the model. In addition, a
diversity-driven training strategy is proposed to improve diversity of
paragraph on the language perspective. Considering that untrimmed videos
generally contain massive but redundant frames, we further augment the video
encoder with keyframe awareness to improve efficiency. Experimental results on
the ActivityNet and Charades datasets show that our proposed model
significantly outperforms the state-of-the-art performance on both accuracy and
diversity metrics without using any event boundary annotations. Code will be
released at https: //github.com/syuqings/video-paragraph.
</summary>
    <author>
      <name>Yuqing Song</name>
    </author>
    <author>
      <name>Shizhe Chen</name>
    </author>
    <author>
      <name>Qin Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVPR 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.14477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.14477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1710.05630v1</id>
    <updated>2017-10-16T11: 32: 27Z</updated>
    <published>2017-10-16T11: 32: 27Z</published>
    <title>Activation of Microwave Fields in a Spin-Torque Nano-Oscillator by
  Neuronal Action Potentials</title>
    <summary>  Action potentials are the basic unit of information in the nervous system and
their reliable detection and decoding holds the key to understanding how the
brain generates complex thought and behavior. Transducing these signals into
microwave field oscillations can enable wireless sensors that report on brain
activity through magnetic induction. In the present work we demonstrate that
action potentials from crayfish lateral giant neuron can trigger microwave
oscillations in spin-torque nano-oscillators. These nanoscale devices take as
input small currents and convert them to microwave current oscillations that
can wirelessly broadcast neuronal activity, opening up the possibility for
compact neuro-sensors. We show that action potentials activate microwave
oscillations in spin-torque nano-oscillators with an amplitude that follows the
action potential signal, demonstrating that the device has both the sensitivity
and temporal resolution to respond to action potentials from a single neuron.
The activation of magnetic oscillations by action potentials, together with the
small footprint and the high frequency tunability, makes these devices
promising candidates for high resolution sensing of bioelectric signals from
neural tissues. These device attributes may be useful for design of
high-throughput bi-directional brain-machine interfaces.
</summary>
    <author>
      <name>J. M. Algarin</name>
    </author>
    <author>
      <name>B. Ramaswamy</name>
    </author>
    <author>
      <name>L. Venuti</name>
    </author>
    <author>
      <name>M. E. Swierzbinski</name>
    </author>
    <author>
      <name>J. Baker-McKee</name>
    </author>
    <author>
      <name>I. N. Weinberg</name>
    </author>
    <author>
      <name>Y. J. Chen</name>
    </author>
    <author>
      <name>I. N. Krivorotov</name>
    </author>
    <author>
      <name>J. A. Katine</name>
    </author>
    <author>
      <name>J. Herberholz</name>
    </author>
    <author>
      <name>R. C. Araneda</name>
    </author>
    <author>
      <name>B. Shapiro</name>
    </author>
    <author>
      <name>E. Waks</name>
    </author>
    <link href="http://arxiv.org/abs/1710.05630v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.05630v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1910.06540v1</id>
    <updated>2019-10-15T05: 44: 28Z</updated>
    <published>2019-10-15T05: 44: 28Z</published>
    <title>Real-time monitoring of driver drowsiness on mobile platforms using 3D
  neural networks</title>
    <summary>  Driver drowsiness increases crash risk, leading to substantial road trauma
each year. Drowsiness detection methods have received considerable attention,
but few studies have investigated the implementation of a detection approach on
a mobile phone. Phone applications reduce the need for specialised hardware and
hence, enable a cost-effective roll-out of the technology across the driving
population. While it has been shown that three-dimensional (3D) operations are
more suitable for spatiotemporal feature learning, current methods for
drowsiness detection commonly use frame-based, multi-step approaches. However,
computationally expensive techniques that achieve superior results on action
recognition benchmarks (e.g. 3D convolutions, optical flow extraction) create
bottlenecks for real-time, safety-critical applications on mobile devices.
Here, we show how depthwise separable 3D convolutions, combined with an early
fusion of spatial and temporal information, can achieve a balance between high
prediction accuracy and real-time inference requirements. In particular,
increased accuracy is achieved when assessment requires motion information, for
example, when sunglasses conceal the eyes. Further, a custom TensorFlow-based
smartphone application shows the true impact of various approaches on inference
times and demonstrates the effectiveness of real-time monitoring based on
out-of-sample data to alert a drowsy driver. Our model is pre-trained on
ImageNet and Kinetics and fine-tuned on a publicly available Driver Drowsiness
Detection dataset. Fine-tuning on large naturalistic driving datasets could
further improve accuracy to obtain robust in-vehicle performance. Overall, our
research is a step towards practical deep learning applications, potentially
preventing micro-sleeps and reducing road trauma.
</summary>
    <author>
      <name>Jasper S. Wijnands</name>
    </author>
    <author>
      <name>Jason Thompson</name>
    </author>
    <author>
      <name>Kerry A. Nice</name>
    </author>
    <author>
      <name>Gideon D. P. A. Aschwanden</name>
    </author>
    <author>
      <name>Mark Stevenson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00521-019-04506-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00521-019-04506-0" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages,
            2 figures, 'Online First' version. For associated mp4
  files, see journal website</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Computing and Applications (2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.06540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T45 (Primary) 68U10 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.9; I.4.8; I.2.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2203.07112v1</id>
    <updated>2022-03-14T13: 56: 12Z</updated>
    <published>2022-03-14T13: 56: 12Z</published>
    <title>RCL: Recurrent Continuous Localization for Temporal Action Detection</title>
    <summary>  Temporal representation is the cornerstone of modern action detection
techniques. State-of-the-art methods mostly rely on a dense anchoring scheme,
where anchors are sampled uniformly over the temporal domain with a discretized
grid, and then regress the accurate boundaries. In this paper, we revisit this
foundational stage and introduce Recurrent Continuous Localization (RCL), which
learns a fully continuous anchoring representation. Specifically, the proposed
representation builds upon an explicit model conditioned with video embeddings
and temporal coordinates, which ensure the capability of detecting segments
with arbitrary length. To optimize the continuous representation, we develop an
effective scale-invariant sampling strategy and recurrently refine the
prediction in subsequent iterations. Our continuous anchoring scheme is fully
differentiable, allowing to be seamlessly integrated into existing detectors,
e.g., BMN and G-TAD. Extensive experiments on two benchmarks demonstrate that
our continuous representation steadily surpasses other discretized counterparts
by ~2% mAP. As a result, RCL achieves 52.92% mAP@0.5 on THUMOS14 and 37.65% mAP
on ActivtiyNet v1.3, outperforming all existing single-model detectors.
</summary>
    <author>
      <name>Qiang Wang</name>
    </author>
    <author>
      <name>Yanhao Zhang</name>
    </author>
    <author>
      <name>Yun Zheng</name>
    </author>
    <author>
      <name>Pan Pan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages,
            7 figures, CVPR2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.07112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.07112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2408.03612v2</id>
    <updated>2024-09-17T06: 25: 38Z</updated>
    <published>2024-08-07T08: 08: 08Z</published>
    <title>JARViS: Detecting Actions in Video Using Unified Actor-Scene Context
  Relation Modeling</title>
    <summary>  Video action detection (VAD) is a formidable vision task that involves the
localization and classification of actions within the spatial and temporal
dimensions of a video clip. Among the myriad VAD architectures, two-stage VAD
methods utilize a pre-trained person detector to extract the region of interest
features, subsequently employing these features for action detection. However,
the performance of two-stage VAD methods has been limited as they depend solely
on localized actor features to infer action semantics. In this study, we
propose a new two-stage VAD framework called Joint Actor-scene context Relation
modeling based on Visual Semantics (JARViS), which effectively consolidates
cross-modal action semantics distributed globally across spatial and temporal
dimensions using Transformer attention. JARViS employs a person detector to
produce densely sampled actor features from a keyframe. Concurrently, it uses a
video backbone to create spatio-temporal scene features from a video clip.
Finally, the fine-grained interactions between actors and scenes are modeled
through a Unified Action-Scene Context Transformer to directly output the final
set of actions in parallel. Our experimental results demonstrate that JARViS
outperforms existing methods by significant margins and achieves
state-of-the-art performance on three popular VAD datasets, including AVA,
UCF101-24, and JHMDB51-21.
</summary>
    <author>
      <name>Seok Hwan Lee</name>
    </author>
    <author>
      <name>Taein Son</name>
    </author>
    <author>
      <name>Soo Won Seo</name>
    </author>
    <author>
      <name>Jisong Kim</name>
    </author>
    <author>
      <name>Jun Won Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages,
            10 figures, update references</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.03612v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.03612v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2102.13493v1</id>
    <updated>2021-02-26T14: 06: 31Z</updated>
    <published>2021-02-26T14: 06: 31Z</published>
    <title>ACDnet: An action detection network for real-time edge computing based
  on flow-guided feature approximation and memory aggregation</title>
    <summary>  Interpreting human actions requires understanding the spatial and temporal
context of the scenes. State-of-the-art action detectors based on Convolutional
Neural Network (CNN) have demonstrated remarkable results by adopting
two-stream or 3D CNN architectures. However, these methods typically operate in
a non-real-time, ofline fashion due to system complexity to reason
spatio-temporal information. Consequently, their high computational cost is not
compliant with emerging real-world scenarios such as service robots or public
surveillance where detection needs to take place at resource-limited edge
devices. In this paper, we propose ACDnet, a compact action detection network
targeting real-time edge computing which addresses both efficiency and
accuracy. It intelligently exploits the temporal coherence between successive
video frames to approximate their CNN features rather than naively extracting
them. It also integrates memory feature aggregation from past video frames to
enhance current detection stability, implicitly modeling long temporal cues
over time. Experiments conducted on the public benchmark datasets UCF-24 and
JHMDB-21 demonstrate that ACDnet, when integrated with the SSD detector, can
robustly achieve detection well above real-time (75 FPS). At the same time, it
retains reasonable accuracy (70.92 and 49.53 frame mAP) compared to other
top-performing methods using far heavier configurations. Codes will be
available at https: //github.com/dginhac/ACDnet.
</summary>
    <author>
      <name>Yu Liu</name>
    </author>
    <author>
      <name>Fan Yang</name>
    </author>
    <author>
      <name>Dominique Ginhac</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.patrec.2021.02.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.patrec.2021.02.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Pattern Recognition Letters</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pattern Recognition Letters,
            145,
            118-126,
            2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2102.13493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.13493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1708.00666v1</id>
    <updated>2017-08-02T09: 38: 26Z</updated>
    <published>2017-08-02T09: 38: 26Z</published>
    <title>Temporal Dynamic Graph LSTM for Action-driven Video Object Detection</title>
    <summary>  In this paper, we investigate a weakly-supervised object detection framework.
Most existing frameworks focus on using static images to learn object
detectors. However, these detectors often fail to generalize to videos because
of the existing domain shift. Therefore, we investigate learning these
detectors directly from boring videos of daily activities. Instead of using
bounding boxes, we explore the use of action descriptions as supervision since
they are relatively easy to gather. A common issue, however, is that objects of
interest that are not involved in human actions are often absent in global
action descriptions known as "missing label". To tackle this problem, we
propose a novel temporal dynamic graph Long Short-Term Memory network (TD-Graph
LSTM). TD-Graph LSTM enables global temporal reasoning by constructing a
dynamic graph that is based on temporal correlations of object proposals and
spans the entire video. The missing label issue for each individual frame can
thus be significantly alleviated by transferring knowledge across correlated
objects proposals in the whole video. Extensive evaluations on a large-scale
daily-life action dataset (i.e., Charades) demonstrates the superiority of our
proposed method. We also release object bounding-box annotations for more than
5,
            000 frames in Charades. We believe this annotated data can also benefit other
research on video-based object recognition in the future.
</summary>
    <author>
      <name>Yuan Yuan</name>
    </author>
    <author>
      <name>Xiaodan Liang</name>
    </author>
    <author>
      <name>Xiaolong Wang</name>
    </author>
    <author>
      <name>Dit-Yan Yeung</name>
    </author>
    <author>
      <name>Abhinav Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ICCV 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.00666v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00666v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2309.08476v1</id>
    <updated>2023-09-15T15: 34: 17Z</updated>
    <published>2023-09-15T15: 34: 17Z</published>
    <title>A Spiking Binary Neuron -- Detector of Causal Links</title>
    <summary>  Causal relationship recognition is a fundamental operation in neural networks
aimed at learning behavior, action planning, and inferring external world
dynamics. This operation is particularly crucial for reinforcement learning
(RL). In the context of spiking neural networks (SNNs), events are represented
as spikes emitted by network neurons or input nodes. Detecting causal
relationships within these events is essential for effective RL implementation.
This research paper presents a novel approach to realize causal relationship
recognition using a simple spiking binary neuron. The proposed method leverages
specially designed synaptic plasticity rules, which are both straightforward
and efficient. Notably, our approach accounts for the temporal aspects of
detected causal links and accommodates the representation of spiking signals as
single spikes or tight spike sequences (bursts), as observed in biological
brains. Furthermore, this study places a strong emphasis on the
hardware-friendliness of the proposed models, ensuring their efficient
implementation on modern and future neuroprocessors. Being compared with
precise machine learning techniques, such as decision tree algorithms and
convolutional neural networks, our neuron demonstrates satisfactory accuracy
despite its simplicity. In conclusion, we introduce a multi-neuron structure
capable of operating in more complex environments with enhanced accuracy,
making it a promising candidate for the advancement of RL applications in SNNs.
</summary>
    <author>
      <name>Mikhail Kiselev</name>
    </author>
    <author>
      <name>Denis Larionov</name>
    </author>
    <author>
      <name>Andrey Urusov</name>
    </author>
    <link href="http://arxiv.org/abs/2309.08476v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.08476v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2211.05337v2</id>
    <updated>2024-04-15T00: 19: 41Z</updated>
    <published>2022-11-10T04: 40: 31Z</published>
    <title>Spatiotemporal k-means</title>
    <summary>  Spatiotemporal data is increasingly available due to emerging sensor and data
acquisition technologies that track moving objects. Spatiotemporal clustering
addresses the need to efficiently discover patterns and trends in moving object
behavior without human supervision. One application of interest is the
discovery of moving clusters, where clusters have a static identity, but their
location and content can change over time. We propose a two phase
spatiotemporal clustering method called spatiotemporal k-means (STkM) that is
able to analyze the multi-scale relationships within spatiotemporal data. By
optimizing an objective function that is unified over space and time, the
method can track dynamic clusters at both short and long timescales with
minimal parameter tuning and no post-processing. We begin by proposing a
theoretical generating model for spatiotemporal data and prove the efficacy of
STkM in this setting. We then evaluate STkM on a recently developed collective
animal behavior benchmark dataset and show that STkM outperforms baseline
methods in the low-data limit, which is a critical regime of consideration in
many emerging applications. Finally, we showcase how STkM can be extended to
more complex machine learning tasks, particularly unsupervised region of
interest detection and tracking in videos.
</summary>
    <author>
      <name>Olga Dorabiala</name>
    </author>
    <author>
      <name>Devavrat Vivek Dabke</name>
    </author>
    <author>
      <name>Jennifer Webster</name>
    </author>
    <author>
      <name>Nathan Kutz</name>
    </author>
    <author>
      <name>Aleksandr Aravkin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages,
            5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.05337v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.05337v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91C20, 62H30, 68T05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1506.01929v2</id>
    <updated>2015-09-27T11: 21: 16Z</updated>
    <published>2015-06-05T14: 48: 46Z</published>
    <title>Learning to track for spatio-temporal action localization</title>
    <summary>  We propose an effective approach for spatio-temporal action localization in
realistic videos. The approach first detects proposals at the frame-level and
scores them with a combination of static and motion CNN features. It then
tracks high-scoring proposals throughout the video using a
tracking-by-detection approach. Our tracker relies simultaneously on
instance-level and class-level detectors. The tracks are scored using a
spatio-temporal motion histogram, a descriptor at the track level, in
combination with the CNN features. Finally, we perform temporal localization of
the action using a sliding-window approach at the track level. We present
experimental results for spatio-temporal localization on the UCF-Sports, J-HMDB
and UCF-101 action localization datasets, where our approach outperforms the
state of the art with a margin of 15%,
            7% and 12% respectively in mAP.
</summary>
    <author>
      <name>Philippe Weinzaepfel</name>
    </author>
    <author>
      <name>Zaid Harchaoui</name>
    </author>
    <author>
      <name>Cordelia Schmid</name>
    </author>
    <link href="http://arxiv.org/abs/1506.01929v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.01929v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1904.03870v1</id>
    <updated>2019-04-08T07: 17: 30Z</updated>
    <published>2019-04-08T07: 17: 30Z</published>
    <title>Streamlined Dense Video Captioning</title>
    <summary>  Dense video captioning is an extremely challenging task since accurate and
coherent description of events in a video requires holistic understanding of
video contents as well as contextual reasoning of individual events. Most
existing approaches handle this problem by first detecting event proposals from
a video and then captioning on a subset of the proposals. As a result, the
generated sentences are prone to be redundant or inconsistent since they fail
to consider temporal dependency between events. To tackle this challenge, we
propose a novel dense video captioning framework, which models temporal
dependency across events in a video explicitly and leverages visual and
linguistic context from prior events for coherent storytelling. This objective
is achieved by 1) integrating an event sequence generation network to select a
sequence of event proposals adaptively, and 2) feeding the sequence of event
proposals to our sequential video captioning network, which is trained by
reinforcement learning with two-level rewards at both event and episode levels
for better context modeling. The proposed technique achieves outstanding
performances on ActivityNet Captions dataset in most metrics.
</summary>
    <author>
      <name>Jonghwan Mun</name>
    </author>
    <author>
      <name>Linjie Yang</name>
    </author>
    <author>
      <name>Zhou Ren</name>
    </author>
    <author>
      <name>Ning Xu</name>
    </author>
    <author>
      <name>Bohyung Han</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.03870v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.03870v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1909.00239v1</id>
    <updated>2019-08-31T16: 30: 28Z</updated>
    <published>2019-08-31T16: 30: 28Z</published>
    <title>WSLLN: Weakly Supervised Natural Language Localization Networks</title>
    <summary>  We propose weakly supervised language localization networks (WSLLN) to detect
events in long, untrimmed videos given language queries. To learn the
correspondence between visual segments and texts, most previous methods require
temporal coordinates (start and end times) of events for training, which leads
to high costs of annotation. WSLLN relieves the annotation burden by training
with only video-sentence pairs without accessing to temporal locations of
events. With a simple end-to-end structure, WSLLN measures segment-text
consistency and conducts segment selection (conditioned on the text)
simultaneously. Results from both are merged and optimized as a video-sentence
matching problem. Experiments on ActivityNet Captions and DiDeMo demonstrate
that WSLLN achieves state-of-the-art performance.
</summary>
    <author>
      <name>Mingfei Gao</name>
    </author>
    <author>
      <name>Larry S. Davis</name>
    </author>
    <author>
      <name>Richard Socher</name>
    </author>
    <author>
      <name>Caiming Xiong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by EMNLP2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.00239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.00239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2302.09850v1</id>
    <updated>2023-02-20T09: 14: 41Z</updated>
    <published>2023-02-20T09: 14: 41Z</published>
    <title>Constraint and Union for Partially-Supervised Temporal Sentence
  Grounding</title>
    <summary>  Temporal sentence grounding aims to detect the event timestamps described by
the natural language query from given untrimmed videos. The existing
fully-supervised setting achieves great performance but requires expensive
annotation costs; while the weakly-supervised setting adopts cheap labels but
performs poorly. To pursue high performance with less annotation cost, this
paper introduces an intermediate partially-supervised setting, i.e., only
short-clip or even single-frame labels are available during training. To take
full advantage of partial labels, we propose a novel quadruple constraint
pipeline to comprehensively shape event-query aligned representations, covering
intra- and inter-samples, uni- and multi-modalities. The former raises
intra-cluster compactness and inter-cluster separability; while the latter
enables event-background separation and event-query gather. To achieve more
powerful performance with explicit grounding optimization, we further introduce
a partial-full union framework, i.e., bridging with an additional
fully-supervised branch, to enjoy its impressive grounding bonus, and be robust
to partial annotations. Extensive experiments and ablations on Charades-STA and
ActivityNet Captions demonstrate the significance of partial supervision and
our superior performance.
</summary>
    <author>
      <name>Chen Ju</name>
    </author>
    <author>
      <name>Haicheng Wang</name>
    </author>
    <author>
      <name>Jinxiang Liu</name>
    </author>
    <author>
      <name>Chaofan Ma</name>
    </author>
    <author>
      <name>Ya Zhang</name>
    </author>
    <author>
      <name>Peisen Zhao</name>
    </author>
    <author>
      <name>Jianlong Chang</name>
    </author>
    <author>
      <name>Qi Tian</name>
    </author>
    <link href="http://arxiv.org/abs/2302.09850v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.09850v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2406.14206v1</id>
    <updated>2024-06-20T11: 25: 16Z</updated>
    <published>2024-06-20T11: 25: 16Z</published>
    <title>Live Video Captioning</title>
    <summary>  Dense video captioning is the task that involves the detection and
description of events within video sequences. While traditional approaches
focus on offline solutions where the entire video of analysis is available for
the captioning model, in this work we introduce a paradigm shift towards Live
Video Captioning (LVC). In LVC, dense video captioning models must generate
captions for video streams in an online manner, facing important constraints
such as having to work with partial observations of the video, the need for
temporal anticipation and, of course, ensuring ideally a real-time response. In
this work we formally introduce the novel problem of LVC and propose new
evaluation metrics tailored for the online scenario, demonstrating their
superiority over traditional metrics. We also propose an LVC model integrating
deformable transformers and temporal filtering to address the LVC new
challenges. Experimental evaluations on the ActivityNet Captions dataset
validate the effectiveness of our approach, highlighting its performance in LVC
compared to state-of-the-art offline methods. Results of our model as well as
an evaluation kit with the novel metrics integrated are made publicly available
to encourage further research on LVC.
</summary>
    <author>
      <name>Eduardo Blanco-Fernández</name>
    </author>
    <author>
      <name>Carlos Gutiérrez-Álvarez</name>
    </author>
    <author>
      <name>Nadia Nasri</name>
    </author>
    <author>
      <name>Saturnino Maldonado-Bascón</name>
    </author>
    <author>
      <name>Roberto J. López-Sastre</name>
    </author>
    <link href="http://arxiv.org/abs/2406.14206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.14206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2502.12917v1</id>
    <updated>2025-02-18T14: 59: 18Z</updated>
    <published>2025-02-18T14: 59: 18Z</published>
    <title>Contrast-Unity for Partially-Supervised Temporal Sentence Grounding</title>
    <summary>  Temporal sentence grounding aims to detect event timestamps described by the
natural language query from given untrimmed videos. The existing
fully-supervised setting achieves great results but requires expensive
annotation costs; while the weakly-supervised setting adopts cheap labels but
performs poorly. To pursue high performance with less annotation costs, this
paper introduces an intermediate partially-supervised setting, i.e., only
short-clip is available during training. To make full use of partial labels, we
specially design one contrast-unity framework, with the two-stage goal of
implicit-explicit progressive grounding. In the implicit stage, we align
event-query representations at fine granularity using comprehensive quadruple
contrastive learning: event-query gather, event-background separation,
intra-cluster compactness and inter-cluster separability. Then, high-quality
representations bring acceptable grounding pseudo-labels. In the explicit
stage, to explicitly optimize grounding objectives, we train one
fully-supervised model using obtained pseudo-labels for grounding refinement
and denoising. Extensive experiments and thoroughly ablations on Charades-STA
and ActivityNet Captions demonstrate the significance of partial supervision,
as well as our superior performance.
</summary>
    <author>
      <name>Haicheng Wang</name>
    </author>
    <author>
      <name>Chen Ju</name>
    </author>
    <author>
      <name>Weixiong Lin</name>
    </author>
    <author>
      <name>Chaofan Ma</name>
    </author>
    <author>
      <name>Shuai Xiao</name>
    </author>
    <author>
      <name>Ya Zhang</name>
    </author>
    <author>
      <name>Yanfeng Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICASSP 2025.The first two authors share the same
  contribution. arXiv admin note: text overlap with arXiv: 2302.09850</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.12917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.12917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2307.05721v1</id>
    <updated>2023-07-09T08: 44: 46Z</updated>
    <published>2023-07-09T08: 44: 46Z</published>
    <title>HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly
  Knowledge Understanding</title>
    <summary>  Understanding comprehensive assembly knowledge from videos is critical for
futuristic ultra-intelligent industry. To enable technological breakthrough, we
present HA-ViD - the first human assembly video dataset that features
representative industrial assembly scenarios, natural procedural knowledge
acquisition process, and consistent human-robot shared annotations.
Specifically, HA-ViD captures diverse collaboration patterns of real-world
assembly, natural human behaviors and learning progression during assembly, and
granulate action annotations to subject, action verb, manipulated object,
target object, and tool. We provide 3222 multi-view, multi-modality videos
(each video contains one assembly task),
            1.5M frames,
            96K temporal labels and
2M spatial labels. We benchmark four foundational video understanding tasks:
action recognition, action segmentation, object detection and multi-object
tracking. Importantly, we analyze their performance for comprehending knowledge
in assembly progress, process efficiency, task collaboration, skill parameters
and human intention. Details of HA-ViD is available at:
https: //iai-hrc.github.io/ha-vid.
</summary>
    <author>
      <name>Hao Zheng</name>
    </author>
    <author>
      <name>Regina Lee</name>
    </author>
    <author>
      <name>Yuqian Lu</name>
    </author>
    <link href="http://arxiv.org/abs/2307.05721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.05721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2312.03799v2</id>
    <updated>2024-03-19T16: 08: 37Z</updated>
    <published>2023-12-06T14: 58: 03Z</published>
    <title>Low-power, Continuous Remote Behavioral Localization with Event Cameras</title>
    <summary>  Researchers in natural science need reliable methods for quantifying animal
behavior. Recently, numerous computer vision methods emerged to automate the
process. However, observing wild species at remote locations remains a
challenging task due to difficult lighting conditions and constraints on power
supply and data storage. Event cameras offer unique advantages for
battery-dependent remote monitoring due to their low power consumption and high
dynamic range capabilities. We use this novel sensor to quantify a behavior in
Chinstrap penguins called ecstatic display. We formulate the problem as a
temporal action detection task, determining the start and end times of the
behavior. For this purpose, we recorded a colony of breeding penguins in
Antarctica for several weeks and labeled event data on 16 nests. The developed
method consists of a generator of candidate time intervals (proposals) and a
classifier of the actions within them. The experiments show that the event
cameras' natural response to motion is effective for continuous behavior
monitoring and detection, reaching a mean average precision (mAP) of 58% (which
increases to 63% in good weather conditions). The results also demonstrate the
robustness against various lighting conditions contained in the challenging
dataset. The low-power capabilities of the event camera allow it to record
significantly longer than with a conventional camera. This work pioneers the
use of event cameras for remote wildlife observation, opening new
interdisciplinary opportunities. https: //tub-rip.github.io/eventpenguins/
</summary>
    <author>
      <name>Friedhelm Hamann</name>
    </author>
    <author>
      <name>Suman Ghosh</name>
    </author>
    <author>
      <name>Ignacio Juarez Martinez</name>
    </author>
    <author>
      <name>Tom Hart</name>
    </author>
    <author>
      <name>Alex Kacelnik</name>
    </author>
    <author>
      <name>Guillermo Gallego</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages,
            8 figures,
            12 tables, Project page:
  https: //tub-rip.github.io/eventpenguins/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  Seattle,
            2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2312.03799v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.03799v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.23214v1</id>
    <updated>2025-03-29T20: 32: 22Z</updated>
    <published>2025-03-29T20: 32: 22Z</published>
    <title>Action Recognition in Real-World Ambient Assisted Living Environment</title>
    <summary>  The growing ageing population and their preference to maintain independence
by living in their own homes require proactive strategies to ensure safety and
support. Ambient Assisted Living (AAL) technologies have emerged to facilitate
ageing in place by offering continuous monitoring and assistance within the
home. Within AAL technologies, action recognition plays a crucial role in
interpreting human activities and detecting incidents like falls, mobility
decline, or unusual behaviours that may signal worsening health conditions.
However, action recognition in practical AAL applications presents challenges,
including occlusions, noisy data, and the need for real-time performance. While
advancements have been made in accuracy, robustness to noise, and computation
efficiency, achieving a balance among them all remains a challenge. To address
this challenge, this paper introduces the Robust and Efficient Temporal
Convolution network (RE-TCN), which comprises three main elements: Adaptive
Temporal Weighting (ATW), Depthwise Separable Convolutions (DSC), and data
augmentation techniques. These elements aim to enhance the model's accuracy,
robustness against noise and occlusion, and computational efficiency within
real-world AAL contexts. RE-TCN outperforms existing models in terms of
accuracy, noise and occlusion robustness, and has been validated on four
benchmark datasets: NTU RGB+D 60, Northwestern-UCLA, SHREC'17, and DHG-14/28.
The code is publicly available at: https: //github.com/Gbouna/RE-TCN
</summary>
    <author>
      <name>Vincent Gbouna Zakka</name>
    </author>
    <author>
      <name>Zhuangzhuang Dai</name>
    </author>
    <author>
      <name>Luis J. Manso</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.26599/BDMA.2025.9020003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.26599/BDMA.2025.9020003" rel="related"/>
    <link href="http://arxiv.org/abs/2503.23214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.23214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1905.11575v1</id>
    <updated>2019-05-28T02: 29: 12Z</updated>
    <published>2019-05-28T02: 29: 12Z</published>
    <title>Improving Action Localization by Progressive Cross-stream Cooperation</title>
    <summary>  Spatio-temporal action localization consists of three levels of tasks:
spatial localization, action classification, and temporal segmentation. In this
work, we propose a new Progressive Cross-stream Cooperation (PCSC) framework to
use both region proposals and features from one stream (i.e. Flow/RGB) to help
another stream (i.e. RGB/Flow) to iteratively improve action localization
results and generate better bounding boxes in an iterative fashion.
Specifically, we first generate a larger set of region proposals by combining
the latest region proposals from both streams, from which we can readily obtain
a larger set of labelled training samples to help learn better action detection
models. Second, we also propose a new message passing approach to pass
information from one stream to another stream in order to learn better
representations, which also leads to better action detection models. As a
result, our iterative framework progressively improves action localization
results at the frame level. To improve action localization results at the video
level, we additionally propose a new strategy to train class-specific
actionness detectors for better temporal segmentation, which can be readily
learnt by focusing on "confusing" samples from the same action class.
Comprehensive experiments on two benchmark datasets UCF-101-24 and J-HMDB
demonstrate the effectiveness of our newly proposed approaches for
spatio-temporal action localization in realistic scenarios.
</summary>
    <author>
      <name>Rui Su</name>
    </author>
    <author>
      <name>Wanli Ouyang</name>
    </author>
    <author>
      <name>Luping Zhou</name>
    </author>
    <author>
      <name>Dong Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.11575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.11575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1811.12248v1</id>
    <updated>2018-11-29T15: 29: 43Z</updated>
    <published>2018-11-29T15: 29: 43Z</published>
    <title>Discovering Spatio-Temporal Action Tubes</title>
    <summary>  In this paper, we address the challenging problem of spatial and temporal
action detection in videos. We first develop an effective approach to localize
frame-level action regions through integrating static and kinematic information
by the early- and late-fusion detection scheme. With the intention of exploring
important temporal connections among the detected action regions, we propose a
tracking-by-point-matching algorithm to stitch the discrete action regions into
a continuous spatio-temporal action tube. Recurrent 3D convolutional neural
network is used to predict action categories and determine temporal boundaries
of the generated tubes. We then introduce an action footprint map to refine the
candidate tubes based on the action-specific spatial characteristics preserved
in the convolutional layers of R3DCNN. In the extensive experiments, our method
achieves superior detection results on the three public benchmark datasets:
UCFSports, J-HMDB and UCF101.
</summary>
    <author>
      <name>Yuancheng Ye</name>
    </author>
    <author>
      <name>Xiaodong Yang</name>
    </author>
    <author>
      <name>Yingli Tian</name>
    </author>
    <link href="http://arxiv.org/abs/1811.12248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.12248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2105.11107v3</id>
    <updated>2022-10-20T10: 30: 59Z</updated>
    <published>2021-05-24T06: 06: 32Z</published>
    <title>FineAction: A Fine-Grained Video Dataset for Temporal Action
  Localization</title>
    <summary>  Temporal action localization (TAL) is an important and challenging problem in
video understanding. However, most existing TAL benchmarks are built upon the
coarse granularity of action classes, which exhibits two major limitations in
this task. First, coarse-level actions can make the localization models overfit
in high-level context information, and ignore the atomic action details in the
video. Second, the coarse action classes often lead to the ambiguous
annotations of temporal boundaries, which are inappropriate for temporal action
localization. To tackle these problems, we develop a novel large-scale and
fine-grained video dataset, coined as FineAction, for temporal action
localization. In total, FineAction contains 103K temporal instances of 106
action categories, annotated in 17K untrimmed videos. Compared to the existing
TAL datasets, our FineAction takes distinct characteristics of fine action
classes with rich diversity, dense annotations of multiple instances, and
co-occurring actions of different classes, which introduces new opportunities
and challenges for temporal action localization. To benchmark FineAction, we
systematically investigate the performance of several popular temporal
localization methods on it, and deeply analyze the influence of fine-grained
instances in temporal action localization. As a minor contribution, we present
a simple baseline approach for handling the fine-grained action detection,
which achieves an mAP of 13.17% on our FineAction. We believe that FineAction
can advance research of temporal action localization and beyond.
</summary>
    <author>
      <name>Yi Liu</name>
    </author>
    <author>
      <name>Limin Wang</name>
    </author>
    <author>
      <name>Yali Wang</name>
    </author>
    <author>
      <name>Xiao Ma</name>
    </author>
    <author>
      <name>Yu Qiao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IEEE T-IP. HomePage:
  https: //deeperaction.github.io/datasets/fineaction</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.11107v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.11107v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2112.03902v2</id>
    <updated>2022-03-29T13: 02: 47Z</updated>
    <published>2021-12-07T18: 57: 37Z</published>
    <title>MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection</title>
    <summary>  Action detection is an essential and challenging task, especially for densely
labelled datasets of untrimmed videos. The temporal relation is complex in
those datasets, including challenges like composite action, and co-occurring
action. For detecting actions in those complex videos, efficiently capturing
both short-term and long-term temporal information in the video is critical. To
this end, we propose a novel ConvTransformer network for action detection. This
network comprises three main components: (1) Temporal Encoder module
extensively explores global and local temporal relations at multiple temporal
resolutions. (2) Temporal Scale Mixer module effectively fuses the multi-scale
features to have a unified feature representation. (3) Classification module is
used to learn the instance center-relative position and predict the frame-level
classification scores. The extensive experiments on multiple datasets,
including Charades, TSU and MultiTHUMOS, confirm the effectiveness of our
proposed method. Our network outperforms the state-of-the-art methods on all
three datasets.
</summary>
    <author>
      <name>Rui Dai</name>
    </author>
    <author>
      <name>Srijan Das</name>
    </author>
    <author>
      <name>Kumara Kahatapitiya</name>
    </author>
    <author>
      <name>Michael S. Ryoo</name>
    </author>
    <author>
      <name>Francois Bremond</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in CVPR 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.03902v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.03902v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2007.06866v1</id>
    <updated>2020-07-14T07: 20: 14Z</updated>
    <published>2020-07-14T07: 20: 14Z</published>
    <title>Alleviating Over-segmentation Errors by Detecting Action Boundaries</title>
    <summary>  We propose an effective framework for the temporal action segmentation task,
namely an Action Segment Refinement Framework (ASRF). Our model architecture
consists of a long-term feature extractor and two branches: the Action
Segmentation Branch (ASB) and the Boundary Regression Branch (BRB). The
long-term feature extractor provides shared features for the two branches with
a wide temporal receptive field. The ASB classifies video frames with action
classes, while the BRB regresses the action boundary probabilities. The action
boundaries predicted by the BRB refine the output from the ASB, which results
in a significant performance improvement. Our contributions are three-fold: (i)
We propose a framework for temporal action segmentation, the ASRF, which
divides temporal action segmentation into frame-wise action classification and
action boundary regression. Our framework refines frame-level hypotheses of
action classes using predicted action boundaries. (ii) We propose a loss
function for smoothing the transition of action probabilities, and analyze
combinations of various loss functions for temporal action segmentation. (iii)
Our framework outperforms state-of-the-art methods on three challenging
datasets, offering an improvement of up to 13.7% in terms of segmental edit
distance and up to 16.1% in terms of segmental F1 score. Our code will be
publicly available soon.
</summary>
    <author>
      <name>Yuchi Ishikawa</name>
    </author>
    <author>
      <name>Seito Kasai</name>
    </author>
    <author>
      <name>Yoshimitsu Aoki</name>
    </author>
    <author>
      <name>Hirokatsu Kataoka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.06866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.06866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2104.02036v1</id>
    <updated>2021-04-05T17: 42: 34Z</updated>
    <published>2021-04-05T17: 42: 34Z</published>
    <title>Modelling and Analysis of Magnetic Fields from Skeletal Muscle for
  Valuable Physiological Measurements</title>
    <summary>  MagnetoMyoGraphy (MMG) is a method of studying muscle function via weak
magnetic fields generated from human active organs and tissues. The
correspondence between MMG and electromyography means directly derived from the
Maxwell-Amp\`ere law. Here, upon briefly describing the principles of voltage
distribution inside skeletal muscles due to the electrical stimulation, we
provide a protocol to determine the effects of the magnetic field generated
from a time-changing action potential propagating in a group of skeletal muscle
cells. The position-dependent and the magnetic field behaviour on account of
the different currents in muscle fibres are performed in temporal, spectral and
spatial domains. The procedure covers identification of the fibre
subpopulations inside the fascicles of a given nerve section, characterization
of soleus skeletal muscle currents, check of axial intracellular currents,
calculation of the generated magnetic field ultimately. We expect this protocol
to take approximately 2-3 hours to complete for the whole finite-element
analysis.
</summary>
    <author>
      <name>Siming Zuo</name>
    </author>
    <author>
      <name>Kianoush Nazarpour</name>
    </author>
    <author>
      <name>Dario Farina</name>
    </author>
    <author>
      <name>Philip Broser</name>
    </author>
    <author>
      <name>Hadi Heidari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be submitted to Nature Protocols. 37 Pages and includes 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.02036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.02036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2107.13648v1</id>
    <updated>2021-07-28T21: 37: 18Z</updated>
    <published>2021-07-28T21: 37: 18Z</published>
    <title>Spot What Matters: Learning Context Using Graph Convolutional Networks
  for Weakly-Supervised Action Detection</title>
    <summary>  The dominant paradigm in spatiotemporal action detection is to classify
actions using spatiotemporal features learned by 2D or 3D Convolutional
Networks. We argue that several actions are characterized by their context,
such as relevant objects and actors present in the video. To this end, we
introduce an architecture based on self-attention and Graph Convolutional
Networks in order to model contextual cues, such as actor-actor and
actor-object interactions, to improve human action detection in video. We are
interested in achieving this in a weakly-supervised setting, i.e. using as less
annotations as possible in terms of action bounding boxes. Our model aids
explainability by visualizing the learned context as an attention map, even for
actions and objects unseen during training. We evaluate how well our model
highlights the relevant context by introducing a quantitative metric based on
recall of objects retrieved by attention maps. Our model relies on a 3D
convolutional RGB stream, and does not require expensive optical flow
computation. We evaluate our models on the DALY dataset, which consists of
human-object interaction actions. Experimental results show that our
contextualized approach outperforms a baseline action detection approach by
more than 2 points in Video-mAP. Code is available at
\url{https: //github.com/micts/acgcn}
</summary>
    <author>
      <name>Michail Tsiaousis</name>
    </author>
    <author>
      <name>Gertjan Burghouts</name>
    </author>
    <author>
      <name>Fieke Hillerström</name>
    </author>
    <author>
      <name>Peter van der Putten</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-68799-1_9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-68799-1_9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the International Workshop on Deep Learning for
  Human-Centric Activity Understanding (DL-HAU2020), January 11,
                2021</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Workshop on Deep Learning for Human-Centric Activity
  Understanding (DL-HAU2020), January 11,
                2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2107.13648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.13648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1705.01180v1</id>
    <updated>2017-05-02T21: 45: 21Z</updated>
    <published>2017-05-02T21: 45: 21Z</published>
    <title>Cascaded Boundary Regression for Temporal Action Detection</title>
    <summary>  Temporal action detection in long videos is an important problem.
State-of-the-art methods address this problem by applying action classifiers on
sliding windows. Although sliding windows may contain an identifiable portion
of the actions, they may not necessarily cover the entire action instance,
which would lead to inferior performance. We adapt a two-stage temporal action
detection pipeline with Cascaded Boundary Regression (CBR) model.
Class-agnostic proposals and specific actions are detected respectively in the
first and the second stage. CBR uses temporal coordinate regression to refine
the temporal boundaries of the sliding windows. The salient aspect of the
refinement process is that, inside each stage, the temporal boundaries are
adjusted in a cascaded way by feeding the refined windows back to the system
for further boundary refinement. We test CBR on THUMOS-14 and TVSeries, and
achieve state-of-the-art performance on both datasets. The performance gain is
especially remarkable under high IoU thresholds, e.g. map@tIoU=0.5 on THUMOS-14
is improved from 19.0% to 31.0%.
</summary>
    <author>
      <name>Jiyang Gao</name>
    </author>
    <author>
      <name>Zhenheng Yang</name>
    </author>
    <author>
      <name>Ram Nevatia</name>
    </author>
    <link href="http://arxiv.org/abs/1705.01180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2110.13473v2</id>
    <updated>2022-07-11T12: 55: 51Z</updated>
    <published>2021-10-26T08: 15: 47Z</published>
    <title>CTRN: Class-Temporal Relational Network for Action Detection</title>
    <summary>  Action detection is an essential and challenging task, especially for densely
labelled datasets of untrimmed videos. There are many real-world challenges in
those datasets, such as composite action, co-occurring action, and high
temporal variation of instance duration. For handling these challenges, we
propose to explore both the class and temporal relations of detected actions.
In this work, we introduce an end-to-end network: Class-Temporal Relational
Network (CTRN). It contains three key components: (1) The Representation
Transform Module filters the class-specific features from the mixed
representations to build graph-structured data. (2) The Class-Temporal Module
models the class and temporal relations in a sequential manner. (3)
G-classifier leverages the privileged knowledge of the snippet-wise
co-occurring action pairs to further improve the co-occurring action detection.
We evaluate CTRN on three challenging densely labelled datasets and achieve
state-of-the-art performance, reflecting the effectiveness and robustness of
our method.
</summary>
    <author>
      <name>Rui Dai</name>
    </author>
    <author>
      <name>Srijan Das</name>
    </author>
    <author>
      <name>Francois Bremond</name>
    </author>
    <link href="http://arxiv.org/abs/2110.13473v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.13473v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2004.11006v1</id>
    <updated>2020-04-23T07: 17: 20Z</updated>
    <published>2020-04-23T07: 17: 20Z</published>
    <title>Stratospheric temperature anomalies as imprints from the dark Universe</title>
    <summary>  The manifestation of the dark Universe begun with unexpected large scale
astronomical observations. Here we are investigating the origin of small scale
anomalies, like that of the annually observed temperature anomalies in the
stratosphere. Unexpectedly, we observe a planetary relationship of the daily
stratospheric temperature distribution. Its spectral shape does not match
concurrent solar activity, or solar EUV emission, whose impact on the
atmosphere is unequivocal. This behavior points at an additional energy source
of exosolar origin. A viable concept behind such observations is based on
possible gravitational focusing by the solar system towards the Earth of low
speed invisible matter. We denote generic constituents from the dark Universe
as invisible matter, in order to distinguish them from ordinary dark matter
candidates like axions or WIMPs, which cannot have any noticeable impact on the
stratosphere. The observed peaking planetary relations exclude on their own any
conventional explanation. Only a somehow strongly interacting invisible
streaming matter with the little screened upper stratosphere (noverhead about 1
gr/cm 2) can be behind the occasionally observed temperature increases. We also
estimate an associated energy deposition O(W/m 2), which is variable over the
11 years solar cycle. For the widely assumed picture of a quasi not interacting
dark Universe, this new exosolar energy is enormous. Since the atmosphere is
uninterruptedly monitored since decades, it can serve also parasitically as a
novel low threshold detector for the dark Universe, with built-in
spatiotemporal resolution and the solar system acting temporally as signal
amplifier. In future, analyzing more observations, for example, from the
anomalous ionosphere, or, the transient sudden stratospheric warmings, the
nature of the assumed invisible streams could be deciphered.
</summary>
    <author>
      <name>K. Zioutas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Patras</arxiv:affiliation>
    </author>
    <author>
      <name>A. Argiriou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Patras</arxiv:affiliation>
    </author>
    <author>
      <name>H. Fischer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Freiburg</arxiv:affiliation>
    </author>
    <author>
      <name>S. Hofmann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Munich</arxiv:affiliation>
    </author>
    <author>
      <name>M. Maroudas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Patras</arxiv:affiliation>
    </author>
    <author>
      <name>A. Pappa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Patras</arxiv:affiliation>
    </author>
    <author>
      <name>Y. K. Semertzidis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Daejeon</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.dark.2020.100497</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.dark.2020.100497" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages,
                11 Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physics of the Dark Universe 28 (2020) 100497</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2004.11006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.11006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2402.02085v8</id>
    <updated>2025-04-20T11: 47: 02Z</updated>
    <published>2024-02-03T08: 52: 06Z</published>
    <title>Detecting AI-Generated Video via Frame Consistency</title>
    <summary>  The escalating quality of video generated by advanced video generation
methods results in new security challenges, while there have been few relevant
research efforts: 1) There is no open-source dataset for generated video
detection,
                2) No generated video detection method has been proposed so far. To
this end, we propose an open-source dataset and a detection method for
generated video for the first time. First, we propose a scalable dataset
consisting of 964 prompts, covering various forgery targets, scenes, behaviors,
and actions, as well as various generation models with different architectures
and generation methods, including the most popular commercial models like
OpenAI's Sora and Google's Veo. Second, we found via probing experiments that
spatial artifact-based detectors lack generalizability. Hence, we propose a
simple yet effective \textbf{de
                }tection model based on \textbf{f
                }rame
\textbf{co
                }nsistency (\textbf{DeCoF
                }), which focuses on temporal artifacts by
eliminating the impact of spatial artifacts during feature learning. Extensive
experiments demonstrate the efficacy of DeCoF in detecting videos generated by
unseen video generation models and confirm its powerful generalizability across
several commercially proprietary models.
</summary>
    <author>
      <name>Long Ma</name>
    </author>
    <author>
      <name>Zhiyuan Yan</name>
    </author>
    <author>
      <name>Qinglang Guo</name>
    </author>
    <author>
      <name>Yong Liao</name>
    </author>
    <author>
      <name>Haiyang Yu</name>
    </author>
    <author>
      <name>Pengyuan Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2402.02085v8" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.02085v8" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2203.01542v1</id>
    <updated>2022-03-03T06: 52: 13Z</updated>
    <published>2022-03-03T06: 52: 13Z</published>
    <title>SegTAD: Precise Temporal Action Detection via Semantic Segmentation</title>
    <summary>  Temporal action detection (TAD) is an important yet challenging task in video
analysis. Most existing works draw inspiration from image object detection and
tend to reformulate it as a proposal generation - classification problem.
However, there are two caveats with this paradigm. First, proposals are not
equipped with annotated labels, which have to be empirically compiled, thus the
information in the annotations is not necessarily precisely employed in the
model training process. Second, there are large variations in the temporal
scale of actions, and neglecting this fact may lead to deficient representation
in the video features. To address these issues and precisely model temporal
action detection, we formulate the task of temporal action detection in a novel
perspective of semantic segmentation. Owing to the 1-dimensional property of
TAD, we are able to convert the coarse-grained detection annotations to
fine-grained semantic segmentation annotations for free. We take advantage of
them to provide precise supervision so as to mitigate the impact induced by the
imprecise proposal labels. We propose an end-to-end framework SegTAD composed
of a 1D semantic segmentation network (1D-SSN) and a proposal detection network
(PDN).
</summary>
    <author>
      <name>Chen Zhao</name>
    </author>
    <author>
      <name>Merey Ramazanova</name>
    </author>
    <author>
      <name>Mengmeng Xu</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <link href="http://arxiv.org/abs/2203.01542v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.01542v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1808.00297v1</id>
    <updated>2018-08-01T12: 33: 57Z</updated>
    <published>2018-08-01T12: 33: 57Z</published>
    <title>TraMNet - Transition Matrix Network for Efficient Action Tube Proposals</title>
    <summary>  Current state-of-the-art methods solve spatiotemporal action localisation by
extending 2D anchors to 3D-cuboid proposals on stacks of frames, to generate
sets of temporally connected bounding boxes called \textit{action micro-tubes
                }.
However, they fail to consider that the underlying anchor proposal hypotheses
should also move (transition) from frame to frame, as the actor or the camera
does. Assuming we evaluate $n$ 2D anchors in each frame, then the number of
possible transitions from each 2D anchor to the next, for a sequence of $f$
consecutive frames, is in the order of $O(n^f)$, expensive even for small
values of $f$. To avoid this problem, we introduce a Transition-Matrix-based
Network (TraMNet) which relies on computing transition probabilities between
anchor proposals while maximising their overlap with ground truth bounding
boxes across frames, and enforcing sparsity via a transition threshold. As the
resulting transition matrix is sparse and stochastic, this reduces the proposal
hypothesis search space from $O(n^f)$ to the cardinality of the thresholded
matrix. At training time, transitions are specific to cell locations of the
feature maps, so that a sparse (efficient) transition matrix is used to train
the network. At test time, a denser transition matrix can be obtained either by
decreasing the threshold or by adding to it all the relative transitions
originating from any cell location, allowing the network to handle transitions
in the test data that might not have been present in the training data, and
making detection translation-invariant. Finally, we show that our network can
handle sparse annotations such as those available in the DALY dataset. We
report extensive experiments on the DALY, UCF101-24 and Transformed-UCF101-24
datasets to support our claims.
</summary>
    <author>
      <name>Gurkirt Singh</name>
    </author>
    <author>
      <name>Suman Saha</name>
    </author>
    <author>
      <name>Fabio Cuzzolin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1904.07774v4</id>
    <updated>2020-01-06T02: 46: 05Z</updated>
    <published>2019-04-16T15: 48: 36Z</published>
    <title>Weakly Supervised Gaussian Networks for Action Detection</title>
    <summary>  Detecting temporal extents of human actions in videos is a challenging
computer vision problem that requires detailed manual supervision including
frame-level labels. This expensive annotation process limits deploying action
detectors to a limited number of categories. We propose a novel method, called
WSGN, that learns to detect actions from \emph{weak supervision
                }, using only
video-level labels. WSGN learns to exploit both video-specific and dataset-wide
statistics to predict relevance of each frame to an action category. This
strategy leads to significant gains in action detection for two standard
benchmarks THUMOS14 and Charades. Our method obtains excellent results compared
to state-of-the-art methods that uses similar features and loss functions on
THUMOS14 dataset. Similarly, our weakly supervised method is only 0.3% mAP
behind a state-of-the-art supervised method on challenging Charades dataset for
action localization.
</summary>
    <author>
      <name>Basura Fernando</name>
    </author>
    <author>
      <name>Cheston Tan Yin Chet</name>
    </author>
    <author>
      <name>Hakan Bilen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in WACV 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.07774v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.07774v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1706.07251v1</id>
    <updated>2017-06-22T10: 59: 33Z</updated>
    <published>2017-06-22T10: 59: 33Z</published>
    <title>A Self-Adaptive Proposal Model for Temporal Action Detection based on
  Reinforcement Learning</title>
    <summary>  Existing action detection algorithms usually generate action proposals
through an extensive search over the video at multiple temporal scales, which
brings about huge computational overhead and deviates from the human perception
procedure. We argue that the process of detecting actions should be naturally
one of observation and refinement: observe the current window and refine the
span of attended window to cover true action regions. In this paper, we propose
an active action proposal model that learns to find actions through
continuously adjusting the temporal bounds in a self-adaptive way. The whole
process can be deemed as an agent, which is firstly placed at a position in the
video at random, adopts a sequence of transformations on the current attended
region to discover actions according to a learned policy. We utilize
reinforcement learning, especially the Deep Q-learning algorithm to learn the
agent's decision policy. In addition, we use temporal pooling operation to
extract more effective feature representation for the long temporal window, and
design a regression network to adjust the position offsets between predicted
results and the ground truth. Experiment results on THUMOS 2014 validate the
effectiveness of the proposed approach, which can achieve competitive
performance with current action detection algorithms via much fewer proposals.
</summary>
    <author>
      <name>Jingjia Huang</name>
    </author>
    <author>
      <name>Nannan Li</name>
    </author>
    <author>
      <name>Tao Zhang</name>
    </author>
    <author>
      <name>Ge Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Deep Reinforcement Learning, Action Temporal Detection, Temporal
  Location Regression</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2105.07404v2</id>
    <updated>2021-08-18T05: 27: 50Z</updated>
    <published>2021-05-16T10: 40: 30Z</published>
    <title>MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized
  Sports Actions</title>
    <summary>  Spatio-temporal action detection is an important and challenging problem in
video understanding. The existing action detection benchmarks are limited in
aspects of small numbers of instances in a trimmed video or low-level atomic
actions. This paper aims to present a new multi-person dataset of
spatio-temporal localized sports actions, coined as MultiSports. We first
analyze the important ingredients of constructing a realistic and challenging
dataset for spatio-temporal action detection by proposing three criteria: (1)
multi-person scenes and motion dependent identification, (2) with well-defined
boundaries, (3) relatively fine-grained classes of high complexity. Based on
these guide-lines, we build the dataset of MultiSports v1.0 by selecting 4
sports classes, collecting 3200 video clips, and annotating 37701 action
instances with 902k bounding boxes. Our datasets are characterized with
important properties of high diversity, dense annotation, and high quality. Our
Multi-Sports, with its realistic setting and detailed annotations, exposes the
intrinsic challenges of spatio-temporal action detection. To benchmark this, we
adapt several baseline methods to our dataset and give an in-depth analysis on
the action detection results in our dataset. We hope our MultiSports can serve
as a standard benchmark for spatio-temporal action detection in the future. Our
dataset website is at https: //deeperaction.github.io/multisports/.
</summary>
    <author>
      <name>Yixuan Li</name>
    </author>
    <author>
      <name>Lei Chen</name>
    </author>
    <author>
      <name>Runyu He</name>
    </author>
    <author>
      <name>Zhenzhi Wang</name>
    </author>
    <author>
      <name>Gangshan Wu</name>
    </author>
    <author>
      <name>Limin Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV 2021 camera ready version. One track of DeeperAction
  Workshop@ICCV2021. HomePage: https: //deeperaction.github.io/multisports/</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.07404v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.07404v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2204.02932v1</id>
    <updated>2022-04-06T16: 46: 30Z</updated>
    <published>2022-04-06T16: 46: 30Z</published>
    <title>An Empirical Study of End-to-End Temporal Action Detection</title>
    <summary>  Temporal action detection (TAD) is an important yet challenging task in video
understanding. It aims to simultaneously predict the semantic label and the
temporal interval of every action instance in an untrimmed video. Rather than
end-to-end learning, most existing methods adopt a head-only learning paradigm,
where the video encoder is pre-trained for action classification, and only the
detection head upon the encoder is optimized for TAD. The effect of end-to-end
learning is not systematically evaluated. Besides, there lacks an in-depth
study on the efficiency-accuracy trade-off in end-to-end TAD. In this paper, we
present an empirical study of end-to-end temporal action detection. We validate
the advantage of end-to-end learning over head-only learning and observe up to
11\% performance improvement. Besides, we study the effects of multiple design
choices that affect the TAD performance and speed, including detection head,
video encoder, and resolution of input videos. Based on the findings, we build
a mid-resolution baseline detector, which achieves the state-of-the-art
performance of end-to-end methods while running more than 4$\times$ faster. We
hope that this paper can serve as a guide for end-to-end learning and inspire
future research in this field. Code and models are available at
\url{https: //github.com/xlliu7/E2E-TAD}.
</summary>
    <author>
      <name>Xiaolong Liu</name>
    </author>
    <author>
      <name>Song Bai</name>
    </author>
    <author>
      <name>Xiang Bai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVPR 2022. 13 pages, including supplementary</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.02932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.02932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2203.10584v1</id>
    <updated>2022-03-20T15: 41: 47Z</updated>
    <published>2022-03-20T15: 41: 47Z</published>
    <title>Point3D: tracking actions as moving points with 3D CNNs</title>
    <summary>  Spatio-temporal action recognition has been a challenging task that involves
detecting where and when actions occur. Current state-of-the-art action
detectors are mostly anchor-based, requiring sensitive anchor designs and huge
computations due to calculating large numbers of anchor boxes. Motivated by
nascent anchor-free approaches, we propose Point3D, a flexible and
computationally efficient network with high precision for spatio-temporal
action recognition. Our Point3D consists of a Point Head for action
localization and a 3D Head for action classification. Firstly, Point Head is
used to track center points and knot key points of humans to localize the
bounding box of an action. These location features are then piped into a
time-wise attention to learn long-range dependencies across frames. The 3D Head
is later deployed for the final action classification. Our Point3D achieves
state-of-the-art performance on the JHMDB, UCF101-24, and AVA benchmarks in
terms of frame-mAP and video-mAP. Comprehensive ablation studies also
demonstrate the effectiveness of each module proposed in our Point3D.
</summary>
    <author>
      <name>Shentong Mo</name>
    </author>
    <author>
      <name>Jingfei Xia</name>
    </author>
    <author>
      <name>Xiaoqing Tan</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by the 32nd British Machine Vision Conference (BMVC 2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.10584v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.10584v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2311.14775v2</id>
    <updated>2024-07-08T16: 59: 16Z</updated>
    <published>2023-11-24T15: 07: 29Z</published>
    <title>VSViG: Real-time Video-based Seizure Detection via Skeleton-based
  Spatiotemporal ViG</title>
    <summary>  An accurate and efficient epileptic seizure onset detection can significantly
benefit patients. Traditional diagnostic methods, primarily relying on
electroencephalograms (EEGs), often result in cumbersome and non-portable
solutions, making continuous patient monitoring challenging. The video-based
seizure detection system is expected to free patients from the constraints of
scalp or implanted EEG devices and enable remote monitoring in residential
settings. Previous video-based methods neither enable all-day monitoring nor
provide short detection latency due to insufficient resources and ineffective
patient action recognition techniques. Additionally, skeleton-based action
recognition approaches remain limitations in identifying subtle seizure-related
actions. To address these challenges, we propose a novel Video-based Seizure
detection model via a skeleton-based spatiotemporal Vision Graph neural network
(VSViG) for its efficient, accurate and timely purpose in real-time scenarios.
Our experimental results indicate VSViG outperforms previous state-of-the-art
action recognition models on our collected patients' video data with higher
accuracy (5.9% error), lower FLOPs (0.4G), and smaller model size (1.4M).
Furthermore, by integrating a decision-making rule that combines output
probabilities and an accumulative function, we achieve a 5.1 s detection
latency after EEG onset, a 13.1 s detection advance before clinical onset, and
a zero false detection rate. The project homepage is available at:
https: //github.com/xuyankun/VSViG/
</summary>
    <author>
      <name>Yankun Xu</name>
    </author>
    <author>
      <name>Junzhe Wang</name>
    </author>
    <author>
      <name>Yun-Hsuan Chen</name>
    </author>
    <author>
      <name>Jie Yang</name>
    </author>
    <author>
      <name>Wenjie Ming</name>
    </author>
    <author>
      <name>Shuang Wang</name>
    </author>
    <author>
      <name>Mohamad Sawan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ECCV2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.14775v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.14775v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1910.10335v1</id>
    <updated>2019-10-23T04: 02: 20Z</updated>
    <published>2019-10-23T04: 02: 20Z</published>
    <title>USTAR: Online Multimodal Embedding for Modeling User-Guided
  Spatiotemporal Activity</title>
    <summary>  Building spatiotemporal activity models for people's activities in urban
spaces is important for understanding the ever-increasing complexity of urban
dynamics. With the emergence of Geo-Tagged Social Media (GTSM) records,
previous studies demonstrate the potential of GTSM records for spatiotemporal
activity modeling. State-of-the-art methods for this task embed different
modalities (location, time, and text) of GTSM records into a single embedding
space. However, they ignore Non-GeoTagged Social Media (NGTSM) records, which
generally account for the majority of posts (e.g., more than 95\% in Twitter),
and could represent a great source of information to alleviate the sparsity of
GTSM records. Furthermore, in the current spatiotemporal embedding techniques,
less focus has been given to the users, who exhibit spatially motivated
behaviors. To bridge this research gap, this work proposes USTAR, a novel
online learning method for User-guided SpatioTemporal Activity Representation,
which (1) embeds locations, time, and text along with users into the same
embedding space to capture their correlations; (2) uses a novel collaborative
filtering approach based on two different empirically studied user behaviors to
incorporate both NGTSM and GTSM records in learning; and (3) introduces a novel
sampling technique to learn spatiotemporal representations in an online fashion
to accommodate recent information into the embedding space, while avoiding
overfitting to recent records and frequently appearing units in social media
streams. Our results show that USTAR substantially improves the
state-of-the-art for region retrieval and keyword retrieval and its potential
to be applied to other downstream applications such as local event detection.
</summary>
    <author>
      <name>Amila Silva</name>
    </author>
    <author>
      <name>Shanika Karunasekera</name>
    </author>
    <author>
      <name>Christopher Leckie</name>
    </author>
    <author>
      <name>Ling Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, IEEE International Conference on Big Data 2019 (IEEE Big
  Data 2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.10335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.10335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2008.00088v1</id>
    <updated>2020-07-24T20: 55: 57Z</updated>
    <published>2020-07-24T20: 55: 57Z</published>
    <title>A Comparative Study of AI-based Intrusion Detection Techniques in
  Critical Infrastructures</title>
    <summary>  Volunteer computing uses Internet-connected devices (laptops, PCs, smart
devices, etc.), in which their owners volunteer them as storage and computing
power resources, has become an essential mechanism for resource management in
numerous applications. The growth of the volume and variety of data traffic in
the Internet leads to concerns on the robustness of cyberphysical systems
especially for critical infrastructures. Therefore, the implementation of an
efficient Intrusion Detection System for gathering such sensory data has gained
vital importance. In this paper, we present a comparative study of Artificial
Intelligence (AI)-driven intrusion detection systems for wirelessly connected
sensors that track crucial applications. Specifically, we present an in-depth
analysis of the use of machine learning, deep learning and reinforcement
learning solutions to recognize intrusive behavior in the collected traffic. We
evaluate the proposed mechanisms by using KD'99 as real attack data-set in our
simulations. Results present the performance metrics for three different IDSs
namely the Adaptively Supervised and Clustered Hybrid IDS (ASCH-IDS),
Restricted Boltzmann Machine-based Clustered IDS (RBC-IDS) and Q-learning based
IDS (QL-IDS) to detect malicious behaviors. We also present the performance of
different reinforcement learning techniques such as
State-Action-Reward-State-Action Learning (SARSA) and the Temporal Difference
learning (TD). Through simulations, we show that QL-IDS performs with 100%
detection rate while SARSA-IDS and TD-IDS perform at the order of 99.5%.
</summary>
    <author>
      <name>Safa Otoum</name>
    </author>
    <author>
      <name>Burak Kantarci</name>
    </author>
    <author>
      <name>Hussein Mouftah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transaction on Internet Technology,
                    2020 22 pages,
                    11 Figures,
                    3
  Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.00088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.00088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1707.06005v2</id>
    <updated>2017-07-21T21: 40: 33Z</updated>
    <published>2017-07-19T10: 09: 59Z</published>
    <title>Detecting Parts for Action Localization</title>
    <summary>  In this paper, we propose a new framework for action localization that tracks
people in videos and extracts full-body human tubes, i.e., spatio-temporal
regions localizing actions, even in the case of occlusions or truncations. This
is achieved by training a novel human part detector that scores visible parts
while regressing full-body bounding boxes. The core of our method is a
convolutional neural network which learns part proposals specific to certain
body parts. These are then combined to detect people robustly in each frame.
Our tracking algorithm connects the image detections temporally to extract
full-body human tubes. We apply our new tube extraction method on the problem
of human action localization, on the popular JHMDB dataset, and a very recent
challenging dataset DALY (Daily Action Localization in YouTube), showing
state-of-the-art results.
</summary>
    <author>
      <name>Nicolas Chesneau</name>
    </author>
    <author>
      <name>Grégory Rogez</name>
    </author>
    <author>
      <name>Karteek Alahari</name>
    </author>
    <author>
      <name>Cordelia Schmid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BMVC 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.06005v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06005v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1807.03387v1</id>
    <updated>2018-07-09T21: 07: 14Z</updated>
    <published>2018-07-09T21: 07: 14Z</published>
    <title>Process Monitoring Using Maximum Sequence Divergence</title>
    <summary>  Process Monitoring involves tracking a system's behaviors, evaluating the
current state of the system, and discovering interesting events that require
immediate actions. In this paper, we consider monitoring temporal system state
sequences to help detect the changes of dynamic systems, check the divergence
of the system development, and evaluate the significance of the deviation. We
begin with discussions of data reduction, symbolic data representation, and the
anomaly detection in temporal discrete sequences. Time-series representation
methods are also discussed and used in this paper to discretize raw data into
sequences of system states. Markov Chains and stationary state distributions
are continuously generated from temporal sequences to represent snapshots of
the system dynamics in different time frames. We use generalized Jensen-Shannon
Divergence as the measure to monitor changes of the stationary symbol
probability distributions and evaluate the significance of system deviations.
We prove that the proposed approach is able to detect deviations of the systems
we monitor and assess the deviation significance in probabilistic manner.
</summary>
    <author>
      <name>Yihuang Kang</name>
    </author>
    <author>
      <name>Vladimir Zadorozhny</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10115-015-0858-z</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10115-015-0858-z" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Knowledge and Information Systems 48.1 (2016): 81-109</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.03387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2203.04251v3</id>
    <updated>2022-07-01T05: 36: 23Z</updated>
    <published>2022-03-08T18: 11: 25Z</published>
    <title>End-to-End Semi-Supervised Learning for Video Action Detection</title>
    <summary>  In this work, we focus on semi-supervised learning for video action detection
which utilizes both labeled as well as unlabeled data. We propose a simple
end-to-end consistency based approach which effectively utilizes the unlabeled
data. Video action detection requires both, action class prediction as well as
a spatio-temporal localization of actions. Therefore, we investigate two types
of constraints, classification consistency, and spatio-temporal consistency.
The presence of predominant background and static regions in a video makes it
challenging to utilize spatio-temporal consistency for action detection. To
address this, we propose two novel regularization constraints for
spatio-temporal consistency; 1) temporal coherency, and 2) gradient smoothness.
Both these aspects exploit the temporal continuity of action in videos and are
found to be effective for utilizing unlabeled videos for action detection. We
demonstrate the effectiveness of the proposed approach on two different action
detection benchmark datasets, UCF101-24 and JHMDB-21. In addition, we also show
the effectiveness of the proposed approach for video object segmentation on the
Youtube-VOS which demonstrates its generalization capability The proposed
approach achieves competitive performance by using merely 20% of annotations on
UCF101-24 when compared with recent fully supervised methods. On UCF101-24, it
improves the score by +8.9% and +11% at 0.5 f-mAP and v-mAP respectively,
compared to supervised approach.
</summary>
    <author>
      <name>Akash Kumar</name>
    </author>
    <author>
      <name>Yogesh Singh Rawat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR'22</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.04251v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.04251v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2407.17792v2</id>
    <updated>2024-07-26T01: 16: 07Z</updated>
    <published>2024-07-25T06: 03: 02Z</published>
    <title>Harnessing Temporal Causality for Advanced Temporal Action Detection</title>
    <summary>  As a fundamental task in long-form video understanding, temporal action
detection (TAD) aims to capture inherent temporal relations in untrimmed videos
and identify candidate actions with precise boundaries. Over the years, various
networks, including convolutions, graphs, and transformers, have been explored
for effective temporal modeling for TAD. However, these modules typically treat
past and future information equally, overlooking the crucial fact that changes
in action boundaries are essentially causal events. Inspired by this insight,
we propose leveraging the temporal causality of actions to enhance TAD
representation by restricting the model's access to only past or future
context. We introduce CausalTAD, which combines causal attention and causal
Mamba to achieve state-of-the-art performance on multiple benchmarks. Notably,
with CausalTAD, we ranked 1st in the Action Recognition, Action Detection, and
Audio-Based Interaction Detection tracks at the EPIC-Kitchens Challenge 2024,
as well as 1st in the Moment Queries track at the Ego4D Challenge 2024. Our
code is available at https: //github.com/sming256/OpenTAD/.
</summary>
    <author>
      <name>Shuming Liu</name>
    </author>
    <author>
      <name>Lin Sui</name>
    </author>
    <author>
      <name>Chen-Lin Zhang</name>
    </author>
    <author>
      <name>Fangzhou Mu</name>
    </author>
    <author>
      <name>Chen Zhao</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1st in Moment Queries track at the Ego4D Challenge 2024; 1st in
  Action Recognition, Action Detection, and Audio-Based Interaction Detection
  tracks at the EPIC-Kitchens Challenge 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.17792v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.17792v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1903.00304v1</id>
    <updated>2019-03-01T14: 08: 21Z</updated>
    <published>2019-03-01T14: 08: 21Z</published>
    <title>Progress Regression RNN for Online Spatial-Temporal Action Localization
  in Unconstrained Videos</title>
    <summary>  Previous spatial-temporal action localization methods commonly follow the
pipeline of object detection to estimate bounding boxes and labels of actions.
However, the temporal relation of an action has not been fully explored. In
this paper, we propose an end-to-end Progress Regression Recurrent Neural
Network (PR-RNN) for online spatial-temporal action localization, which learns
to infer the action by temporal progress regression. Two new action attributes,
called progression and progress rate, are introduced to describe the temporal
engagement and relative temporal position of an action. In our method,
frame-level features are first extracted by a Fully Convolutional Network
(FCN). Subsequently, detection results and action progress attributes are
regressed by the Convolutional Gated Recurrent Unit (ConvGRU) based on all the
observed frames instead of a single frame or a short clip. Finally, a novel
online linking method is designed to connect single-frame results to
spatial-temporal tubes with the help of the estimated action progress
attributes. Extensive experiments demonstrate that the progress attributes
improve the localization accuracy by providing more precise temporal position
of an action in unconstrained videos. Our proposed PR-RNN achieves the
stateof-the-art performance for most of the IoU thresholds on two benchmark
datasets.
</summary>
    <author>
      <name>Bo Hu</name>
    </author>
    <author>
      <name>Jianfei Cai</name>
    </author>
    <author>
      <name>Tat-Jen Cham</name>
    </author>
    <author>
      <name>Junsong Yuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages,
                    5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.00304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.00304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2005.10640v1</id>
    <updated>2020-05-04T01: 34: 47Z</updated>
    <published>2020-05-04T01: 34: 47Z</published>
    <title>DETECT: A Hierarchical Clustering Algorithm for Behavioural Trends in
  Temporal Educational Data</title>
    <summary>  Techniques for clustering student behaviour offer many opportunities to
improve educational outcomes by providing insight into student learning.
However, one important aspect of student behaviour, namely its evolution over
time, can often be challenging to identify using existing methods. This is
because the objective functions used by these methods do not explicitly aim to
find cluster trends in time, so these trends may not be clearly represented in
the results. This paper presents `DETECT' (Detection of Educational Trends
Elicited by Clustering Time-series data), a novel divisive hierarchical
clustering algorithm that incorporates temporal information into its objective
function to prioritise the detection of behavioural trends. The resulting
clusters are similar in structure to a decision tree, with a hierarchy of
clusters defined by decision rules on features. DETECT is easy to apply, highly
customisable, applicable to a wide range of educational datasets and yields
easily interpretable results. Through a case study of two online programming
courses (N&gt;600), this paper demonstrates two example applications of DETECT: 1)
to identify how cohort behaviour develops over time and 2) to identify student
behaviours that characterise exercises where many students give up.
</summary>
    <author>
      <name>Jessica McBroom</name>
    </author>
    <author>
      <name>Kalina Yacef</name>
    </author>
    <author>
      <name>Irena Koprinska</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-52237-7_30</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-52237-7_30" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages,
                    4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AIED 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2005.10640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.10640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2311.02538v1</id>
    <updated>2023-11-05T01: 45: 31Z</updated>
    <published>2023-11-05T01: 45: 31Z</published>
    <title>Dense Video Captioning: A Survey of Techniques, Datasets and Evaluation
  Protocols</title>
    <summary>  Untrimmed videos have interrelated events, dependencies, context, overlapping
events, object-object interactions, domain specificity, and other semantics
that are worth highlighting while describing a video in natural language. Owing
to such a vast diversity, a single sentence can only correctly describe a
portion of the video. Dense Video Captioning (DVC) aims at detecting and
describing different events in a given video. The term DVC originated in the
2017 ActivityNet challenge, after which considerable effort has been made to
address the challenge. Dense Video Captioning is divided into three sub-tasks:
(1) Video Feature Extraction (VFE), (2) Temporal Event Localization (TEL), and
(3) Dense Caption Generation (DCG). This review aims to discuss all the studies
that claim to perform DVC along with its sub-tasks and summarize their results.
We also discuss all the datasets that have been used for DVC. Lastly, we
highlight some emerging challenges and future trends in the field.
</summary>
    <author>
      <name>Iqra Qasim</name>
    </author>
    <author>
      <name>Alexander Horsch</name>
    </author>
    <author>
      <name>Dilip K. Prasad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages,
                    10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.02538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.02538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1412.6469v1</id>
    <updated>2014-12-19T18: 08: 02Z</updated>
    <published>2014-12-19T18: 08: 02Z</published>
    <title>A hidden Markov model for decoding and the analysis of replay in spike
  trains</title>
    <summary>  We present a hidden Markov model that describes variation in an animal's
position associated with varying levels of activity in action potential spike
trains of individual place cell neurons. The model incorporates a
coarse-graining of position, which we find to be a more parsimonious
description of the system than other models. We use a sequential Monte Carlo
algorithm for Bayesian inference of model parameters, including the state space
dimension, and we explain how to estimate position from spike train
observations (decoding). We obtain greater accuracy over other methods in the
conditions of high temporal resolution and small neuronal sample size. We also
present a novel, model-based approach to the study of replay: the expression of
spike train activity related to behaviour during times of motionlessness or
sleep, thought to be integral to the consolidation of long-term memories. We
demonstrate how we can detect the time, information content and compression
rate of replay events in simulated and real hippocampal data recorded from rats
in two different environments, and verify the correlation between the times of
detected replay events and of sharp wave/ripples in the local field potential.
</summary>
    <author>
      <name>Marc Box</name>
    </author>
    <author>
      <name>Matt W. Jones</name>
    </author>
    <author>
      <name>Nick Whiteley</name>
    </author>
    <link href="http://arxiv.org/abs/1412.6469v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6469v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2004.10774v1</id>
    <updated>2020-04-22T18: 02: 50Z</updated>
    <published>2020-04-22T18: 02: 50Z</published>
    <title>Action recognition in real-world videos</title>
    <summary>  The goal of human action recognition is to temporally or spatially localize
the human action of interest in video sequences. Temporal localization (i.e.
indicating the start and end frames of the action in a video) is referred to as
frame-level detection. Spatial localization, which is more challenging, means
to identify the pixels within each action frame that correspond to the action.
This setting is usually referred to as pixel-level detection. In this chapter,
we are using action, activity, event interchangeably.
</summary>
    <author>
      <name>Waqas Sultani</name>
    </author>
    <author>
      <name>Qazi Ammar Arshad</name>
    </author>
    <author>
      <name>Chen Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2004.10774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.10774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2106.13967v1</id>
    <updated>2021-06-26T08: 34: 19Z</updated>
    <published>2021-06-26T08: 34: 19Z</published>
    <title>Exploring Temporal Context and Human Movement Dynamics for Online Action
  Detection in Videos</title>
    <summary>  Nowadays, the interaction between humans and robots is constantly expanding,
requiring more and more human motion recognition applications to operate in
real time. However, most works on temporal action detection and recognition
perform these tasks in offline manner, i.e. temporally segmented videos are
classified as a whole. In this paper, based on the recently proposed framework
of Temporal Recurrent Networks, we explore how temporal context and human
movement dynamics can be effectively employed for online action detection. Our
approach uses various state-of-the-art architectures and appropriately combines
the extracted features in order to improve action detection. We evaluate our
method on a challenging but widely used dataset for temporal action
localization, THUMOS'14. Our experiments show significant improvement over the
baseline method, achieving state-of-the art results on THUMOS'14.
</summary>
    <author>
      <name>Vasiliki I. Vasileiou</name>
    </author>
    <author>
      <name>Nikolaos Kardaris</name>
    </author>
    <author>
      <name>Petros Maragos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EUSIPCO-2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.13967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.13967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1706.00699v2</id>
    <updated>2018-05-17T17: 30: 28Z</updated>
    <published>2017-06-02T14: 34: 21Z</published>
    <title>Action Sets: Weakly Supervised Action Segmentation without Ordering
  Constraints</title>
    <summary>  Action detection and temporal segmentation of actions in videos are topics of
increasing interest. While fully supervised systems have gained much attention
lately, full annotation of each action within the video is costly and
impractical for large amounts of video data. Thus, weakly supervised action
detection and temporal segmentation methods are of great importance. While most
works in this area assume an ordered sequence of occurring actions to be given,
our approach only uses a set of actions. Such action sets provide much less
supervision since neither action ordering nor the number of action occurrences
are known. In exchange, they can be easily obtained, for instance, from
meta-tags, while ordered sequences still require human annotation. We introduce
a system that automatically learns to temporally segment and label actions in a
video, where the only supervision that is used are action sets. An evaluation
on three datasets shows that our method still achieves good results although
the amount of supervision is significantly smaller than for other related
methods.
</summary>
    <author>
      <name>Alexander Richard</name>
    </author>
    <author>
      <name>Hilde Kuehne</name>
    </author>
    <author>
      <name>Juergen Gall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.00699v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00699v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2101.08540v2</id>
    <updated>2021-01-28T12: 14: 19Z</updated>
    <published>2021-01-21T10: 42: 48Z</published>
    <title>Activity Graph Transformer for Temporal Action Localization</title>
    <summary>  We introduce Activity Graph Transformer, an end-to-end learnable model for
temporal action localization, that receives a video as input and directly
predicts a set of action instances that appear in the video. Detecting and
localizing action instances in untrimmed videos requires reasoning over
multiple action instances in a video. The dominant paradigms in the literature
process videos temporally to either propose action regions or directly produce
frame-level detections. However, sequential processing of videos is problematic
when the action instances have non-sequential dependencies and/or non-linear
temporal ordering, such as overlapping action instances or re-occurrence of
action instances over the course of the video. In this work, we capture this
non-linear temporal structure by reasoning over the videos as non-sequential
entities in the form of graphs. We evaluate our model on challenging datasets:
THUMOS14, Charades, and EPIC-Kitchens-100. Our results show that our proposed
model outperforms the state-of-the-art by a considerable margin.
</summary>
    <author>
      <name>Megha Nawhal</name>
    </author>
    <author>
      <name>Greg Mori</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project webpage: https: //www.sfu.ca/~mnawhal/projects/agt.html; Code
  available at https: //github.com/Nmegha2601/activitygraph_transformer</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.08540v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.08540v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1706.07788v1</id>
    <updated>2017-06-20T16: 41: 01Z</updated>
    <published>2017-06-20T16: 41: 01Z</published>
    <title>Spatial statistics of single-quantum detection</title>
    <summary>  In a single-particle detection experiment, a wavefront impinges on a detector
but observers only see a point response. The extent of the wavefront becomes
evident only in statistical accumulation of many independent detections, with
probability given by the Born rule. Drawing on concepts from quantum optics, we
analyze a simple model to reverse-engineer how this behavior can come about in
terms of wave mechanics alone without a measurement axiom. The model detector
consists of many molecules, each of which can be resonantly excited by the
incoming particle and then emit a detection signature (e.g., localized flash of
light). Different molecules have different resonant energies because local
conditions (proximity of other molecules, Doppler shifts, etc.) vary. The
detector is thus a quasi-continuum, and the incoming particle preferentially
excites the molecule that it matches most closely in energy. (In actuality,
molecules can be so numerous that many could closely match the incoming
particle in energy; but in that case only one will be the first, and there will
be nothing left for the others by the time the first match resonates and then
emits. The model does not explicitly take into account the temporal advance of
the particle wave packet through the detector medium.) The excited molecule can
emit a detection signature, but that process competes with fluctuation-driven
dephasing. We estimate the probability that a given molecule is resonantly
excited, and we also estimate the probability that a detection signature is
produced before being overwhelmed by fluctuations in the detector medium. The
product of these two probabilities is proportional to the absolute-square of
the incoming wavefunction at the molecule in question, i.e. the Born rule. We
discuss ways to probe these mechanisms experimentally, and check the model with
numbers from a neutron interference experiment.
</summary>
    <author>
      <name>Jonathan F. Schonfeld</name>
    </author>
    <link href="http://arxiv.org/abs/1706.07788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2405.12708v1</id>
    <updated>2024-05-21T11: 56: 01Z</updated>
    <published>2024-05-21T11: 56: 01Z</published>
    <title>Multimodal video analysis for crowd anomaly detection using open access
  tourism cameras</title>
    <summary>  In this article, we propose the detection of crowd anomalies through the
extraction of information in the form of time series from video format using a
multimodal approach. Through pattern recognition algorithms and segmentation,
informative measures of the number of people and image occupancy are extracted
at regular intervals, which are then analyzed to obtain trends and anomalous
behaviors. Specifically, through temporal decomposition and residual analysis,
intervals or specific situations of unusual behaviors are identified, which can
be used in decision-making and improvement of actions in sectors related to
human movement such as tourism or security.
  The application of this methodology on the webcam of Turisme Comunitat
Valenciana in the town of Morella (Comunitat Valenciana, Spain) has provided
excellent results. It is shown to correctly detect specific anomalous
situations and unusual overall increases during the previous weekend and during
the festivities in October 2023. These results have been obtained while
preserving the confidentiality of individuals at all times by using measures
that maximize anonymity, without trajectory recording or person recognition.
</summary>
    <author>
      <name>Alejandro Dionis-Ros</name>
    </author>
    <author>
      <name>Joan Vila-Francés</name>
    </author>
    <author>
      <name>Rafael Magdalena-Benedicto</name>
    </author>
    <author>
      <name>Fernando Mateo</name>
    </author>
    <author>
      <name>Antonio J. Serrano-López</name>
    </author>
    <link href="http://arxiv.org/abs/2405.12708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.12708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1704.05643v1</id>
    <updated>2017-04-19T08: 16: 13Z</updated>
    <published>2017-04-19T08: 16: 13Z</published>
    <title>Skeleton Boxes: Solving skeleton based action detection with a single
  deep convolutional neural network</title>
    <summary>  Action recognition from well-segmented 3D skeleton video has been intensively
studied. However, due to the difficulty in representing the 3D skeleton video
and the lack of training data, action detection from streaming 3D skeleton
video still lags far behind its recognition counterpart and image based object
detection. In this paper, we propose a novel approach for this problem, which
leverages both effective skeleton video encoding and deep regression based
object detection from images. Our framework consists of two parts:
skeleton-based video image mapping, which encodes a skeleton video to a color
image in a temporal preserving way, and an end-to-end trainable fast skeleton
action detector (Skeleton Boxes) based on image detection. Experimental results
on the latest and largest PKU-MMD benchmark dataset demonstrate that our method
outperforms the state-of-the-art methods with a large margin. We believe our
idea would inspire and benefit future research in this important area.
</summary>
    <author>
      <name>Bo Li</name>
    </author>
    <author>
      <name>Huahui Chen</name>
    </author>
    <author>
      <name>Yucheng Chen</name>
    </author>
    <author>
      <name>Yuchao Dai</name>
    </author>
    <author>
      <name>Mingyi He</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages,
                    3 figures, icmew 2017</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">icmew 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1704.05643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1811.07391v2</id>
    <updated>2019-03-23T18: 58: 29Z</updated>
    <published>2018-11-18T20: 03: 55Z</published>
    <title>Temporal Recurrent Networks for Online Action Detection</title>
    <summary>  Most work on temporal action detection is formulated as an offline problem,
in which the start and end times of actions are determined after the entire
video is fully observed. However, important real-time applications including
surveillance and driver assistance systems require identifying actions as soon
as each video frame arrives, based only on current and historical observations.
In this paper, we propose a novel framework, Temporal Recurrent Network (TRN),
to model greater temporal context of a video frame by simultaneously performing
online action detection and anticipation of the immediate future. At each
moment in time, our approach makes use of both accumulated historical evidence
and predicted future information to better recognize the action that is
currently occurring, and integrates both of these into a unified end-to-end
architecture. We evaluate our approach on two popular online action detection
datasets, HDD and TVSeries, as well as another widely used dataset, THUMOS'14.
The results show that TRN significantly outperforms the state-of-the-art.
</summary>
    <author>
      <name>Mingze Xu</name>
    </author>
    <author>
      <name>Mingfei Gao</name>
    </author>
    <author>
      <name>Yi-Ting Chen</name>
    </author>
    <author>
      <name>Larry S. Davis</name>
    </author>
    <author>
      <name>David J. Crandall</name>
    </author>
    <link href="http://arxiv.org/abs/1811.07391v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.07391v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1411.6031v1</id>
    <updated>2014-11-21T21: 38: 15Z</updated>
    <published>2014-11-21T21: 38: 15Z</published>
    <title>Finding Action Tubes</title>
    <summary>  We address the problem of action detection in videos. Driven by the latest
progress in object detection from 2D images, we build action models using rich
feature hierarchies derived from shape and kinematic cues. We incorporate
appearance and motion in two ways. First, starting from image region proposals
we select those that are motion salient and thus are more likely to contain the
action. This leads to a significant reduction in the number of regions being
processed and allows for faster computations. Second, we extract
spatio-temporal feature representations to build strong classifiers using
Convolutional Neural Networks. We link our predictions to produce detections
consistent in time, which we call action tubes. We show that our approach
outperforms other techniques in the task of action detection.
</summary>
    <author>
      <name>Georgia Gkioxari</name>
    </author>
    <author>
      <name>Jitendra Malik</name>
    </author>
    <link href="http://arxiv.org/abs/1411.6031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.6031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2005.02808v1</id>
    <updated>2020-05-03T20: 04: 54Z</updated>
    <published>2020-05-03T20: 04: 54Z</published>
    <title>Spatiotemporal Patterns of COVID-19 Impact on Human Activities and
  Environment in China Using Nighttime Light and Air Quality Data</title>
    <summary>  In order to analyze the impact of COVID-19 on people's lives, activities and
the natural environment, this paper investigates the spatial and temporal
characteristics of Night Time Light (NTL) radiance and Air Quality Index (AQI)
before and during the pandemic in mainland China. Our results show that the
monthly average NTL brightness is much lower during the quarantine period than
before. This study categorizes NTL into three classes: residential area,
transportation and public facilities and commercial centers, with NTL radiance
ranges of 5-20,
                    20-40 and greater than 40 nW/(cm*cm*sr), respectively. We found
that the Number Of Pixels (NOP) with NTL detection increased in the residential
area and decreased in the commercial centers for most of the provinces after
the shutdown, while transportation and public facilities generally stayed the
same. More specifically, we examined these factors in Wuhan, where the first
confirmed cases were reported, and where the earliest quarantine measures were
taken. Observations and analysis of pixels associated with commercial centers
were observed to have lower NTL radiance values, indicating a dimming behavior,
while residential area pixels recorded increased levels of brightness, after
the beginning of the lockdown. The study also discovered a significant
decreasing trend in the daily average AQI for the whole country, with cleaner
air in most provinces during February and March, compared to January 2020. In
conclusion, the outbreak and spread of COVID-19 has had a crucial impact on
people's daily lives and activity ranges through the increased implementation
of lockdown and quarantine policies. On the other hand, the air quality of
China has improved with the reduction of non-essential industries and motor
vehicle usage.
</summary>
    <author>
      <name>Qian Liu</name>
    </author>
    <author>
      <name>Dexuan Sha</name>
    </author>
    <author>
      <name>Wei Liu</name>
    </author>
    <author>
      <name>Paul Houser</name>
    </author>
    <author>
      <name>Luyao Zhang</name>
    </author>
    <author>
      <name>Ruizhi Hou</name>
    </author>
    <author>
      <name>Hai Lan</name>
    </author>
    <author>
      <name>Colin Flynn</name>
    </author>
    <author>
      <name>Mingyue Lu</name>
    </author>
    <author>
      <name>Tao Hu</name>
    </author>
    <author>
      <name>Chaowei Yang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/rs12101576</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/rs12101576" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages,
                    5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">https: //www.mdpi.com/2072-4292/12/10/1576</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2005.02808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.02808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2410.09523v1</id>
    <updated>2024-10-12T12: 57: 06Z</updated>
    <published>2024-10-12T12: 57: 06Z</published>
    <title>Monitoring Drug-Induced Brain Activity Changes with Functional
  Ultrasound Imaging and Convolutional Neural Networks</title>
    <summary>  Functional ultrasound imaging (fUSI) is a cutting-edge technology that
measures changes in cerebral blood volume (CBV) by detecting backscattered
echoes from red blood cells moving within its field of view (FOV). It offers
high spatiotemporal resolution and sensitivity, allowing for detailed
visualization of cerebral blood flow dynamics. While fUSI has been utilized in
preclinical drug development studies to explore the mechanisms of action of
various drugs targeting the central nervous system, many of these studies have
primarily focused on predetermined regions of interest (ROIs). This focus may
overlook relevant brain activity outside these specific areas, which could
influence the results. To address this limitation, we combined convolutional
neural networks (CNNs) with fUSI to comprehensively understand the
pharmacokinetic process of Dizocilpine, also known as MK-801, a drug that
blocks the N-Methyl-D-aspartate (NMDA) receptor in the central nervous system.
CNN and class activation mapping (CAM) revealed the spatiotemporal effects of
MK-801, which originated in the cortex and propagated to the hippocampus,
demonstrating the ability to detect dynamic drug effects over time.
Additionally, CNN and CAM assessed the impact of anesthesia on the
spatiotemporal hemodynamics of the brain, revealing no distinct patterns
between early and late stages. The integration of fUSI and CNN provides a
powerful tool to gain insights into the spatiotemporal dynamics of drug action
in the brain. This combination enables a comprehensive and unbiased assessment
of drug effects on brain function, potentially accelerating the development of
new therapies in neuropharmacological studies.
</summary>
    <author>
      <name>Jared Deighton</name>
    </author>
    <author>
      <name>Shan Zhong</name>
    </author>
    <author>
      <name>Kofi Agyeman</name>
    </author>
    <author>
      <name>Wooseong Choi</name>
    </author>
    <author>
      <name>Charles Liu</name>
    </author>
    <author>
      <name>Darrin Lee</name>
    </author>
    <author>
      <name>Vasileios Maroulas</name>
    </author>
    <author>
      <name>Vasileios Christopoulos</name>
    </author>
    <link href="http://arxiv.org/abs/2410.09523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.09523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2001.07501v1</id>
    <updated>2020-01-21T13: 12: 58Z</updated>
    <published>2020-01-21T13: 12: 58Z</published>
    <title>A Comprehensive Study on Temporal Modeling for Online Action Detection</title>
    <summary>  Online action detection (OAD) is a practical yet challenging task, which has
attracted increasing attention in recent years. A typical OAD system mainly
consists of three modules: a frame-level feature extractor which is usually
based on pre-trained deep Convolutional Neural Networks (CNNs), a temporal
modeling module, and an action classifier. Among them, the temporal modeling
module is crucial which aggregates discriminative information from historical
and current features. Though many temporal modeling methods have been developed
for OAD and other topics, their effects are lack of investigation on OAD
fairly. This paper aims to provide a comprehensive study on temporal modeling
for OAD including four meta types of temporal modeling methods, \ie temporal
pooling, temporal convolution, recurrent neural networks, and temporal
attention, and uncover some good practices to produce a state-of-the-art OAD
system. Many of them are explored in OAD for the first time, and extensively
evaluated with various hyper parameters. Furthermore, based on our
comprehensive study, we present several hybrid temporal modeling methods, which
outperform the recent state-of-the-art methods with sizable margins on
THUMOS-14 and TVSeries.
</summary>
    <author>
      <name>Wen Wang</name>
    </author>
    <author>
      <name>Xiaojiang Peng</name>
    </author>
    <author>
      <name>Yu Qiao</name>
    </author>
    <author>
      <name>Jian Cheng</name>
    </author>
    <link href="http://arxiv.org/abs/2001.07501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.07501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2408.16924v1</id>
    <updated>2024-08-29T21: 53: 01Z</updated>
    <published>2024-08-29T21: 53: 01Z</published>
    <title>Enhancing Autism Spectrum Disorder Early Detection with the Parent-Child
  Dyads Block-Play Protocol and an Attention-enhanced GCN-xLSTM Hybrid Deep
  Learning Framework</title>
    <summary>  Autism Spectrum Disorder (ASD) is a rapidly growing neurodevelopmental
disorder. Performing a timely intervention is crucial for the growth of young
children with ASD, but traditional clinical screening methods lack objectivity.
This study introduces an innovative approach to early detection of ASD. The
contributions are threefold. First, this work proposes a novel Parent-Child
Dyads Block-Play (PCB) protocol, grounded in kinesiological and neuroscientific
research, to identify behavioral patterns distinguishing ASD from typically
developing (TD) toddlers. Second, we have compiled a substantial video dataset,
featuring 40 ASD and 89 TD toddlers engaged in block play with parents. This
dataset exceeds previous efforts on both the scale of participants and the
length of individual sessions. Third, our approach to action analysis in videos
employs a hybrid deep learning framework, integrating a two-stream graph
convolution network with attention-enhanced xLSTM (2sGCN-AxLSTM). This
framework is adept at capturing dynamic interactions between toddlers and
parents by extracting spatial features correlated with upper body and head
movements and focusing on global contextual information of action sequences
over time. By learning these global features with spatio-temporal correlations,
our 2sGCN-AxLSTM effectively analyzes dynamic human behavior patterns and
demonstrates an unprecedented accuracy of 89.6\% in early detection of ASD. Our
approach shows strong potential for enhancing early ASD diagnosis by accurately
analyzing parent-child interactions, providing a critical tool to support
timely and informed clinical decision-making.
</summary>
    <author>
      <name>Xiang Li</name>
    </author>
    <author>
      <name>Lizhou Fan</name>
    </author>
    <author>
      <name>Hanbo Wu</name>
    </author>
    <author>
      <name>Kunping Chen</name>
    </author>
    <author>
      <name>Xiaoxiao Yu</name>
    </author>
    <author>
      <name>Chao Che</name>
    </author>
    <author>
      <name>Zhifeng Cai</name>
    </author>
    <author>
      <name>Xiuhong Niu</name>
    </author>
    <author>
      <name>Aihua Cao</name>
    </author>
    <author>
      <name>Xin Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages,
                    8 figures, and 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.16924v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.16924v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2208.07002v1</id>
    <updated>2022-08-15T04: 32: 16Z</updated>
    <published>2022-08-15T04: 32: 16Z</published>
    <title>The effectiveness of using Google Maps Location History data to detect
  joint activities in social networks</title>
    <summary>  This study evaluates the effectiveness of using Google Maps Location History
data to identify joint activities in social networks. To do so, an experiment
was conducted where participants were asked to execute daily schedules designed
to simulate daily travel incorporating joint activities. For Android devices,
detection rates for 4-person group activities ranged from 22% under the
strictest spatiotemporal accuracy criteria to 60% under less strict yet still
operational criteria. The performance of iPhones was markedly worse than
Android devices, irrespective of accuracy criteria. In addition, logit models
were estimated to evaluate factors affecting activity detection given different
spatiotemporal accuracy thresholds. In terms of effect magnitudes, non-trivial
effects on joint activity detection probability were found for floor area ratio
(FAR) at location, activity duration, Android device ratio, device model ratio,
whether the destination was an open space or not, and group size.
  Although current activity detection rates are not ideal, these levels must be
weighed against the potential of observing travel behavior over long periods of
time, and that Google Maps Location History data could potentially be used in
conjunction with other data-gathering methodologies to compensate for some of
its limitations.
</summary>
    <author>
      <name>Giancarlos Parady</name>
    </author>
    <author>
      <name>Keita Suzuki</name>
    </author>
    <author>
      <name>Yuki Oyama</name>
    </author>
    <author>
      <name>Makoto Chikaraishi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.tbs.2022.10.010</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.tbs.2022.10.010" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages,
                    8 tables,
                    7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Travel Behaviour and Society 30 (2023) 344-357</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2208.07002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.07002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2303.15620v2</id>
    <updated>2023-10-06T21: 39: 24Z</updated>
    <published>2023-03-27T22: 29: 27Z</published>
    <title>Optimizing Lead Time in Fall Detection for a Planar Bipedal Robot</title>
    <summary>  For legged robots to operate in complex terrains, they must be robust to the
disturbances and uncertainties they encounter. This paper contributes to
enhancing robustness through the design of fall detection/prediction algorithms
that will provide sufficient lead time for corrective motions to be taken.
Falls can be caused by abrupt (fast-acting), incipient (slow-acting), or
intermittent (non-continuous) faults. Early fall detection is a challenging
task due to the masking effects of controllers (through their disturbance
attenuation actions), the inverse relationship between lead time and false
positive rates, and the temporal behavior of the faults/underlying factors. In
this paper, we propose a fall detection algorithm that is capable of detecting
both incipient and abrupt faults while maximizing lead time and meeting desired
thresholds on the false positive and negative rates.
</summary>
    <author>
      <name>M. Eva Mungai</name>
    </author>
    <author>
      <name>Jessy Grizzle</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICECCME57830.2023.10253317</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICECCME57830.2023.10253317" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">\c{opyright
                    } 2023 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.15620v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.15620v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1808.03619v2</id>
    <updated>2018-10-20T18: 50: 04Z</updated>
    <published>2018-08-10T16: 54: 58Z</published>
    <title>Mitigation of the instrumental noise transient in gravitational-wave
  data surrounding GW170817</title>
    <summary>  In the coming years gravitational-wave detectors will undergo a series of
improvements, with an increase in their detection rate by about an order of
magnitude. Routine detections of gravitational-wave signals promote novel
astrophysical and fundamental theory studies, while simultaneously leading to
an increase in the number of detections temporally overlapping with
instrumentally- or environmentally-induced transients in the detectors
(glitches), often of unknown origin. Indeed, this was the case for the very
first detection by the LIGO and Virgo detectors of a gravitational-wave signal
consistent with a binary neutron star coalescence, GW170817. A loud glitch in
the LIGO-Livingston detector, about one second before the merger, hampered
coincident detection (which was initially achieved solely with LIGO-Hanford
data). Moreover, accurate source characterization depends on specific
assumptions about the behavior of the detector noise that are rendered invalid
by the presence of glitches. In this paper, we present the various techniques
employed for the initial mitigation of the glitch to perform source
characterization of GW170817 and study advantages and disadvantages of each
mitigation method. We show that, despite the presence of instrumental noise
transients louder than the one affecting GW170817, we are still able to produce
unbiased measurements of the intrinsic parameters from simulated injections
with properties similar to GW170817.
</summary>
    <author>
      <name>Chris Pankow</name>
    </author>
    <author>
      <name>Katerina Chatziioannou</name>
    </author>
    <author>
      <name>Eve A. Chase</name>
    </author>
    <author>
      <name>Tyson B. Littenberg</name>
    </author>
    <author>
      <name>Matthew Evans</name>
    </author>
    <author>
      <name>Jessica McIver</name>
    </author>
    <author>
      <name>Neil J. Cornish</name>
    </author>
    <author>
      <name>Carl-Johan Haster</name>
    </author>
    <author>
      <name>Jonah Kanner</name>
    </author>
    <author>
      <name>Vivien Raymond</name>
    </author>
    <author>
      <name>Salvatore Vitale</name>
    </author>
    <author>
      <name>Aaron Zimmerman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevD.98.084016</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevD.98.084016" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages,
                    3 figures, accepted in PRD</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. D 98,
                    084016 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1808.03619v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03619v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2207.08653v2</id>
    <updated>2022-07-21T05: 46: 37Z</updated>
    <published>2022-07-18T14: 52: 37Z</published>
    <title>Leveraging Action Affinity and Continuity for Semi-supervised Temporal
  Action Segmentation</title>
    <summary>  We present a semi-supervised learning approach to the temporal action
segmentation task. The goal of the task is to temporally detect and segment
actions in long, untrimmed procedural videos, where only a small set of videos
are densely labelled, and a large collection of videos are unlabelled. To this
end, we propose two novel loss functions for the unlabelled data: an action
affinity loss and an action continuity loss. The action affinity loss guides
the unlabelled samples learning by imposing the action priors induced from the
labelled set. Action continuity loss enforces the temporal continuity of
actions, which also provides frame-wise classification supervision. In
addition, we propose an Adaptive Boundary Smoothing (ABS) approach to build
coarser action boundaries for more robust and reliable learning. The proposed
loss functions and ABS were evaluated on three benchmarks. Results show that
they significantly improved action segmentation performance with a low amount
(5% and 10%) of labelled data and achieved comparable results to full
supervision with 50% labelled data. Furthermore, ABS succeeded in boosting
performance when integrated into fully-supervised learning.
</summary>
    <author>
      <name>Guodong Ding</name>
    </author>
    <author>
      <name>Angela Yao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages,
                    5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.08653v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.08653v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1906.02073v1</id>
    <updated>2019-06-05T15: 29: 15Z</updated>
    <published>2019-06-05T15: 29: 15Z</published>
    <title>Intrinsically-limited timing jitter in molybdenum silicide
  superconducting nanowire single-photon detectors</title>
    <summary>  Recent progress in the development of superconducting nanowire single-photon
detectors (SNSPDs) has delivered excellent performances, and has had a great
impact on a range of research fields. The timing jitter, which denotes the
temporal resolution of the detection, is a crucial parameter for many
applications. Despite extensive work since their apparition, the lowest jitter
achievable with SNSPDs is still not clear, and the origin of the intrinsic
limits is not fully understood. Understanding its intrinsic behaviour and
limits is a mandatory step toward improvements. Here, we report our
experimental study on the intrinsically-limited timing jitter in molybdenum
silicide (MoSi) SNSPDs. We show that to reach intrinsic jitter, several
detector properties such as the latching current and the kinetic inductance of
the devices have to be understood. The dependence on the nanowire cross-section
and the energy dependence of the intrinsic jitter are exhibited, and the origin
of the limits are explicited. System timing jitter of 6.0 ps at 532 nm and 10.6
ps at 1550 nm photon wavelength have been obtained.
</summary>
    <author>
      <name>Misael Caloz</name>
    </author>
    <author>
      <name>Boris Korzh</name>
    </author>
    <author>
      <name>Edward Ramirez</name>
    </author>
    <author>
      <name>Christian Schönenberger</name>
    </author>
    <author>
      <name>Richard J. Warburton</name>
    </author>
    <author>
      <name>Hugo Zbinden</name>
    </author>
    <author>
      <name>Matthew D. Shaw</name>
    </author>
    <author>
      <name>Félix Bussières</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.5113748</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.5113748" rel="related"/>
    <link href="http://arxiv.org/abs/1906.02073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.02073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.supr-con" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1901.10340v2</id>
    <updated>2020-01-30T10: 29: 09Z</updated>
    <published>2019-01-29T15: 35: 22Z</published>
    <title>e-μ Discrimination at High Energy in the JUNO Detector</title>
    <summary>  Cosmic Ray and neutrino oscillation physics can be studied by using
atmospheric neutrinos. JUNO (Jiangmen Underground Neutrino Observatory) is a
large liquid scintillator detector with low energy detection threshold and
excellent energy resolution. The detector performances allow the atmospheric
neutrino oscillation measurements. In this work, a discrimination algorithm for
different reaction channels of neutrino-nucleon interactions in the JUNO liquid
scintillator, in the GeV/sub-GeV energy region, is presented. The atmospheric
neutrino flux is taken as reference, considering $\overset{(-)
                    }{\nu_\mu
                    }$ and
$\overset{(-)
                    }{\nu_e
                    }$. The different temporal behaviour of the classes of
events have been exploited to build a time profile-based discrimination
algorithm. The results show a good selection power for $\overset{(-)
                    }{\nu_e
                    }$
CC events, while the $\overset{(-)
                    }{\nu_\mu
                    }$ CC component suffers of an
important contamination from NC events at low energy, which is under study.
Preliminary results are presented.
</summary>
    <author>
      <name>Giulio Settanta</name>
    </author>
    <author>
      <name>Stefano Maria Mari</name>
    </author>
    <author>
      <name>Cristina Martellini</name>
    </author>
    <author>
      <name>Paolo Montini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1051/epjconf/201920901011</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1051/epjconf/201920901011" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceeding for poster presented at the 7th Roma International
  Conference on AstroParticle Physics</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPJ Web of Conferences 209,
                    01011 (2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1901.10340v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.10340v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1904.11953v1</id>
    <updated>2019-04-19T21: 23: 28Z</updated>
    <published>2019-04-19T21: 23: 28Z</published>
    <title>Temporal Unet: Sample Level Human Action Recognition using WiFi</title>
    <summary>  Human doing actions will result in WiFi distortion, which is widely explored
for action recognition, such as the elderly fallen detection, hand sign
language recognition, and keystroke estimation. As our best survey, past work
recognizes human action by categorizing one complete distortion series into one
action, which we term as series-level action recognition. In this paper, we
introduce a much more fine-grained and challenging action recognition task into
WiFi sensing domain, i.e., sample-level action recognition. In this task, every
WiFi distortion sample in the whole series should be categorized into one
action, which is a critical technique in precise action localization,
continuous action segmentation, and real-time action recognition. To achieve
WiFi-based sample-level action recognition, we fully analyze approaches in
image-based semantic segmentation as well as in video-based frame-level action
recognition, then propose a simple yet efficient deep convolutional neural
network, i.e., Temporal Unet. Experimental results show that Temporal Unet
achieves this novel task well. Codes have been made publicly available at
https: //github.com/geekfeiw/WiSLAR.
</summary>
    <author>
      <name>Fei Wang</name>
    </author>
    <author>
      <name>Yunpeng Song</name>
    </author>
    <author>
      <name>Jimuyang Zhang</name>
    </author>
    <author>
      <name>Jinsong Han</name>
    </author>
    <author>
      <name>Dong Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages,
                    14 figures,
                    1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.11953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.11953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2303.07347v2</id>
    <updated>2023-03-16T11: 26: 39Z</updated>
    <published>2023-03-13T17: 59: 59Z</published>
    <title>TriDet: Temporal Action Detection with Relative Boundary Modeling</title>
    <summary>  In this paper, we present a one-stage framework TriDet for temporal action
detection. Existing methods often suffer from imprecise boundary predictions
due to the ambiguous action boundaries in videos. To alleviate this problem, we
propose a novel Trident-head to model the action boundary via an estimated
relative probability distribution around the boundary. In the feature pyramid
of TriDet, we propose an efficient Scalable-Granularity Perception (SGP) layer
to mitigate the rank loss problem of self-attention that takes place in the
video features and aggregate information across different temporal
granularities. Benefiting from the Trident-head and the SGP-based feature
pyramid, TriDet achieves state-of-the-art performance on three challenging
benchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational
costs, compared to previous methods. For example, TriDet hits an average mAP of
$69.3\%$ on THUMOS14, outperforming the previous best by $2.5\%$, but with only
$74.6\%$ of its latency. The code is released to
https: //github.com/sssste/TriDet.
</summary>
    <author>
      <name>Dingfeng Shi</name>
    </author>
    <author>
      <name>Yujie Zhong</name>
    </author>
    <author>
      <name>Qiong Cao</name>
    </author>
    <author>
      <name>Lin Ma</name>
    </author>
    <author>
      <name>Jia Li</name>
    </author>
    <author>
      <name>Dacheng Tao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR2023; Temporal Action Detection; Temporal Action Localization</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.07347v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.07347v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2407.11690v1</id>
    <updated>2024-07-16T13: 04: 54Z</updated>
    <published>2024-07-16T13: 04: 54Z</published>
    <title>Using Causality to Infer Coordinated Attacks in Social Media</title>
    <summary>  The rise of social media has been accompanied by a dark side with the ease of
creating fake accounts and disseminating misinformation through coordinated
attacks. Existing methods to identify such attacks often rely on thematic
similarities or network-based approaches, overlooking the intricate causal
relationships that underlie coordinated actions. This work introduces a novel
approach for detecting coordinated attacks using Convergent Cross Mapping
(CCM), a technique that infers causality from temporal relationships between
user activity. We build on the theoretical framework of CCM by incorporating
topic modelling as a basis for further optimizing its performance. We apply CCM
to real-world data from the infamous IRA attack on US elections, achieving F1
scores up to 75.3% in identifying coordinated accounts. Furthermore, we analyse
the output of our model to identify the most influential users in a community.
We apply our model to a case study involving COVID-19 anti-vax related
discussions on Twitter. Our results demonstrate the effectiveness of our model
in uncovering causal structures of coordinated behaviour, offering a promising
avenue for mitigating the threat of malicious campaigns on social media
platforms.
</summary>
    <author>
      <name>Isura Manchanayaka</name>
    </author>
    <author>
      <name>Zainab Razia Zaidi</name>
    </author>
    <author>
      <name>Shanika Karunasekera</name>
    </author>
    <author>
      <name>Christopher Leckie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for International AAAI Conference on Web and Social Media
  (ICWSM 2025)</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.11690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.11690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4; I.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2207.11466v1</id>
    <updated>2022-07-23T08: 58: 57Z</updated>
    <published>2022-07-23T08: 58: 57Z</published>
    <title>Anomaly Detection for Fraud in Cryptocurrency Time Series</title>
    <summary>  Since the inception of Bitcoin in 2009, the market of cryptocurrencies has
grown beyond initial expectations as daily trades exceed $10 billion. As
industries become automated, the need for an automated fraud detector becomes
very apparent. Detecting anomalies in real time prevents potential accidents
and economic losses. Anomaly detection in multivariate time series data poses a
particular challenge because it requires simultaneous consideration of temporal
dependencies and relationships between variables. Identifying an anomaly in
real time is not an easy task specifically because of the exact anomalistic
behavior they observe. Some points may present pointwise global or local
anomalistic behavior, while others may be anomalistic due to their frequency or
seasonal behavior or due to a change in the trend. In this paper we suggested
working on real time series of trades of Ethereum from specific accounts and
surveyed a large variety of different algorithms traditional and new. We
categorized them according to the strategy and the anomalistic behavior which
they search and showed that when bundling them together to different groups,
they can prove to be a good real-time detector with an alarm time of no longer
than a few seconds and with very high confidence.
</summary>
    <author>
      <name>Eran Kaufman</name>
    </author>
    <author>
      <name>Andrey Iaremenko</name>
    </author>
    <link href="http://arxiv.org/abs/2207.11466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.11466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2010.01231v1</id>
    <updated>2020-10-02T23: 45: 41Z</updated>
    <published>2020-10-02T23: 45: 41Z</published>
    <title>Stuttering Speech Disfluency Prediction using Explainable Attribution
  Vectors of Facial Muscle Movements</title>
    <summary>  Speech disorders such as stuttering disrupt the normal fluency of speech by
involuntary repetitions, prolongations and blocking of sounds and syllables. In
addition to these disruptions to speech fluency, most adults who stutter (AWS)
also experience numerous observable secondary behaviors before, during, and
after a stuttering moment, often involving the facial muscles. Recent studies
have explored automatic detection of stuttering using Artificial Intelligence
(AI) based algorithm from respiratory rate, audio, etc. during speech
utterance. However, most methods require controlled environments and/or
invasive wearable sensors, and are unable explain why a decision (fluent vs
stuttered) was made. We hypothesize that pre-speech facial activity in AWS,
which can be captured non-invasively, contains enough information to accurately
classify the upcoming utterance as either fluent or stuttered. Towards this
end, this paper proposes a novel explainable AI (XAI) assisted convolutional
neural network (CNN) classifier to predict near future stuttering by learning
temporal facial muscle movement patterns of AWS and explains the important
facial muscles and actions involved. Statistical analyses reveal significantly
high prevalence of cheek muscles (p&lt;0.005) and lip muscles (p&lt;0.005) to predict
stuttering and shows a behavior conducive of arousal and anticipation to speak.
The temporal study of these upper and lower facial muscles may facilitate early
detection of stuttering, promote automated assessment of stuttering and have
application in behavioral therapies by providing automatic non-invasive
feedback in realtime.
</summary>
    <author>
      <name>Arun Das</name>
    </author>
    <author>
      <name>Jeffrey Mock</name>
    </author>
    <author>
      <name>Henry Chacon</name>
    </author>
    <author>
      <name>Farzan Irani</name>
    </author>
    <author>
      <name>Edward Golob</name>
    </author>
    <author>
      <name>Peyman Najafirad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitting to IEEE Trans. 10 pages,
                    7 figures. Final Manuscript</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1408.3809v4</id>
    <updated>2014-09-22T06: 50: 28Z</updated>
    <published>2014-08-17T10: 34: 47Z</published>
    <title>HOPC: Histogram of Oriented Principal Components of 3D Pointclouds for
  Action Recognition</title>
    <summary>  Existing techniques for 3D action recognition are sensitive to viewpoint
variations because they extract features from depth images which change
significantly with viewpoint. In contrast, we directly process the pointclouds
and propose a new technique for action recognition which is more robust to
noise, action speed and viewpoint variations. Our technique consists of a novel
descriptor and keypoint detection algorithm. The proposed descriptor is
extracted at a point by encoding the Histogram of Oriented Principal Components
(HOPC) within an adaptive spatio-temporal support volume around that point.
Based on this descriptor, we present a novel method to detect Spatio-Temporal
Key-Points (STKPs) in 3D pointcloud sequences. Experimental results show that
the proposed descriptor and STKP detector outperform state-of-the-art
algorithms on three benchmark human activity datasets. We also introduce a new
multiview public dataset and show the robustness of our proposed method to
viewpoint variations.
</summary>
    <author>
      <name>Hossein Rahmani</name>
    </author>
    <author>
      <name>Arif Mahmood</name>
    </author>
    <author>
      <name>Du Q. Huynh</name>
    </author>
    <author>
      <name>Ajmal Mian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.3809v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.3809v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1710.02310v1</id>
    <updated>2017-10-06T08: 30: 05Z</updated>
    <published>2017-10-06T08: 30: 05Z</published>
    <title>Detecting the Moment of Completion: Temporal Models for Localising
  Action Completion</title>
    <summary>  Action completion detection is the problem of modelling the action's
progression towards localising the moment of completion - when the action's
goal is confidently considered achieved. In this work, we assess the ability of
two temporal models, namely Hidden Markov Models (HMM) and Long-Short Term
Memory (LSTM), to localise completion for six object interactions: switch,
plug, open, pull, pick and drink. We use a supervised approach, where
annotations of pre-completion and post-completion frames are available per
action, and fine-tuned CNN features are used to train temporal models. Tested
on the Action-Completion-2016 dataset, we detect completion within 10 frames of
annotations for ~75% of completed action sequences using both temporal models.
Results show that fine-tuned CNN features outperform hand-crafted features for
localisation, and that observing incomplete instances is necessary when
incomplete sequences are also present in the test set.
</summary>
    <author>
      <name>Farnoosh Heidarivincheh</name>
    </author>
    <author>
      <name>Majid Mirmehdi</name>
    </author>
    <author>
      <name>Dima Damen</name>
    </author>
    <link href="http://arxiv.org/abs/1710.02310v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02310v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2206.10080v1</id>
    <updated>2022-06-21T02: 24: 22Z</updated>
    <published>2022-06-21T02: 24: 22Z</published>
    <title>One-stage Action Detection Transformer</title>
    <summary>  In this work, we introduce our solution to the EPIC-KITCHENS-100 2022 Action
Detection challenge. One-stage Action Detection Transformer (OADT) is proposed
to model the temporal connection of video segments. With the help of OADT, both
the category and time boundary can be recognized simultaneously. After
ensembling multiple OADT models trained from different features, our model can
reach 21.28\% action mAP and ranks the 1st on the test-set of the Action
detection challenge.
</summary>
    <author>
      <name>Lijun Li</name>
    </author>
    <author>
      <name>Li'an Zhuo</name>
    </author>
    <author>
      <name>Bang Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2206.10080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.10080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2010.03016v1</id>
    <updated>2020-10-06T20: 43: 50Z</updated>
    <published>2020-10-06T20: 43: 50Z</published>
    <title>Online Action Detection in Streaming Videos with Time Buffers</title>
    <summary>  We formulate the problem of online temporal action detection in live
streaming videos, acknowledging one important property of live streaming videos
that there is normally a broadcast delay between the latest captured frame and
the actual frame viewed by the audience. The standard setting of the online
action detection task requires immediate prediction after a new frame is
captured. We illustrate that its lack of consideration of the delay is imposing
unnecessary constraints on the models and thus not suitable for this problem.
We propose to adopt the problem setting that allows models to make use of the
small `buffer time' incurred by the delay in live streaming videos. We design
an action start and end detection framework for this online with buffer setting
with two major components: flattened I3D and window-based suppression.
Experiments on three standard temporal action detection benchmarks under the
proposed setting demonstrate the effectiveness of the proposed framework. We
show that by having a suitable problem setting for this problem with
wide-applications, we can achieve much better detection accuracy than
off-the-shelf online action detection models.
</summary>
    <author>
      <name>Bowen Zhang</name>
    </author>
    <author>
      <name>Hao Chen</name>
    </author>
    <author>
      <name>Meng Wang</name>
    </author>
    <author>
      <name>Yuanjun Xiong</name>
    </author>
    <link href="http://arxiv.org/abs/2010.03016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.03016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1512.06498v2</id>
    <updated>2015-12-24T04: 37: 51Z</updated>
    <published>2015-12-21T05: 28: 23Z</published>
    <title>Harnessing the Deep Net Object Models for Enhancing Human Action
  Recognition</title>
    <summary>  In this study, the influence of objects is investigated in the scenario of
human action recognition with large number of classes. We hypothesize that the
objects the humans are interacting will have good say in determining the action
being performed. Especially, if the objects are non-moving, such as objects
appearing in the background, features such as spatio-temporal interest points,
dense trajectories may fail to detect them. Hence we propose to detect objects
using pre-trained object detectors in every frame statically. Trained Deep
network models are used as object detectors. Information from different layers
in conjunction with different encoding techniques is extensively studied to
obtain the richest feature vectors. This technique is observed to yield
state-of-the-art performance on HMDB51 and UCF101 datasets.
</summary>
    <author>
      <name>O. V. Ramana Murthy</name>
    </author>
    <author>
      <name>Roland Goecke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages. arXiv admin note: text overlap with arXiv: 1411.4006 by other
  authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.06498v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.06498v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2405.10832v1</id>
    <updated>2024-05-17T14: 52: 47Z</updated>
    <published>2024-05-17T14: 52: 47Z</published>
    <title>Open-Vocabulary Spatio-Temporal Action Detection</title>
    <summary>  Spatio-temporal action detection (STAD) is an important fine-grained video
understanding task. Current methods require box and label supervision for all
action classes in advance. However, in real-world applications, it is very
likely to come across new action classes not seen in training because the
action category space is large and hard to enumerate. Also, the cost of data
annotation and model training for new classes is extremely high for traditional
methods, as we need to perform detailed box annotations and re-train the whole
network from scratch. In this paper, we propose a new challenging setting by
performing open-vocabulary STAD to better mimic the situation of action
detection in an open world. Open-vocabulary spatio-temporal action detection
(OV-STAD) requires training a model on a limited set of base classes with box
and label supervision, which is expected to yield good generalization
performance on novel action classes. For OV-STAD, we build two benchmarks based
on the existing STAD datasets and propose a simple but effective method based
on pretrained video-language models (VLM). To better adapt the holistic VLM for
the fine-grained action detection task, we carefully fine-tune it on the
localized video region-text pairs. This customized fine-tuning endows the VLM
with better motion understanding, thus contributing to a more accurate
alignment between video regions and texts. Local region feature and global
video feature fusion before alignment is adopted to further improve the action
detection performance by providing global context. Our method achieves a
promising performance on novel classes.
</summary>
    <author>
      <name>Tao Wu</name>
    </author>
    <author>
      <name>Shuqiu Ge</name>
    </author>
    <author>
      <name>Jie Qin</name>
    </author>
    <author>
      <name>Gangshan Wu</name>
    </author>
    <author>
      <name>Limin Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2405.10832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.10832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2409.07588v1</id>
    <updated>2024-09-11T19: 36: 12Z</updated>
    <published>2024-09-11T19: 36: 12Z</published>
    <title>2D bidirectional gated recurrent unit convolutional Neural networks for
  end-to-end violence detection In videos</title>
    <summary>  Abnormal behavior detection, action recognition, fight and violence detection
in videos is an area that has attracted a lot of interest in recent years. In
this work, we propose an architecture that combines a Bidirectional Gated
Recurrent Unit (BiGRU) and a 2D Convolutional Neural Network (CNN) to detect
violence in video sequences. A CNN is used to extract spatial characteristics
from each frame, while the BiGRU extracts temporal and local motion
characteristics using CNN extracted features from multiple frames. The proposed
end-to-end deep learning network is tested in three public datasets with
varying scene complexities. The proposed network achieves accuracies up to 98%.
The obtained results are promising and show the performance of the proposed
end-to-end approach.
</summary>
    <author>
      <name>Abdarahmane Traoré</name>
    </author>
    <author>
      <name>Moulay A. Akhloufi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-50347-5_14</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-50347-5_14" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,
                    6 figures,
                    2020 International Conference on Image Analysis
  and Recognition (ICIAR)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2020 International Conference on Image Analysis and Recognition
  (ICIAR)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2409.07588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.07588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1705.05345v1</id>
    <updated>2017-04-28T11: 15: 38Z</updated>
    <published>2017-04-28T11: 15: 38Z</published>
    <title>Frequency-Multiplexed bias and readout of a 16-pixel Superconducting
  Nanowire Single-Photon Detector Array</title>
    <summary>  We demonstrate a 16-pixel array of radio-frequency superconducting nanowire
single-photon detectors with an integrated and scalable frequency-division
multiplexing architecture, reducing the required bias and readout lines to a
single microwave feed line. The electrical behavior of the photon-sensitive
nanowires, embedded in a resonant circuit, as well as the optical performance
and timing jitter of the single detectors is discussed. Besides the single
pixel measurements we also demonstrate the operation of the 16-pixel array with
a temporal, spatial and photon-number resolution.
</summary>
    <author>
      <name>Steffen Doerner</name>
    </author>
    <author>
      <name>Artem Kuzmin</name>
    </author>
    <author>
      <name>Stefan Wuensch</name>
    </author>
    <author>
      <name>Ilya Charaev</name>
    </author>
    <author>
      <name>Florian Boes</name>
    </author>
    <author>
      <name>Thomas Zwick</name>
    </author>
    <author>
      <name>Michael Siegel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/1.4993779</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/1.4993779" rel="related"/>
    <link href="http://arxiv.org/abs/1705.05345v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05345v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.supr-con" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1809.08875v3</id>
    <updated>2019-03-14T12: 11: 48Z</updated>
    <published>2018-09-24T12: 39: 21Z</published>
    <title>A Probabilistic Semi-Supervised Approach to Multi-Task Human Activity
  Modeling</title>
    <summary>  Human behavior is a continuous stochastic spatio-temporal process which is
governed by semantic actions and affordances as well as latent factors.
Therefore, video-based human activity modeling is concerned with a number of
tasks such as inferring current and future semantic labels, predicting future
continuous observations as well as imagining possible future label and feature
sequences. In this paper we present a semi-supervised probabilistic deep latent
variable model that can represent both discrete labels and continuous
observations as well as latent dynamics over time. This allows the model to
solve several tasks at once without explicit fine-tuning. We focus here on the
tasks of action classification, detection, prediction and anticipation as well
as motion prediction and synthesis based on 3D human activity data recorded
with Kinect. We further extend the model to capture hierarchical label
structure and to model the dependencies between multiple entities, such as a
human and objects. Our experiments demonstrate that our principled approach to
human activity modeling can be used to detect current and anticipate future
semantic labels and to predict and synthesize future label and feature
sequences. When comparing our model to state-of-the-art approaches, which are
specifically designed for e.g. action classification, we find that our
probabilistic formulation outperforms or is comparable to these task specific
models.
</summary>
    <author>
      <name>Judith Bütepage</name>
    </author>
    <author>
      <name>Hedvig Kjellström</name>
    </author>
    <author>
      <name>Danica Kragic</name>
    </author>
    <link href="http://arxiv.org/abs/1809.08875v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.08875v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1705.02168v1</id>
    <updated>2017-05-05T10: 57: 32Z</updated>
    <published>2017-05-05T10: 57: 32Z</published>
    <title>Computer Algorithms for Automated Detection and Analysis of Local Ca2+
  Releases in Spontaneously Beating Cardiac Pacemaker Cells</title>
    <summary>  Local Ca Releases (LCRs) are crucial events involved in cardiac pacemaker
cell function. However, specific algorithms for automatic LCR detection and
analysis have not been developed in live, spontaneously beating pacemaker
cells. Here we measured LCRs using a high-speed 2D-camera in spontaneously
contracting sinoatrial (SA) node cells isolated from rabbit and guinea pig and
developed a new algorithm capable of detecting and analyzing the LCRs spatially
in two-dimensions, and in time. Our algorithm tracks points along the midline
of the contracting cell. It uses these points as a coordinate system for affine
transform, producing a transformed image series where the cell does not
contract. Action potential-induced Ca transients and LCRs were thereafter
isolated from recording noise by applying a series of spatial filters. The LCR
birth and death events were detected by a differential (frame-to-frame)
sensitivity algorithm. An LCR was detected when its signal changes sufficiently
quickly within a sufficiently large area. The LCR is considered to have died
when its amplitude decays substantially, or when it merges into the rising
whole cell Ca transient. Our algorithm provides major LCR parameters such as
period, signal mass, duration, and path area. As LCRs propagate within cells,
the algorithm identifies splitting and merging behaviors, indicating the
importance of Ca-induced-Ca-release for the fate of LCRs and for generating a
powerful ensemble Ca signal. Thus, our new computer algorithms eliminate motion
artifacts and detect 2D local spatiotemporal Ca release events from recording
noise and global signals. While the algorithms detect LCRs in sinoatrial nodal
cells, they have the potential to be used in other applications in biophysics
and cell physiology, for example, to detect Ca wavelets (abortive waves),
sparks and embers in muscle cells and Ca puffs and syntillas in neurons.
</summary>
    <author>
      <name>Alexander V. Maltsev</name>
    </author>
    <author>
      <name>Sean P. Parsons</name>
    </author>
    <author>
      <name>Mary S. Kim</name>
    </author>
    <author>
      <name>Kenta Tsutsui</name>
    </author>
    <author>
      <name>Michael D. Stern</name>
    </author>
    <author>
      <name>Edward G Lakatta</name>
    </author>
    <author>
      <name>Victor A. Maltsev</name>
    </author>
    <author>
      <name>Oliver Monfredi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0179419</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0179419" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages,
                    7 figures,
                    2 tables,
                    5 videos (with web links)</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.02168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.02168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.SC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.SC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.01486v1</id>
    <updated>2024-04-01T21: 11: 43Z</updated>
    <published>2024-04-01T21: 11: 43Z</published>
    <title>QuAD: Query-based Interpretable Neural Motion Planning for Autonomous
  Driving</title>
    <summary>  A self-driving vehicle must understand its environment to determine the
appropriate action. Traditional autonomy systems rely on object detection to
find the agents in the scene. However, object detection assumes a discrete set
of objects and loses information about uncertainty, so any errors compound when
predicting the future behavior of those agents. Alternatively, dense occupancy
grid maps have been utilized to understand free-space. However, predicting a
grid for the entire scene is wasteful since only certain spatio-temporal
regions are reachable and relevant to the self-driving vehicle. We present a
unified, interpretable, and efficient autonomy framework that moves away from
cascading modules that first perceive, then predict, and finally plan. Instead,
we shift the paradigm to have the planner query occupancy at relevant
spatio-temporal points, restricting the computation to those regions of
interest. Exploiting this representation, we evaluate candidate trajectories
around key factors such as collision avoidance, comfort, and progress for
safety and interpretability. Our approach achieves better highway driving
quality than the state-of-the-art in high-fidelity closed-loop simulations.
</summary>
    <author>
      <name>Sourav Biswas</name>
    </author>
    <author>
      <name>Sergio Casas</name>
    </author>
    <author>
      <name>Quinlan Sykora</name>
    </author>
    <author>
      <name>Ben Agro</name>
    </author>
    <author>
      <name>Abbas Sadat</name>
    </author>
    <author>
      <name>Raquel Urtasun</name>
    </author>
    <link href="http://arxiv.org/abs/2404.01486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.01486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2407.05311v2</id>
    <updated>2025-03-30T10: 25: 39Z</updated>
    <published>2024-07-07T09: 45: 14Z</published>
    <title>MMAD: Multi-label Micro-Action Detection in Videos</title>
    <summary>  Human body actions are an important form of non-verbal communication in
social interactions. This paper specifically focuses on a subset of body
actions known as micro-actions, which are subtle, low-intensity body movements
with promising applications in human emotion analysis. In real-world scenarios,
human micro-actions often temporally co-occur, with multiple micro-actions
overlapping in time, such as concurrent head and hand movements. However,
current research primarily focuses on recognizing individual micro-actions
while overlooking their co-occurring nature. To address this gap, we propose a
new task named Multi-label Micro-Action Detection (MMAD), which involves
identifying all micro-actions in a given short video, determining their start
and end times, and categorizing them. Accomplishing this requires a model
capable of accurately capturing both long-term and short-term action
relationships to detect multiple overlapping micro-actions. To facilitate the
MMAD task, we introduce a new dataset named Multi-label Micro-Action-52
(MMA-52) and propose a baseline method equipped with a dual-path
spatial-temporal adapter to address the challenges of subtle visual change in
MMAD. We hope that MMA-52 can stimulate research on micro-action analysis in
videos and prompt the development of spatio-temporal modeling in human-centric
video understanding. The proposed MMA-52 dataset is available at:
https: //github.com/VUT-HFUT/Micro-Action.
</summary>
    <author>
      <name>Kun Li</name>
    </author>
    <author>
      <name>Pengyu Liu</name>
    </author>
    <author>
      <name>Dan Guo</name>
    </author>
    <author>
      <name>Fei Wang</name>
    </author>
    <author>
      <name>Zhiliang Wu</name>
    </author>
    <author>
      <name>Hehe Fan</name>
    </author>
    <author>
      <name>Meng Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2407.05311v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.05311v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2410.22862v1</id>
    <updated>2024-10-30T09: 55: 30Z</updated>
    <published>2024-10-30T09: 55: 30Z</published>
    <title>AtGCN: A Graph Convolutional Network For Ataxic Gait Detection</title>
    <summary>  Video-based gait analysis can be defined as the task of diagnosing
pathologies, such as ataxia, using videos of patients walking in front of a
camera. This paper presents a graph convolution network called AtGCN for
detecting ataxic gait and identifying its severity using 2D videos. The problem
is especially challenging as the deviation of an ataxic gait from a healthy
gait is very subtle. The datasets for ataxic gait detection are also quite
small, with the largest dataset having only 149 videos. The paper addresses the
first problem using special spatiotemporal graph convolution that successfully
captures important gait-related features. To handle the small dataset size, a
deep spatiotemporal graph convolution network pre-trained on an action
recognition dataset is systematically truncated and then fine-tuned on the
ataxia dataset to obtain the AtGCN model. The paper also presents an
augmentation strategy that segments a video sequence into multiple gait cycles.
The proposed AtGCN model then operates on a graph of body part locations
belonging to a single gait cycle. The evaluation results support the strength
of the proposed AtGCN model, as it outperforms the state-of-the-art in
detection and severity prediction with an accuracy of 93.46% and a MAE of
0.4169, respectively.
</summary>
    <author>
      <name>Karan Bania</name>
    </author>
    <author>
      <name>Tanmay Verlekar</name>
    </author>
    <link href="http://arxiv.org/abs/2410.22862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.22862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2501.06138v2</id>
    <updated>2025-03-13T18: 31: 23Z</updated>
    <published>2025-01-10T17: 52: 47Z</published>
    <title>MS-Temba : Multi-Scale Temporal Mamba for Efficient Temporal Action
  Detection</title>
    <summary>  Temporal Action Detection (TAD) in untrimmed videos requires models that can
efficiently (1) process long-duration videos, (2) capture temporal variations
within action classes, and (3) handle dense, overlapping actions, all while
remaining suitable for resource-constrained edge deployment. While
Transformer-based methods achieve high accuracy, their quadratic complexity
hinders deployment in such scenarios. Given the recent popularity of linear
complexity Mamba-based models, leveraging them for TAD is a natural choice.
However, naively adapting Mamba from language or vision tasks fails to provide
an optimal solution and does not address the challenges of long, untrimmed
videos. Therefore, we propose Multi-Scale Temporal Mamba (MS-Temba), the first
Mamba-based architecture specifically designed for densely labeled TAD tasks.
MS-Temba features Temporal Mamba Blocks (Temba Blocks), consisting of Temporal
Convolutional Module (TCM) and Dilated SSM (D-SSM). TCM captures short-term
dependencies using dilated convolutions, while D-SSM introduces a novel dilated
state-space mechanism to model long-range temporal relationships effectively at
each temporal scale. These multi-scale representations are aggregated by
Scale-Aware State Fuser, which learns a unified representation for detecting
densely overlapping actions. Experiments show that MS-Temba achieves
state-of-the-art performance on long-duration videos, remains competitive on
shorter segments, and reduces model complexity by 88%. Its efficiency and
effectiveness make MS-Temba well-suited for real-world edge deployment.
</summary>
    <author>
      <name>Arkaprava Sinha</name>
    </author>
    <author>
      <name>Monish Soundar Raj</name>
    </author>
    <author>
      <name>Pu Wang</name>
    </author>
    <author>
      <name>Ahmed Helmy</name>
    </author>
    <author>
      <name>Srijan Das</name>
    </author>
    <link href="http://arxiv.org/abs/2501.06138v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.06138v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1806.11008v1</id>
    <updated>2018-06-28T14: 34: 00Z</updated>
    <published>2018-06-28T14: 34: 00Z</published>
    <title>Modeling Spatio-Temporal Human Track Structure for Action Localization</title>
    <summary>  This paper addresses spatio-temporal localization of human actions in video.
In order to localize actions in time, we propose a recurrent localization
network (RecLNet) designed to model the temporal structure of actions on the
level of person tracks. Our model is trained to simultaneously recognize and
localize action classes in time and is based on two layer gated recurrent units
(GRU) applied separately to two streams, i.e. appearance and optical flow
streams. When used together with state-of-the-art person detection and
tracking, our model is shown to improve substantially spatio-temporal action
localization in videos. The gain is shown to be mainly due to improved temporal
localization. We evaluate our method on two recent datasets for spatio-temporal
action localization, UCF101-24 and DALY, demonstrating a significant
improvement of the state of the art.
</summary>
    <author>
      <name>Guilhem Chéron</name>
    </author>
    <author>
      <name>Anton Osokin</name>
    </author>
    <author>
      <name>Ivan Laptev</name>
    </author>
    <author>
      <name>Cordelia Schmid</name>
    </author>
    <link href="http://arxiv.org/abs/1806.11008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.11008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1908.03231v1</id>
    <updated>2019-08-08T18: 16: 37Z</updated>
    <published>2019-08-08T18: 16: 37Z</published>
    <title>Sparse Coding of Shape Trajectories for Facial Expression and Action
  Recognition</title>
    <summary>  The detection and tracking of human landmarks in video streams has gained in
reliability partly due to the availability of affordable RGB-D sensors. The
analysis of such time-varying geometric data is playing an important role in
the automatic human behavior understanding. However, suitable shape
representations as well as their temporal evolution, termed trajectories, often
lie to nonlinear manifolds. This puts an additional constraint (i.e.,
nonlinearity) in using conventional Machine Learning techniques. As a solution,
this paper accommodates the well-known Sparse Coding and Dictionary Learning
approach to study time-varying shapes on the Kendall shape spaces of 2D and 3D
landmarks. We illustrate effective coding of 3D skeletal sequences for action
recognition and 2D facial landmark sequences for macro- and micro-expression
recognition. To overcome the inherent nonlinearity of the shape spaces,
intrinsic and extrinsic solutions were explored. As main results, shape
trajectories give rise to more discriminative time-series with suitable
computational properties, including sparsity and vector space structure.
Extensive experiments conducted on commonly-used datasets demonstrate the
competitiveness of the proposed approaches with respect to state-of-the-art.
</summary>
    <author>
      <name>Amor Ben Tanfous</name>
    </author>
    <author>
      <name>Hassen Drira</name>
    </author>
    <author>
      <name>Boulbaba Ben Amor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages,
                    5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Pattern Analysis and Machine Intelligence
  2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1908.03231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.03231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1904.07442v1</id>
    <updated>2019-04-16T03: 50: 57Z</updated>
    <published>2019-04-16T03: 50: 57Z</published>
    <title>Decoupling Localization and Classification in Single Shot Temporal
  Action Detection</title>
    <summary>  Video temporal action detection aims to temporally localize and recognize the
action in untrimmed videos. Existing one-stage approaches mostly focus on
unifying two subtasks, i.e., localization of action proposals and
classification of each proposal through a fully shared backbone. However, such
design of encapsulating all components of two subtasks in one single network
might restrict the training by ignoring the specialized characteristic of each
subtask. In this paper, we propose a novel Decoupled Single Shot temporal
Action Detection (Decouple-SSAD) method to mitigate such problem by decoupling
the localization and classification in a one-stage scheme. Particularly, two
separate branches are designed in parallel to enable each component to own
representations privately for accurate localization or classification. Each
branch produces a set of action anchor layers by applying deconvolution to the
feature maps of the main stream. Each branch produces a set of feature maps by
applying deconvolution to the feature maps of the main stream. High-level
semantic information from deeper layers is thus incorporated to enhance the
feature representations. We conduct extensive experiments on THUMOS14 dataset
and demonstrate superior performance over state-of-the-art methods. Our code is
available online.
</summary>
    <author>
      <name>Yupan Huang</name>
    </author>
    <author>
      <name>Qi Dai</name>
    </author>
    <author>
      <name>Yutong Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICME 2019. https: //github.com/HYPJUDY/Decouple-SSAD</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.07442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.07442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2409.09953v2</id>
    <updated>2025-01-18T16: 09: 00Z</updated>
    <published>2024-09-16T02: 53: 49Z</published>
    <title>Uncertainty-Guided Appearance-Motion Association Network for
  Out-of-Distribution Action Detection</title>
    <summary>  Out-of-distribution (OOD) detection targets to detect and reject test samples
with semantic shifts, to prevent models trained on in-distribution (ID) dataset
from producing unreliable predictions. Existing works only extract the
appearance features on image datasets, and cannot handle dynamic multimedia
scenarios with much motion information. Therefore, we target a more realistic
and challenging OOD detection task: OOD action detection (ODAD). Given an
untrimmed video, ODAD first classifies the ID actions and recognizes the OOD
actions, and then localizes ID and OOD actions. To this end, in this paper, we
propose a novel Uncertainty-Guided Appearance-Motion Association Network
(UAAN), which explores both appearance features and motion contexts to reason
spatial-temporal inter-object interaction for ODAD.Firstly, we design separate
appearance and motion branches to extract corresponding appearance-oriented and
motion-aspect object representations. In each branch, we construct a
spatial-temporal graph to reason appearance-guided and motion-driven
inter-object interaction. Then, we design an appearance-motion attention module
to fuse the appearance and motion features for final action detection.
Experimental results on two challenging datasets show that UAAN beats
state-of-the-art methods by a significant margin, illustrating its
effectiveness.
</summary>
    <author>
      <name>Xiang Fang</name>
    </author>
    <author>
      <name>Arvind Easwaran</name>
    </author>
    <author>
      <name>Blaise Genest</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by MIPR 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.09953v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.09953v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2106.15258v1</id>
    <updated>2021-06-29T11: 29: 16Z</updated>
    <published>2021-06-29T11: 29: 16Z</published>
    <title>SRF-Net: Selective Receptive Field Network for Anchor-Free Temporal
  Action Detection</title>
    <summary>  Temporal action detection (TAD) is a challenging task which aims to
temporally localize and recognize the human action in untrimmed videos. Current
mainstream one-stage TAD approaches localize and classify action proposals
relying on pre-defined anchors, where the location and scale for action
instances are set by designers. Obviously, such an anchor-based TAD method
limits its generalization capability and will lead to performance degradation
when videos contain rich action variation. In this study, we explore to remove
the requirement of pre-defined anchors for TAD methods. A novel TAD model
termed as Selective Receptive Field Network (SRF-Net) is developed, in which
the location offsets and classification scores at each temporal location can be
directly estimated in the feature map and SRF-Net is trained in an end-to-end
manner. Innovatively, a building block called Selective Receptive Field
Convolution (SRFC) is dedicatedly designed which is able to adaptively adjust
its receptive field size according to multiple scales of input information at
each temporal location in the feature map. Extensive experiments are conducted
on the THUMOS14 dataset, and superior results are reported comparing to
state-of-the-art TAD approaches.
</summary>
    <author>
      <name>Ranyu Ning</name>
    </author>
    <author>
      <name>Can Zhang</name>
    </author>
    <author>
      <name>Yuexian Zou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICASSP 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.15258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.15258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2207.08184v1</id>
    <updated>2022-07-17T13: 59: 46Z</updated>
    <published>2022-07-17T13: 59: 46Z</published>
    <title>Zero-Shot Temporal Action Detection via Vision-Language Prompting</title>
    <summary>  Existing temporal action detection (TAD) methods rely on large training data
including segment-level annotations, limited to recognizing previously seen
classes alone during inference. Collecting and annotating a large training set
for each class of interest is costly and hence unscalable. Zero-shot TAD
(ZS-TAD) resolves this obstacle by enabling a pre-trained model to recognize
any unseen action classes. Meanwhile, ZS-TAD is also much more challenging with
significantly less investigation. Inspired by the success of zero-shot image
classification aided by vision-language (ViL) models such as CLIP, we aim to
tackle the more complex TAD task. An intuitive method is to integrate an
off-the-shelf proposal detector with CLIP style classification. However, due to
the sequential localization (e.g, proposal generation) and classification
design, it is prone to localization error propagation. To overcome this
problem, in this paper we propose a novel zero-Shot Temporal Action detection
model via Vision-LanguagE prompting (STALE). Such a novel design effectively
eliminates the dependence between localization and classification by breaking
the route for error propagation in-between. We further introduce an interaction
mechanism between classification and localization for improved optimization.
Extensive experiments on standard ZS-TAD video benchmarks show that our STALE
significantly outperforms state-of-the-art alternatives. Besides, our model
also yields superior results on supervised TAD over recent strong competitors.
The PyTorch implementation of STALE is available at
https: //github.com/sauradip/STALE.
</summary>
    <author>
      <name>Sauradip Nag</name>
    </author>
    <author>
      <name>Xiatian Zhu</name>
    </author>
    <author>
      <name>Yi-Zhe Song</name>
    </author>
    <author>
      <name>Tao Xiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2022; Code available at https: //github.com/sauradip/STALE</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.08184v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.08184v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2101.08581v1</id>
    <updated>2021-01-21T12: 50: 02Z</updated>
    <published>2021-01-21T12: 50: 02Z</published>
    <title>Hierarchical Graph-RNNs for Action Detection of Multiple Activities</title>
    <summary>  In this paper, we propose an approach that spatially localizes the activities
in a video frame where each person can perform multiple activities at the same
time. Our approach takes the temporal scene context as well as the relations of
the actions of detected persons into account. While the temporal context is
modeled by a temporal recurrent neural network (RNN), the relations of the
actions are modeled by a graph RNN. Both networks are trained together and the
proposed approach achieves state of the art results on the AVA dataset.
</summary>
    <author>
      <name>Sovan Biswas</name>
    </author>
    <author>
      <name>Yaser Souri</name>
    </author>
    <author>
      <name>Juergen Gall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ICIP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.08581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.08581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2206.00614v2</id>
    <updated>2022-11-03T23: 02: 29Z</updated>
    <published>2022-06-01T16: 32: 25Z</published>
    <title>Dual-stream spatiotemporal networks with feature sharing for monitoring
  animals in the home cage</title>
    <summary>  This paper presents a spatiotemporal deep learning approach for mouse
behavioural classification in the home-cage. Using a series of dual-stream
architectures with assorted modifications to increase performance, we introduce
a novel feature sharing approach that jointly processes the streams at regular
intervals throughout the network. To investigate the efficacy of this approach,
models were evaluated by dissociating the streams and training/testing in the
same rigorous manner as the main classifiers. Using an annotated, publicly
available dataset of a singly-housed mice, we achieve prediction accuracy of
86.47% using an ensemble of a Inception-based network and an attention-based
network, both of which utilize this feature sharing. We also demonstrate
through ablation studies that for all models, the feature-sharing architectures
consistently perform better than conventional ones having separate streams. The
best performing models were further evaluated on other activity datasets, both
mouse and human. Future work will investigate the effectiveness of feature
sharing to behavioural classification in the unsupervised anomaly detection
domain.
</summary>
    <author>
      <name>Ezechukwu I. Nwokedi</name>
    </author>
    <author>
      <name>Rasneer S. Bains</name>
    </author>
    <author>
      <name>Luc Bidaut</name>
    </author>
    <author>
      <name>Xujiong Ye</name>
    </author>
    <author>
      <name>Sara Wells</name>
    </author>
    <author>
      <name>James M. Brown</name>
    </author>
    <link href="http://arxiv.org/abs/2206.00614v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.00614v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2012.02426v1</id>
    <updated>2020-12-04T06: 23: 40Z</updated>
    <published>2020-12-04T06: 23: 40Z</published>
    <title>Spatial-Temporal Alignment Network for Action Recognition and Detection</title>
    <summary>  This paper studies how to introduce viewpoint-invariant feature
representations that can help action recognition and detection. Although we
have witnessed great progress of action recognition in the past decade, it
remains challenging yet interesting how to efficiently model the geometric
variations in large scale datasets. This paper proposes a novel
Spatial-Temporal Alignment Network (STAN) that aims to learn geometric
invariant representations for action recognition and action detection. The STAN
model is very light-weighted and generic, which could be plugged into existing
action recognition models like ResNet3D and the SlowFast with a very low extra
computational cost. We test our STAN model extensively on AVA, Kinetics-400,
AVA-Kinetics, Charades, and Charades-Ego datasets. The experimental results
show that the STAN model can consistently improve the state of the arts in both
action detection and action recognition tasks. We will release our data, models
and code.
</summary>
    <author>
      <name>Junwei Liang</name>
    </author>
    <author>
      <name>Liangliang Cao</name>
    </author>
    <author>
      <name>Xuehan Xiong</name>
    </author>
    <author>
      <name>Ting Yu</name>
    </author>
    <author>
      <name>Alexander Hauptmann</name>
    </author>
    <link href="http://arxiv.org/abs/2012.02426v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.02426v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1710.03383v1</id>
    <updated>2017-10-10T02: 50: 37Z</updated>
    <published>2017-10-10T02: 50: 37Z</published>
    <title>Real-Time Action Detection in Video Surveillance using Sub-Action
  Descriptor with Multi-CNN</title>
    <summary>  When we say a person is texting, can you tell the person is walking or
sitting? Emphatically, no. In order to solve this incomplete representation
problem, this paper presents a sub-action descriptor for detailed action
detection. The sub-action descriptor consists of three levels: the posture, the
locomotion, and the gesture level. The three levels give three sub-action
categories for one action to address the representation problem. The proposed
action detection model simultaneously localizes and recognizes the actions of
multiple individuals in video surveillance using appearance-based temporal
features with multi-CNN. The proposed approach achieved a mean average
precision (mAP) of 76.6% at the frame-based and 83.5% at the video-based
measurement on the new large-scale ICVL video surveillance dataset that the
authors introduce and make available to the community with this paper.
Extensive experiments on the benchmark KTH dataset demonstrate that the
proposed approach achieved better performance, which in turn boosts the action
recognition performance over the state-of-the-art. The action detection model
can run at around 25 fps on the ICVL and more than 80 fps on the KTH dataset,
which is suitable for real-time surveillance applications.
</summary>
    <author>
      <name>Cheng-Bin Jin</name>
    </author>
    <author>
      <name>Shengzhe Li</name>
    </author>
    <author>
      <name>Hakil Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages,
                    16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.03383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2406.06187v1</id>
    <updated>2024-06-10T11: 33: 34Z</updated>
    <published>2024-06-10T11: 33: 34Z</published>
    <title>An Effective-Efficient Approach for Dense Multi-Label Action Detection</title>
    <summary>  Unlike the sparse label action detection task, where a single action occurs
in each timestamp of a video, in a dense multi-label scenario, actions can
overlap. To address this challenging task, it is necessary to simultaneously
learn (i) temporal dependencies and (ii) co-occurrence action relationships.
Recent approaches model temporal information by extracting multi-scale features
through hierarchical transformer-based networks. However, the self-attention
mechanism in transformers inherently loses temporal positional information. We
argue that combining this with multiple sub-sampling processes in hierarchical
designs can lead to further loss of positional information. Preserving this
information is essential for accurate action detection. In this paper, we
address this issue by proposing a novel transformer-based network that (a)
employs a non-hierarchical structure when modelling different ranges of
temporal dependencies and (b) embeds relative positional encoding in its
transformer layers. Furthermore, to model co-occurrence action relationships,
current methods explicitly embed class relations into the transformer network.
However, these approaches are not computationally efficient, as the network
needs to compute all possible pair action class relations. We also overcome
this challenge by introducing a novel learning paradigm that allows the network
to benefit from explicitly modelling temporal co-occurrence action dependencies
without imposing their additional computational costs during inference. We
evaluate the performance of our proposed approach on two challenging dense
multi-label benchmark datasets and show that our method improves the current
state-of-the-art results.
</summary>
    <author>
      <name>Faegheh Sardari</name>
    </author>
    <author>
      <name>Armin Mustafa</name>
    </author>
    <author>
      <name>Philip J. B. Jackson</name>
    </author>
    <author>
      <name>Adrian Hilton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages. arXiv admin note: substantial text overlap with
  arXiv: 2308.05051</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.06187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.06187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2107.02050v2</id>
    <updated>2021-11-09T15: 57: 19Z</updated>
    <published>2021-07-05T14: 23: 30Z</published>
    <title>Multimessenger Analysis Strategy for Core-Collapse Supernova Search:
  Gravitational Waves and Low-energy Neutrinos</title>
    <summary>  Core-collapse supernovae are fascinating astrophysical objects for
multimessenger studies. Gravitational waves (GWs) are expected to play a role
in the supernova explosion mechanism, but their modelling is also challenging
due to the stochastic nature of the dynamics and the vast possible progenitors,
and moreover, the GW detection from these objects is still elusive with the
already advanced detectors. Low-energy neutrinos will be emitted enormously
during the core-collapse explosion and can help for the gravitational wave
counterpart search. In this work we develop a multi-messenger strategy to
search for such astrophysical objects by exploiting a global network of both
low-energy neutrino and gravitational wave detectors. First, we discuss how to
improve the detection potential of the neutrino sub-network by exploiting the
temporal behaviour of a neutrino burst from a core-collapse supernova. We show
that with the proposed approach neutrino detectors can gain at least $10\%$ of
detection efficiency at the distance where their efficiency drops. Then, we
combine the information provided by GW and neutrino in a multimessenger
strategy. In particular, we obtain an increase of the probability to detect the
GW signal from a CCSN at $60$ kpc from zero when using only GW analysis to
$\sim 33\%$ with our combined GW-$\nu$ approach.
  Keywords: multimessenger, supernova, core-collapse, low-energy neutrino,
gravitational wave.
</summary>
    <author>
      <name>Odysse Halim</name>
    </author>
    <author>
      <name>Claudio Casentini</name>
    </author>
    <author>
      <name>Marco Drago</name>
    </author>
    <author>
      <name>Viviana Fafone</name>
    </author>
    <author>
      <name>Kate Scholberg</name>
    </author>
    <author>
      <name>Carlo Francesco Vigorito</name>
    </author>
    <author>
      <name>Giulia Pagliaroli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1475-7516/2021/11/021</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1475-7516/2021/11/021" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages,
                    15 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Odysse Halim et al JCAP11(2021)021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2107.02050v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.02050v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2001.11122v3</id>
    <updated>2020-09-30T17: 17: 06Z</updated>
    <published>2020-01-29T22: 51: 06Z</published>
    <title>Joint Visual-Temporal Embedding for Unsupervised Learning of Actions in
  Untrimmed Sequences</title>
    <summary>  Understanding the structure of complex activities in untrimmed videos is a
challenging task in the area of action recognition. One problem here is that
this task usually requires a large amount of hand-annotated minute- or even
hour-long video data, but annotating such data is very time consuming and can
not easily be automated or scaled. To address this problem, this paper proposes
an approach for the unsupervised learning of actions in untrimmed video
sequences based on a joint visual-temporal embedding space. To this end, we
combine a visual embedding based on a predictive U-Net architecture with a
temporal continuous function. The resulting representation space allows
detecting relevant action clusters based on their visual as well as their
temporal appearance. The proposed method is evaluated on three standard
benchmark datasets, Breakfast Actions, INRIA YouTube Instructional Videos, and
50 Salads. We show that the proposed approach is able to provide a meaningful
visual and temporal embedding out of the visual cues present in contiguous
video frames and is suitable for the task of unsupervised temporal segmentation
of actions.
</summary>
    <author>
      <name>Rosaura G. VidalMata</name>
    </author>
    <author>
      <name>Walter J. Scheirer</name>
    </author>
    <author>
      <name>Anna Kukleva</name>
    </author>
    <author>
      <name>David Cox</name>
    </author>
    <author>
      <name>Hilde Kuehne</name>
    </author>
    <link href="http://arxiv.org/abs/2001.11122v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.11122v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2310.02523v4</id>
    <updated>2024-09-09T10: 57: 46Z</updated>
    <published>2023-10-04T01: 47: 36Z</published>
    <title>A Spatio-Temporal Attention-Based Method for Detecting Student Classroom
  Behaviors</title>
    <summary>  Accurately detecting student behavior from classroom videos is beneficial for
analyzing their classroom status and improving teaching efficiency. However,
low accuracy in student classroom behavior detection is a prevalent issue. To
address this issue, we propose a Spatio-Temporal Attention-Based Method for
Detecting Student Classroom Behaviors (BDSTA). Firstly, the SlowFast network is
used to generate motion and environmental information feature maps from the
video. Then, the spatio-temporal attention module is applied to the feature
maps, including information aggregation, compression and stimulation processes.
Subsequently, attention maps in the time, channel and space dimensions are
obtained, and multi-label behavior classification is performed based on these
attention maps. To solve the long-tail data problem that exists in student
classroom behavior datasets, we use an improved focal loss function to assign
more weight to the tail class data during training. Experimental results are
conducted on a self-made student classroom behavior dataset named STSCB.
Compared with the SlowFast model, the average accuracy of student behavior
classification detection improves by 8.94\% using BDSTA.
</summary>
    <author>
      <name>Fan Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2310.02523v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.02523v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2204.03596v1</id>
    <updated>2022-04-07T17: 16: 37Z</updated>
    <published>2022-04-07T17: 16: 37Z</published>
    <title>Controlling Golog Programs against MTL Constraints</title>
    <summary>  While Golog is an expressive programming language to control the high-level
behavior of a robot, it is often tedious to use on a real robotic system. On an
actual robot, the user needs to consider low-level details, such as enabling
and disabling hardware components, e.g., a camera to detect objects for
grasping. In other words, high-level actions usually pose implicit temporal
constraints on the low-level platform, which are typically independent of the
concrete program to be executed. In this paper, we propose to make these
constraints explicit by modeling them as MTL formulas, which enforce the
execution of certain low-level platform operations in addition to the main
program. Based on results from timed automata controller synthesis, we describe
a method to synthesize a controller that executes both the high-level program
and the low-level platform operations concurrently in order to satisfy the MTL
specification. This allows the user to focus on the high-level behavior without
the need to consider low-level operations. We present an extension to Golog by
clocks together with the required theoretical foundations as well as
decidability results.
</summary>
    <author>
      <name>Till Hofmann</name>
    </author>
    <author>
      <name>Stefan Schupp</name>
    </author>
    <link href="http://arxiv.org/abs/2204.03596v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.03596v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2304.08261v1</id>
    <updated>2023-04-13T20: 35: 03Z</updated>
    <published>2023-04-13T20: 35: 03Z</published>
    <title>DeepSegmenter: Temporal Action Localization for Detecting Anomalies in
  Untrimmed Naturalistic Driving Videos</title>
    <summary>  Identifying unusual driving behaviors exhibited by drivers during driving is
essential for understanding driver behavior and the underlying causes of
crashes. Previous studies have primarily approached this problem as a
classification task, assuming that naturalistic driving videos come
discretized. However, both activity segmentation and classification are
required for this task due to the continuous nature of naturalistic driving
videos. The current study therefore departs from conventional approaches and
introduces a novel methodological framework, DeepSegmenter, that simultaneously
performs activity segmentation and classification in a single framework. The
proposed framework consists of four major modules namely Data Module, Activity
Segmentation Module, Classification Module and Postprocessing Module. Our
proposed method won 8th place in the 2023 AI City Challenge, Track 3, with an
activity overlap score of 0.5426 on experimental validation data. The
experimental results demonstrate the effectiveness, efficiency, and robustness
of the proposed system.
</summary>
    <author>
      <name>Armstrong Aboah</name>
    </author>
    <author>
      <name>Ulas Bagci</name>
    </author>
    <author>
      <name>Abdul Rashid Mussah</name>
    </author>
    <author>
      <name>Neema Jakisa Owor</name>
    </author>
    <author>
      <name>Yaw Adu-Gyamfi</name>
    </author>
    <link href="http://arxiv.org/abs/2304.08261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.08261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2411.18798v1</id>
    <updated>2024-11-27T22: 52: 36Z</updated>
    <published>2024-11-27T22: 52: 36Z</published>
    <title>Formal Verification of Digital Twins with TLA and Information Leakage
  Control</title>
    <summary>  Verifying the correctness of a digital twin provides a formal guarantee that
the digital twin operates as intended. Digital twin verification is challenging
due to the presence of uncertainties in the virtual representation, the
physical environment, and the bidirectional flow of information between
physical and virtual. A further challenge is that a digital twin of a complex
system is composed of distributed components. This paper presents a methodology
to specify and verify digital twin behavior, translating uncertain processes
into a formally verifiable finite state machine. We use the Temporal Logic of
Actions (TLA) to create a specification, an implementation abstraction that
defines the properties required for correct system behavior. Our approach
includes a novel weakening of formal security properties, allowing controlled
information leakage while preserving theoretical guarantees. We demonstrate
this approach on a digital twin of an unmanned aerial vehicle, verifying
synchronization of physical-to-virtual and virtual-to-digital data flows to
detect unintended misalignments.
</summary>
    <author>
      <name>Luwen Huang</name>
    </author>
    <author>
      <name>Lav R. Varshney</name>
    </author>
    <author>
      <name>Karen E. Willcox</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.18798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.18798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1706.08218v1</id>
    <updated>2017-06-26T03: 52: 15Z</updated>
    <published>2017-06-26T03: 52: 15Z</published>
    <title>YoTube: Searching Action Proposal via Recurrent and Static Regression
  Networks</title>
    <summary>  In this paper, we present YoTube-a novel network fusion framework for
searching action proposals in untrimmed videos, where each action proposal
corresponds to a spatialtemporal video tube that potentially locates one human
action. Our method consists of a recurrent YoTube detector and a static YoTube
detector, where the recurrent YoTube explores the regression capability of RNN
for candidate bounding boxes predictions using learnt temporal dynamics and the
static YoTube produces the bounding boxes using rich appearance cues in a
single frame. Both networks are trained using rgb and optical flow in order to
fully exploit the rich appearance, motion and temporal context, and their
outputs are fused to produce accurate and robust proposal boxes. Action
proposals are finally constructed by linking these boxes using dynamic
programming with a novel trimming method to handle the untrimmed video
effectively and efficiently. Extensive experiments on the challenging UCF-101
and UCF-Sports datasets show that our proposed technique obtains superior
performance compared with the state-of-the-art.
</summary>
    <author>
      <name>Hongyuan Zhu</name>
    </author>
    <author>
      <name>Romain Vial</name>
    </author>
    <author>
      <name>Shijian Lu</name>
    </author>
    <author>
      <name>Yonghong Tian</name>
    </author>
    <author>
      <name>Xianbin Cao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2018.2806279</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2018.2806279" rel="related"/>
    <link href="http://arxiv.org/abs/1706.08218v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08218v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2501.18509v2</id>
    <updated>2025-03-11T12: 34: 08Z</updated>
    <published>2025-01-30T17: 20: 42Z</published>
    <title>Reframing Dense Action Detection (RefDense): A Paradigm Shift in Problem
  Solving &amp; a Novel Optimization Strategy</title>
    <summary>  Dense action detection involves detecting multiple co-occurring actions while
action classes are often ambiguous and represent overlapping concepts. We argue
that handling the dual challenge of temporal and class overlaps is too complex
to effectively be tackled by a single network. To address this, we propose to
decompose the task of detecting dense ambiguous actions into detecting dense,
unambiguous sub-concepts that form the action classes (i.e., action entities
and action motions), and assigning these sub-tasks to distinct sub-networks. By
isolating these unambiguous concepts, the sub-networks can focus exclusively on
resolving a single challenge, dense temporal overlaps. Furthermore,
simultaneous actions in a video often exhibit interrelationships, and
exploiting these relationships can improve the method performance. However,
current dense action detection networks fail to effectively learn these
relationships due to their reliance on binary cross-entropy optimization, which
treats each class independently. To address this limitation, we propose
providing explicit supervision on co-occurring concepts during network
optimization through a novel language-guided contrastive learning loss. Our
extensive experiments demonstrate the superiority of our approach over
state-of-the-art methods, achieving substantial improvements of 3.8% and 1.7%
on average across all metrics on the challenging benchmark datasets, Charades
and MultiTHUMOS.
</summary>
    <author>
      <name>Faegheh Sardari</name>
    </author>
    <author>
      <name>Armin Mustafa</name>
    </author>
    <author>
      <name>Philip J. B. Jackson</name>
    </author>
    <author>
      <name>Adrian Hilton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Vision</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.18509v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.18509v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1704.04671v1</id>
    <updated>2017-04-15T18: 10: 21Z</updated>
    <published>2017-04-15T18: 10: 21Z</published>
    <title>Temporal Action Localization by Structured Maximal Sums</title>
    <summary>  We address the problem of temporal action localization in videos. We pose
action localization as a structured prediction over arbitrary-length temporal
windows, where each window is scored as the sum of frame-wise classification
scores. Additionally, our model classifies the start, middle, and end of each
action as separate components, allowing our system to explicitly model each
action's temporal evolution and take advantage of informative temporal
dependencies present in this structure. In this framework, we localize actions
by searching for the structured maximal sum, a problem for which we develop a
novel, provably-efficient algorithmic solution. The frame-wise classification
scores are computed using features from a deep Convolutional Neural Network
(CNN), which are trained end-to-end to directly optimize for a novel structured
objective. We evaluate our system on the THUMOS 14 action detection benchmark
and achieve competitive performance.
</summary>
    <author>
      <name>Zehuan Yuan</name>
    </author>
    <author>
      <name>Jonathan C. Stroud</name>
    </author>
    <author>
      <name>Tong Lu</name>
    </author>
    <author>
      <name>Jia Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CVPR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.04671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2106.13014v1</id>
    <updated>2021-06-24T13: 46: 30Z</updated>
    <published>2021-06-24T13: 46: 30Z</published>
    <title>Exploring Stronger Feature for Temporal Action Localization</title>
    <summary>  Temporal action localization aims to localize starting and ending time with
action category. Limited by GPU memory, mainstream methods pre-extract features
for each video. Therefore, feature quality determines the upper bound of
detection performance. In this technical report, we explored classic
convolution-based backbones and the recent surge of transformer-based
backbones. We found that the transformer-based methods can achieve better
classification performance than convolution-based, but they cannot generate
accuracy action proposals. In addition, extracting features with larger frame
resolution to reduce the loss of spatial information can also effectively
improve the performance of temporal action localization. Finally, we achieve
42.42% in terms of mAP on validation set with a single SlowFast feature by a
simple combination: BMN+TCANet, which is 1.87% higher than the result of 2020's
multi-model ensemble. Finally, we achieve Rank 1st on the CVPR2021 HACS
supervised Temporal Action Localization Challenge.
</summary>
    <author>
      <name>Zhiwu Qing</name>
    </author>
    <author>
      <name>Xiang Wang</name>
    </author>
    <author>
      <name>Ziyuan Huang</name>
    </author>
    <author>
      <name>Yutong Feng</name>
    </author>
    <author>
      <name>Shiwei Zhang</name>
    </author>
    <author>
      <name>jianwen Jiang</name>
    </author>
    <author>
      <name>Mingqian Tang</name>
    </author>
    <author>
      <name>Changxin Gao</name>
    </author>
    <author>
      <name>Nong Sang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Rank 1st on the CVPR2021 HACS supervised Temporal Action Localization
  Challenge</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.13014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.13014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1809.03669v1</id>
    <updated>2018-09-11T03: 29: 28Z</updated>
    <published>2018-09-11T03: 29: 28Z</published>
    <title>Temporal-Spatial Mapping for Action Recognition</title>
    <summary>  Deep learning models have enjoyed great success for image related computer
vision tasks like image classification and object detection. For video related
tasks like human action recognition, however, the advancements are not as
significant yet. The main challenge is the lack of effective and efficient
models in modeling the rich temporal spatial information in a video. We
introduce a simple yet effective operation, termed Temporal-Spatial Mapping
(TSM), for capturing the temporal evolution of the frames by jointly analyzing
all the frames of a video. We propose a video level 2D feature representation
by transforming the convolutional features of all frames to a 2D feature map,
referred to as VideoMap. With each row being the vectorized feature
representation of a frame, the temporal-spatial features are compactly
represented, while the temporal dynamic evolution is also well embedded. Based
on the VideoMap representation, we further propose a temporal attention model
within a shallow convolutional neural network to efficiently exploit the
temporal-spatial dynamics. The experiment results show that the proposed scheme
achieves the state-of-the-art performance, with 4.2% accuracy gain over
Temporal Segment Network (TSN), a competing baseline method, on the challenging
human action benchmark dataset HMDB51.
</summary>
    <author>
      <name>Xiaolin Song</name>
    </author>
    <author>
      <name>Cuiling Lan</name>
    </author>
    <author>
      <name>Wenjun Zeng</name>
    </author>
    <author>
      <name>Junliang Xing</name>
    </author>
    <author>
      <name>Jingyu Yang</name>
    </author>
    <author>
      <name>Xiaoyan Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1809.03669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.03669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2011.10830v3</id>
    <updated>2021-03-26T11: 01: 35Z</updated>
    <published>2020-11-21T17: 46: 24Z</published>
    <title>Boundary-sensitive Pre-training for Temporal Localization in Videos</title>
    <summary>  Many video analysis tasks require temporal localization thus detection of
content changes. However, most existing models developed for these tasks are
pre-trained on general video action classification tasks. This is because large
scale annotation of temporal boundaries in untrimmed videos is expensive.
Therefore no suitable datasets exist for temporal boundary-sensitive
pre-training. In this paper for the first time, we investigate model
pre-training for temporal localization by introducing a novel
boundary-sensitive pretext (BSP) task. Instead of relying on costly manual
annotations of temporal boundaries, we propose to synthesize temporal
boundaries in existing video action classification datasets. With the
synthesized boundaries, BSP can be simply conducted via classifying the
boundary types. This enables the learning of video representations that are
much more transferable to downstream temporal localization tasks. Extensive
experiments show that the proposed BSP is superior and complementary to the
existing action classification based pre-training counterpart, and achieves new
state-of-the-art performance on several temporal localization tasks.
</summary>
    <author>
      <name>Mengmeng Xu</name>
    </author>
    <author>
      <name>Juan-Manuel Perez-Rua</name>
    </author>
    <author>
      <name>Victor Escorcia</name>
    </author>
    <author>
      <name>Brais Martinez</name>
    </author>
    <author>
      <name>Xiatian Zhu</name>
    </author>
    <author>
      <name>Li Zhang</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <author>
      <name>Tao Xiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages,
                    4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.10830v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.10830v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.14524v1</id>
    <updated>2025-03-15T08: 01: 36Z</updated>
    <published>2025-03-15T08: 01: 36Z</published>
    <title>Salient Temporal Encoding for Dynamic Scene Graph Generation</title>
    <summary>  Representing a dynamic scene using a structured spatial-temporal scene graph
is a novel and particularly challenging task. To tackle this task, it is
crucial to learn the temporal interactions between objects in addition to their
spatial relations. Due to the lack of explicitly annotated temporal relations
in current benchmark datasets, most of the existing spatial-temporal scene
graph generation methods build dense and abstract temporal connections among
all objects across frames. However, not all temporal connections are encoding
meaningful temporal dynamics. We propose a novel spatial-temporal scene graph
generation method that selectively builds temporal connections only between
temporal-relevant objects pairs and represents the temporal relations as
explicit edges in the scene graph. The resulting sparse and explicit temporal
representation allows us to improve upon strong scene graph generation
baselines by up to $4.4\%$ in Scene Graph Detection. In addition, we show that
our approach can be leveraged to improve downstream vision tasks. Particularly,
applying our approach to action recognition, shows 0.6\% gain in mAP in
comparison to the state-of-the-art
</summary>
    <author>
      <name>Zhihao Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2503.14524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.14524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2209.02241v1</id>
    <updated>2022-09-06T06: 31: 09Z</updated>
    <published>2022-09-06T06: 31: 09Z</published>
    <title>Real-Time Cattle Interaction Recognition via Triple-stream Network</title>
    <summary>  In stockbreeding of beef cattle, computer vision-based approaches have been
widely employed to monitor cattle conditions (e.g. the physical, physiology,
and health). To this end, the accurate and effective recognition of cattle
action is a prerequisite. Generally, most existing models are confined to
individual behavior that uses video-based methods to extract spatial-temporal
features for recognizing the individual actions of each cattle. However, there
is sociality among cattle and their interaction usually reflects important
conditions, e.g. estrus, and also video-based method neglects the real-time
capability of the model. Based on this, we tackle the challenging task of
real-time recognizing interactions between cattle in a single frame in this
paper. The pipeline of our method includes two main modules: Cattle
Localization Network and Interaction Recognition Network. At every moment,
cattle localization network outputs high-quality interaction proposals from
every detected cattle and feeds them into the interaction recognition network
with a triple-stream architecture. Such a triple-stream network allows us to
fuse different features relevant to recognizing interactions. Specifically, the
three kinds of features are a visual feature that extracts the appearance
representation of interaction proposals, a geometric feature that reflects the
spatial relationship between cattle, and a semantic feature that captures our
prior knowledge of the relationship between the individual action and
interaction of cattle. In addition, to solve the problem of insufficient
quantity of labeled data, we pre-train the model based on self-supervised
learning. Qualitative and quantitative evaluation evidences the performance of
our framework as an effective method to recognize cattle interaction in real
time.
</summary>
    <author>
      <name>Yang Yang</name>
    </author>
    <author>
      <name>Mizuka Komatsu</name>
    </author>
    <author>
      <name>Kenji Oyama</name>
    </author>
    <author>
      <name>Takenao Ohkawa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in ICMLA2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.02241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.02241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1610.06906v2</id>
    <updated>2016-11-01T16: 06: 47Z</updated>
    <published>2016-10-21T19: 36: 49Z</published>
    <title>Review of Action Recognition and Detection Methods</title>
    <summary>  In computer vision, action recognition refers to the act of classifying an
action that is present in a given video and action detection involves locating
actions of interest in space and/or time. Videos, which contain photometric
information (e.g. RGB, intensity values) in a lattice structure, contain
information that can assist in identifying the action that has been imaged. The
process of action recognition and detection often begins with extracting useful
features and encoding them to ensure that the features are specific to serve
the task of action recognition and detection. Encoded features are then
processed through a classifier to identify the action class and their spatial
and/or temporal locations. In this report, a thorough review of various action
recognition and detection algorithms in computer vision is provided by
analyzing the two-step process of a typical action recognition and detection
algorithm: (i) extraction and encoding of features, and (ii) classifying
features into action classes. In efforts to ensure that computer vision-based
algorithms reach the capabilities that humans have of identifying actions
irrespective of various nuisance variables that may be present within the field
of view, the state-of-the-art methods are reviewed and some remaining problems
are addressed in the final chapter.
</summary>
    <author>
      <name>Soo Min Kang</name>
    </author>
    <author>
      <name>Richard P. Wildes</name>
    </author>
    <link href="http://arxiv.org/abs/1610.06906v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.06906v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2307.07977v1</id>
    <updated>2023-07-16T07: 58: 29Z</updated>
    <published>2023-07-16T07: 58: 29Z</published>
    <title>Integrating Human Parsing and Pose Network for Human Action Recognition</title>
    <summary>  Human skeletons and RGB sequences are both widely-adopted input modalities
for human action recognition. However, skeletons lack appearance features and
color data suffer large amount of irrelevant depiction. To address this, we
introduce human parsing feature map as a novel modality, since it can
selectively retain spatiotemporal features of the body parts, while filtering
out noises regarding outfits, backgrounds, etc. We propose an Integrating Human
Parsing and Pose Network (IPP-Net) for action recognition, which is the first
to leverage both skeletons and human parsing feature maps in dual-branch
approach. The human pose branch feeds compact skeletal representations of
different modalities in graph convolutional network to model pose features. In
human parsing branch, multi-frame body-part parsing features are extracted with
human detector and parser, which is later learnt using a convolutional
backbone. A late ensemble of two branches is adopted to get final predictions,
considering both robust keypoints and rich semantic body-part features.
Extensive experiments on NTU RGB+D and NTU RGB+D 120 benchmarks consistently
verify the effectiveness of the proposed IPP-Net, which outperforms the
existing action recognition methods. Our code is publicly available at
https: //github.com/liujf69/IPP-Net-Parsing .
</summary>
    <author>
      <name>Runwei Ding</name>
    </author>
    <author>
      <name>Yuhang Wen</name>
    </author>
    <author>
      <name>Jinfu Liu</name>
    </author>
    <author>
      <name>Nan Dai</name>
    </author>
    <author>
      <name>Fanyang Meng</name>
    </author>
    <author>
      <name>Mengyuan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CICAI 2023 Camera-ready Version</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.07977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.07977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2108.03619v1</id>
    <updated>2021-08-08T12: 04: 14Z</updated>
    <published>2021-08-08T12: 04: 14Z</published>
    <title>Learning an Augmented RGB Representation with Cross-Modal Knowledge
  Distillation for Action Detection</title>
    <summary>  In video understanding, most cross-modal knowledge distillation (KD) methods
are tailored for classification tasks, focusing on the discriminative
representation of the trimmed videos. However, action detection requires not
only categorizing actions, but also localizing them in untrimmed videos.
Therefore, transferring knowledge pertaining to temporal relations is critical
for this task which is missing in the previous cross-modal KD frameworks. To
this end, we aim at learning an augmented RGB representation for action
detection, taking advantage of additional modalities at training time through
KD. We propose a KD framework consisting of two levels of distillation. On one
hand, atomic-level distillation encourages the RGB student to learn the
sub-representation of the actions from the teacher in a contrastive manner. On
the other hand, sequence-level distillation encourages the student to learn the
temporal knowledge from the teacher, which consists of transferring the Global
Contextual Relations and the Action Boundary Saliency. The result is an
Augmented-RGB stream that can achieve competitive performance as the two-stream
network while using only RGB at inference time. Extensive experimental analysis
shows that our proposed distillation framework is generic and outperforms other
popular cross-modal distillation methods in action detection task.
</summary>
    <author>
      <name>Rui Dai</name>
    </author>
    <author>
      <name>Srijan Das</name>
    </author>
    <author>
      <name>Francois Bremond</name>
    </author>
    <link href="http://arxiv.org/abs/2108.03619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.03619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1812.11631v3</id>
    <updated>2020-05-10T23: 01: 24Z</updated>
    <published>2018-12-30T23: 12: 27Z</published>
    <title>Actor Conditioned Attention Maps for Video Action Detection</title>
    <summary>  While observing complex events with multiple actors, humans do not assess
each actor separately, but infer from the context. The surrounding context
provides essential information for understanding actions. To this end, we
propose to replace region of interest(RoI) pooling with an attention module,
which ranks each spatio-temporal region's relevance to a detected actor instead
of cropping. We refer to these as Actor-Conditioned Attention Maps (ACAM),
which amplify/dampen the features extracted from the entire scene. The
resulting actor-conditioned features focus the model on regions that are
relevant to the conditioned actor. For actor localization, we leverage
pre-trained object detectors, which transfer better. The proposed model is
efficient and our action detection pipeline achieves near real-time
performance. Experimental results on AVA 2.1 and JHMDB demonstrate the
effectiveness of attention maps, with improvements of 7 mAP on AVA and 4 mAP on
JHMDB.
</summary>
    <author>
      <name>Oytun Ulutan</name>
    </author>
    <author>
      <name>Swati Rallapalli</name>
    </author>
    <author>
      <name>Mudhakar Srivatsa</name>
    </author>
    <author>
      <name>Carlos Torres</name>
    </author>
    <author>
      <name>B. S. Manjunath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">WACV2020 Paper</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In The IEEE Winter Conference on Applications of Computer Vision
  (pp. 527-536) 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.11631v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.11631v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2007.15726v1</id>
    <updated>2020-07-30T20: 26: 15Z</updated>
    <published>2020-07-30T20: 26: 15Z</published>
    <title>Dynamic Hypergames for Synthesis of Deceptive Strategies with Temporal
  Logic Objectives</title>
    <summary>  In this paper, we study the use of deception for strategic planning in
adversarial environments. We model the interaction between the agent (player 1)
and the adversary (player 2) as a two-player concurrent game in which the
adversary has incomplete information about the agent's task specification in
temporal logic. During the online interaction, the adversary can infer the
agent's intention from observations and adapt its strategy so as to prevent the
agent from satisfying the task. To plan against such an adaptive opponent, the
agent must leverage its knowledge about the adversary's incomplete information
to influence the behavior of the opponent, and thereby being deceptive. To
synthesize a deceptive strategy, we introduce a class of hypergame models that
capture the interaction between the agent and its adversary given asymmetric,
incomplete information. A hypergame is a hierarchy of games, perceived
differently by the agent and its adversary. We develop the solution concept of
this class of hypergames and show that the subjectively rationalizable strategy
for the agent is deceptive and maximizes the probability of satisfying the task
in temporal logic. This deceptive strategy is obtained by modeling the opponent
evolving perception of the interaction and integrating the opponent model into
proactive planning. Following the deceptive strategy, the agent chooses actions
to influence the game history as well as to manipulate the adversary's
perception so that it takes actions that benefit the goal of the agent. We
demonstrate the correctness of our deceptive planning algorithm using robot
motion planning examples with temporal logic objectives and design a detection
mechanism to notify the agent of potential errors in modeling of the
adversary's behavior.
</summary>
    <author>
      <name>Lening Li</name>
    </author>
    <author>
      <name>Haoxiang Ma</name>
    </author>
    <author>
      <name>Abhishek N. Kulkarni</name>
    </author>
    <author>
      <name>Jie Fu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages,
                    4 figures, submitted to IEEE Transactions on Automation
  Science and Engineering</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.15726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.15726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1907.05023v2</id>
    <updated>2020-04-10T06: 13: 38Z</updated>
    <published>2019-07-11T07: 00: 47Z</published>
    <title>Micro-expression Action Unit Detection with Spatio-temporal Adaptive
  Pooling</title>
    <summary>  Action Unit (AU) detection plays an important role for facial expression
recognition. To the best of our knowledge, there is little research about AU
analysis for micro-expressions. In this paper, we focus on AU detection in
micro-expressions. Microexpression AU detection is challenging due to the small
quantity of micro-expression databases, low intensity, short duration of facial
muscle change, and class imbalance. In order to alleviate the problems, we
propose a novel Spatio-Temporal Adaptive Pooling (STAP) network for AU
detection in micro-expressions. Firstly, STAP is aggregated by a series of
convolutional filters of different sizes. In this way, STAP can obtain
multi-scale information on spatial and temporal domains. On the other hand,
STAP contains less parameters, thus it has less computational cost and is
suitable for micro-expression AU detection on very small databases.
Furthermore, STAP module is designed to pool discriminative information for
micro-expression AUs on spatial and temporal domains.Finally, Focal loss is
employed to prevent the vast number of negatives from overwhelming the
microexpression AU detector. In experiments, we firstly polish the AU
annotations on three commonly used databases. We conduct intensive experiments
on three micro-expression databases, and provide several baseline results on
micro-expression AU detection. The results show that our proposed approach
outperforms the basic Inflated inception-v1 (I3D) in terms of an average of F1-
score. We also evaluate the performance of our proposed method on
cross-database protocol. It demonstrates that our proposed approach is feasible
for cross-database micro-expression AU detection. Importantly, the results on
three micro-expression databases and cross-database protocol provide extensive
baseline results for future research on micro-expression AU detection.
</summary>
    <author>
      <name>Yante Li</name>
    </author>
    <author>
      <name>Xiaohua Huang</name>
    </author>
    <author>
      <name>Guoying Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">There is a bug in the method. The results are not correct</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.05023v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.05023v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2204.07892v1</id>
    <updated>2022-04-17T00: 42: 14Z</updated>
    <published>2022-04-17T00: 42: 14Z</published>
    <title>Video Action Detection: Analysing Limitations and Challenges</title>
    <summary>  Beyond possessing large enough size to feed data hungry machines (eg,
transformers), what attributes measure the quality of a dataset? Assuming that
the definitions of such attributes do exist, how do we quantify among their
relative existences? Our work attempts to explore these questions for video
action detection. The task aims to spatio-temporally localize an actor and
assign a relevant action class. We first analyze the existing datasets on video
action detection and discuss their limitations. Next, we propose a new dataset,
Multi Actor Multi Action (MAMA) which overcomes these limitations and is more
suitable for real world applications. In addition, we perform a biasness study
which analyzes a key property differentiating videos from static images: the
temporal aspect. This reveals if the actions in these datasets really need the
motion information of an actor, or whether they predict the occurrence of an
action even by looking at a single frame. Finally, we investigate the widely
held assumptions on the importance of temporal ordering: is temporal ordering
important for detecting these actions? Such extreme experiments show existence
of biases which have managed to creep into existing methods inspite of careful
modeling.
</summary>
    <author>
      <name>Rajat Modi</name>
    </author>
    <author>
      <name>Aayush Jung Rana</name>
    </author>
    <author>
      <name>Akash Kumar</name>
    </author>
    <author>
      <name>Praveen Tirupattur</name>
    </author>
    <author>
      <name>Shruti Vyas</name>
    </author>
    <author>
      <name>Yogesh Singh Rawat</name>
    </author>
    <author>
      <name>Mubarak Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPRW'22</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.07892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.07892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1912.04316v3</id>
    <updated>2021-03-01T10: 37: 54Z</updated>
    <published>2019-12-09T19: 01: 46Z</published>
    <title>Video action detection by learning graph-based spatio-temporal
  interactions</title>
    <summary>  Action Detection is a complex task that aims to detect and classify human
actions in video clips. Typically, it has been addressed by processing
fine-grained features extracted from a video classification backbone. Recently,
thanks to the robustness of object and people detectors, a deeper focus has
been added on relationship modelling. Following this line, we propose a
graph-based framework to learn high-level interactions between people and
objects, in both space and time. In our formulation, spatio-temporal
relationships are learned through self-attention on a multi-layer graph
structure which can connect entities from consecutive clips, thus considering
long-range spatial and temporal dependencies. The proposed module is backbone
independent by design and does not require end-to-end training. Extensive
experiments are conducted on the AVA dataset, where our model demonstrates
state-of-the-art results and consistent improvements over baselines built with
different backbones. Code is publicly available at
https: //github.com/aimagelab/STAGE_action_detection.
</summary>
    <author>
      <name>Matteo Tomei</name>
    </author>
    <author>
      <name>Lorenzo Baraldi</name>
    </author>
    <author>
      <name>Simone Calderara</name>
    </author>
    <author>
      <name>Simone Bronzin</name>
    </author>
    <author>
      <name>Rita Cucchiara</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cviu.2021.103187</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cviu.2021.103187" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the authors version of an article accepted for publication in
  Computer Vision and Image Understanding (CVIU), available online February
  2021</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Vision and Image Understanding (CVIU),
                    2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1912.04316v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.04316v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2003.14065v1</id>
    <updated>2020-03-31T10: 02: 51Z</updated>
    <published>2020-03-31T10: 02: 51Z</published>
    <title>Long Short-Term Relation Networks for Video Action Detection</title>
    <summary>  It has been well recognized that modeling human-object or object-object
relations would be helpful for detection task. Nevertheless, the problem is not
trivial especially when exploring the interactions between human actor, object
and scene (collectively as human-context) to boost video action detectors. The
difficulty originates from the aspect that reliable relations in a video should
depend on not only short-term human-context relation in the present clip but
also the temporal dynamics distilled over a long-range span of the video. This
motivates us to capture both short-term and long-term relations in a video. In
this paper, we present a new Long Short-Term Relation Networks, dubbed as LSTR,
that novelly aggregates and propagates relation to augment features for video
action detection. Technically, Region Proposal Networks (RPN) is remoulded to
first produce 3D bounding boxes, i.e., tubelets, in each video clip. LSTR then
models short-term human-context interactions within each clip through
spatio-temporal attention mechanism and reasons long-term temporal dynamics
across video clips via Graph Convolutional Networks (GCN) in a cascaded manner.
Extensive experiments are conducted on four benchmark datasets, and superior
results are reported when comparing to state-of-the-art methods.
</summary>
    <author>
      <name>Dong Li</name>
    </author>
    <author>
      <name>Ting Yao</name>
    </author>
    <author>
      <name>Zhaofan Qiu</name>
    </author>
    <author>
      <name>Houqiang Li</name>
    </author>
    <author>
      <name>Tao Mei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a full paper for ACMMM 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.14065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.14065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2105.11731v2</id>
    <updated>2021-06-24T09: 10: 17Z</updated>
    <published>2021-05-25T07: 54: 35Z</published>
    <title>ST-HOI: A Spatial-Temporal Baseline for Human-Object Interaction
  Detection in Videos</title>
    <summary>  Detecting human-object interactions (HOI) is an important step toward a
comprehensive visual understanding of machines. While detecting non-temporal
HOIs (e.g., sitting on a chair) from static images is feasible, it is unlikely
even for humans to guess temporal-related HOIs (e.g., opening/closing a door)
from a single video frame, where the neighboring frames play an essential role.
However, conventional HOI methods operating on only static images have been
used to predict temporal-related interactions, which is essentially guessing
without temporal contexts and may lead to sub-optimal performance. In this
paper, we bridge this gap by detecting video-based HOIs with explicit temporal
information. We first show that a naive temporal-aware variant of a common
action detection baseline does not work on video-based HOIs due to a
feature-inconsistency issue. We then propose a simple yet effective
architecture named Spatial-Temporal HOI Detection (ST-HOI) utilizing temporal
information such as human and object trajectories, correctly-localized visual
features, and spatial-temporal masking pose features. We construct a new video
HOI benchmark dubbed VidHOI where our proposed approach serves as a solid
baseline.
</summary>
    <author>
      <name>Meng-Jiun Chiou</name>
    </author>
    <author>
      <name>Chun-Yu Liao</name>
    </author>
    <author>
      <name>Li-Wei Wang</name>
    </author>
    <author>
      <name>Roger Zimmermann</name>
    </author>
    <author>
      <name>Jiashi Feng</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3463944.3469097</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3463944.3469097" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ACM ICMR'21 Workshop on Intelligent Cross-Data Analysis
  and Retrieval. The dataset and source code are available at
  https: //github.com/coldmanck/VidHOI</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.11731v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.11731v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1610.09334v1</id>
    <updated>2016-10-28T18: 15: 31Z</updated>
    <published>2016-10-28T18: 15: 31Z</published>
    <title>Real-time Online Action Detection Forests using Spatio-temporal Contexts</title>
    <summary>  Online action detection (OAD) is challenging since 1) robust yet
computationally expensive features cannot be straightforwardly used due to the
real-time processing requirements and 2) the localization and classification of
actions have to be performed even before they are fully observed. We propose a
new random forest (RF)-based online action detection framework that addresses
these challenges. Our algorithm uses computationally efficient skeletal joint
features. High accuracy is achieved by using robust convolutional neural
network (CNN)-based features which are extracted from the raw RGBD images, plus
the temporal relationships between the current frame of interest, and the past
and future frames. While these high-quality features are not available in
real-time testing scenario, we demonstrate that they can be effectively
exploited in training RF classifiers: We use these spatio-temporal contexts to
craft RF's new split functions improving RFs' leaf node statistics. Experiments
with challenging MSRAction3D, G3D, and OAD datasets demonstrate that our
algorithm significantly improves the accuracy over the state-of-the-art online
action detection algorithms while achieving the real-time efficiency of
existing skeleton-based RF classifiers.
</summary>
    <author>
      <name>Seungryul Baek</name>
    </author>
    <author>
      <name>Kwang In Kim</name>
    </author>
    <author>
      <name>Tae-Kyun Kim</name>
    </author>
    <link href="http://arxiv.org/abs/1610.09334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.09334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.13344v2</id>
    <updated>2025-03-20T10: 11: 27Z</updated>
    <published>2025-03-17T16: 22: 00Z</published>
    <title>STEP: Simultaneous Tracking and Estimation of Pose for Animals and
  Humans</title>
    <summary>  We introduce STEP, a novel framework utilizing Transformer-based
discriminative model prediction for simultaneous tracking and estimation of
pose across diverse animal species and humans. We are inspired by the fact that
the human brain exploits spatiotemporal continuity and performs concurrent
localization and pose estimation despite the specialization of brain areas for
form and motion processing. Traditional discriminative models typically require
predefined target states for determining model weights, a challenge we address
through Gaussian Map Soft Prediction (GMSP) and Offset Map Regression Adapter
(OMRA) Modules. These modules remove the necessity of keypoint target states as
input, streamlining the process. Our method starts with a known target state in
the initial frame of a given video sequence. It then seamlessly tracks the
target and estimates keypoints of anatomical importance as output for
subsequent frames. Unlike prevalent top-down pose estimation methods, our
approach doesn't rely on per-frame target detections due to its tracking
capability. This facilitates a significant advancement in inference efficiency
and potential applications. We train and validate our approach on datasets
encompassing diverse species. Our experiments demonstrate superior results
compared to existing methods, opening doors to various applications, including
but not limited to action recognition and behavioral analysis.
</summary>
    <author>
      <name>Shashikant Verma</name>
    </author>
    <author>
      <name>Harish Katti</name>
    </author>
    <author>
      <name>Soumyaratna Debnath</name>
    </author>
    <author>
      <name>Yamuna Swamy</name>
    </author>
    <author>
      <name>Shanmuganathan Raman</name>
    </author>
    <link href="http://arxiv.org/abs/2503.13344v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.13344v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2002.07917v1</id>
    <updated>2020-02-18T22: 56: 40Z</updated>
    <published>2020-02-18T22: 56: 40Z</published>
    <title>TIES: Temporal Interaction Embeddings For Enhancing Social Media
  Integrity At Facebook</title>
    <summary>  Since its inception, Facebook has become an integral part of the online
social community. People rely on Facebook to make connections with others and
build communities. As a result, it is paramount to protect the integrity of
such a rapidly growing network in a fast and scalable manner. In this paper, we
present our efforts to protect various social media entities at Facebook from
people who try to abuse our platform. We present a novel Temporal Interaction
EmbeddingS (TIES) model that is designed to capture rogue social interactions
and flag them for further suitable actions. TIES is a supervised, deep
learning, production ready model at Facebook-scale networks. Prior works on
integrity problems are mostly focused on capturing either only static or
certain dynamic features of social entities. In contrast, TIES can capture both
these variant behaviors in a unified model owing to the recent strides made in
the domains of graph embedding and deep sequential pattern learning. To show
the real-world impact of TIES, we present a few applications especially for
preventing spread of misinformation, fake account detection, and reducing ads
payment risks in order to enhance the platform's integrity.
</summary>
    <author>
      <name>Nima Noorshams</name>
    </author>
    <author>
      <name>Saurabh Verma</name>
    </author>
    <author>
      <name>Aude Hofleitner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to KDD 2020 applied DS track</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.07917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.07917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2103.02758v1</id>
    <updated>2021-03-03T23: 43: 55Z</updated>
    <published>2021-03-03T23: 43: 55Z</published>
    <title>Learning Asynchronous and Sparse Human-Object Interaction in Videos</title>
    <summary>  Human activities can be learned from video. With effective modeling it is
possible to discover not only the action labels but also the temporal
structures of the activities such as the progression of the sub-activities.
Automatically recognizing such structure from raw video signal is a new
capability that promises authentic modeling and successful recognition of
human-object interactions. Toward this goal, we introduce Asynchronous-Sparse
Interaction Graph Networks (ASSIGN), a recurrent graph network that is able to
automatically detect the structure of interaction events associated with
entities in a video scene. ASSIGN pioneers learning of autonomous behavior of
video entities including their dynamic structure and their interaction with the
coexisting neighbors. Entities' lives in our model are asynchronous to those of
others therefore more flexible in adaptation to complex scenarios. Their
interactions are sparse in time hence more faithful to the true underlying
nature and more robust in inference and learning. ASSIGN is tested on
human-object interaction recognition and shows superior performance in
segmenting and labeling of human sub-activities and object affordances from raw
videos. The native ability for discovering temporal structures of the model
also eliminates the dependence on external segmentation that was previously
mandatory for this task.
</summary>
    <author>
      <name>Romero Morais</name>
    </author>
    <author>
      <name>Vuong Le</name>
    </author>
    <author>
      <name>Svetha Venkatesh</name>
    </author>
    <author>
      <name>Truyen Tran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in CVPR'21</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.02758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.02758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2403.11440v1</id>
    <updated>2024-03-18T03: 28: 01Z</updated>
    <published>2024-03-18T03: 28: 01Z</published>
    <title>Boosting Continuous Emotion Recognition with Self-Pretraining using
  Masked Autoencoders, Temporal Convolutional Networks, and Transformers</title>
    <summary>  Human emotion recognition holds a pivotal role in facilitating seamless
human-computer interaction. This paper delineates our methodology in tackling
the Valence-Arousal (VA) Estimation Challenge, Expression (Expr) Classification
Challenge, and Action Unit (AU) Detection Challenge within the ambit of the 6th
Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW). Our
study advocates a novel approach aimed at refining continuous emotion
recognition. We achieve this by initially harnessing pre-training with Masked
Autoencoders (MAE) on facial datasets, followed by fine-tuning on the aff-wild2
dataset annotated with expression (Expr) labels. The pre-trained model serves
as an adept visual feature extractor, thereby enhancing the model's robustness.
Furthermore, we bolster the performance of continuous emotion recognition by
integrating Temporal Convolutional Network (TCN) modules and Transformer
Encoder modules into our framework.
</summary>
    <author>
      <name>Weiwei Zhou</name>
    </author>
    <author>
      <name>Jiada Lu</name>
    </author>
    <author>
      <name>Chenkun Ling</name>
    </author>
    <author>
      <name>Weifeng Wang</name>
    </author>
    <author>
      <name>Shaowei Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2403.11440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.11440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1703.10664v3</id>
    <updated>2017-08-02T04: 52: 02Z</updated>
    <published>2017-03-30T20: 28: 31Z</published>
    <title>Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos</title>
    <summary>  Deep learning has been demonstrated to achieve excellent results for image
classification and object detection. However, the impact of deep learning on
video analysis (e.g. action detection and recognition) has been limited due to
complexity of video data and lack of annotations. Previous convolutional neural
networks (CNN) based video action detection approaches usually consist of two
major steps: frame-level action proposal detection and association of proposals
across frames. Also, these methods employ two-stream CNN framework to handle
spatial and temporal feature separately. In this paper, we propose an
end-to-end deep network called Tube Convolutional Neural Network (T-CNN) for
action detection in videos. The proposed architecture is a unified network that
is able to recognize and localize action based on 3D convolution features. A
video is first divided into equal length clips and for each clip a set of tube
proposals are generated next based on 3D Convolutional Network (ConvNet)
features. Finally, the tube proposals of different clips are linked together
employing network flow and spatio-temporal action detection is performed using
these linked video proposals. Extensive experiments on several video datasets
demonstrate the superior performance of T-CNN for classifying and localizing
actions in both trimmed and untrimmed videos compared to state-of-the-arts.
</summary>
    <author>
      <name>Rui Hou</name>
    </author>
    <author>
      <name>Chen Chen</name>
    </author>
    <author>
      <name>Mubarak Shah</name>
    </author>
    <link href="http://arxiv.org/abs/1703.10664v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.10664v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1410.5861v1</id>
    <updated>2014-10-21T21: 25: 45Z</updated>
    <published>2014-10-21T21: 25: 45Z</published>
    <title>Compositional Structure Learning for Action Understanding</title>
    <summary>  The focus of the action understanding literature has predominately been
classification, how- ever, there are many applications demanding richer action
understanding such as mobile robotics and video search, with solutions to
classification, localization and detection. In this paper, we propose a
compositional model that leverages a new mid-level representation called
compositional trajectories and a locally articulated spatiotemporal deformable
parts model (LALSDPM) for fully action understanding. Our methods is
advantageous in capturing the variable structure of dynamic human activity over
a long range. First, the compositional trajectories capture long-ranging,
frequently co-occurring groups of trajectories in space time and represent them
in discriminative hierarchies, where human motion is largely separated from
camera motion; second, LASTDPM learns a structured model with multi-layer
deformable parts to capture multiple levels of articulated motion. We implement
our methods and demonstrate state of the art performance on all three problems:
action detection, localization, and recognition.
</summary>
    <author>
      <name>Ran Xu</name>
    </author>
    <author>
      <name>Gang Chen</name>
    </author>
    <author>
      <name>Caiming Xiong</name>
    </author>
    <author>
      <name>Wei Chen</name>
    </author>
    <author>
      <name>Jason J. Corso</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.5861v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.5861v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2112.00302v1</id>
    <updated>2021-12-01T06: 36: 59Z</updated>
    <published>2021-12-01T06: 36: 59Z</published>
    <title>Graph Convolutional Module for Temporal Action Localization in Videos</title>
    <summary>  Temporal action localization has long been researched in computer vision.
Existing state-of-the-art action localization methods divide each video into
multiple action units (i.e., proposals in two-stage methods and segments in
one-stage methods) and then perform action recognition/regression on each of
them individually, without explicitly exploiting their relations during
learning. In this paper, we claim that the relations between action units play
an important role in action localization, and a more powerful action detector
should not only capture the local content of each action unit but also allow a
wider field of view on the context related to it. To this end, we propose a
general graph convolutional module (GCM) that can be easily plugged into
existing action localization methods, including two-stage and one-stage
paradigms. To be specific, we first construct a graph, where each action unit
is represented as a node and their relations between two action units as an
edge. Here, we use two types of relations, one for capturing the temporal
connections between different action units, and the other one for
characterizing their semantic relationship. Particularly for the temporal
connections in two-stage methods, we further explore two different kinds of
edges, one connecting the overlapping action units and the other one connecting
surrounding but disjointed units. Upon the graph we built, we then apply graph
convolutional networks (GCNs) to model the relations among different action
units, which is able to learn more informative representations to enhance
action localization. Experimental results show that our GCM consistently
improves the performance of existing action localization methods, including
two-stage methods (e.g., CBR and R-C3D) and one-stage methods (e.g., D-SSAD),
verifying the generality and effectiveness of our GCM.
</summary>
    <author>
      <name>Runhao Zeng</name>
    </author>
    <author>
      <name>Wenbing Huang</name>
    </author>
    <author>
      <name>Mingkui Tan</name>
    </author>
    <author>
      <name>Yu Rong</name>
    </author>
    <author>
      <name>Peilin Zhao</name>
    </author>
    <author>
      <name>Junzhou Huang</name>
    </author>
    <author>
      <name>Chuang Gan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by T-PAMI</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.00302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.00302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2303.17297v2</id>
    <updated>2023-09-16T12: 42: 11Z</updated>
    <published>2023-03-30T11: 16: 58Z</published>
    <title>Understanding the Robustness of 3D Object Detection with Bird's-Eye-View
  Representations in Autonomous Driving</title>
    <summary>  3D object detection is an essential perception task in autonomous driving to
understand the environments. The Bird's-Eye-View (BEV) representations have
significantly improved the performance of 3D detectors with camera inputs on
popular benchmarks. However, there still lacks a systematic understanding of
the robustness of these vision-dependent BEV models, which is closely related
to the safety of autonomous driving systems. In this paper, we evaluate the
natural and adversarial robustness of various representative models under
extensive settings, to fully understand their behaviors influenced by explicit
BEV features compared with those without BEV. In addition to the classic
settings, we propose a 3D consistent patch attack by applying adversarial
patches in the 3D space to guarantee the spatiotemporal consistency, which is
more realistic for the scenario of autonomous driving. With substantial
experiments, we draw several findings: 1) BEV models tend to be more stable
than previous methods under different natural conditions and common corruptions
due to the expressive spatial representations; 2) BEV models are more
vulnerable to adversarial noises, mainly caused by the redundant BEV features;
3) Camera-LiDAR fusion models have superior performance under different
settings with multi-modal inputs, but BEV fusion model is still vulnerable to
adversarial noises of both point cloud and image. These findings alert the
safety issue in the applications of BEV detectors and could facilitate the
development of more robust models.
</summary>
    <author>
      <name>Zijian Zhu</name>
    </author>
    <author>
      <name>Yichi Zhang</name>
    </author>
    <author>
      <name>Hai Chen</name>
    </author>
    <author>
      <name>Yinpeng Dong</name>
    </author>
    <author>
      <name>Shu Zhao</name>
    </author>
    <author>
      <name>Wenbo Ding</name>
    </author>
    <author>
      <name>Jiachen Zhong</name>
    </author>
    <author>
      <name>Shibao Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, CVPR2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.17297v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.17297v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2110.06383v1</id>
    <updated>2021-10-12T22: 09: 29Z</updated>
    <published>2021-10-12T22: 09: 29Z</published>
    <title>Real-time Drift Detection on Time-series Data</title>
    <summary>  Practical machine learning applications involving time series data, such as
firewall log analysis to proactively detect anomalous behavior, are concerned
with real time analysis of streaming data. Consequently, we need to update the
ML models as the statistical characteristics of such data may shift frequently
with time. One alternative explored in the literature is to retrain models with
updated data whenever the models accuracy is observed to degrade. However,
these methods rely on near real time availability of ground truth, which is
rarely fulfilled. Further, in applications with seasonal data, temporal concept
drift is confounded by seasonal variation. In this work, we propose an approach
called Unsupervised Temporal Drift Detector or UTDD to flexibly account for
seasonal variation, efficiently detect temporal concept drift in time series
data in the absence of ground truth, and subsequently adapt our ML models to
concept drift for better generalization.
</summary>
    <author>
      <name>Nandini Ramanan</name>
    </author>
    <author>
      <name>Rasool Tahmasbi</name>
    </author>
    <author>
      <name>Marjorie Sayer</name>
    </author>
    <author>
      <name>Deokwoo Jung</name>
    </author>
    <author>
      <name>Shalini Hemachandran</name>
    </author>
    <author>
      <name>Claudionor Nunes Coelho Jr</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages,
                    5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.06383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.06383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2504.00149v1</id>
    <updated>2025-03-31T18: 57: 57Z</updated>
    <published>2025-03-31T18: 57: 57Z</published>
    <title>Towards Precise Action Spotting: Addressing Temporal Misalignment in
  Labels with Dynamic Label Assignment</title>
    <summary>  Precise action spotting has attracted considerable attention due to its
promising applications. While existing methods achieve substantial performance
by employing well-designed model architecture, they overlook a significant
challenge: the temporal misalignment inherent in ground-truth labels. This
misalignment arises when frames labeled as containing events do not align
accurately with the actual event times, often as a result of human annotation
errors or the inherent difficulties in precisely identifying event boundaries
across neighboring frames. To tackle this issue, we propose a novel dynamic
label assignment strategy that allows predictions to have temporal offsets from
ground-truth action times during training, ensuring consistent event spotting.
Our method extends the concept of minimum-cost matching, which is utilized in
the spatial domain for object detection, to the temporal domain. By calculating
matching costs based on predicted action class scores and temporal offsets, our
method dynamically assigns labels to the most likely predictions, even when the
predicted times of these predictions deviate from ground-truth times,
alleviating the negative effects of temporal misalignment in labels. We conduct
extensive experiments and demonstrate that our method achieves state-of-the-art
performance, particularly in conditions where events are visually distinct and
temporal misalignment in labels is common.
</summary>
    <author>
      <name>Masato Tamura</name>
    </author>
    <link href="http://arxiv.org/abs/2504.00149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.00149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1502.00258v1</id>
    <updated>2015-02-01T13: 49: 31Z</updated>
    <published>2015-02-01T13: 49: 31Z</published>
    <title>Learning Latent Spatio-Temporal Compositional Model for Human Action
  Recognition</title>
    <summary>  Action recognition is an important problem in multimedia understanding. This
paper addresses this problem by building an expressive compositional action
model. We model one action instance in the video with an ensemble of
spatio-temporal compositions: a number of discrete temporal anchor frames, each
of which is further decomposed to a layout of deformable parts. In this way,
our model can identify a Spatio-Temporal And-Or Graph (STAOG) to represent the
latent structure of actions e.g. triple jumping, swinging and high jumping. The
STAOG model comprises four layers: (i) a batch of leaf-nodes in bottom for
detecting various action parts within video patches; (ii) the or-nodes over
bottom, i.e. switch variables to activate their children leaf-nodes for
structural variability; (iii) the and-nodes within an anchor frame for
verifying spatial composition; and (iv) the root-node at top for aggregating
scores over temporal anchor frames. Moreover, the contextual interactions are
defined between leaf-nodes in both spatial and temporal domains. For model
training, we develop a novel weakly supervised learning algorithm which
iteratively determines the structural configuration (e.g. the production of
leaf-nodes associated with the or-nodes) along with the optimization of
multi-layer parameters. By fully exploiting spatio-temporal compositions and
interactions, our approach handles well large intra-class action variance (e.g.
different views, individual appearances, spatio-temporal structures). The
experimental results on the challenging databases demonstrate superior
performance of our approach over other competing methods.
</summary>
    <author>
      <name>Xiaodan Liang</name>
    </author>
    <author>
      <name>Liang Lin</name>
    </author>
    <author>
      <name>Liangliang Cao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2502081.2502089</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2502081.2502089" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This manuscript has 10 pages with 7 figures, and a preliminary
  version was published in ACM MM'13</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.00258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5; I.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2207.11931v1</id>
    <updated>2022-07-25T06: 52: 55Z</updated>
    <published>2022-07-25T06: 52: 55Z</published>
    <title>Hybrid Classifiers for Spatio-temporal Real-time Abnormal Behaviors
  Detection, Tracking, and Recognition in Massive Hajj Crowds</title>
    <summary>  Individual abnormal behaviors vary depending on crowd sizes, contexts, and
scenes. Challenges such as partial occlusions, blurring, large-number abnormal
behavior, and camera viewing occur in large-scale crowds when detecting,
tracking, and recognizing individuals with abnormal behaviors. In this paper,
our contribution is twofold. First, we introduce an annotated and labeled
large-scale crowd abnormal behaviors Hajj dataset (HAJJv2). Second, we propose
two methods of hybrid Convolutional Neural Networks (CNNs) and Random Forests
(RFs) to detect and recognize Spatio-temporal abnormal behaviors in small and
large-scales crowd videos. In small-scale crowd videos, a ResNet-50 pre-trained
CNN model is fine-tuned to verify whether every frame is normal or abnormal in
the spatial domain. If anomalous behaviors are observed, a motion-based
individuals detection method based on the magnitudes and orientations of
Horn-Schunck optical flow is used to locate and track individuals with abnormal
behaviors. A Kalman filter is employed in large-scale crowd videos to predict
and track the detected individuals in the subsequent frames. Then, means,
variances, and standard deviations statistical features are computed and fed to
the RF to classify individuals with abnormal behaviors in the temporal domain.
In large-scale crowds, we fine-tune the ResNet-50 model using YOLOv2 object
detection technique to detect individuals with abnormal behaviors in the
spatial domain.
</summary>
    <author>
      <name>Tarik Alafif</name>
    </author>
    <author>
      <name>Anas Hadi</name>
    </author>
    <author>
      <name>Manal Allahyani</name>
    </author>
    <author>
      <name>Bander Alzahrani</name>
    </author>
    <author>
      <name>Areej Alhothali</name>
    </author>
    <author>
      <name>Reem Alotaibi</name>
    </author>
    <author>
      <name>Ahmed Barnawi</name>
    </author>
    <link href="http://arxiv.org/abs/2207.11931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.11931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1504.04923v1</id>
    <updated>2015-04-20T02: 41: 03Z</updated>
    <published>2015-04-20T02: 41: 03Z</published>
    <title>Learning discriminative trajectorylet detector sets for accurate
  skeleton-based action recognition</title>
    <summary>  The introduction of low-cost RGB-D sensors has promoted the research in
skeleton-based human action recognition. Devising a representation suitable for
characterising actions on the basis of noisy skeleton sequences remains a
challenge, however. We here provide two insights into this challenge. First, we
show that the discriminative information of a skeleton sequence usually resides
in a short temporal interval and we propose a simple-but-effective local
descriptor called trajectorylet to capture the static and kinematic information
within this interval. Second, we further propose to encode each trajectorylet
with a discriminative trajectorylet detector set which is selected from a large
number of candidate detectors trained through exemplar-SVMs. The action-level
representation is obtained by pooling trajectorylet encodings. Evaluating on
standard datasets acquired from the Kinect sensor, it is demonstrated that our
method obtains superior results over existing approaches under various
experimental setups.
</summary>
    <author>
      <name>Ruizhi Qiao</name>
    </author>
    <author>
      <name>Lingqiao Liu</name>
    </author>
    <author>
      <name>Chunhua Shen</name>
    </author>
    <author>
      <name>Anton von den Hengel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.04923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.04923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1703.01515v2</id>
    <updated>2017-06-13T04: 14: 57Z</updated>
    <published>2017-03-04T20: 00: 44Z</published>
    <title>CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action
  Localization in Untrimmed Videos</title>
    <summary>  Temporal action localization is an important yet challenging problem. Given a
long, untrimmed video consisting of multiple action instances and complex
background contents, we need not only to recognize their action categories, but
also to localize the start time and end time of each instance. Many
state-of-the-art systems use segment-level classifiers to select and rank
proposal segments of pre-determined boundaries. However, a desirable model
should move beyond segment-level and make dense predictions at a fine
granularity in time to determine precise temporal boundaries. To this end, we
design a novel Convolutional-De-Convolutional (CDC) network that places CDC
filters on top of 3D ConvNets, which have been shown to be effective for
abstracting action semantics but reduce the temporal length of the input data.
The proposed CDC filter performs the required temporal upsampling and spatial
downsampling operations simultaneously to predict actions at the frame-level
granularity. It is unique in jointly modeling action semantics in space-time
and fine-grained temporal dynamics. We train the CDC network in an end-to-end
manner efficiently. Our model not only achieves superior performance in
detecting actions in every frame, but also significantly boosts the precision
of localizing temporal boundaries. Finally, the CDC network demonstrates a very
high efficiency with the ability to process 500 frames per second on a single
GPU server. We will update the camera-ready version and publish the source
codes online soon.
</summary>
    <author>
      <name>Zheng Shou</name>
    </author>
    <author>
      <name>Jonathan Chan</name>
    </author>
    <author>
      <name>Alireza Zareian</name>
    </author>
    <author>
      <name>Kazuyuki Miyazawa</name>
    </author>
    <author>
      <name>Shih-Fu Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
                    2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.01515v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01515v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1909.07725v1</id>
    <updated>2019-09-17T11: 30: 52Z</updated>
    <published>2019-09-17T11: 30: 52Z</published>
    <title>Deep Point-wise Prediction for Action Temporal Proposal</title>
    <summary>  Detecting actions in videos is an important yet challenging task. Previous
works usually utilize (a) sliding window paradigms, or (b) per-frame action
scoring and grouping to enumerate the possible temporal locations. Their
performances are also limited to the designs of sliding windows or grouping
strategies. In this paper, we present a simple and effective method for
temporal action proposal generation, named Deep Point-wise Prediction (DPP).
DPP simultaneously predicts the action existing possibility and the
corresponding temporal locations, without the utilization of any handcrafted
sliding window or grouping. The whole system is end-to-end trained with joint
loss of temporal action proposal classification and location prediction. We
conduct extensive experiments to verify its effectiveness, generality and
robustness on standard THUMOS14 dataset. DPP runs more than 1000 frames per
second, which largely satisfies the real-time requirement. The code is
available at https: //github.com/liluxuan1997/DPP.
</summary>
    <author>
      <name>Luxuan Li</name>
    </author>
    <author>
      <name>Tao Kong</name>
    </author>
    <author>
      <name>Fuchun Sun</name>
    </author>
    <author>
      <name>Huaping Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by ICONIP2019 oral presentation (International Conference on
  Neural Information Processing)</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.07725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.07725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2010.08365v1</id>
    <updated>2020-10-16T13: 08: 50Z</updated>
    <published>2020-10-16T13: 08: 50Z</published>
    <title>Toward Accurate Person-level Action Recognition in Videos of Crowded
  Scenes</title>
    <summary>  Detecting and recognizing human action in videos with crowded scenes is a
challenging problem due to the complex environment and diversity events. Prior
works always fail to deal with this problem in two aspects: (1) lacking
utilizing information of the scenes; (2) lacking training data in the crowd and
complex scenes. In this paper, we focus on improving spatio-temporal action
recognition by fully-utilizing the information of scenes and collecting new
data. A top-down strategy is used to overcome the limitations. Specifically, we
adopt a strong human detector to detect the spatial location of each frame. We
then apply action recognition models to learn the spatio-temporal information
from video frames on both the HIE dataset and new data with diverse scenes from
the internet, which can improve the generalization ability of our model.
Besides, the scenes information is extracted by the semantic segmentation model
to assistant the process. As a result, our method achieved an average 26.05
wf\_mAP (ranking 1st place in the ACM MM grand challenge 2020: Human in
Events).
</summary>
    <author>
      <name>Li Yuan</name>
    </author>
    <author>
      <name>Yichen Zhou</name>
    </author>
    <author>
      <name>Shuning Chang</name>
    </author>
    <author>
      <name>Ziyuan Huang</name>
    </author>
    <author>
      <name>Yunpeng Chen</name>
    </author>
    <author>
      <name>Xuecheng Nie</name>
    </author>
    <author>
      <name>Tao Wang</name>
    </author>
    <author>
      <name>Jiashi Feng</name>
    </author>
    <author>
      <name>Shuicheng Yan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3394171.3416301</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3394171.3416301" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1'st Place in ACM Multimedia Grand Challenge: Human in Events,
  Track4: Person-level Action Recognition in Complex Events</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Multimedia 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.08365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.08365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2203.03677v4</id>
    <updated>2023-05-19T09: 21: 10Z</updated>
    <published>2022-03-07T19: 28: 39Z</published>
    <title>Object-centric and memory-guided normality reconstruction for video
  anomaly detection</title>
    <summary>  This paper addresses video anomaly detection problem for videosurveillance.
Due to the inherent rarity and heterogeneity of abnormal events, the problem is
viewed as a normality modeling strategy, in which our model learns
object-centric normal patterns without seeing anomalous samples during
training. The main contributions consist in coupling pretrained object-level
action features prototypes with a cosine distance-based anomaly estimation
function, therefore extending previous methods by introducing additional
constraints to the mainstream reconstruction-based strategy. Our framework
leverages both appearance and motion information to learn object-level behavior
and captures prototypical patterns within a memory module. Experiments on
several well-known datasets demonstrate the effectiveness of our method as it
outperforms current state-of-the-art on most relevant spatio-temporal
evaluation metrics.
</summary>
    <author>
      <name>Khalil Bergaoui</name>
    </author>
    <author>
      <name>Yassine Naji</name>
    </author>
    <author>
      <name>Aleksandr Setkov</name>
    </author>
    <author>
      <name>Angélique Loesch</name>
    </author>
    <author>
      <name>Michèle Gouiffès</name>
    </author>
    <author>
      <name>Romaric Audigier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ICIP 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.03677v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.03677v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2411.15354v1</id>
    <updated>2024-11-22T21: 40: 19Z</updated>
    <published>2024-11-22T21: 40: 19Z</published>
    <title>Unified Semantic Log Parsing and Causal Graph Construction for Attack
  Attribution</title>
    <summary>  Multi-source logs provide a comprehensive overview of ongoing system
activities, allowing for in-depth analysis to detect potential threats. A
practical approach for threat detection involves explicit extraction of entity
triples (subject, action, object) towards building provenance graphs to
facilitate the analysis of system behavior. However, current log parsing
methods mainly focus on retrieving parameters and events from raw logs while
approaches based on entity extraction are limited to processing a single type
of log. To address these gaps, we contribute with a novel unified framework,
coined UTLParser. UTLParser adopts semantic analysis to construct causal graphs
by merging multiple sub-graphs from individual log sources in labeled log
dataset. It leverages domain knowledge in threat hunting such as Points of
Interest. We further explore log generation delays and provide interfaces for
optimized temporal graph querying. Our experiments showcase that UTLParser
overcomes drawbacks of other log parsing methods. Furthermore, UTLParser
precisely extracts explicit causal threat information while being compatible
with enormous downstream tasks.
</summary>
    <author>
      <name>Zhuoran Tan</name>
    </author>
    <author>
      <name>Christos Anagnostopoulos</name>
    </author>
    <author>
      <name>Shameem P. Parambath</name>
    </author>
    <author>
      <name>Jeremy Singer</name>
    </author>
    <link href="http://arxiv.org/abs/2411.15354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.15354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2007.10703v1</id>
    <updated>2020-07-21T10: 45: 05Z</updated>
    <published>2020-07-21T10: 45: 05Z</published>
    <title>Uncertainty-Aware Weakly Supervised Action Detection from Untrimmed
  Videos</title>
    <summary>  Despite the recent advances in video classification, progress in
spatio-temporal action recognition has lagged behind. A major contributing
factor has been the prohibitive cost of annotating videos frame-by-frame. In
this paper, we present a spatio-temporal action recognition model that is
trained with only video-level labels, which are significantly easier to
annotate. Our method leverages per-frame person detectors which have been
trained on large image datasets within a Multiple Instance Learning framework.
We show how we can apply our method in cases where the standard Multiple
Instance Learning assumption, that each bag contains at least one instance with
the specified label, is invalid using a novel probabilistic variant of MIL
where we estimate the uncertainty of each prediction. Furthermore, we report
the first weakly-supervised results on the AVA dataset and state-of-the-art
results among weakly-supervised methods on UCF101-24.
</summary>
    <author>
      <name>Anurag Arnab</name>
    </author>
    <author>
      <name>Chen Sun</name>
    </author>
    <author>
      <name>Arsha Nagrani</name>
    </author>
    <author>
      <name>Cordelia Schmid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.10703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.10703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2405.02961v2</id>
    <updated>2024-08-03T18: 49: 02Z</updated>
    <published>2024-05-05T15: 01: 00Z</published>
    <title>JOSENet: A Joint Stream Embedding Network for Violence Detection in
  Surveillance Videos</title>
    <summary>  The increasing proliferation of video surveillance cameras and the escalating
demand for crime prevention have intensified interest in the task of violence
detection within the research community. Compared to other action recognition
tasks, violence detection in surveillance videos presents additional issues,
such as the wide variety of real fight scenes. Unfortunately, existing datasets
for violence detection are relatively small in comparison to those for other
action recognition tasks. Moreover, surveillance footage often features
different individuals in each video and varying backgrounds for each camera. In
addition, fast detection of violent actions in real-life surveillance videos is
crucial to prevent adverse outcomes, thus necessitating models that are
optimized for reduced memory usage and computational costs. These challenges
complicate the application of traditional action recognition methods. To tackle
all these issues, we introduce JOSENet, a novel self-supervised framework that
provides outstanding performance for violence detection in surveillance videos.
The proposed model processes two spatiotemporal video streams, namely RGB
frames and optical flows, and incorporates a new regularized self-supervised
learning approach for videos. JOSENet demonstrates improved performance
compared to state-of-the-art methods, while utilizing only one-fourth of the
frames per video segment and operating at a reduced frame rate. The source code
is available at https: //github.com/ispamm/JOSENet.
</summary>
    <author>
      <name>Pietro Nardelli</name>
    </author>
    <author>
      <name>Danilo Comminiello</name>
    </author>
    <link href="http://arxiv.org/abs/2405.02961v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.02961v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2210.11219v1</id>
    <updated>2022-10-20T12: 51: 39Z</updated>
    <published>2022-10-20T12: 51: 39Z</published>
    <title>YOWO-Plus: An Incremental Improvement</title>
    <summary>  In this technical report, we would like to introduce our updates to YOWO, a
real-time method for spatio-temporal action detection. We make a bunch of
little design changes to make it better. For network structure, we use the same
ones of official implemented YOWO, including 3D-ResNext-101 and YOLOv2, but we
use a better pretrained weight of our reimplemented YOLOv2, which is better
than the official YOLOv2. We also optimize the label assignment used in YOWO.
To accurately detection action instances, we deploy GIoU loss for box
regression. After our incremental improvement, YOWO achieves 84.9\% frame mAP
and 50.5\% video mAP on the UCF101-24, significantly higher than the official
YOWO. On the AVA, our optimized YOWO achieves 20.6\% frame mAP with 16 frames,
also exceeding the official YOWO. With 32 frames, our YOWO achieves 21.6 frame
mAP with 25 FPS on an RTX 3090 GPU. We name the optimized YOWO as YOWO-Plus.
Moreover, we replace the 3D-ResNext-101 with the efficient 3D-ShuffleNet-v2 to
design a lightweight action detector, YOWO-Nano. YOWO-Nano achieves 81.0 \%
frame mAP and 49.7\% video frame mAP with over 90 FPS on the UCF101-24. It also
achieves 18.4 \% frame mAP with about 90 FPS on the AVA. As far as we know,
YOWO-Nano is the fastest state-of-the-art action detector. Our code is
available on https: //github.com/yjh0410/PyTorch_YOWO.
</summary>
    <author>
      <name>Jianhua Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages,
                    1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.11219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.11219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1912.04461v3</id>
    <updated>2020-03-31T03: 48: 06Z</updated>
    <published>2019-12-10T02: 45: 37Z</published>
    <title>Learning to Discriminate Information for Online Action Detection</title>
    <summary>  From a streaming video, online action detection aims to identify actions in
the present. For this task, previous methods use recurrent networks to model
the temporal sequence of current action frames. However, these methods overlook
the fact that an input image sequence includes background and irrelevant
actions as well as the action of interest. For online action detection, in this
paper, we propose a novel recurrent unit to explicitly discriminate the
information relevant to an ongoing action from others. Our unit, named
Information Discrimination Unit (IDU), decides whether to accumulate input
information based on its relevance to the current action. This enables our
recurrent network with IDU to learn a more discriminative representation for
identifying ongoing actions. In experiments on two benchmark datasets, TVSeries
and THUMOS-14, the proposed method outperforms state-of-the-art methods by a
significant margin. Moreover, we demonstrate the effectiveness of our recurrent
unit by conducting comprehensive ablation studies.
</summary>
    <author>
      <name>Hyunjun Eun</name>
    </author>
    <author>
      <name>Jinyoung Moon</name>
    </author>
    <author>
      <name>Jongyoul Park</name>
    </author>
    <author>
      <name>Chanho Jung</name>
    </author>
    <author>
      <name>Changick Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in CVPR 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.04461v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.04461v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2011.07915v1</id>
    <updated>2020-11-16T13: 08: 47Z</updated>
    <published>2020-11-16T13: 08: 47Z</published>
    <title>LAP-Net: Adaptive Features Sampling via Learning Action Progression for
  Online Action Detection</title>
    <summary>  Online action detection is a task with the aim of identifying ongoing actions
from streaming videos without any side information or access to future frames.
Recent methods proposed to aggregate fixed temporal ranges of invisible but
anticipated future frames representations as supplementary features and
achieved promising performance. They are based on the observation that human
beings often detect ongoing actions by contemplating the future vision
simultaneously. However, we observed that at different action progressions, the
optimal supplementary features should be obtained from distinct temporal ranges
instead of simply fixed future temporal ranges. To this end, we introduce an
adaptive features sampling strategy to overcome the mentioned variable-ranges
of optimal supplementary features. Specifically, in this paper, we propose a
novel Learning Action Progression Network termed LAP-Net, which integrates an
adaptive features sampling strategy. At each time step, this sampling strategy
first estimates current action progression and then decide what temporal ranges
should be used to aggregate the optimal supplementary features. We evaluated
our LAP-Net on three benchmark datasets, TVSeries, THUMOS-14 and HDD. The
extensive experiments demonstrate that with our adaptive feature sampling
strategy, the proposed LAP-Net can significantly outperform current
state-of-the-art methods with a large margin.
</summary>
    <author>
      <name>Sanqing Qu</name>
    </author>
    <author>
      <name>Guang Chen</name>
    </author>
    <author>
      <name>Dan Xu</name>
    </author>
    <author>
      <name>Jinhu Dong</name>
    </author>
    <author>
      <name>Fan Lu</name>
    </author>
    <author>
      <name>Alois Knoll</name>
    </author>
    <link href="http://arxiv.org/abs/2011.07915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.07915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2407.18849v1</id>
    <updated>2024-07-26T16: 17: 53Z</updated>
    <published>2024-07-26T16: 17: 53Z</published>
    <title>MNTD: An Efficient Dynamic Community Detector Based on Nonnegative
  Tensor Decomposition</title>
    <summary>  Dynamic community detection is crucial for elucidating the temporal evolution
of social structures, information dissemination, and interactive behaviors
within complex networks. Nonnegative matrix factorization provides an efficient
framework for identifying communities in static networks but fall short in
depicting temporal variations in community affiliations. To solve this problem,
this paper proposes a Modularity maximization-incorporated Nonnegative Tensor
RESCAL Decomposition (MNTD) model for dynamic community detection. This method
serves two primary functions: a) Nonnegative tensor RESCAL decomposition
extracts latent community structures in different time slots, highlighting the
persistence and transformation of communities; and b) Incorporating an initial
community structure into the modularity maximization algorithm, facilitating
more precise community segmentations. Comparative analysis of real-world
datasets shows that the MNTD is superior to state-of-the-art dynamic community
detection methods in the accuracy of community detection.
</summary>
    <author>
      <name>Hao Fang</name>
    </author>
    <author>
      <name>Qu Wang</name>
    </author>
    <author>
      <name>Qicong Hu</name>
    </author>
    <author>
      <name>Hao Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,
                    5 figures,This paper will be published on 2024 IEEE
  International Conference on Systems, Man, and Cybernetics(SMC)</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.18849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.18849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2410.07161v1</id>
    <updated>2024-10-09T17: 57: 06Z</updated>
    <published>2024-10-09T17: 57: 06Z</published>
    <title>Spatiotemporal Modeling and Forecasting at Scale with Dynamic
  Generalized Linear Models</title>
    <summary>  Spatiotemporal data consisting of timestamps, GPS coordinates, and IDs occurs
in many settings. Modeling approaches for this type of data must address
challenges in terms of sensor noise, uneven sampling rates, and non-persistent
IDs. In this work, we characterize and forecast human mobility at scale with
dynamic generalized linear models (DGLMs). We represent mobility data as
occupancy counts of spatial cells over time and use DGLMs to model the
occupancy counts for each spatial cell in an area of interest. DGLMs are
flexible to varying numbers of occupancy counts across spatial cells, are
dynamic, and easily incorporate daily and weekly seasonality in the
aggregate-level behavior. Our overall approach is robust to various types of
noise and scales linearly in the number of spatial cells, time bins, and
agents. Our results show that DGLMs provide accurate occupancy count forecasts
over a variety of spatial resolutions and forecast horizons. We also present
scaling results for spatiotemporal data consisting of hundreds of millions of
observations. Our approach is flexible to support several downstream
applications, including characterizing human mobility, forecasting occupancy
counts, and anomaly detection for aggregate-level behaviors.
</summary>
    <author>
      <name>Pranay Pherwani</name>
    </author>
    <author>
      <name>Nicholas Hass</name>
    </author>
    <author>
      <name>Anna K. Yanchenko</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3681765.3698449</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3681765.3698449" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages,
                    12 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In 1st ACM SIGSPATIAL International Workshop on Geospatial Anomaly
  Detection (GeoAnomalies'24), October 29,
                    2024, Atlanta, GA, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2410.07161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.07161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1612.07736v2</id>
    <updated>2018-02-13T21: 11: 58Z</updated>
    <published>2016-12-22T18: 18: 39Z</published>
    <title>Collaborative Alerts Ranking for Anomaly Detection</title>
    <summary>  Given a large number of low-level heterogeneous categorical alerts from an
anomaly detection system, how to characterize complex relationships between
different alerts, filter out false positives, and deliver trustworthy rankings
and suggestions to end users? This problem is motivated by and generalized from
applications in enterprise security and attack scenario reconstruction. While
existing techniques focus on either reconstructing abnormal scenarios or
filtering out false positive alerts, it can be more advantageous to consider
the two perspectives simultaneously in order to improve detection accuracy and
better understand anomaly behaviors. In this paper, we propose CAR, a
collaborative alerts ranking framework that exploits both temporal and content
correlations from heterogeneous categorical alerts. CAR first builds a
tree-based model to capture both short-term correlations and long-term
dependencies in each alert sequence, which identifies abnormal action
sequences. Then, an embedding-based model is employed to learn the content
correlations between alerts via their heterogeneous categorical attributes.
Finally, by incorporating both temporal and content dependencies into one
optimization framework, CAR ranks both alerts and their corresponding alert
patterns. Our experiments, using real-world enterprise monitoring data and real
attacks launched by professional hackers, show that CAR can accurately identify
true positive alerts and successfully reconstruct attack scenarios at the same
time.
</summary>
    <author>
      <name>Ying Lin</name>
    </author>
    <author>
      <name>Zhengzhang Chen</name>
    </author>
    <author>
      <name>Cheng Cao</name>
    </author>
    <author>
      <name>Lu-an Tang</name>
    </author>
    <author>
      <name>Kai Zhang</name>
    </author>
    <author>
      <name>Zhichun Li</name>
    </author>
    <author>
      <name>Haifeng Chen</name>
    </author>
    <author>
      <name>Guofei Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">We want to withdraw the paper due to some error in it. We don't want
  to confuse the readers</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.07736v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.07736v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.22405v2</id>
    <updated>2025-04-02T04: 50: 12Z</updated>
    <published>2025-03-28T13: 16: 02Z</published>
    <title>Modeling Multiple Normal Action Representations for Error Detection in
  Procedural Tasks</title>
    <summary>  Error detection in procedural activities is essential for consistent and
correct outcomes in AR-assisted and robotic systems. Existing methods often
focus on temporal ordering errors or rely on static prototypes to represent
normal actions. However, these approaches typically overlook the common
scenario where multiple, distinct actions are valid following a given sequence
of executed actions. This leads to two issues: (1) the model cannot effectively
detect errors using static prototypes when the inference environment or action
execution distribution differs from training; and (2) the model may also use
the wrong prototypes to detect errors if the ongoing action label is not the
same as the predicted one. To address this problem, we propose an Adaptive
Multiple Normal Action Representation (AMNAR) framework. AMNAR predicts all
valid next actions and reconstructs their corresponding normal action
representations, which are compared against the ongoing action to detect
errors. Extensive experiments demonstrate that AMNAR achieves state-of-the-art
performance, highlighting the effectiveness of AMNAR and the importance of
modeling multiple valid next actions in error detection. The code is available
at https: //github.com/iSEE-Laboratory/AMNAR.
</summary>
    <author>
      <name>Wei-Jin Huang</name>
    </author>
    <author>
      <name>Yuan-Ming Li</name>
    </author>
    <author>
      <name>Zhi-Wei Xia</name>
    </author>
    <author>
      <name>Yu-Ming Tang</name>
    </author>
    <author>
      <name>Kun-Yu Lin</name>
    </author>
    <author>
      <name>Jian-Fang Hu</name>
    </author>
    <author>
      <name>Wei-Shi Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/2503.22405v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.22405v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2207.10448v1</id>
    <updated>2022-07-21T12: 38: 05Z</updated>
    <published>2022-07-21T12: 38: 05Z</published>
    <title>An Efficient Spatio-Temporal Pyramid Transformer for Action Detection</title>
    <summary>  The task of action detection aims at deducing both the action category and
localization of the start and end moment for each action instance in a long,
untrimmed video. While vision Transformers have driven the recent advances in
video understanding, it is non-trivial to design an efficient architecture for
action detection due to the prohibitively expensive self-attentions over a long
sequence of video clips. To this end, we present an efficient hierarchical
Spatio-Temporal Pyramid Transformer (STPT) for action detection, building upon
the fact that the early self-attention layers in Transformers still focus on
local patterns. Specifically, we propose to use local window attention to
encode rich local spatio-temporal representations in the early stages while
applying global attention modules to capture long-term space-time dependencies
in the later stages. In this way, our STPT can encode both locality and
dependency with largely reduced redundancy, delivering a promising trade-off
between accuracy and efficiency. For example, with only RGB input, the proposed
STPT achieves 53.6% mAP on THUMOS14, surpassing I3D+AFSD RGB model by over 10%
and performing favorably against state-of-the-art AFSD that uses additional
flow features with 31% fewer GFLOPs, which serves as an effective and efficient
end-to-end Transformer-based framework for action detection.
</summary>
    <author>
      <name>Yuetian Weng</name>
    </author>
    <author>
      <name>Zizheng Pan</name>
    </author>
    <author>
      <name>Mingfei Han</name>
    </author>
    <author>
      <name>Xiaojun Chang</name>
    </author>
    <author>
      <name>Bohan Zhuang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ECCV 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.10448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.10448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2205.10450v2</id>
    <updated>2022-07-11T19: 30: 44Z</updated>
    <published>2022-05-20T22: 14: 02Z</published>
    <title>Temporally Precise Action Spotting in Soccer Videos Using Dense
  Detection Anchors</title>
    <summary>  We present a model for temporally precise action spotting in videos, which
uses a dense set of detection anchors, predicting a detection confidence and
corresponding fine-grained temporal displacement for each anchor. We experiment
with two trunk architectures, both of which are able to incorporate large
temporal contexts while preserving the smaller-scale features required for
precise localization: a one-dimensional version of a u-net, and a Transformer
encoder (TE). We also suggest best practices for training models of this kind,
by applying Sharpness-Aware Minimization (SAM) and mixup data augmentation. We
achieve a new state-of-the-art on SoccerNet-v2, the largest soccer video
dataset of its kind, with marked improvements in temporal localization.
Additionally, our ablations show: the importance of predicting the temporal
displacements; the trade-offs between the u-net and TE trunks; and the benefits
of training with SAM and mixup.
</summary>
    <author>
      <name>João V. B. Soares</name>
    </author>
    <author>
      <name>Avijit Shah</name>
    </author>
    <author>
      <name>Topojoy Biswas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in International Conference on Image Processing (ICIP),
                    2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.10450v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.10450v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2410.01281v1</id>
    <updated>2024-10-02T06: 57: 08Z</updated>
    <published>2024-10-02T06: 57: 08Z</published>
    <title>Uncertainty-aware Human Mobility Modeling and Anomaly Detection</title>
    <summary>  Given the GPS coordinates of a large collection of human agents over time,
how can we model their mobility behavior toward effective anomaly detection
(e.g. for bad-actor or malicious behavior detection) without any labeled data?
Human mobility and trajectory modeling have been studied extensively with
varying capacity to handle complex input, and performance-efficiency
trade-offs. With the arrival of more expressive models in machine learning, we
attempt to model GPS data as a sequence of stay-point events, each with a set
of characterizing spatiotemporal features, and leverage modern sequence models
such as Transformers for un/self-supervised training and inference. Notably,
driven by the inherent stochasticity of certain individuals' behavior, we equip
our model with aleatoric/data uncertainty estimation. In addition, to handle
data sparsity of a large variety of behaviors, we incorporate epistemic/model
uncertainty into our model. Together, aleatoric and epistemic uncertainty
enable a robust loss and training dynamics, as well as uncertainty-aware
decision making in anomaly scoring. Experiments on large expert-simulated
datasets with tens of thousands of agents demonstrate the effectiveness of our
model against both forecasting and anomaly detection baselines.
</summary>
    <author>
      <name>Haomin Wen</name>
    </author>
    <author>
      <name>Shurui Cao</name>
    </author>
    <author>
      <name>Leman Akoglu</name>
    </author>
    <link href="http://arxiv.org/abs/2410.01281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.01281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2409.04205v1</id>
    <updated>2024-09-06T11: 52: 42Z</updated>
    <published>2024-09-06T11: 52: 42Z</published>
    <title>Introducing Gating and Context into Temporal Action Detection</title>
    <summary>  Temporal Action Detection (TAD), the task of localizing and classifying
actions in untrimmed video, remains challenging due to action overlaps and
variable action durations. Recent findings suggest that TAD performance is
dependent on the structural design of transformers rather than on the
self-attention mechanism. Building on this insight, we propose a refined
feature extraction process through lightweight, yet effective operations.
First, we employ a local branch that employs parallel convolutions with varying
window sizes to capture both fine-grained and coarse-grained temporal features.
This branch incorporates a gating mechanism to select the most relevant
features. Second, we introduce a context branch that uses boundary frames as
key-value pairs to analyze their relationship with the central frame through
cross-attention. The proposed method captures temporal dependencies and
improves contextual understanding. Evaluations of the gating mechanism and
context branch on challenging datasets (THUMOS14 and EPIC-KITCHEN 100) show a
consistent improvement over the baseline and existing methods.
</summary>
    <author>
      <name>Aglind Reka</name>
    </author>
    <author>
      <name>Diana Laura Borza</name>
    </author>
    <author>
      <name>Dominick Reilly</name>
    </author>
    <author>
      <name>Michal Balazia</name>
    </author>
    <author>
      <name>Francois Bremond</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at the ECCV 2024 ABAW Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.04205v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.04205v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1502.01228v6</id>
    <updated>2015-12-28T07: 40: 11Z</updated>
    <published>2015-02-04T15: 13: 04Z</published>
    <title>Linear-time Online Action Detection From 3D Skeletal Data Using Bags of
  Gesturelets</title>
    <summary>  Sliding window is one direct way to extend a successful recognition system to
handle the more challenging detection problem. While action recognition decides
only whether or not an action is present in a pre-segmented video sequence,
action detection identifies the time interval where the action occurred in an
unsegmented video stream. Sliding window approaches for action detection can
however be slow as they maximize a classifier score over all possible
sub-intervals. Even though new schemes utilize dynamic programming to speed up
the search for the optimal sub-interval, they require offline processing on the
whole video sequence. In this paper, we propose a novel approach for online
action detection based on 3D skeleton sequences extracted from depth data. It
identifies the sub-interval with the maximum classifier score in linear time.
Furthermore, it is invariant to temporal scale variations and is suitable for
real-time applications with low latency.
</summary>
    <author>
      <name>Moustafa Meshry</name>
    </author>
    <author>
      <name>Mohamed E. Hussein</name>
    </author>
    <author>
      <name>Marwan Torki</name>
    </author>
    <link href="http://arxiv.org/abs/1502.01228v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.01228v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2402.18319v1</id>
    <updated>2024-02-28T13: 29: 28Z</updated>
    <published>2024-02-28T13: 29: 28Z</published>
    <title>A Multimodal Handover Failure Detection Dataset and Baselines</title>
    <summary>  An object handover between a robot and a human is a coordinated action which
is prone to failure for reasons such as miscommunication, incorrect actions and
unexpected object properties. Existing works on handover failure detection and
prevention focus on preventing failures due to object slip or external
disturbances. However, there is a lack of datasets and evaluation methods that
consider unpreventable failures caused by the human participant. To address
this deficit, we present the multimodal Handover Failure Detection dataset,
which consists of failures induced by the human participant, such as ignoring
the robot or not releasing the object. We also present two baseline methods for
handover failure detection: (i) a video classification method using 3D CNNs and
(ii) a temporal action segmentation approach which jointly classifies the human
action, robot action and overall outcome of the action. The results show that
video is an important modality, but using force-torque data and gripper
position help improve failure detection and action segmentation accuracy.
</summary>
    <author>
      <name>Santosh Thoduka</name>
    </author>
    <author>
      <name>Nico Hochgeschwender</name>
    </author>
    <author>
      <name>Juergen Gall</name>
    </author>
    <author>
      <name>Paul G. Plöger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICRA57147.2024.10610143</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICRA57147.2024.10610143" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ICRA 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.18319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.18319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2201.00354v2</id>
    <updated>2022-10-28T20: 25: 54Z</updated>
    <published>2022-01-02T13: 09: 40Z</published>
    <title>Toward Causal-Aware RL: State-Wise Action-Refined Temporal Difference</title>
    <summary>  Although it is well known that exploration plays a key role in Reinforcement
Learning (RL), prevailing exploration strategies for continuous control tasks
in RL are mainly based on naive isotropic Gaussian noise regardless of the
causality relationship between action space and the task and consider all
dimensions of actions equally important. In this work, we propose to conduct
interventions on the primal action space to discover the causal relationship
between the action space and the task reward. We propose the method of
State-Wise Action Refined (SWAR), which addresses the issue of action space
redundancy and promote causality discovery in RL. We formulate causality
discovery in RL tasks as a state-dependent action space selection problem and
propose two practical algorithms as solutions. The first approach, TD-SWAR,
detects task-related actions during temporal difference learning, while the
second approach, Dyn-SWAR, reveals important actions through dynamic model
prediction. Empirically, both methods provide approaches to understand the
decisions made by RL agents and improve learning efficiency in action-redundant
tasks.
</summary>
    <author>
      <name>Hao Sun</name>
    </author>
    <author>
      <name>Taiyi Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2201.00354v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.00354v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1905.10608v2</id>
    <updated>2019-05-29T17: 13: 46Z</updated>
    <published>2019-05-25T14: 55: 48Z</published>
    <title>Exploring Feature Representation and Training strategies in Temporal
  Action Localization</title>
    <summary>  Temporal action localization has recently attracted significant interest in
the Computer Vision community. However, despite the great progress, it is hard
to identify which aspects of the proposed methods contribute most to the
increase in localization performance. To address this issue, we conduct
ablative experiments on feature extraction methods, fixed-size feature
representation methods and training strategies, and report how each influences
the overall performance. Based on our findings, we propose a two-stage detector
that outperforms the state of the art in THUMOS14, achieving a mAP@tIoU=0.5
equal to 44.2%.
</summary>
    <author>
      <name>Tingting Xie</name>
    </author>
    <author>
      <name>Xiaoshan Yang</name>
    </author>
    <author>
      <name>Tianzhu Zhang</name>
    </author>
    <author>
      <name>Changsheng Xu</name>
    </author>
    <author>
      <name>Ioannis Patras</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICIP19 Camera Ready</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.10608v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.10608v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2407.13053v1</id>
    <updated>2024-05-24T10: 17: 43Z</updated>
    <published>2024-05-24T10: 17: 43Z</published>
    <title>E2Vec: Feature Embedding with Temporal Information for Analyzing Student
  Actions in E-Book Systems</title>
    <summary>  Digital textbook (e-book) systems record student interactions with textbooks
as a sequence of events called EventStream data. In the past, researchers
extracted meaningful features from EventStream, and utilized them as inputs for
downstream tasks such as grade prediction and modeling of student behavior.
Previous research evaluated models that mainly used statistical-based features
derived from EventStream logs, such as the number of operation types or access
frequencies. While these features are useful for providing certain insights,
they lack temporal information that captures fine-grained differences in
learning behaviors among different students. This study proposes E2Vec, a novel
feature representation method based on word embeddings. The proposed method
regards operation logs and their time intervals for each student as a string
sequence of characters and generates a student vector of learning activity
features that incorporates time information. We applied fastText to generate an
embedding vector for each of 305 students in a dataset from two years of
computer science courses. Then, we investigated the effectiveness of E2Vec in
an at-risk detection task, demonstrating potential for generalizability and
performance.
</summary>
    <author>
      <name>Yuma Miyazaki</name>
    </author>
    <author>
      <name>Valdemar Švábenský</name>
    </author>
    <author>
      <name>Yuta Taniguchi</name>
    </author>
    <author>
      <name>Fumiya Okubo</name>
    </author>
    <author>
      <name>Tsubasa Minematsu</name>
    </author>
    <author>
      <name>Atsushi Shimada</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in proceedings of the 17th Educational Data Mining
  Conference (EDM 2024)</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.13053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.13053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1911.11450v3</id>
    <updated>2019-12-10T09: 27: 45Z</updated>
    <published>2019-11-26T10: 55: 25Z</published>
    <title>Expanding Core-Collapse Supernova Search Horizon of Neutrino Detectors</title>
    <summary>  Core-Collapse Supernovae, failed supernovae and quark novae are expected to
release an energy of few $10^{
                        53
                    }$ ergs through MeV neutrinos and a network of
detectors is operative to look online for these events. However, when the
source distance increases and/or the average energy of emitted neutrinos
decreases, the signal statistics drops and the identification of these low
statistic astrophysical bursts could be challenging. In a standard search,
neutrino detectors characterise the observed clusters of events with a
parameter called multiplicity, i.e. the number of collected events in a fixed
time-window. We discuss a new parameter called $\xi$ (=multiplicity/duration of
the cluster) in order to add the information on the temporal behaviour of the
expected signal with respect to background. By adding this parameter to the
multiplicity we optimise the search of astrophysical bursts and we increase
their detection horizon. Moreover, the use of the $\xi$ can be easily
implemented in an online system and can apply also to a network of detectors
like SNEWS. For these reasons this work is relevant in the multi-messengers era
when fast alerts with high significance are mandatory.
</summary>
    <author>
      <name>Odysse Halim</name>
    </author>
    <author>
      <name>Carlo Vigorito</name>
    </author>
    <author>
      <name>Claudio Casentini</name>
    </author>
    <author>
      <name>Giulia Pagliaroli</name>
    </author>
    <author>
      <name>Marco Drago</name>
    </author>
    <author>
      <name>Viviana Fafone</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1742-6596/1468/1/012154</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1742-6596/1468/1/012154" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages,
                    2 figures. this contribution was accepted by IOP Conference
  Series for the conference: TAUP2019, Toyama, Japan.
  http: //taup2019.icrr.u-tokyo.ac.jp/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Physics: Conference Series Volume 1468 (2020) 012154</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.11450v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.11450v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2011.12311v3</id>
    <updated>2021-03-14T12: 24: 02Z</updated>
    <published>2020-11-24T19: 00: 05Z</published>
    <title>TRAP: A temporal systematics model for improved direct detection of
  exoplanets at small angular separations</title>
    <summary>  High-contrast imaging surveys for exoplanet detection have shown giant
planets at large separations to be rare. It is important to push towards
detections at smaller separations, the part of the parameter space containing
most planets. The performance of traditional methods for post-processing of
pupil-stabilized observations decreases at smaller separations, due to the
larger field-rotation required to displace a source on the detector in addition
to the intrinsic difficulty of higher stellar contamination. We developed a
method of extracting exoplanet signals that improves performance at small
angular separations. A data-driven model of the temporal behavior of the
systematics for each pixel can be created using reference pixels at a different
position, assuming the underlying causes of the systematics are shared across
multiple pixels. This is mostly true for the speckle pattern in high-contrast
imaging. In our causal regression model, we simultaneously fit the model of a
planet signal "transiting" over detector pixels and non-local reference
lightcurves describing a basis of shared temporal trends of the speckle pattern
to find the best fitting temporal model describing the signal. With our
implementation of a spatially non-local, temporal systematics model, called
TRAP, we show that it is possible to gain up to a factor of 6 in contrast at
close separations ($&lt;3\lambda/D$) compared to a model based on spatial
correlations between images displaced in time. We show that better temporal
sampling resulting in significantly better contrasts. At short integration
times for $\beta$ Pic data, we increase the SNR of the planet by a factor of 4
compared to the spatial systematics model. Finally, we show that the temporal
model can be used on unaligned data which has only been dark and flat
corrected, without the need for further pre-processing.
</summary>
    <author>
      <name>M. Samland</name>
    </author>
    <author>
      <name>J. Bouwman</name>
    </author>
    <author>
      <name>D. W. Hogg</name>
    </author>
    <author>
      <name>W. Brandner</name>
    </author>
    <author>
      <name>T. Henning</name>
    </author>
    <author>
      <name>M. Janson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1051/0004-6361/201937308</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1051/0004-6361/201937308" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has 21 pages of which 17 are the main body and 4 pages are
  appendix. 16 main figures and 4 figures in appendix</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">A&amp;A 646, A24 (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2011.12311v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.12311v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2403.13731v2</id>
    <updated>2024-03-23T06: 31: 11Z</updated>
    <published>2024-03-19T12: 26: 53Z</published>
    <title>Emotion Recognition Using Transformers with Masked Learning</title>
    <summary>  In recent years, deep learning has achieved innovative advancements in
various fields, including the analysis of human emotions and behaviors.
Initiatives such as the Affective Behavior Analysis in-the-wild (ABAW)
competition have been particularly instrumental in driving research in this
area by providing diverse and challenging datasets that enable precise
evaluation of complex emotional states. This study leverages the Vision
Transformer (ViT) and Transformer models to focus on the estimation of
Valence-Arousal (VA), which signifies the positivity and intensity of emotions,
recognition of various facial expressions, and detection of Action Units (AU)
representing fundamental muscle movements. This approach transcends traditional
Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) based
methods, proposing a new Transformer-based framework that maximizes the
understanding of temporal and spatial features. The core contributions of this
research include the introduction of a learning technique through random frame
masking and the application of Focal loss adapted for imbalanced data,
enhancing the accuracy and applicability of emotion and behavior analysis in
real-world settings. This approach is expected to contribute to the advancement
of emotional computing and deep learning methodologies.
</summary>
    <author>
      <name>Seongjae Min</name>
    </author>
    <author>
      <name>Junseok Yang</name>
    </author>
    <author>
      <name>Sangjun Lim</name>
    </author>
    <author>
      <name>Junyong Lee</name>
    </author>
    <author>
      <name>Sangwon Lee</name>
    </author>
    <author>
      <name>Sejoon Lim</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) Workshops,
                    2024, pp. 4860-4865</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2403.13731v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.13731v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2111.13675v2</id>
    <updated>2023-02-05T00: 12: 36Z</updated>
    <published>2021-11-26T18: 59: 28Z</published>
    <title>Weakly-guided Self-supervised Pretraining for Temporal Activity
  Detection</title>
    <summary>  Temporal Activity Detection aims to predict activity classes per frame, in
contrast to video-level predictions in Activity Classification (i.e., Activity
Recognition). Due to the expensive frame-level annotations required for
detection, the scale of detection datasets is limited. Thus, commonly, previous
work on temporal activity detection resorts to fine-tuning a classification
model pretrained on large-scale classification datasets (e.g., Kinetics-400).
However, such pretrained models are not ideal for downstream detection, due to
the disparity between the pretraining and the downstream fine-tuning tasks. In
this work, we propose a novel 'weakly-guided self-supervised' pretraining
method for detection. We leverage weak labels (classification) to introduce a
self-supervised pretext task (detection) by generating frame-level pseudo
labels, multi-action frames, and action segments. Simply put, we design a
detection task similar to downstream, on large-scale classification data,
without extra annotations. We show that the models pretrained with the proposed
weakly-guided self-supervised detection task outperform prior work on multiple
challenging activity detection benchmarks, including Charades and MultiTHUMOS.
Our extensive ablations further provide insights on when and how to use the
proposed models for activity detection. Code is available at
https: //github.com/kkahatapitiya/SSDet.
</summary>
    <author>
      <name>Kumara Kahatapitiya</name>
    </author>
    <author>
      <name>Zhou Ren</name>
    </author>
    <author>
      <name>Haoxiang Li</name>
    </author>
    <author>
      <name>Zhenyu Wu</name>
    </author>
    <author>
      <name>Michael S. Ryoo</name>
    </author>
    <author>
      <name>Gang Hua</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at AAAI 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.13675v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.13675v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2410.07912v1</id>
    <updated>2024-10-10T13: 39: 17Z</updated>
    <published>2024-10-10T13: 39: 17Z</published>
    <title>Understanding Spatio-Temporal Relations in Human-Object Interaction
  using Pyramid Graph Convolutional Network</title>
    <summary>  Human activities recognition is an important task for an intelligent robot,
especially in the field of human-robot collaboration, it requires not only the
label of sub-activities but also the temporal structure of the activity. In
order to automatically recognize both the label and the temporal structure in
sequence of human-object interaction, we propose a novel Pyramid Graph
Convolutional Network (PGCN), which employs a pyramidal encoder-decoder
architecture consisting of an attention based graph convolution network and a
temporal pyramid pooling module for downsampling and upsampling interaction
sequence on the temporal axis, respectively. The system represents the 2D or 3D
spatial relation of human and objects from the detection results in video data
as a graph. To learn the human-object relations, a new attention graph
convolutional network is trained to extract condensed information from the
graph representation. To segment action into sub-actions, a novel temporal
pyramid pooling module is proposed, which upsamples compressed features back to
the original time scale and classifies actions per frame.
  We explore various attention layers, namely spatial attention, temporal
attention and channel attention, and combine different upsampling decoders to
test the performance on action recognition and segmentation. We evaluate our
model on two challenging datasets in the field of human-object interaction
recognition, i.e. Bimanual Actions and IKEA Assembly datasets. We demonstrate
that our classifier significantly improves both framewise action recognition
and segmentation, e.g., F1 micro and F1@50 scores on Bimanual Actions dataset
are improved by $4.3\%$ and $8.5\%$ respectively.
</summary>
    <author>
      <name>Hao Xing</name>
    </author>
    <author>
      <name>Darius Burschka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages,
                    6 figures, IROS 2022 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.07912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.07912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2106.15171v1</id>
    <updated>2021-06-29T08: 33: 48Z</updated>
    <published>2021-06-29T08: 33: 48Z</published>
    <title>Spatio-Temporal Context for Action Detection</title>
    <summary>  Research in action detection has grown in the recentyears, as it plays a key
role in video understanding. Modelling the interactions (either spatial or
temporal) between actors and their context has proven to be essential for this
task. While recent works use spatial features with aggregated temporal
information, this work proposes to use non-aggregated temporal information.
This is done by adding an attention based method that leverages spatio-temporal
interactions between elements in the scene along the clip.The main contribution
of this work is the introduction of two cross attention blocks to effectively
model the spatial relations and capture short range temporal
interactions.Experiments on the AVA dataset show the advantages of the proposed
approach that models spatio-temporal relations between relevant elements in the
scene, outperforming other methods that model actor interactions with their
context by +0.31 mAP.
</summary>
    <author>
      <name>Manuel Sarmiento Calderó</name>
    </author>
    <author>
      <name>David Varas</name>
    </author>
    <author>
      <name>Elisenda Bou-Balust</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Vision and Pattern Recognition Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.15171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.15171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2011.08819v2</id>
    <updated>2021-03-04T02: 41: 43Z</updated>
    <published>2020-11-17T18: 36: 38Z</published>
    <title>Spatio-Temporal Analysis of Facial Actions using Lifecycle-Aware Capsule
  Networks</title>
    <summary>  Most state-of-the-art approaches for Facial Action Unit (AU) detection rely
upon evaluating facial expressions from static frames, encoding a snapshot of
heightened facial activity. In real-world interactions, however, facial
expressions are usually more subtle and evolve in a temporal manner requiring
AU detection models to learn spatial as well as temporal information. In this
paper, we focus on both spatial and spatio-temporal features encoding the
temporal evolution of facial AU activation. For this purpose, we propose the
Action Unit Lifecycle-Aware Capsule Network (AULA-Caps) that performs AU
detection using both frame and sequence-level features. While at the
frame-level the capsule layers of AULA-Caps learn spatial feature primitives to
determine AU activations, at the sequence-level, it learns temporal
dependencies between contiguous frames by focusing on relevant spatio-temporal
segments in the sequence. The learnt feature capsules are routed together such
that the model learns to selectively focus more on spatial or spatio-temporal
information depending upon the AU lifecycle. The proposed model is evaluated on
the commonly used BP4D and GFT benchmark datasets obtaining state-of-the-art
results on both the datasets.
</summary>
    <author>
      <name>Nikhil Churamani</name>
    </author>
    <author>
      <name>Sinan Kalkan</name>
    </author>
    <author>
      <name>Hatice Gunes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Updated Figure 6 and the Acknowledgements. Corrected typos. 11 pages,
                    6 figures,
                    3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.08819v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.08819v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2204.04796v2</id>
    <updated>2022-05-02T23: 39: 56Z</updated>
    <published>2022-04-10T23: 27: 19Z</published>
    <title>SOS! Self-supervised Learning Over Sets Of Handled Objects In Egocentric
  Action Recognition</title>
    <summary>  Learning an egocentric action recognition model from video data is
challenging due to distractors (e.g., irrelevant objects) in the background.
Further integrating object information into an action model is hence
beneficial. Existing methods often leverage a generic object detector to
identify and represent the objects in the scene. However, several important
issues remain. Object class annotations of good quality for the target domain
(dataset) are still required for learning good object representation. Besides,
previous methods deeply couple the existing action models and need to retrain
them jointly with object representation, leading to costly and inflexible
integration. To overcome both limitations, we introduce Self-Supervised
Learning Over Sets (SOS), an approach to pre-train a generic Objects In Contact
(OIC) representation model from video object regions detected by an
off-the-shelf hand-object contact detector. Instead of augmenting object
regions individually as in conventional self-supervised learning, we view the
action process as a means of natural data transformations with unique
spatio-temporal continuity and exploit the inherent relationships among
per-video object sets. Extensive experiments on two datasets, EPIC-KITCHENS-100
and EGTEA, show that our OIC significantly boosts the performance of multiple
state-of-the-art video classification models.
</summary>
    <author>
      <name>Victor Escorcia</name>
    </author>
    <author>
      <name>Ricardo Guerrero</name>
    </author>
    <author>
      <name>Xiatian Zhu</name>
    </author>
    <author>
      <name>Brais Martinez</name>
    </author>
    <link href="http://arxiv.org/abs/2204.04796v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.04796v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2207.06580v2</id>
    <updated>2022-08-19T17: 41: 11Z</updated>
    <published>2022-07-14T00: 46: 51Z</published>
    <title>Proposal-Free Temporal Action Detection via Global Segmentation Mask
  Learning</title>
    <summary>  Existing temporal action detection (TAD) methods rely on generating an
overwhelmingly large number of proposals per video. This leads to complex model
designs due to proposal generation and/or per-proposal action instance
evaluation and the resultant high computational cost. In this work, for the
first time, we propose a proposal-free Temporal Action detection model with
Global Segmentation mask (TAGS). Our core idea is to learn a global
segmentation mask of each action instance jointly at the full video length. The
TAGS model differs significantly from the conventional proposal-based methods
by focusing on global temporal representation learning to directly detect local
start and end points of action instances without proposals. Further, by
modeling TAD holistically rather than locally at the individual proposal level,
TAGS needs a much simpler model architecture with lower computational cost.
Extensive experiments show that despite its simpler design, TAGS outperforms
existing TAD methods, achieving new state-of-the-art performance on two
benchmarks. Importantly, it is ~ 20x faster to train and ~1.6x more efficient
for inference. Our PyTorch implementation of TAGS is available at
https: //github.com/sauradip/TAGS .
</summary>
    <author>
      <name>Sauradip Nag</name>
    </author>
    <author>
      <name>Xiatian Zhu</name>
    </author>
    <author>
      <name>Yi-Zhe Song</name>
    </author>
    <author>
      <name>Tao Xiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2022; Code available at https: //github.com/sauradip/TAGS</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.06580v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.06580v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2301.02059v1</id>
    <updated>2023-01-05T13: 29: 36Z</updated>
    <published>2023-01-05T13: 29: 36Z</published>
    <title>Zen: LSTM-based generation of individual spatiotemporal cellular traffic
  with interactions</title>
    <summary>  Domain-wide recognized by their high value in human presence and activity
studies, cellular network datasets (i.e., Charging Data Records, named CdRs),
however, present accessibility, usability, and privacy issues, restricting
their exploitation and research reproducibility.This paper tackles such
challenges by modeling Cdrs that fulfill real-world data attributes. Our
designed framework, named Zen follows a four-fold methodology related to (i)
the LTSM-based modeling of users' traffic behavior, (ii) the realistic and
flexible emulation of spatiotemporal mobility behavior, (iii) the structure of
lifelike cellular network infrastructure and social interactions, and (iv) the
combination of the three previous modules into realistic Cdrs traces with an
individual basis, realistically. Results show that Zen's first and third models
accurately capture individual and global distributions of a fully anonymized
real-world Cdrs dataset, while the second model is consistent with the
literature's revealed features in human mobility. Finally, we validate Zen Cdrs
ability of reproducing daily cellular behaviors of the urban population and its
usefulness in practical networking applications such as dynamic population
tracing, Radio Access Network's power savings, and anomaly detection as
compared to real-world CdRs.
</summary>
    <author>
      <name>Anne Josiane Kouam</name>
    </author>
    <author>
      <name>Aline Carneiro Viana</name>
    </author>
    <author>
      <name>Alain Tchana</name>
    </author>
    <link href="http://arxiv.org/abs/2301.02059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.02059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1903.04354v1</id>
    <updated>2019-03-11T15: 11: 56Z</updated>
    <published>2019-03-11T15: 11: 56Z</published>
    <title>ADS-ME: Anomaly Detection System for Micro-expression Spotting</title>
    <summary>  Micro-expressions (MEs) are infrequent and uncontrollable facial events that
can highlight emotional deception and appear in a high-stakes environment. This
paper propose an algorithm for spatiotemporal MEs spotting. Since MEs are
unusual events, we treat them as abnormal patterns that diverge from expected
Normal Facial Behaviour (NFBs) patterns. NFBs correspond to facial muscle
activations, eye blink/gaze events and mouth opening/closing movements that are
all facial deformation but not MEs. We propose a probabilistic model to
estimate the probability density function that models the spatiotemporal
distributions of NFBs patterns. To rank the outputs, we compute the negative
log-likelihood and we developed an adaptive thresholding technique to identify
MEs from NFBs. While working only with NFBs data, the main challenge is to
capture intrinsic spatiotemoral features, hence we design a recurrent
convolutional autoencoder for feature representation. Finally, we show that our
system is superior to previous works for MEs spotting.
</summary>
    <author>
      <name>Dawood Al Chanti</name>
    </author>
    <author>
      <name>Alice Caplier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages,
                    9 figures,
                    3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.04354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.04354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2304.00254v2</id>
    <updated>2023-04-04T07: 08: 41Z</updated>
    <published>2023-04-01T08: 06: 43Z</published>
    <title>DOAD: Decoupled One Stage Action Detection Network</title>
    <summary>  Localizing people and recognizing their actions from videos is a challenging
task towards high-level video understanding. Existing methods are mostly
two-stage based, with one stage for person bounding box generation and the
other stage for action recognition. However, such two-stage methods are
generally with low efficiency. We observe that directly unifying detection and
action recognition normally suffers from (i) inferior learning due to different
desired properties of context representation for detection and action
recognition; (ii) optimization difficulty with insufficient training data. In
this work, we present a decoupled one-stage network dubbed DOAD, to mitigate
above issues and improve the efficiency for spatio-temporal action detection.
To achieve it, we decouple detection and action recognition into two branches.
Specifically, one branch focuses on detection representation for actor
detection, and the other one for action recognition. For the action branch, we
design a transformer-based module (TransPC) to model pairwise relationships
between people and context. Different from commonly used vector-based dot
product in self-attention, it is built upon a novel matrix-based key and value
for Hadamard attention to model person-context information. It not only
exploits relationships between person pairs but also takes into account context
and relative position information. The results on AVA and UCF101-24 datasets
show that our method is competitive with two-stage state-of-the-art methods
with significant efficiency improvement.
</summary>
    <author>
      <name>Shuning Chang</name>
    </author>
    <author>
      <name>Pichao Wang</name>
    </author>
    <author>
      <name>Fan Wang</name>
    </author>
    <author>
      <name>Jiashi Feng</name>
    </author>
    <author>
      <name>Mike Zheng Show</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.00254v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.00254v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2104.04715v1</id>
    <updated>2021-04-10T08: 56: 58Z</updated>
    <published>2021-04-10T08: 56: 58Z</published>
    <title>Object Priors for Classifying and Localizing Unseen Actions</title>
    <summary>  This work strives for the classification and localization of human actions in
videos, without the need for any labeled video training examples. Where
existing work relies on transferring global attribute or object information
from seen to unseen action videos, we seek to classify and spatio-temporally
localize unseen actions in videos from image-based object information only. We
propose three spatial object priors, which encode local person and object
detectors along with their spatial relations. On top we introduce three
semantic object priors, which extend semantic matching through word embeddings
with three simple functions that tackle semantic ambiguity, object
discrimination, and object naming. A video embedding combines the spatial and
semantic object priors. It enables us to introduce a new video retrieval task
that retrieves action tubes in video collections based on user-specified
objects, spatial relations, and object size. Experimental evaluation on five
action datasets shows the importance of spatial and semantic object priors for
unseen actions. We find that persons and objects have preferred spatial
relations that benefit unseen action localization, while using multiple
languages and simple object filtering directly improves semantic matching,
leading to state-of-the-art results for both unseen action classification and
localization.
</summary>
    <author>
      <name>Pascal Mettes</name>
    </author>
    <author>
      <name>William Thong</name>
    </author>
    <author>
      <name>Cees G. M. Snoek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IJCV</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.04715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2309.05590v1</id>
    <updated>2023-09-11T16: 17: 50Z</updated>
    <published>2023-09-11T16: 17: 50Z</published>
    <title>Temporal Action Localization with Enhanced Instant Discriminability</title>
    <summary>  Temporal action detection (TAD) aims to detect all action boundaries and
their corresponding categories in an untrimmed video. The unclear boundaries of
actions in videos often result in imprecise predictions of action boundaries by
existing methods. To resolve this issue, we propose a one-stage framework named
TriDet. First, we propose a Trident-head to model the action boundary via an
estimated relative probability distribution around the boundary. Then, we
analyze the rank-loss problem (i.e. instant discriminability deterioration) in
transformer-based methods and propose an efficient scalable-granularity
perception (SGP) layer to mitigate this issue. To further push the limit of
instant discriminability in the video backbone, we leverage the strong
representation capability of pretrained large models and investigate their
performance on TAD. Last, considering the adequate spatial-temporal context for
classification, we design a decoupled feature pyramid network with separate
feature pyramids to incorporate rich spatial context from the large model for
localization. Experimental results demonstrate the robustness of TriDet and its
state-of-the-art performance on multiple TAD datasets, including hierarchical
(multilabel) TAD datasets.
</summary>
    <author>
      <name>Dingfeng Shi</name>
    </author>
    <author>
      <name>Qiong Cao</name>
    </author>
    <author>
      <name>Yujie Zhong</name>
    </author>
    <author>
      <name>Shan An</name>
    </author>
    <author>
      <name>Jian Cheng</name>
    </author>
    <author>
      <name>Haogang Zhu</name>
    </author>
    <author>
      <name>Dacheng Tao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An extended version of the CVPR paper arXiv: 2303.07347, submitted to
  IJCV</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.05590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.05590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1904.07846v1</id>
    <updated>2019-04-16T17: 49: 50Z</updated>
    <published>2019-04-16T17: 49: 50Z</published>
    <title>Temporal Cycle-Consistency Learning</title>
    <summary>  We introduce a self-supervised representation learning method based on the
task of temporal alignment between videos. The method trains a network using
temporal cycle consistency (TCC), a differentiable cycle-consistency loss that
can be used to find correspondences across time in multiple videos. The
resulting per-frame embeddings can be used to align videos by simply matching
frames using the nearest-neighbors in the learned embedding space.
  To evaluate the power of the embeddings, we densely label the Pouring and
Penn Action video datasets for action phases. We show that (i) the learned
embeddings enable few-shot classification of these action phases, significantly
reducing the supervised training requirements; and (ii) TCC is complementary to
other methods of self-supervised learning in videos, such as Shuffle and Learn
and Time-Contrastive Networks. The embeddings are also used for a number of
applications based on alignment (dense temporal correspondence) between video
pairs, including transfer of metadata of synchronized modalities between videos
(sounds, temporal semantic labels), synchronized playback of multiple videos,
and anomaly detection. Project webpage:
https: //sites.google.com/view/temporal-cycle-consistency .
</summary>
    <author>
      <name>Debidatta Dwibedi</name>
    </author>
    <author>
      <name>Yusuf Aytar</name>
    </author>
    <author>
      <name>Jonathan Tompson</name>
    </author>
    <author>
      <name>Pierre Sermanet</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CVPR 2019. Project webpage:
  https: //sites.google.com/view/temporal-cycle-consistency</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.07846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.07846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2010.06235v1</id>
    <updated>2020-10-13T08: 49: 35Z</updated>
    <published>2020-10-13T08: 49: 35Z</published>
    <title>Robust Two-Stream Multi-Feature Network for Driver Drowsiness Detection</title>
    <summary>  Drowsiness driving is a major cause of traffic accidents and thus numerous
previous researches have focused on driver drowsiness detection. Many drive
relevant factors have been taken into consideration for fatigue detection and
can lead to high precision, but there are still several serious constraints,
such as most existing models are environmentally susceptible. In this paper,
fatigue detection is considered as temporal action detection problem instead of
image classification. The proposed detection system can be divided into four
parts: (1) Localize the key patches of the detected driver picture which are
critical for fatigue detection and calculate the corresponding optical flow.
(2) Contrast Limited Adaptive Histogram Equalization (CLAHE) is used in our
system to reduce the impact of different light conditions. (3) Three individual
two-stream networks combined with attention mechanism are designed for each
feature to extract temporal information. (4) The outputs of the three
sub-networks will be concatenated and sent to the fully-connected network,
which judges the status of the driver. The drowsiness detection system is
trained and evaluated on the famous Nation Tsing Hua University Driver
Drowsiness Detection (NTHU-DDD) dataset and we obtain an accuracy of 94.46%,
which outperforms most existing fatigue detection models.
</summary>
    <author>
      <name>Qi Shen</name>
    </author>
    <author>
      <name>Shengjie Zhao</name>
    </author>
    <author>
      <name>Rongqing Zhang</name>
    </author>
    <author>
      <name>Bin Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2010.06235v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.06235v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2402.09820v2</id>
    <updated>2024-02-18T11: 29: 45Z</updated>
    <published>2024-02-15T09: 35: 57Z</published>
    <title>Utilizing Deep Learning for Enhancing Network Resilience in Finance</title>
    <summary>  In the age of the Internet, people's lives are increasingly dependent on
today's network technology. Maintaining network integrity and protecting the
legitimate interests of users is at the heart of network construction. Threat
detection is an important part of a complete and effective defense system. How
to effectively detect unknown threats is one of the concerns of network
protection. Currently, network threat detection is usually based on rules and
traditional machine learning methods, which create artificial rules or extract
common spatiotemporal features, which cannot be applied to large-scale data
applications, and the emergence of unknown risks causes the detection accuracy
of the original model to decline. With this in mind, this paper uses deep
learning for advanced threat detection to improve protective measures in the
financial industry. Many network researchers have shifted their focus to
exception-based intrusion detection techniques. The detection technology mainly
uses statistical machine learning methods - collecting normal program and
network behavior data, extracting multidimensional features, and training
decision machine learning models on this basis (commonly used include naive
Bayes, decision trees, support vector machines, random forests, etc.).
</summary>
    <author>
      <name>Yulu Gong</name>
    </author>
    <author>
      <name>Mengran Zhu</name>
    </author>
    <author>
      <name>Shuning Huo</name>
    </author>
    <author>
      <name>Yafei Xiang</name>
    </author>
    <author>
      <name>Hanyi Yu</name>
    </author>
    <link href="http://arxiv.org/abs/2402.09820v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.09820v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2106.08827v2</id>
    <updated>2021-11-24T04: 40: 27Z</updated>
    <published>2021-06-16T14: 43: 46Z</published>
    <title>JRDB-Act: A Large-scale Dataset for Spatio-temporal Action, Social Group
  and Activity Detection</title>
    <summary>  The availability of large-scale video action understanding datasets has
facilitated advances in the interpretation of visual scenes containing people.
However, learning to recognise human actions and their social interactions in
an unconstrained real-world environment comprising numerous people, with
potentially highly unbalanced and long-tailed distributed action labels from a
stream of sensory data captured from a mobile robot platform remains a
significant challenge, not least owing to the lack of a reflective large-scale
dataset. In this paper, we introduce JRDB-Act, as an extension of the existing
JRDB, which is captured by a social mobile manipulator and reflects a real
distribution of human daily-life actions in a university campus environment.
JRDB-Act has been densely annotated with atomic actions, comprises over 2.8M
action labels, constituting a large-scale spatio-temporal action detection
dataset. Each human bounding box is labeled with one pose-based action label
and multiple~(optional) interaction-based action labels. Moreover JRDB-Act
provides social group annotation, conducive to the task of grouping individuals
based on their interactions in the scene to infer their social
activities~(common activities in each social group). Each annotated label in
JRDB-Act is tagged with the annotators' confidence level which contributes to
the development of reliable evaluation strategies. In order to demonstrate how
one can effectively utilise such annotations, we develop an end-to-end
trainable pipeline to learn and infer these tasks, i.e. individual action and
social group detection. The data and the evaluation code is publicly available
at https: //jrdb.erc.monash.edu/.
</summary>
    <author>
      <name>Mahsa Ehsanpour</name>
    </author>
    <author>
      <name>Fatemeh Saleh</name>
    </author>
    <author>
      <name>Silvio Savarese</name>
    </author>
    <author>
      <name>Ian Reid</name>
    </author>
    <author>
      <name>Hamid Rezatofighi</name>
    </author>
    <link href="http://arxiv.org/abs/2106.08827v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.08827v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2104.10122v2</id>
    <updated>2021-10-16T18: 45: 49Z</updated>
    <published>2021-04-20T17: 10: 13Z</published>
    <title>Improving state-of-the-art in Detecting Student Engagement with Resnet
  and TCN Hybrid Network</title>
    <summary>  Automatic detection of students' engagement in online learning settings is a
key element to improve the quality of learning and to deliver personalized
learning materials to them. Varying levels of engagement exhibited by students
in an online classroom is an affective behavior that takes place over space and
time. Therefore, we formulate detecting levels of students' engagement from
videos as a spatio-temporal classification problem. In this paper, we present a
novel end-to-end Residual Network (ResNet) and Temporal Convolutional Network
(TCN) hybrid neural network architecture for students' engagement level
detection in videos. The 2D ResNet extracts spatial features from consecutive
video frames, and the TCN analyzes the temporal changes in video frames to
detect the level of engagement. The spatial and temporal arms of the hybrid
network are jointly trained on raw video frames of a large publicly available
students' engagement detection dataset, DAiSEE. We compared our method with
several competing students' engagement detection methods on this dataset. The
ResNet+TCN architecture outperforms all other studied methods, improves the
state-of-the-art engagement level detection accuracy, and sets a new baseline
for future research.
</summary>
    <author>
      <name>Ali Abedi</name>
    </author>
    <author>
      <name>Shehroz S. Khan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages,
                    3 figures,
                    1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.10122v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.10122v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2501.17429v1</id>
    <updated>2025-01-29T06: 09: 25Z</updated>
    <published>2025-01-29T06: 09: 25Z</published>
    <title>Algorithmic Segmentation and Behavioral Profiling for Ransomware
  Detection Using Temporal-Correlation Graphs</title>
    <summary>  The rapid evolution of cyber threats has outpaced traditional detection
methodologies, necessitating innovative approaches capable of addressing the
adaptive and complex behaviors of modern adversaries. A novel framework was
introduced, leveraging Temporal-Correlation Graphs to model the intricate
relationships and temporal patterns inherent in malicious operations. The
approach dynamically captured behavioral anomalies, offering a robust mechanism
for distinguishing between benign and malicious activities in real-time
scenarios. Extensive experiments demonstrated the framework's effectiveness
across diverse ransomware families, with consistently high precision, recall,
and overall detection accuracy. Comparative evaluations highlighted its better
performance over traditional signature-based and heuristic methods,
particularly in handling polymorphic and previously unseen ransomware variants.
The architecture was designed with scalability and modularity in mind, ensuring
compatibility with enterprise-scale environments while maintaining resource
efficiency. Analysis of encryption speeds, anomaly patterns, and temporal
correlations provided deeper insights into the operational strategies of
ransomware, validating the framework's adaptability to evolving threats. The
research contributes to advancing cybersecurity technologies by integrating
dynamic graph analytics and machine learning for future innovations in threat
detection. Results from this study underline the potential for transforming the
way organizations detect and mitigate complex cyberattacks.
</summary>
    <author>
      <name>Ignatius Rollere</name>
    </author>
    <author>
      <name>Caspian Hartsfield</name>
    </author>
    <author>
      <name>Seraphina Courtenay</name>
    </author>
    <author>
      <name>Lucian Fenwick</name>
    </author>
    <author>
      <name>Aurelia Grunwald</name>
    </author>
    <link href="http://arxiv.org/abs/2501.17429v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.17429v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2310.03377v1</id>
    <updated>2023-10-05T08: 28: 26Z</updated>
    <published>2023-10-05T08: 28: 26Z</published>
    <title>ACT-Net: Anchor-context Action Detection in Surgery Videos</title>
    <summary>  Recognition and localization of surgical detailed actions is an essential
component of developing a context-aware decision support system. However, most
existing detection algorithms fail to provide high-accuracy action classes even
having their locations, as they do not consider the surgery procedure's
regularity in the whole video. This limitation hinders their application.
Moreover, implementing the predictions in clinical applications seriously needs
to convey model confidence to earn entrustment, which is unexplored in surgical
action prediction. In this paper, to accurately detect fine-grained actions
that happen at every moment, we propose an anchor-context action detection
network (ACTNet), including an anchor-context detection (ACD) module and a
class conditional diffusion (CCD) module, to answer the following questions: 1)
where the actions happen; 2) what actions are; 3) how confidence predictions
are. Specifically, the proposed ACD module spatially and temporally highlights
the regions interacting with the extracted anchor in surgery video, which
outputs action location and its class distribution based on anchor-context
interactions. Considering the full distribution of action classes in videos,
the CCD module adopts a denoising diffusion-based generative model conditioned
on our ACD estimator to further reconstruct accurately the action predictions.
Moreover, we utilize the stochastic nature of the diffusion model outputs to
access model confidence for each prediction. Our method reports the
state-of-the-art performance, with improvements of 4.0% mAP against baseline on
the surgical video dataset.
</summary>
    <author>
      <name>Luoying Hao</name>
    </author>
    <author>
      <name>Yan Hu</name>
    </author>
    <author>
      <name>Wenjun Lin</name>
    </author>
    <author>
      <name>Qun Wang</name>
    </author>
    <author>
      <name>Heng Li</name>
    </author>
    <author>
      <name>Huazhu Fu</name>
    </author>
    <author>
      <name>Jinming Duan</name>
    </author>
    <author>
      <name>Jiang Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted early by MICCAI2023 (Oral)</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.03377v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.03377v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.22121v2</id>
    <updated>2025-04-13T18: 17: 29Z</updated>
    <published>2025-03-28T03: 49: 00Z</published>
    <title>Detecting Localized Deepfake Manipulations Using Action Unit-Guided
  Video Representations</title>
    <summary>  With rapid advancements in generative modeling, deepfake techniques are
increasingly narrowing the gap between real and synthetic videos, raising
serious privacy and security concerns. Beyond traditional face swapping and
reenactment, an emerging trend in recent state-of-the-art deepfake generation
methods involves localized edits such as subtle manipulations of specific
facial features like raising eyebrows, altering eye shapes, or modifying mouth
expressions. These fine-grained manipulations pose a significant challenge for
existing detection models, which struggle to capture such localized variations.
To the best of our knowledge, this work presents the first detection approach
explicitly designed to generalize to localized edits in deepfake videos by
leveraging spatiotemporal representations guided by facial action units. Our
method leverages a cross-attention-based fusion of representations learned from
pretext tasks like random masking and action unit detection, to create an
embedding that effectively encodes subtle, localized changes. Comprehensive
evaluations across multiple deepfake generation methods demonstrate that our
approach, despite being trained solely on the traditional FF+ dataset, sets a
new benchmark in detecting recent deepfake-generated videos with fine-grained
local edits, achieving a $20\%$ improvement in accuracy over current
state-of-the-art detection methods. Additionally, our method delivers
competitive performance on standard datasets, highlighting its robustness and
generalization across diverse types of local and global forgeries.
</summary>
    <author>
      <name>Tharun Anand</name>
    </author>
    <author>
      <name>Siva Sankar Sajeev</name>
    </author>
    <author>
      <name>Pravin Nair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CVPR-W 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.22121v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.22121v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2403.06592v3</id>
    <updated>2024-05-20T13: 01: 23Z</updated>
    <published>2024-03-11T10: 35: 58Z</published>
    <title>Exploiting Style Latent Flows for Generalizing Deepfake Video Detection</title>
    <summary>  This paper presents a new approach for the detection of fake videos, based on
the analysis of style latent vectors and their abnormal behavior in temporal
changes in the generated videos. We discovered that the generated facial videos
suffer from the temporal distinctiveness in the temporal changes of style
latent vectors, which are inevitable during the generation of temporally stable
videos with various facial expressions and geometric transformations. Our
framework utilizes the StyleGRU module, trained by contrastive learning, to
represent the dynamic properties of style latent vectors. Additionally, we
introduce a style attention module that integrates StyleGRU-generated features
with content-based features, enabling the detection of visual and temporal
artifacts. We demonstrate our approach across various benchmark scenarios in
deepfake detection, showing its superiority in cross-dataset and
cross-manipulation scenarios. Through further analysis, we also validate the
importance of using temporal changes of style latent vectors to improve the
generality of deepfake video detection.
</summary>
    <author>
      <name>Jongwook Choi</name>
    </author>
    <author>
      <name>Taehoon Kim</name>
    </author>
    <author>
      <name>Yonghyun Jeong</name>
    </author>
    <author>
      <name>Seungryul Baek</name>
    </author>
    <author>
      <name>Jongwon Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint version, final version will be available at
  https: //openaccess.thecvf.com The IEEE / CVF Computer Vision and Pattern
  Recognition Conference (CVPR) (2024) Published by: IEEE &amp; CVF</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.06592v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.06592v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2109.02376v2</id>
    <updated>2021-10-01T10: 03: 22Z</updated>
    <published>2021-09-06T11: 45: 45Z</published>
    <title>Robust Event Detection based on Spatio-Temporal Latent Action Unit using
  Skeletal Information</title>
    <summary>  This paper propose a novel dictionary learning approach to detect event
action using skeletal information extracted from RGBD video. The event action
is represented as several latent atoms and composed of latent spatial and
temporal attributes. We perform the method at the example of fall event
detection. The skeleton frames are clustered by an initial K-means method. Each
skeleton frame is assigned with a varying weight parameter and fed into our
Gradual Online Dictionary Learning (GODL) algorithm. During the training
process, outlier frames will be gradually filtered by reducing the weight that
is inversely proportional to a cost. In order to strictly distinguish the event
action from similar actions and robustly acquire its action unit, we build a
latent unit temporal structure for each sub-action. We evaluate the proposed
method on parts of the NTURGB+D dataset, which includes 209 fall videos,
                    405
ground-lift videos,
                    420 sit-down videos, and 280 videos of 46 otheractions. We
present the experimental validation of the achieved accuracy, recall and
precision. Our approach achieves the bestperformance on precision and accuracy
of human fall event detection, compared with other existing dictionary learning
methods. With increasing noise ratio, our method remains the highest accuracy
and the lowest variance.
</summary>
    <author>
      <name>Hao Xing</name>
    </author>
    <author>
      <name>Yuxuan Xue</name>
    </author>
    <author>
      <name>Mingchuan Zhou</name>
    </author>
    <author>
      <name>Darius Burschka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2021 IROS</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.02376v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.02376v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; I.5.2; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.00653v1</id>
    <updated>2024-03-31T11: 43: 39Z</updated>
    <published>2024-03-31T11: 43: 39Z</published>
    <title>Dual DETRs for Multi-Label Temporal Action Detection</title>
    <summary>  Temporal Action Detection (TAD) aims to identify the action boundaries and
the corresponding category within untrimmed videos. Inspired by the success of
DETR in object detection, several methods have adapted the query-based
framework to the TAD task. However, these approaches primarily followed DETR to
predict actions at the instance level (i.e., identify each action by its center
point), leading to sub-optimal boundary localization. To address this issue, we
propose a new Dual-level query-based TAD framework, namely DualDETR, to detect
actions from both instance-level and boundary-level. Decoding at different
levels requires semantics of different granularity, therefore we introduce a
two-branch decoding structure. This structure builds distinctive decoding
processes for different levels, facilitating explicit capture of temporal cues
and semantics at each level. On top of the two-branch design, we present a
joint query initialization strategy to align queries from both levels.
Specifically, we leverage encoder proposals to match queries from each level in
a one-to-one manner. Then, the matched queries are initialized using position
and content prior from the matched action proposal. The aligned dual-level
queries can refine the matched proposal with complementary cues during
subsequent decoding. We evaluate DualDETR on three challenging multi-label TAD
benchmarks. The experimental results demonstrate the superior performance of
DualDETR to the existing state-of-the-art methods, achieving a substantial
improvement under det-mAP and delivering impressive results under seg-mAP.
</summary>
    <author>
      <name>Yuhan Zhu</name>
    </author>
    <author>
      <name>Guozhen Zhang</name>
    </author>
    <author>
      <name>Jing Tan</name>
    </author>
    <author>
      <name>Gangshan Wu</name>
    </author>
    <author>
      <name>Limin Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVPR 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.00653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.00653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2302.06848v2</id>
    <updated>2023-06-08T01: 49: 33Z</updated>
    <published>2023-02-14T05: 52: 45Z</published>
    <title>YOWOv2: A Stronger yet Efficient Multi-level Detection Framework for
  Real-time Spatio-temporal Action Detection</title>
    <summary>  Designing a real-time framework for the spatio-temporal action detection task
is still a challenge. In this paper, we propose a novel real-time action
detection framework, YOWOv2. In this new framework, YOWOv2 takes advantage of
both the 3D backbone and 2D backbone for accurate action detection. A
multi-level detection pipeline is designed to detect action instances of
different scales. To achieve this goal, we carefully build a simple and
efficient 2D backbone with a feature pyramid network to extract different
levels of classification features and regression features. For the 3D backbone,
we adopt the existing efficient 3D CNN to save development time. By combining
3D backbones and 2D backbones of different sizes, we design a YOWOv2 family
including YOWOv2-Tiny, YOWOv2-Medium, and YOWOv2-Large. We also introduce the
popular dynamic label assignment strategy and anchor-free mechanism to make the
YOWOv2 consistent with the advanced model architecture design. With our
improvement, YOWOv2 is significantly superior to YOWO, and can still keep
real-time detection. Without any bells and whistles, YOWOv2 achieves 87.0 %
frame mAP and 52.8 % video mAP with over 20 FPS on the UCF101-24. On the AVA,
YOWOv2 achieves 21.7 % frame mAP with over 20 FPS. Our code is available on
https: //github.com/yjh0410/YOWOv2.
</summary>
    <author>
      <name>Jianhua Yang</name>
    </author>
    <author>
      <name>Kun Dai</name>
    </author>
    <link href="http://arxiv.org/abs/2302.06848v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.06848v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2412.17788v2</id>
    <updated>2024-12-28T00: 40: 45Z</updated>
    <published>2024-12-23T18: 49: 18Z</published>
    <title>Spectro-temporal symmetry in action-detected optical spectroscopy:
  highlighting excited-state dynamics in large systems</title>
    <summary>  Multidimensional optical spectroscopy observes transient excitation dynamics
through the time evolution of spectral correlations. Its action-detected
variants offer several advantages over the coherent detection and are thus
becoming increasingly widespread. Nevertheless, a drawback of action-detected
spectra is the presence of a large stationary background of so-called
incoherent mixing of excitations from independent states that resembles a
product of ground-state absorption spectra and obscures the excited-state
signal. This issue is especially problematic in fluorescence-detected
two-dimensional electronic spectroscopy (F-2DES) and fluorescence-detected
pump--probe spectroscopy (F-PP) of extended systems, where large incoherent
mixing arises from efficient exciton--exciton annihilation. In this work, we
demonstrate on the example of F-2DES and F-PP an inherent spectro-temporal
symmetry of action-detected spectra, which allows general, system-independent
subtraction of any stationary signals including incoherent mixing. We derive
the expressions for spectra with normal and reversed time ordering of the
pulses, relating these to the symmetry of the system response. As we
demonstrate both analytically and numerically, the difference signal
constructed from spectra with normal and reversed pulse ordering is free of
incoherent mixing and highlights the excitation dynamics. We further verify the
approach on the experimental F-PP spectra of a molecular squaraine heterodimer
and the F-2DES spectra of the photosynthetic antenna LH2 of purple bacteria.
The approach is generally applicable to action-detected 2DES and pump--probe
spectroscopy without experimental modifications and independent of the studied
system, enabling their application to large systems such as molecular
complexes.
</summary>
    <author>
      <name>Kateřina Charvátová</name>
    </author>
    <author>
      <name>Pavel Malý</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages,
                    6 figures, plus Supporting Information (16 pages,
                    4
  figures)</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.17788v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.17788v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/0705.3835v1</id>
    <updated>2007-05-25T19: 35: 29Z</updated>
    <published>2007-05-25T19: 35: 29Z</published>
    <title>Oscillation Effects and Time Variation of the Supernova Neutrino Signal</title>
    <summary>  The neutrinos detected from the next Galactic core-collapse supernova will
contain valuable information on the internal dynamics of the explosion. One
mechanism leading to a temporal evolution of the neutrino signal is the
variation of the induced neutrino flavor mixing driven by changes in the
density profile. With one and two dimensional hydrodynamical simulations we
identify the behavior and properties of prominent features of the explosion.
Using these results we demonstrate the time variation of the neutrino crossing
probabilities due to changes in the MSW neutrino transformations as the star
explodes by using the S-matrix - Monte Carlo - approach to neutrino
propagation. After adopting spectra for the neutrinos emitted from the
proto-neutron star we calculate for a Galactic supernova the evolution of the
positron spectra within a water Cerenkov detector and the ratio of charged
current to neutral current event rates for a heavy water - SNO like - detector
and find that these detector signals are feasible probes of a number of
explosion features.
</summary>
    <author>
      <name>James P. Kneller</name>
    </author>
    <author>
      <name>Gail C. McLaughlin</name>
    </author>
    <author>
      <name>Justin Brockman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevD.77.045023</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevD.77.045023" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys.Rev.D77: 045023,
                    2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.3835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.3835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2409.04035v1</id>
    <updated>2024-09-06T05: 57: 49Z</updated>
    <published>2024-09-06T05: 57: 49Z</published>
    <title>MultiCounter: Multiple Action Agnostic Repetition Counting in Untrimmed
  Videos</title>
    <summary>  Multi-instance Repetitive Action Counting (MRAC) aims to estimate the number
of repetitive actions performed by multiple instances in untrimmed videos,
commonly found in human-centric domains like sports and exercise. In this
paper, we propose MultiCounter, a fully end-to-end deep learning framework that
enables simultaneous detection, tracking, and counting of repetitive actions of
multiple human instances. Specifically, MultiCounter incorporates two novel
modules: 1) mixed spatiotemporal interaction for efficient context correlation
across consecutive frames, and 2) task-specific heads for accurate perception
of periodic boundaries and generalization for action-agnostic human instances.
We train MultiCounter on a synthetic dataset called MultiRep generated from
annotated real-world videos. Experiments on the MultiRep dataset validate the
fundamental challenge of MRAC tasks and showcase the superiority of our
proposed model. Compared to ByteTrack+RepNet, a solution that combines an
advanced tracker with a single repetition counter, MultiCounter substantially
improves Period-mAP by 41.0%, reduces AvgMAE by 58.6%, and increases AvgOBO
1.48 times. This sets a new benchmark in the field of MRAC. Moreover,
MultiCounter runs in real-time on a commodity GPU server and is insensitive to
the number of human instances in a video.
</summary>
    <author>
      <name>Yin Tang</name>
    </author>
    <author>
      <name>Wei Luo</name>
    </author>
    <author>
      <name>Jinrui Zhang</name>
    </author>
    <author>
      <name>Wei Huang</name>
    </author>
    <author>
      <name>Ruihai Jing</name>
    </author>
    <author>
      <name>Deyu Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ECAI 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.04035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.04035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1805.06749v2</id>
    <updated>2018-07-23T22: 06: 19Z</updated>
    <published>2018-05-17T13: 21: 43Z</published>
    <title>Action Completion: A Temporal Model for Moment Detection</title>
    <summary>  We introduce completion moment detection for actions - the problem of
locating the moment of completion, when the action's goal is confidently
considered achieved. The paper proposes a joint classification-regression
recurrent model that predicts completion from a given frame, and then
integrates frame-level contributions to detect sequence-level completion
moment. We introduce a recurrent voting node that predicts the frame's relative
position of the completion moment by either classification or regression. The
method is also capable of detecting incompletion. For example, the method is
capable of detecting a missed ball-catch, as well as the moment at which the
ball is safely caught. We test the method on 16 actions from three public
datasets, covering sports as well as daily actions. Results show that when
combining contributions from frames prior to the completion moment as well as
frames post completion, the completion moment is detected within one second in
89% of all tested sequences.
</summary>
    <author>
      <name>Farnoosh Heidarivincheh</name>
    </author>
    <author>
      <name>Majid Mirmehdi</name>
    </author>
    <author>
      <name>Dima Damen</name>
    </author>
    <link href="http://arxiv.org/abs/1805.06749v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06749v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2207.10275v1</id>
    <updated>2022-07-21T03: 00: 22Z</updated>
    <published>2022-07-21T03: 00: 22Z</published>
    <title>Adversary Detection and Resilient Control for Multi-Agent Systems</title>
    <summary>  This paper presents an adversary detection mechanism and a resilient control
framework for multi-agent systems under spatiotemporal constraints. Safety in
multi-agent systems is typically addressed under the assumption that all agents
collaborate to ensure the forward invariance of a desired safe set. This work
analyzes agent behaviors based on certain behavior metrics, and designs a
proactive adversary detection mechanism based on the notion of the critical
region for the system operation. In particular, the presented detection
mechanism not only identifies adversarial agents, but also ensures all-time
safety for intact agents. Then, based on the analysis and detection results, a
resilient QP-based controller is presented to ensure safety and liveness
constraints for intact agents. Simulation results validate the efficacy of the
presented theoretical contributions.
</summary>
    <author>
      <name>Aquib Mustafa</name>
    </author>
    <author>
      <name>Dimitra Panagou</name>
    </author>
    <link href="http://arxiv.org/abs/2207.10275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.10275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2111.01604v1</id>
    <updated>2021-11-02T14: 00: 33Z</updated>
    <published>2021-11-02T14: 00: 33Z</published>
    <title>A Critical Study on the Recent Deep Learning Based Semi-Supervised Video
  Anomaly Detection Methods</title>
    <summary>  Video anomaly detection is one of the hot research topics in computer vision
nowadays, as abnormal events contain a high amount of information. Anomalies
are one of the main detection targets in surveillance systems, usually needing
real-time actions. Regarding the availability of labeled data for training
(i.e., there is not enough labeled data for abnormalities), semi-supervised
anomaly detection approaches have gained interest recently. This paper
introduces the researchers of the field to a new perspective and reviews the
recent deep-learning based semi-supervised video anomaly detection approaches,
based on a common strategy they use for anomaly detection. Our goal is to help
researchers develop more effective video anomaly detection methods. As the
selection of a right Deep Neural Network plays an important role for several
parts of this task, a quick comparative review on DNNs is prepared first.
Unlike previous surveys, DNNs are reviewed from a spatiotemporal feature
extraction viewpoint, customized for video anomaly detection. This part of the
review can help researchers in this field select suitable networks for
different parts of their methods. Moreover, some of the state-of-the-art
anomaly detection methods, based on their detection strategy, are critically
surveyed. The review provides a novel and deep look at existing methods and
results in stating the shortcomings of these approaches, which can be a hint
for future works.
</summary>
    <author>
      <name>Mohammad Baradaran</name>
    </author>
    <author>
      <name>Robert Bergevin</name>
    </author>
    <link href="http://arxiv.org/abs/2111.01604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.01604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1910.02993v1</id>
    <updated>2019-10-07T18: 18: 37Z</updated>
    <published>2019-10-07T18: 18: 37Z</published>
    <title>Rekall: Specifying Video Events using Compositions of Spatiotemporal
  Labels</title>
    <summary>  Many real-world video analysis applications require the ability to identify
domain-specific events in video, such as interviews and commercials in TV news
broadcasts, or action sequences in film. Unfortunately, pre-trained models to
detect all the events of interest in video may not exist, and training new
models from scratch can be costly and labor-intensive. In this paper, we
explore the utility of specifying new events in video in a more traditional
manner: by writing queries that compose outputs of existing, pre-trained
models. To write these queries, we have developed Rekall, a library that
exposes a data model and programming model for compositional video event
specification. Rekall represents video annotations from different sources
(object detectors, transcripts, etc.) as spatiotemporal labels associated with
continuous volumes of spacetime in a video, and provides operators for
composing labels into queries that model new video events. We demonstrate the
use of Rekall in analyzing video from cable TV news broadcasts, films,
static-camera vehicular video streams, and commercial autonomous vehicle logs.
In these efforts, domain experts were able to quickly (in a few hours to a day)
author queries that enabled the accurate detection of new events (on par with,
and in some cases much more accurate than, learned approaches) and to rapidly
retrieve video clips for human-in-the-loop tasks such as video content curation
and training data curation. Finally, in a user study, novice users of Rekall
were able to author queries to retrieve new events in video given just one hour
of query development time.
</summary>
    <author>
      <name>Daniel Y. Fu</name>
    </author>
    <author>
      <name>Will Crichton</name>
    </author>
    <author>
      <name>James Hong</name>
    </author>
    <author>
      <name>Xinwei Yao</name>
    </author>
    <author>
      <name>Haotian Zhang</name>
    </author>
    <author>
      <name>Anh Truong</name>
    </author>
    <author>
      <name>Avanika Narayan</name>
    </author>
    <author>
      <name>Maneesh Agrawala</name>
    </author>
    <author>
      <name>Christopher Ré</name>
    </author>
    <author>
      <name>Kayvon Fatahalian</name>
    </author>
    <link href="http://arxiv.org/abs/1910.02993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2308.07893v1</id>
    <updated>2023-08-15T17: 34: 54Z</updated>
    <published>2023-08-15T17: 34: 54Z</published>
    <title>Memory-and-Anticipation Transformer for Online Action Understanding</title>
    <summary>  Most existing forecasting systems are memory-based methods, which attempt to
mimic human forecasting ability by employing various memory mechanisms and have
progressed in temporal modeling for memory dependency. Nevertheless, an obvious
weakness of this paradigm is that it can only model limited historical
dependence and can not transcend the past. In this paper, we rethink the
temporal dependence of event evolution and propose a novel
memory-anticipation-based paradigm to model an entire temporal structure,
including the past, present, and future. Based on this idea, we present
Memory-and-Anticipation Transformer (MAT), a memory-anticipation-based
approach, to address the online action detection and anticipation tasks. In
addition, owing to the inherent superiority of MAT, it can process online
action detection and anticipation tasks in a unified manner. The proposed MAT
model is tested on four challenging benchmarks TVSeries, THUMOS'14, HDD, and
EPIC-Kitchens-100, for online action detection and anticipation tasks, and it
significantly outperforms all existing methods. Code is available at
https: //github.com/Echo0125/Memory-and-Anticipation-Transformer.
</summary>
    <author>
      <name>Jiahao Wang</name>
    </author>
    <author>
      <name>Guo Chen</name>
    </author>
    <author>
      <name>Yifei Huang</name>
    </author>
    <author>
      <name>Limin Wang</name>
    </author>
    <author>
      <name>Tong Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV 2023 Camera Ready</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.07893v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.07893v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2101.08851v1</id>
    <updated>2021-01-21T21: 01: 46Z</updated>
    <published>2021-01-21T21: 01: 46Z</published>
    <title>Bridging the gap between Human Action Recognition and Online Action
  Detection</title>
    <summary>  Action recognition, early prediction, and online action detection are
complementary disciplines that are often studied independently. Most online
action detection networks use a pre-trained feature extractor, which might not
be optimal for its new task. We address the task-specific feature extraction
with a teacher-student framework between the aforementioned disciplines, and a
novel training strategy. Our network, Online Knowledge Distillation Action
Detection network (OKDAD), embeds online early prediction and online temporal
segment proposal subnetworks in parallel. Low interclass and high intraclass
similarity are encouraged during teacher training. Knowledge distillation to
the OKDAD network is ensured via layer reuse and cosine similarity between
teacher-student feature vectors. Layer reuse and similarity learning
significantly improve our baseline which uses a generic feature extractor. We
evaluate our framework on infrared videos from two popular datasets, NTU RGB+D
(action recognition, early prediction) and PKU MMD (action detection). Unlike
previous attempts on those datasets, our student networks perform without any
knowledge of the future. Even with this added difficulty, we achieve
state-of-the-art results on both datasets. Moreover, our networks use infrared
from RGB-D cameras, which we are the first to use for online action detection,
to our knowledge.
</summary>
    <author>
      <name>Alban Main de Boissiere</name>
    </author>
    <author>
      <name>Rita Noumeir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages,
                    6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.08851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.08851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2007.01089v1</id>
    <updated>2020-07-02T13: 23: 03Z</updated>
    <published>2020-07-02T13: 23: 03Z</published>
    <title>Estimating Blink Probability for Highlight Detection in Figure Skating
  Videos</title>
    <summary>  Highlight detection in sports videos has a broad viewership and huge
commercial potential. It is thus imperative to detect highlight scenes more
suitably for human interest with high temporal accuracy. Since people
instinctively suppress blinks during attention-grabbing events and
synchronously generate blinks at attention break points in videos, the
instantaneous blink rate can be utilized as a highly accurate temporal
indicator of human interest. Therefore, in this study, we propose a novel,
automatic highlight detection method based on the blink rate. The method trains
a one-dimensional convolution network (1D-CNN) to assess blink rates at each
video frame from the spatio-temporal pose features of figure skating videos.
Experiments show that the method successfully estimates the blink rate in 94%
of the video clips and predicts the temporal change in the blink rate around a
jump event with high accuracy. Moreover, the method detects not only the
representative athletic action, but also the distinctive artistic expression of
figure skating performance as key frames. This suggests that the
blink-rate-based supervised learning approach enables high-accuracy highlight
detection that more closely matches human sensibility.
</summary>
    <author>
      <name>Tamami Nakano</name>
    </author>
    <author>
      <name>Atsuya Sakata</name>
    </author>
    <author>
      <name>Akihiro Kishimoto</name>
    </author>
    <link href="http://arxiv.org/abs/2007.01089v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.01089v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2311.04190v1</id>
    <updated>2023-11-07T18: 33: 08Z</updated>
    <published>2023-11-07T18: 33: 08Z</published>
    <title>Spatio-Temporal Anomaly Detection with Graph Networks for Data Quality
  Monitoring of the Hadron Calorimeter</title>
    <summary>  The compact muon solenoid (CMS) experiment is a general-purpose detector for
high-energy collision at the large hadron collider (LHC) at CERN. It employs an
online data quality monitoring (DQM) system to promptly spot and diagnose
particle data acquisition problems to avoid data quality loss. In this study,
we present semi-supervised spatio-temporal anomaly detection (AD) monitoring
for the physics particle reading channels of the hadronic calorimeter (HCAL) of
the CMS using three-dimensional digi-occupancy map data of the DQM. We propose
the GraphSTAD system, which employs convolutional and graph neural networks to
learn local spatial characteristics induced by particles traversing the
detector, and global behavior owing to shared backend circuit connections and
housing boxes of the channels, respectively. Recurrent neural networks capture
the temporal evolution of the extracted spatial features. We have validated the
accuracy of the proposed AD system in capturing diverse channel fault types
using the LHC Run-2 collision data sets. The GraphSTAD system has achieved
production-level accuracy and is being integrated into the CMS core production
system--for real-time monitoring of the HCAL. We have also provided a
quantitative performance comparison with alternative benchmark models to
demonstrate the promising leverage of the presented system.
</summary>
    <author>
      <name>Mulugeta Weldezgina Asres</name>
    </author>
    <author>
      <name>Christian Walter Omlin</name>
    </author>
    <author>
      <name>Long Wang</name>
    </author>
    <author>
      <name>David Yu</name>
    </author>
    <author>
      <name>Pavel Parygin</name>
    </author>
    <author>
      <name>Jay Dittmann</name>
    </author>
    <author>
      <name>Georgia Karapostoli</name>
    </author>
    <author>
      <name>Markus Seidel</name>
    </author>
    <author>
      <name>Rosamaria Venditti</name>
    </author>
    <author>
      <name>Luka Lambrecht</name>
    </author>
    <author>
      <name>Emanuele Usai</name>
    </author>
    <author>
      <name>Muhammad Ahmad</name>
    </author>
    <author>
      <name>Javier Fernandez Menendez</name>
    </author>
    <author>
      <name>Kaori Maeshima</name>
    </author>
    <author>
      <name>the CMS-HCAL Collaboration</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/s23249679</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/s23249679" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages,
                    15 figures,
                    3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.04190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.04190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2206.05291v1</id>
    <updated>2022-06-10T16: 30: 55Z</updated>
    <published>2022-06-10T16: 30: 55Z</published>
    <title>ProActive: Self-Attentive Temporal Point Process Flows for Activity
  Sequences</title>
    <summary>  Any human activity can be represented as a temporal sequence of actions
performed to achieve a certain goal. Unlike machine-made time series, these
action sequences are highly disparate as the time taken to finish a similar
action might vary between different persons. Therefore, understanding the
dynamics of these sequences is essential for many downstream tasks such as
activity length prediction, goal prediction, etc. Existing neural approaches
that model an activity sequence are either limited to visual data or are task
specific, i.e., limited to next action or goal prediction. In this paper, we
present ProActive, a neural marked temporal point process (MTPP) framework for
modeling the continuous-time distribution of actions in an activity sequence
while simultaneously addressing three high-impact problems -- next action
prediction, sequence-goal prediction, and end-to-end sequence generation.
Specifically, we utilize a self-attention module with temporal normalizing
flows to model the influence and the inter-arrival times between actions in a
sequence. Moreover, for time-sensitive prediction, we perform an early
detection of sequence goal via a constrained margin-based optimization
procedure. This in-turn allows ProActive to predict the sequence goal using a
limited number of actions. Extensive experiments on sequences derived from
three activity recognition datasets show the significant accuracy boost of
ProActive over the state-of-the-art in terms of action and goal prediction, and
the first-ever application of end-to-end action sequence generation.
</summary>
    <author>
      <name>Vinayak Gupta</name>
    </author>
    <author>
      <name>Srikanta Bedathur</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3534678.3539477</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3534678.3539477" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">KDD 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.05291v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.05291v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2010.09211v1</id>
    <updated>2020-10-19T04: 25: 10Z</updated>
    <published>2020-10-19T04: 25: 10Z</published>
    <title>Unsupervised Domain Adaptation for Spatio-Temporal Action Localization</title>
    <summary>  Spatio-temporal action localization is an important problem in computer
vision that involves detecting where and when activities occur, and therefore
requires modeling of both spatial and temporal features. This problem is
typically formulated in the context of supervised learning, where the learned
classifiers operate on the premise that both training and test data are sampled
from the same underlying distribution. However, this assumption does not hold
when there is a significant domain shift, leading to poor generalization
performance on the test data. To address this, we focus on the hard and novel
task of generalizing training models to test samples without access to any
labels from the latter for spatio-temporal action localization by proposing an
end-to-end unsupervised domain adaptation algorithm. We extend the
state-of-the-art object detection framework to localize and classify actions.
In order to minimize the domain shift, three domain adaptation modules at image
level (temporal and spatial) and instance level (temporal) are designed and
integrated. We design a new experimental setup and evaluate the proposed method
and different adaptation modules on the UCF-Sports, UCF-101 and JHMDB benchmark
datasets. We show that significant performance gain can be achieved when
spatial and temporal features are adapted separately, or jointly for the most
effective results.
</summary>
    <author>
      <name>Nakul Agarwal</name>
    </author>
    <author>
      <name>Yi-Ting Chen</name>
    </author>
    <author>
      <name>Behzad Dariush</name>
    </author>
    <author>
      <name>Ming-Hsuan Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in BMVC 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.09211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1505.02137v2</id>
    <updated>2015-05-28T16: 05: 07Z</updated>
    <published>2015-05-06T18: 17: 56Z</published>
    <title>Human Social Interaction Modeling Using Temporal Deep Networks</title>
    <summary>  We present a novel approach to computational modeling of social interactions
based on modeling of essential social interaction predicates (ESIPs) such as
joint attention and entrainment. Based on sound social psychological theory and
methodology, we collect a new "Tower Game" dataset consisting of audio-visual
capture of dyadic interactions labeled with the ESIPs. We expect this dataset
to provide a new avenue for research in computational social interaction
modeling. We propose a novel joint Discriminative Conditional Restricted
Boltzmann Machine (DCRBM) model that combines a discriminative component with
the generative power of CRBMs. Such a combination enables us to uncover
actionable constituents of the ESIPs in two steps. First, we train the DCRBM
model on the labeled data and get accurate (76\%-49\% across various ESIPs)
detection of the predicates. Second, we exploit the generative capability of
DCRBMs to activate the trained model so as to generate the lower-level data
corresponding to the specific ESIP that closely matches the actual training
data (with mean square error 0.01-0.1 for generating 100 frames). We are thus
able to decompose the ESIPs into their constituent actionable behaviors. Such a
purely computational determination of how to establish an ESIP such as
engagement is unprecedented.
</summary>
    <author>
      <name>Mohamed R. Amer</name>
    </author>
    <author>
      <name>Behjat Siddiquie</name>
    </author>
    <author>
      <name>Amir Tamrakar</name>
    </author>
    <author>
      <name>David A. Salter</name>
    </author>
    <author>
      <name>Brian Lande</name>
    </author>
    <author>
      <name>Darius Mehri</name>
    </author>
    <author>
      <name>Ajay Divakaran</name>
    </author>
    <link href="http://arxiv.org/abs/1505.02137v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.02137v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2501.07249v2</id>
    <updated>2025-04-13T10: 01: 13Z</updated>
    <published>2025-01-13T11: 58: 14Z</published>
    <title>Self-organized institutions in evolutionary dynamical-systems game</title>
    <summary>  Social institutions are systems of shared norms and rules that regulate
people's behaviors, often emerging without external enforcement. They provide
criteria to distinguish cooperation from defection and establish rules to
sustain cooperation, shaped through long-term trial and error. While principles
for successful institutions have been proposed, the mechanisms underlying their
emergence remain poorly understood. Here, we introduce the evolutionary
dynamical-systems game, a framework that couples game actions with
environmental dynamics and explores the evolution of cognitive frameworks for
decision-making. We analyze a minimal model of common-pool resource management,
where resources grow naturally and are harvested. Players use decision-making
functions to determine whether to harvest at each step, based on environmental
and peer monitoring. As these functions evolve, players detect selfish
harvesting and punish it by degrading the environment through harvesting. This
process leads to the self-organization of norms that classify harvesting
actions as cooperative, defective, or punitive. The emergent norms for
``cooperativeness'' and rules of punishment serve as institutions. The
environmental and players' states converge to distinct modes characterized by
limit-cycles, representing temporal regularities in socio-ecological systems.
These modes remain stable despite slight variations in decision-making,
illustrating the stability of institutions. The evolutionary robustness of
decision-making functions serves as a measure of the evolutionary favorability
of institutions, highlighting the role of plasticity in responding to diverse
opponents. This work introduces foundational concepts in evolutionary
dynamical-systems games and elucidates the mechanisms underlying the
self-organization of institutions by modeling the interplay between ecological
dynamics and human decision-making.
</summary>
    <author>
      <name>Kenji Itao</name>
    </author>
    <author>
      <name>Kunihiko Kaneko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 + 7pages,
                    6 + 12 figures,
                    2 + 1 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.07249v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.07249v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1905.06116v3</id>
    <updated>2019-06-16T04: 51: 35Z</updated>
    <published>2019-05-10T11: 35: 14Z</published>
    <title>BuSCOPE : Fusing Individual &amp; Aggregated Mobility Behavior for "Live"
  Smart City Services</title>
    <summary>  While analysis of urban commuting data has a long and demonstrated history of
providing useful insights into human mobility behavior, such analysis has been
performed largely in offline fashion and to aid medium-to-long term urban
planning. In this work, we demonstrate the power of applying predictive
analytics on real-time mobility data, specifically the smart-card generated
trip data of millions of public bus commuters in Singapore, to create two novel
and "live" smart city services. The key analytical novelty in our work lies in
combining two aspects of urban mobility: (a) conformity: which reflects the
predictability in the aggregated flow of commuters along bus routes, and (b)
regularity: which captures the repeated trip patterns of each individual
commuter. We demonstrate that the fusion of these two measures of behavior can
be performed at city-scale using our BuScope platform, and can be used to
create two innovative smart city applications. The Last-Mile Demand Generator
provides O(mins) lookahead into the number of disembarking passengers at
neighborhood bus stops; it achieves over 85% accuracy in predicting such
disembarkations by an ingenious combination of individual-level regularity with
aggregate-level conformity. By moving driverless vehicles proactively to match
this predicted demand, we can reduce wait times for disembarking passengers by
over 75%. Independently, the Neighborhood Event Detector uses outlier measures
of currently operating buses to detect and spatiotemporally localize dynamic
urban events, as much as 1.5 hours in advance, with a localization error of 450
meters.
</summary>
    <author>
      <name>Lakmal Meegahapola</name>
    </author>
    <author>
      <name>Thivya Kandappu</name>
    </author>
    <author>
      <name>Kasthuri Jayarajah</name>
    </author>
    <author>
      <name>Leman Akoglu</name>
    </author>
    <author>
      <name>Shili Xiang</name>
    </author>
    <author>
      <name>Archan Misra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3307334.3326091</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3307334.3326091" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM MobiSys 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.06116v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.06116v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1612.07403v2</id>
    <updated>2017-04-04T17: 49: 19Z</updated>
    <published>2016-12-22T00: 37: 42Z</published>
    <title>Efficient Action Detection in Untrimmed Videos via Multi-Task Learning</title>
    <summary>  This paper studies the joint learning of action recognition and temporal
localization in long, untrimmed videos. We employ a multi-task learning
framework that performs the three highly related steps of action proposal,
action recognition, and action localization refinement in parallel instead of
the standard sequential pipeline that performs the steps in order. We develop a
novel temporal actionness regression module that estimates what proportion of a
clip contains action. We use it for temporal localization but it could have
other applications like video retrieval, surveillance, summarization, etc. We
also introduce random shear augmentation during training to simulate viewpoint
change. We evaluate our framework on three popular video benchmarks. Results
demonstrate that our joint model is efficient in terms of storage and
computation in that we do not need to compute and cache dense trajectory
features, and that it is several times faster than its sequential ConvNets
counterpart. Yet, despite being more efficient, it outperforms state-of-the-art
methods with respect to accuracy.
</summary>
    <author>
      <name>Yi Zhu</name>
    </author>
    <author>
      <name>Shawn Newsam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">WACV 2017 camera ready, minor updates about test time efficiency</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.07403v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.07403v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1912.04051v1</id>
    <updated>2019-12-09T14: 03: 21Z</updated>
    <published>2019-12-09T14: 03: 21Z</published>
    <title>Building Executable Secure Design Models for Smart Contracts with Formal
  Methods</title>
    <summary>  Smart contracts are appealing because they are self-executing business
agreements between parties with the predefined and immutable obligations and
rights. However, as with all software, smart contracts may contain
vulnerabilities because of design flaws, which may be exploited by one of the
parties to defraud the others. In this paper, we demonstrate a systematic
approach to building secure design models for smart contracts using formal
methods. To build the secure models, we first model the behaviors of
participating parties as state machines, and then, we model the predefined
obligations and rights of contracts, which specify the interactions among state
machines for achieving the business goal. After that, we illustrate executable
secure model design patterns in TLA+ (Temporal Logic of Actions) to against
well-known smart contract vulnerabilities in terms of state machines and
obligations and rights at the design level. These vulnerabilities are found in
Ethereum contracts, including Call to the unknown, Gasless send, Reentrancy,
Lost in the transfer, and Unpredictable state. The resultant TLA+
specifications are called secure models. We illustrate our approach to detect
the vulnerabilities using a real-estate contract example at the design level.
</summary>
    <author>
      <name>Weifeng Xu</name>
    </author>
    <author>
      <name>Glenn A. Fink</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 3rd Workshop on Trusted Smart Contracts In Association with
  Financial Cryptography 2019, St. Kitts, Feb. 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1912.04051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.04051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2203.12891v1</id>
    <updated>2022-03-24T07: 25: 23Z</updated>
    <published>2022-03-24T07: 25: 23Z</published>
    <title>An Ensemble Approach for Facial Expression Analysis in Video</title>
    <summary>  Human emotions recognization contributes to the development of human-computer
interaction. The machines understanding human emotions in the real world will
significantly contribute to life in the future. This paper will introduce the
Affective Behavior Analysis in-the-wild (ABAW3) 2022 challenge. The paper
focuses on solving the problem of the valence-arousal estimation and action
unit detection. For valence-arousal estimation, we conducted two stages:
creating new features from multimodel and temporal learning to predict
valence-arousal. First, we make new features; the Gated Recurrent Unit (GRU)
and Transformer are combined using a Regular Networks (RegNet) feature, which
is extracted from the image. The next step is the GRU combined with Local
Attention to predict valence-arousal. The Concordance Correlation Coefficient
(CCC) was used to evaluate the model.
</summary>
    <author>
      <name>Hong-Hai Nguyen</name>
    </author>
    <author>
      <name>Van-Thong Huynh</name>
    </author>
    <author>
      <name>Soo-Hyung Kim</name>
    </author>
    <link href="http://arxiv.org/abs/2203.12891v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.12891v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2303.08356v3</id>
    <updated>2023-09-06T08: 15: 03Z</updated>
    <published>2023-03-15T04: 15: 57Z</published>
    <title>Leveraging TCN and Transformer for effective visual-audio fusion in
  continuous emotion recognition</title>
    <summary>  Human emotion recognition plays an important role in human-computer
interaction. In this paper, we present our approach to the Valence-Arousal (VA)
Estimation Challenge, Expression (Expr) Classification Challenge, and Action
Unit (AU) Detection Challenge of the 5th Workshop and Competition on Affective
Behavior Analysis in-the-wild (ABAW). Specifically, we propose a novel
multi-modal fusion model that leverages Temporal Convolutional Networks (TCN)
and Transformer to enhance the performance of continuous emotion recognition.
Our model aims to effectively integrate visual and audio information for
improved accuracy in recognizing emotions. Our model outperforms the baseline
and ranks 3 in the Expression Classification challenge.
</summary>
    <author>
      <name>Weiwei Zhou</name>
    </author>
    <author>
      <name>Jiada Lu</name>
    </author>
    <author>
      <name>Zhaolong Xiong</name>
    </author>
    <author>
      <name>Weifeng Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CVPRW59228.2023.00610</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CVPRW59228.2023.00610" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition
  Workshops (CVPRW)</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.08356v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.08356v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2407.13923v1</id>
    <updated>2024-07-18T22: 24: 57Z</updated>
    <published>2024-07-18T22: 24: 57Z</published>
    <title>Existence of Trust-field in Vehicular Ad Hoc Networks: Empirical
  Evidence</title>
    <summary>  Vehicular Ad Hoc Networks (VANETs) play a crucial role in enhancing road
safety and traffic efficiency by enabling communication between vehicles (V2V)
and between vehicles and infrastructure (V2I). Robust trust management is
necessary to ensure the reliability of information in decentralized systems.
This paper presents the notion of a ``Trust Field" in VANETs, conceptualized as
the behavior of the nodes that represents trust levels evolving in both spatial
and temporal dimensions. Using the LogitTrust model, we provide empirical
evidence of how trust fields in vehicular networks change over time in
different scenarios, including when malicious nodes are present. The results of
our study demonstrate that the trust domain can adjust to fluctuations in
network conditions, thereby offering a comprehensive metric for assessing the
reliability of nodes. This innovative method improves the dependability of
VANET applications by efficiently detecting and mitigating malicious actions.
</summary>
    <author>
      <name>Md Mahmudul Islam</name>
    </author>
    <author>
      <name>Shaurya Agarwal</name>
    </author>
    <link href="http://arxiv.org/abs/2407.13923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.13923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.09929v1</id>
    <updated>2025-03-13T01: 02: 06Z</updated>
    <published>2025-03-13T01: 02: 06Z</published>
    <title>Emotion Recognition with CLIP and Sequential Learning</title>
    <summary>  Human emotion recognition plays a crucial role in facilitating seamless
interactions between humans and computers. In this paper, we present our
innovative methodology for tackling the Valence-Arousal (VA) Estimation
Challenge, the Expression Recognition Challenge, and the Action Unit (AU)
Detection Challenge, all within the framework of the 8th Workshop and
Competition on Affective Behavior Analysis in-the-wild (ABAW).
  Our approach introduces a novel framework aimed at enhancing continuous
emotion recognition. This is achieved by fine-tuning the CLIP model with the
aff-wild2 dataset, which provides annotated expression labels. The result is a
fine-tuned model that serves as an efficient visual feature extractor,
significantly improving its robustness. To further boost the performance of
continuous emotion recognition, we incorporate Temporal Convolutional Network
(TCN) modules alongside Transformer Encoder modules into our system
architecture. The integration of these advanced components allows our model to
outperform baseline performance, demonstrating its ability to recognize human
emotions with greater accuracy and efficiency.
</summary>
    <author>
      <name>Weiwei Zhou</name>
    </author>
    <author>
      <name>Chenkun Ling</name>
    </author>
    <author>
      <name>Zefeng Cai</name>
    </author>
    <link href="http://arxiv.org/abs/2503.09929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.09929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2301.01380v2</id>
    <updated>2023-05-19T22: 23: 48Z</updated>
    <published>2023-01-03T22: 22: 34Z</published>
    <title>Ego-Only: Egocentric Action Detection without Exocentric Transferring</title>
    <summary>  We present Ego-Only, the first approach that enables state-of-the-art action
detection on egocentric (first-person) videos without any form of exocentric
(third-person) transferring. Despite the content and appearance gap separating
the two domains, large-scale exocentric transferring has been the default
choice for egocentric action detection. This is because prior works found that
egocentric models are difficult to train from scratch and that transferring
from exocentric representations leads to improved accuracy. However, in this
paper, we revisit this common belief. Motivated by the large gap separating the
two domains, we propose a strategy that enables effective training of
egocentric models without exocentric transferring. Our Ego-Only approach is
simple. It trains the video representation with a masked autoencoder finetuned
for temporal segmentation. The learned features are then fed to an
off-the-shelf temporal action localization method to detect actions. We find
that this renders exocentric transferring unnecessary by showing remarkably
strong results achieved by this simple Ego-Only approach on three established
egocentric video datasets: Ego4D, EPIC-Kitchens-100, and Charades-Ego. On both
action detection and action recognition, Ego-Only outperforms previous best
exocentric transferring methods that use orders of magnitude more labels.
Ego-Only sets new state-of-the-art results on these datasets and benchmarks
without exocentric data.
</summary>
    <author>
      <name>Huiyu Wang</name>
    </author>
    <author>
      <name>Mitesh Kumar Singh</name>
    </author>
    <author>
      <name>Lorenzo Torresani</name>
    </author>
    <link href="http://arxiv.org/abs/2301.01380v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.01380v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1807.10982v1</id>
    <updated>2018-07-28T22: 48: 27Z</updated>
    <published>2018-07-28T22: 48: 27Z</published>
    <title>Actor-Centric Relation Network</title>
    <summary>  Current state-of-the-art approaches for spatio-temporal action localization
rely on detections at the frame level and model temporal context with 3D
ConvNets. Here, we go one step further and model spatio-temporal relations to
capture the interactions between human actors, relevant objects and scene
elements essential to differentiate similar human actions. Our approach is
weakly supervised and mines the relevant elements automatically with an
actor-centric relational network (ACRN). ACRN computes and accumulates
pair-wise relation information from actor and global scene features, and
generates relation features for action classification. It is implemented as
neural networks and can be trained jointly with an existing action detection
system. We show that ACRN outperforms alternative approaches which capture
relation information, and that the proposed framework improves upon the
state-of-the-art performance on JHMDB and AVA. A visualization of the learned
relation features confirms that our approach is able to attend to the relevant
relations for each action.
</summary>
    <author>
      <name>Chen Sun</name>
    </author>
    <author>
      <name>Abhinav Shrivastava</name>
    </author>
    <author>
      <name>Carl Vondrick</name>
    </author>
    <author>
      <name>Kevin Murphy</name>
    </author>
    <author>
      <name>Rahul Sukthankar</name>
    </author>
    <author>
      <name>Cordelia Schmid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2018 camera ready</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2109.03393v3</id>
    <updated>2022-11-18T05: 37: 41Z</updated>
    <published>2021-09-08T01: 51: 51Z</published>
    <title>Learning to Discriminate Information for Online Action Detection:
  Analysis and Application</title>
    <summary>  Online action detection, which aims to identify an ongoing action from a
streaming video, is an important subject in real-world applications. For this
task, previous methods use recurrent neural networks for modeling temporal
relations in an input sequence. However, these methods overlook the fact that
the input image sequence includes not only the action of interest but
background and irrelevant actions. This would induce recurrent units to
accumulate unnecessary information for encoding features on the action of
interest. To overcome this problem, we propose a novel recurrent unit, named
Information Discrimination Unit (IDU), which explicitly discriminates the
information relevancy between an ongoing action and others to decide whether to
accumulate the input information. This enables learning more discriminative
representations for identifying an ongoing action. In this paper, we further
present a new recurrent unit, called Information Integration Unit (IIU), for
action anticipation. Our IIU exploits the outputs from IDU as pseudo action
labels as well as RGB frames to learn enriched features of observed actions
effectively. In experiments on TVSeries and THUMOS-14, the proposed methods
outperform state-of-the-art methods by a significant margin in online action
detection and action anticipation. Moreover, we demonstrate the effectiveness
of the proposed units by conducting comprehensive ablation studies.
</summary>
    <author>
      <name>Sumin Lee</name>
    </author>
    <author>
      <name>Hyunjun Eun</name>
    </author>
    <author>
      <name>Jinyoung Moon</name>
    </author>
    <author>
      <name>Seokeon Choi</name>
    </author>
    <author>
      <name>Yoonhyung Kim</name>
    </author>
    <author>
      <name>Chanho Jung</name>
    </author>
    <author>
      <name>Changick Kim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TPAMI.2022.3204808</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TPAMI.2022.3204808" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in TPAMI. arXiv admin note: substantial text overlap with
  arXiv: 1912.04461</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.03393v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.03393v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2103.04680v1</id>
    <updated>2021-03-08T11: 42: 05Z</updated>
    <published>2021-03-08T11: 42: 05Z</published>
    <title>Time and Frequency Network for Human Action Detection in Videos</title>
    <summary>  Currently, spatiotemporal features are embraced by most deep learning
approaches for human action detection in videos, however, they neglect the
important features in frequency domain. In this work, we propose an end-to-end
network that considers the time and frequency features simultaneously, named
TFNet. TFNet holds two branches, one is time branch formed of three-dimensional
convolutional neural network(3D-CNN), which takes the image sequence as input
to extract time features; and the other is frequency branch, extracting
frequency features through two-dimensional convolutional neural network(2D-CNN)
from DCT coefficients. Finally, to obtain the action patterns, these two
features are deeply fused under the attention mechanism. Experimental results
on the JHMDB51-21 and UCF101-24 datasets demonstrate that our approach achieves
remarkable performance for frame-mAP.
</summary>
    <author>
      <name>Changhai Li</name>
    </author>
    <author>
      <name>Huawei Chen</name>
    </author>
    <author>
      <name>Jingqing Lu</name>
    </author>
    <author>
      <name>Yang Huang</name>
    </author>
    <author>
      <name>Yingying Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,
                    6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.04680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.04680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2108.05821v1</id>
    <updated>2021-08-12T16: 01: 34Z</updated>
    <published>2021-08-12T16: 01: 34Z</published>
    <title>TF-Blender: Temporal Feature Blender for Video Object Detection</title>
    <summary>  Video objection detection is a challenging task because isolated video frames
may encounter appearance deterioration, which introduces great confusion for
detection. One of the popular solutions is to exploit the temporal information
and enhance per-frame representation through aggregating features from
neighboring frames. Despite achieving improvements in detection, existing
methods focus on the selection of higher-level video frames for aggregation
rather than modeling lower-level temporal relations to increase the feature
representation. To address this limitation, we propose a novel solution named
TF-Blender,which includes three modules: 1) Temporal relation mod-els the
relations between the current frame and its neighboring frames to preserve
spatial information. 2). Feature adjustment enriches the representation of
every neigh-boring feature map; 3) Feature blender combines outputs from the
first two modules and produces stronger features for the later detection tasks.
For its simplicity, TF-Blender can be effortlessly plugged into any detection
network to improve detection behavior. Extensive evaluations on ImageNet VID
and YouTube-VIS benchmarks indicate the performance guarantees of using
TF-Blender on recent state-of-the-art methods.
</summary>
    <author>
      <name>Yiming Cui</name>
    </author>
    <author>
      <name>Liqi Yan</name>
    </author>
    <author>
      <name>Zhiwen Cao</name>
    </author>
    <author>
      <name>Dongfang Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2108.05821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.05821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1608.06495v1</id>
    <updated>2016-08-23T13: 08: 30Z</updated>
    <published>2016-08-23T13: 08: 30Z</published>
    <title>Searching Action Proposals via Spatial Actionness Estimation and
  Temporal Path Inference and Tracking</title>
    <summary>  In this paper, we address the problem of searching action proposals in
unconstrained video clips. Our approach starts from actionness estimation on
frame-level bounding boxes, and then aggregates the bounding boxes belonging to
the same actor across frames via linking, associating, tracking to generate
spatial-temporal continuous action paths. To achieve the target, a novel
actionness estimation method is firstly proposed by utilizing both human
appearance and motion cues. Then, the association of the action paths is
formulated as a maximum set coverage problem with the results of actionness
estimation as a priori. To further promote the performance, we design an
improved optimization objective for the problem and provide a greedy search
algorithm to solve it. Finally, a tracking-by-detection scheme is designed to
further refine the searched action paths. Extensive experiments on two
challenging datasets, UCF-Sports and UCF-101, show that the proposed approach
advances state-of-the-art proposal generation performance in terms of both
accuracy and proposal quantity.
</summary>
    <author>
      <name>Nannan Li</name>
    </author>
    <author>
      <name>Dan Xu</name>
    </author>
    <author>
      <name>Zhenqiang Ying</name>
    </author>
    <author>
      <name>Zhihao Li</name>
    </author>
    <author>
      <name>Ge Li</name>
    </author>
    <link href="http://arxiv.org/abs/1608.06495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1811.08815v5</id>
    <updated>2019-11-06T21: 37: 45Z</updated>
    <published>2018-11-21T16: 34: 53Z</published>
    <title>Learning Motion in Feature Space: Locally-Consistent Deformable
  Convolution Networks for Fine-Grained Action Detection</title>
    <summary>  Fine-grained action detection is an important task with numerous applications
in robotics and human-computer interaction. Existing methods typically utilize
a two-stage approach including extraction of local spatio-temporal features
followed by temporal modeling to capture long-term dependencies. While most
recent papers have focused on the latter (long-temporal modeling), here, we
focus on producing features capable of modeling fine-grained motion more
efficiently. We propose a novel locally-consistent deformable convolution,
which utilizes the change in receptive fields and enforces a local coherency
constraint to capture motion information effectively. Our model jointly learns
spatio-temporal features (instead of using independent spatial and temporal
streams). The temporal component is learned from the feature space instead of
pixel space, e.g. optical flow. The produced features can be flexibly used in
conjunction with other long-temporal modeling networks, e.g. ST-CNN,
DilatedTCN, and ED-TCN. Overall, our proposed approach robustly outperforms the
original long-temporal models on two fine-grained action datasets: 50 Salads
and GTEA, achieving F1 scores of 80.22% and 75.39% respectively.
</summary>
    <author>
      <name>Khoi-Nguyen C. Mac</name>
    </author>
    <author>
      <name>Dhiraj Joshi</name>
    </author>
    <author>
      <name>Raymond A. Yeh</name>
    </author>
    <author>
      <name>Jinjun Xiong</name>
    </author>
    <author>
      <name>Rogerio S. Feris</name>
    </author>
    <author>
      <name>Minh N. Do</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ICCV 2019 as oral</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.08815v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.08815v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2311.12541v1</id>
    <updated>2023-11-21T11: 39: 04Z</updated>
    <published>2023-11-21T11: 39: 04Z</published>
    <title>Insights into the interplay between hydrodynamic and acoustic fields in
  a turbulent combustor via community-based dimensionality reduction of
  vortical networks</title>
    <summary>  This study examines the interplay between acoustic pressure oscillations and
interactions between regions of intense vorticity in the reactive field of a
turbulent combustor. These regions of intense vortical interactions are
identified as vortical communities in the network space of weighted directed
vortical networks constructed from two-dimensional velocity data. The vortical
interactions in the high-dimensional reactive flow are condensed into a
low-dimensional network representation by leveraging the inter-community
strengths and the weighted community centroids. Subsequently, we show that the
mean and the maximum of all inter-community interactions exhibit a strong
delayed correlation with the acoustic pressure oscillations during the state of
thermoacoustic instability. In contrast, during the state of combustion noise,
the correlation between the acoustic pressure oscillations and the
inter-community interactions is lost due to the incoherent spatiotemporal
behaviour of acoustic and hydrodynamic fields in turbulent thermoacoustic
system. Spatiotemporal evolution of pairs of vortical communities with the
maximum inter-community interactions provides insight into explaining the
critical regions detected in the reaction field during the states of
intermittency and thermoacoustic instability in previous studies. We further
demonstrate that the high correlations between network measures and acoustic
pressure oscillations during the state of thermoacoustic instability weaken
when steady air jets are introduced within the critical region to suppress the
thermoacoustic oscillations. This work sheds light on the intricate
relationship between the vortical interactions and the acoustic behaviour in a
turbulent combustor, thus offering insights for understanding thermoacoustic
instabilities in combustion systems and for unsteady flows in general fluid
dynamics literature.
</summary>
    <author>
      <name>Ankit Sahay</name>
    </author>
    <author>
      <name>Muralikrishnan Gopalakrishnan Meena</name>
    </author>
    <author>
      <name>R. I. Sujith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages,
                    18 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.12541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.12541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1806.06496v3</id>
    <updated>2019-06-23T00: 23: 12Z</updated>
    <published>2018-06-18T04: 28: 18Z</published>
    <title>Power-Grid Controller Anomaly Detection with Enhanced Temporal Deep
  Learning</title>
    <summary>  Controllers of security-critical cyber-physical systems, like the power grid,
are a very important class of computer systems. Attacks against the control
code of a power-grid system, especially zero-day attacks, can be catastrophic.
Earlier detection of the anomalies can prevent further damage. However,
detecting zero-day attacks is extremely challenging because they have no known
code and have unknown behavior. Furthermore, if data collected from the
controller is transferred to a server through networks for analysis and
detection of anomalous behavior, this creates a very large attack surface and
also delays detection.
  In order to address this problem, we propose Reconstruction Error
Distribution (RED) of Hardware Performance Counters (HPCs), and a data-driven
defense system based on it. Specifically, we first train a temporal deep
learning model, using only normal HPC readings from legitimate processes that
run daily in these power-grid systems, to model the normal behavior of the
power-grid controller. Then, we run this model using real-time data from
commonly available HPCs. We use the proposed RED to enhance the temporal deep
learning detection of anomalous behavior, by estimating distribution deviations
from the normal behavior with an effective statistical test. Experimental
results on a real power-grid controller show that we can detect anomalous
behavior with high accuracy (&gt;99.9%), nearly zero false positives and short
(&lt;360ms) latency.
</summary>
    <author>
      <name>Zecheng He</name>
    </author>
    <author>
      <name>Aswin Raghavan</name>
    </author>
    <author>
      <name>Guangyuan Hu</name>
    </author>
    <author>
      <name>Sek Chai</name>
    </author>
    <author>
      <name>Ruby Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in the 18th IEEE International Conference on Trust, Security
  and Privacy in Computing and Communications (TrustCom'19)</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06496v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06496v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1704.04952v2</id>
    <updated>2017-08-06T14: 18: 44Z</updated>
    <published>2017-04-17T13: 04: 46Z</published>
    <title>AMTnet: Action-Micro-Tube Regression by End-to-end Trainable Deep
  Architecture</title>
    <summary>  Dominant approaches to action detection can only provide sub-optimal
solutions to the problem, as they rely on seeking frame-level detections, to
later compose them into "action tubes" in a post-processing step. With this
paper we radically depart from current practice, and take a first step towards
the design and implementation of a deep network architecture able to classify
and regress whole video subsets, so providing a truly optimal solution of the
action detection problem. In this work, in particular, we propose a novel deep
net framework able to regress and classify 3D region proposals spanning two
successive video frames, whose core is an evolution of classical region
proposal networks (RPNs). As such, our 3D-RPN net is able to effectively encode
the temporal aspect of actions by purely exploiting appearance, as opposed to
methods which heavily rely on expensive flow maps. The proposed model is
end-to-end trainable and can be jointly optimised for action localisation and
classification in a single step. At test time the network predicts
"micro-tubes" encompassing two successive frames, which are linked up into
complete action tubes via a new algorithm which exploits the temporal encoding
learned by the network and cuts computation time by 50%. Promising results on
the J-HMDB-21 and UCF-101 action detection datasets show that our model does
outperform the state-of-the-art when relying purely on appearance.
</summary>
    <author>
      <name>Suman Saha</name>
    </author>
    <author>
      <name>Gurkirt Singh</name>
    </author>
    <author>
      <name>Fabio Cuzzolin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Update to version in ICCV 2017 proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.04952v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04952v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2408.15996v3</id>
    <updated>2024-12-05T14: 38: 12Z</updated>
    <published>2024-08-28T17: 59: 05Z</published>
    <title>Spatio-Temporal Context Prompting for Zero-Shot Action Detection</title>
    <summary>  Spatio-temporal action detection encompasses the tasks of localizing and
classifying individual actions within a video. Recent works aim to enhance this
process by incorporating interaction modeling, which captures the relationship
between people and their surrounding context. However, these approaches have
primarily focused on fully-supervised learning, and the current limitation lies
in the lack of generalization capability to recognize unseen action categories.
In this paper, we aim to adapt the pretrained image-language models to detect
unseen actions. To this end, we propose a method which can effectively leverage
the rich knowledge of visual-language models to perform Person-Context
Interaction. Meanwhile, our Context Prompting module will utilize contextual
information to prompt labels, thereby enhancing the generation of more
representative text features. Moreover, to address the challenge of recognizing
distinct actions by multiple people at the same timestamp, we design the
Interest Token Spotting mechanism which employs pretrained visual knowledge to
find each person's interest context tokens, and then these tokens will be used
for prompting to generate text features tailored to each individual. To
evaluate the ability to detect unseen actions, we propose a comprehensive
benchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that our
method achieves superior results compared to previous approaches and can be
further extended to multi-action videos, bringing it closer to real-world
applications. The code and data can be found in
https: //webber2933.github.io/ST-CLIP-project-page.
</summary>
    <author>
      <name>Wei-Jhe Huang</name>
    </author>
    <author>
      <name>Min-Hung Chen</name>
    </author>
    <author>
      <name>Shang-Hong Lai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by WACV2025. Project page:
  https: //webber2933.github.io/ST-CLIP-project-page</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.15996v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.15996v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2310.16267v4</id>
    <updated>2024-09-09T10: 52: 54Z</updated>
    <published>2023-10-25T00: 46: 26Z</published>
    <title>Student Classroom Behavior Detection based on Spatio-Temporal Network
  and Multi-Model Fusion</title>
    <summary>  Using deep learning methods to detect students' classroom behavior
automatically is a promising approach for analyzing their class performance and
improving teaching effectiveness. However, the lack of publicly available
spatio-temporal datasets on student behavior, as well as the high cost of
manually labeling such datasets, pose significant challenges for researchers in
this field. To address this issue, we proposed a method for extending the
spatio-temporal behavior dataset in Student Classroom Scenarios
(SCB-ST-Dataset4) through image dataset. Our SCB-ST-Dataset4 comprises 757265
images with 25810 labels, focusing on 3 behaviors: hand-raising, reading,
writing. Our proposed method can rapidly generate spatio-temporal behavior
datasets without requiring extra manual labeling. Furthermore, we proposed a
Behavior Similarity Index (BSI) to explore the similarity of behaviors. We
evaluated the dataset using the YOLOv5, YOLOv7, YOLOv8, and SlowFast
algorithms, achieving a mean average precision (map) of up to 82.3%. Last, we
fused multiple models to generate student behavior-related data from various
perspectives. The experiment further demonstrates the effectiveness of our
method. And SCB-ST-Dataset4 provides a robust foundation for future research in
student behavior detection, potentially contributing to advancements in this
field. The SCB-ST-Dataset4 is available for download at:
https: //github.com/Whiffe/SCB-dataset.
</summary>
    <author>
      <name>Fan Yang</name>
    </author>
    <author>
      <name>Xiaofei Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv: 2310.02522;
  text overlap with arXiv: 2306.03318</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.16267v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.16267v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2105.12976v1</id>
    <updated>2021-05-27T07: 40: 30Z</updated>
    <published>2021-05-27T07: 40: 30Z</published>
    <title>Spatio-Temporal Investigation of Brain-Wide Sequences</title>
    <summary>  In "The Organization of Behavior" (Hebb,
                    1949), Hebb suggested that the
propagation of activity between transiently grouped neurons plays an important
role in behavior. Since then, multiple studies have provided evidence
supporting Hebb's claim; however, most findings have been found locally in
confined brain regions during unimodal tasks. Here we report on brain-wide
behavioral-specific sequences in humans performing a multimodal task. To
investigate the structure of these sequences, we used MEG to record brain
activity in multiple brain regions simultaneously in participants performing a
sensory-motor synchronization task. We detected local transient events
corresponding to synchronously activating populations of pyramidal neurons and
searched for their global organization as spatio-temporal patterns of
activation sequences between distant neural populations. We focused our
analysis on two types of spatio-temporal patterns: the most frequently
repeating patterns and the most discriminative patterns, to concentrate on
patterns with high relevancy to behavior. The findings revealed that global
temporally precise sequences can be found and that these sequences have
partially stereotypical characteristics, both temporally and spatially, with
consistent properties across subjects. By implementing a simplistic
single-trial decoding approach, we found that brain-wide sequences have a
temporal precision of 17-31 milliseconds, which resembles the temporal
precision found locally in neural assemblies.
</summary>
    <author>
      <name>Ohad Felsenstein</name>
    </author>
    <author>
      <name>Moshe Abeles</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages,
                    10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.12976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.12976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2412.02808v1</id>
    <updated>2024-12-03T20: 19: 20Z</updated>
    <published>2024-12-03T20: 19: 20Z</published>
    <title>Temporally Consistent Dynamic Scene Graphs: An End-to-End Approach for
  Action Tracklet Generation</title>
    <summary>  Understanding video content is pivotal for advancing real-world applications
like activity recognition, autonomous systems, and human-computer interaction.
While scene graphs are adept at capturing spatial relationships between objects
in individual frames, extending these representations to capture dynamic
interactions across video sequences remains a significant challenge. To address
this, we present TCDSG, Temporally Consistent Dynamic Scene Graphs, an
innovative end-to-end framework that detects, tracks, and links subject-object
relationships across time, generating action tracklets, temporally consistent
sequences of entities and their interactions. Our approach leverages a novel
bipartite matching mechanism, enhanced by adaptive decoder queries and feedback
loops, ensuring temporal coherence and robust tracking over extended sequences.
This method not only establishes a new benchmark by achieving over 60%
improvement in temporal recall@k on the Action Genome, OpenPVSG, and MEVA
datasets but also pioneers the augmentation of MEVA with persistent object ID
annotations for comprehensive tracklet generation. By seamlessly integrating
spatial and temporal dynamics, our work sets a new standard in multi-frame
video analysis, opening new avenues for high-impact applications in
surveillance, autonomous navigation, and beyond.
</summary>
    <author>
      <name>Raphael Ruschel</name>
    </author>
    <author>
      <name>Md Awsafur Rahman</name>
    </author>
    <author>
      <name>Hardik Prajapati</name>
    </author>
    <author>
      <name>Suya You</name>
    </author>
    <author>
      <name>B. S. Manjuanth</name>
    </author>
    <link href="http://arxiv.org/abs/2412.02808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.02808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1401.3291v2</id>
    <updated>2014-01-16T19: 09: 47Z</updated>
    <published>2014-01-14T19: 15: 04Z</published>
    <title>Detection of Anomalous Crowd Behavior Using Spatio-Temporal
  Multiresolution Model and Kronecker Sum Decompositions</title>
    <summary>  In this work we consider the problem of detecting anomalous spatio-temporal
behavior in videos. Our approach is to learn the normative multiframe pixel
joint distribution and detect deviations from it using a likelihood based
approach. Due to the extreme lack of available training samples relative to the
dimension of the distribution, we use a mean and covariance approach and
consider methods of learning the spatio-temporal covariance in the low-sample
regime. Our approach is to estimate the covariance using parameter reduction
and sparse models. The first method considered is the representation of the
covariance as a sum of Kronecker products as in (Greenewald et al 2013), which
is found to be an accurate approximation in this setting. We propose learning
algorithms relevant to our problem. We then consider the sparse multiresolution
model of (Choi et al 2010) and apply the Kronecker product methods to it for
further parameter reduction, as well as introducing modifications for enhanced
efficiency and greater applicability to spatio-temporal covariance matrices. We
apply our methods to the detection of crowd behavior anomalies in the
University of Minnesota crowd anomaly dataset, and achieve competitive results.
</summary>
    <author>
      <name>Kristjan Greenewald</name>
    </author>
    <author>
      <name>Alfred Hero</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.3291v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.3291v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1405.0085v1</id>
    <updated>2014-05-01T03: 53: 36Z</updated>
    <published>2014-05-01T03: 53: 36Z</published>
    <title>Relative Facial Action Unit Detection</title>
    <summary>  This paper presents a subject-independent facial action unit (AU) detection
method by introducing the concept of relative AU detection, for scenarios where
the neutral face is not provided. We propose a new classification objective
function which analyzes the temporal neighborhood of the current frame to
decide if the expression recently increased, decreased or showed no change.
This approach is a significant change from the conventional absolute method
which decides about AU classification using the current frame, without an
explicit comparison with its neighboring frames. Our proposed method improves
robustness to individual differences such as face scale and shape, age-related
wrinkles, and transitions among expressions (e.g., lower intensity of
expressions). Our experiments on three publicly available datasets (Extended
Cohn-Kanade (CK+), Bosphorus, and DISFA databases) show significant improvement
of our approach over conventional absolute techniques. Keywords: facial action
coding system (FACS); relative facial action unit detection; temporal
information;
</summary>
    <author>
      <name>Mahmoud Khademi</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at IEEE Winter Conference on Applications of Computer
  Vision, Steamboat Springs Colorado, USA,
                    2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.0085v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.0085v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2012.14371v3</id>
    <updated>2021-08-28T17: 35: 50Z</updated>
    <published>2020-12-28T17: 27: 18Z</published>
    <title>Tensor Representations for Action Recognition</title>
    <summary>  Human actions in video sequences are characterized by the complex interplay
between spatial features and their temporal dynamics. In this paper, we propose
novel tensor representations for compactly capturing such higher-order
relationships between visual features for the task of action recognition. We
propose two tensor-based feature representations, viz. (i) sequence
compatibility kernel (SCK) and (ii) dynamics compatibility kernel (DCK). SCK
builds on the spatio-temporal correlations between features, whereas DCK
explicitly models the action dynamics of a sequence. We also explore
generalization of SCK, coined SCK(+), that operates on subsequences to capture
the local-global interplay of correlations, which can incorporate multi-modal
inputs e.g., skeleton 3D body-joints and per-frame classifier scores obtained
from deep learning models trained on videos. We introduce linearization of
these kernels that lead to compact and fast descriptors. We provide experiments
on (i) 3D skeleton action sequences, (ii) fine-grained video sequences, and
(iii) standard non-fine-grained videos. As our final representations are
tensors that capture higher-order relationships of features, they relate to
co-occurrences for robust fine-grained recognition. We use higher-order tensors
and so-called Eigenvalue Power Normalization (EPN) which have been long
speculated to perform spectral detection of higher-order occurrences, thus
detecting fine-grained relationships of features rather than merely count
features in action sequences. We prove that a tensor of order r, built from Z*
dimensional features, coupled with EPN indeed detects if at least one
higher-order occurrence is `projected' into one of its binom(Z*,r) subspaces of
dim. r represented by the tensor, thus forming a Tensor Power Normalization
metric endowed with binom(Z*,r) such `detectors'.
</summary>
    <author>
      <name>Piotr Koniusz</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Anoop Cherian</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TPAMI.2021.3107160</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TPAMI.2021.3107160" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published with TPAMI, 2020. arXiv admin note: text overlap with
  arXiv: 1604.00239</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.14371v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.14371v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1508.04900v3</id>
    <updated>2017-02-24T16: 37: 27Z</updated>
    <published>2015-08-20T07: 22: 55Z</published>
    <title>Detecting intraday financial market states using temporal clustering</title>
    <summary>  We propose the application of a high-speed maximum likelihood clustering
algorithm to detect temporal financial market states, using correlation
matrices estimated from intraday market microstructure features. We first
determine the ex-ante intraday temporal cluster configurations to identify
market states, and then study the identified temporal state features to extract
state signature vectors which enable online state detection. The state
signature vectors serve as low-dimensional state descriptors which can be used
in learning algorithms for optimal planning in the high-frequency trading
domain. We present a feasible scheme for real-time intraday state detection
from streaming market data feeds. This study identifies an interesting
hierarchy of system behaviour which motivates the need for time-scale-specific
state space reduction for participating agents.
</summary>
    <author>
      <name>Dieter Hendricks</name>
    </author>
    <author>
      <name>Tim Gebbie</name>
    </author>
    <author>
      <name>Diane Wilcox</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/14697688.2016.1171378</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/14697688.2016.1171378" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages,
                    16 figures,
                    8 tables, published in Quantitative Finance</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Quantitative Finance, (2016),
                    16: 11,
                    1657-1678</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1508.04900v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.04900v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.TR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.TR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1904.04189v1</id>
    <updated>2019-04-08T17: 05: 31Z</updated>
    <published>2019-04-08T17: 05: 31Z</published>
    <title>Unsupervised learning of action classes with continuous temporal
  embedding</title>
    <summary>  The task of temporally detecting and segmenting actions in untrimmed videos
has seen an increased attention recently. One problem in this context arises
from the need to define and label action boundaries to create annotations for
training which is very time and cost intensive. To address this issue, we
propose an unsupervised approach for learning action classes from untrimmed
video sequences. To this end, we use a continuous temporal embedding of
framewise features to benefit from the sequential nature of activities. Based
on the latent space created by the embedding, we identify clusters of temporal
segments across all videos that correspond to semantic meaningful action
classes. The approach is evaluated on three challenging datasets, namely the
Breakfast dataset, YouTube Instructions, and the 50Salads dataset. While
previous works assumed that the videos contain the same high level activity, we
furthermore show that the proposed approach can also be applied to a more
general setting where the content of the videos is unknown.
</summary>
    <author>
      <name>Anna Kukleva</name>
    </author>
    <author>
      <name>Hilde Kuehne</name>
    </author>
    <author>
      <name>Fadime Sener</name>
    </author>
    <author>
      <name>Juergen Gall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.04189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.04189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2207.07097v1</id>
    <updated>2022-07-14T17: 46: 37Z</updated>
    <published>2022-07-14T17: 46: 37Z</published>
    <title>ReAct: Temporal Action Detection with Relational Queries</title>
    <summary>  This work aims at advancing temporal action detection (TAD) using an
encoder-decoder framework with action queries, similar to DETR, which has shown
great success in object detection. However, the framework suffers from several
problems if directly applied to TAD: the insufficient exploration of
inter-query relation in the decoder, the inadequate classification training due
to a limited number of training samples, and the unreliable classification
scores at inference. To this end, we first propose a relational attention
mechanism in the decoder, which guides the attention among queries based on
their relations. Moreover, we propose two losses to facilitate and stabilize
the training of action classification. Lastly, we propose to predict the
localization quality of each action query at inference in order to distinguish
high-quality queries. The proposed method, named ReAct, achieves the
state-of-the-art performance on THUMOS14, with much lower computational costs
than previous methods. Besides, extensive ablation studies are conducted to
verify the effectiveness of each proposed component. The code is available at
https: //github.com/sssste/React.
</summary>
    <author>
      <name>Dingfeng Shi</name>
    </author>
    <author>
      <name>Yujie Zhong</name>
    </author>
    <author>
      <name>Qiong Cao</name>
    </author>
    <author>
      <name>Jing Zhang</name>
    </author>
    <author>
      <name>Lin Ma</name>
    </author>
    <author>
      <name>Jia Li</name>
    </author>
    <author>
      <name>Dacheng Tao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.07097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.07097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2207.10213v1</id>
    <updated>2022-07-20T22: 15: 07Z</updated>
    <published>2022-07-20T22: 15: 07Z</published>
    <title>Spotting Temporally Precise, Fine-Grained Events in Video</title>
    <summary>  We introduce the task of spotting temporally precise, fine-grained events in
video (detecting the precise moment in time events occur). Precise spotting
requires models to reason globally about the full-time scale of actions and
locally to identify subtle frame-to-frame appearance and motion differences
that identify events during these actions. Surprisingly, we find that top
performing solutions to prior video understanding tasks such as action
detection and segmentation do not simultaneously meet both requirements. In
response, we propose E2E-Spot, a compact, end-to-end model that performs well
on the precise spotting task and can be trained quickly on a single GPU. We
demonstrate that E2E-Spot significantly outperforms recent baselines adapted
from the video action detection, segmentation, and spotting literature to the
precise spotting task. Finally, we contribute new annotations and splits to
several fine-grained sports action datasets to make these datasets suitable for
future work on precise spotting.
</summary>
    <author>
      <name>James Hong</name>
    </author>
    <author>
      <name>Haotian Zhang</name>
    </author>
    <author>
      <name>Michaël Gharbi</name>
    </author>
    <author>
      <name>Matthew Fisher</name>
    </author>
    <author>
      <name>Kayvon Fatahalian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2022; Website URL: https: //jhong93.github.io/projects/spot.html</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.10213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.10213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2310.06403v4</id>
    <updated>2024-06-07T10: 19: 33Z</updated>
    <published>2023-10-10T08: 14: 24Z</published>
    <title>Boundary Discretization and Reliable Classification Network for Temporal
  Action Detection</title>
    <summary>  Temporal action detection aims to recognize the action category and determine
each action instance's starting and ending time in untrimmed videos. The mixed
methods have achieved remarkable performance by seamlessly merging anchor-based
and anchor-free approaches. Nonetheless, there are still two crucial issues
within the mixed framework: (1) Brute-force merging and handcrafted anchor
design hinder the substantial potential and practicality of the mixed methods.
(2) Within-category predictions show a significant abundance of false
positives. In this paper, we propose a novel Boundary Discretization and
Reliable Classification Network (BDRC-Net) that addresses the issues above by
introducing boundary discretization and reliable classification modules.
Specifically, the boundary discretization module (BDM) elegantly merges
anchor-based and anchor-free approaches in the form of boundary discretization,
eliminating the need for the traditional handcrafted anchor design.
Furthermore, the reliable classification module (RCM) predicts reliable global
action categories to reduce false positives. Extensive experiments conducted on
different benchmarks demonstrate that our proposed method achieves competitive
detection performance. The code will be released at
https: //github.com/zhenyingfang/BDRC-Net.
</summary>
    <author>
      <name>Zhenying Fang</name>
    </author>
    <author>
      <name>Jun Yu</name>
    </author>
    <author>
      <name>Richang Hong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, Source code: https: //github.com/zhenyingfang/BDRC-Net</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.06403v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.06403v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2311.16446v1</id>
    <updated>2023-11-28T03: 02: 00Z</updated>
    <published>2023-11-28T03: 02: 00Z</published>
    <title>Centre Stage: Centricity-based Audio-Visual Temporal Action Detection</title>
    <summary>  Previous one-stage action detection approaches have modelled temporal
dependencies using only the visual modality. In this paper, we explore
different strategies to incorporate the audio modality, using multi-scale
cross-attention to fuse the two modalities. We also demonstrate the correlation
between the distance from the timestep to the action centre and the accuracy of
the predicted boundaries. Thus, we propose a novel network head to estimate the
closeness of timesteps to the action centre, which we call the centricity
score. This leads to increased confidence for proposals that exhibit more
precise boundaries. Our method can be integrated with other one-stage
anchor-free architectures and we demonstrate this on three recent baselines on
the EPIC-Kitchens-100 action detection benchmark where we achieve
state-of-the-art performance. Detailed ablation studies showcase the benefits
of fusing audio and our proposed centricity scores. Code and models for our
proposed method are publicly available at
https: //github.com/hanielwang/Audio-Visual-TAD.git
</summary>
    <author>
      <name>Hanyuan Wang</name>
    </author>
    <author>
      <name>Majid Mirmehdi</name>
    </author>
    <author>
      <name>Dima Damen</name>
    </author>
    <author>
      <name>Toby Perrett</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to VUA workshop at BMVC 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.16446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.16446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.12969v1</id>
    <updated>2025-03-17T09: 26: 06Z</updated>
    <published>2025-03-17T09: 26: 06Z</published>
    <title>Action tube generation by person query matching for spatio-temporal
  action detection</title>
    <summary>  This paper proposes a method for spatio-temporal action detection (STAD) that
directly generates action tubes from the original video without relying on
post-processing steps such as IoU-based linking and clip splitting. Our
approach applies query-based detection (DETR) to each frame and matches DETR
queries to link the same person across frames. We introduce the Query Matching
Module (QMM), which uses metric learning to bring queries for the same person
closer together across frames compared to queries for different people. Action
classes are predicted using the sequence of queries obtained from QMM matching,
allowing for variable-length inputs from videos longer than a single clip.
Experimental results on JHMDB, UCF101-24, and AVA datasets demonstrate that our
method performs well for large position changes of people while offering
superior computational efficiency and lower resource requirements.
</summary>
    <author>
      <name>Kazuki Omi</name>
    </author>
    <author>
      <name>Jion Oshima</name>
    </author>
    <author>
      <name>Toru Tamaki</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5220/0013089500003912</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5220/0013089500003912" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">extended version of VISAPP2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.12969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.12969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2009.09443v1</id>
    <updated>2020-09-20T14: 49: 34Z</updated>
    <published>2020-09-20T14: 49: 34Z</published>
    <title>Unsupervised Anomaly Detection on Temporal Multiway Data</title>
    <summary>  Temporal anomaly detection looks for irregularities over space-time.
Unsupervised temporal models employed thus far typically work on sequences of
feature vectors, and much less on temporal multiway data. We focus our
investigation on two-way data, in which a data matrix is observed at each time
step. Leveraging recent advances in matrix-native recurrent neural networks, we
investigated strategies for data arrangement and unsupervised training for
temporal multiway anomaly detection. These include compressing-decompressing,
encoding-predicting, and temporal data differencing. We conducted a
comprehensive suite of experiments to evaluate model behaviors under various
settings on synthetic data, moving digits, and ECG recordings. We found
interesting phenomena not previously reported. These include the capacity of
the compact matrix LSTM to compress noisy data near perfectly, making the
strategy of compressing-decompressing data ill-suited for anomaly detection
under the noise. Also, long sequence of vectors can be addressed directly by
matrix models that allow very long context and multiple step prediction.
Overall, the encoding-predicting strategy works very well for the matrix LSTMs
in the conducted experiments, thanks to its compactness and better fit to the
data dynamics.
</summary>
    <author>
      <name>Duc Nguyen</name>
    </author>
    <author>
      <name>Phuoc Nguyen</name>
    </author>
    <author>
      <name>Kien Do</name>
    </author>
    <author>
      <name>Santu Rana</name>
    </author>
    <author>
      <name>Sunil Gupta</name>
    </author>
    <author>
      <name>Truyen Tran</name>
    </author>
    <link href="http://arxiv.org/abs/2009.09443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1505.02810v1</id>
    <updated>2015-05-11T21: 31: 24Z</updated>
    <published>2015-05-11T21: 31: 24Z</published>
    <title>Tempus Fugit: The Impact of Time in Knowledge Mobilization Networks</title>
    <summary>  The temporal component of social networks is often neglected in their
analysis, and statistical measures are typically performed on a "static"
representation of the network. As a result, measures of importance (like
betweenness centrality) cannot reveal any temporal role of the entities
involved. Our goal is to start filling this limitation by proposing a form of
temporal betweenness measure, and by using it to analyse a knowledge
mobilization network. We show that this measure, which takes time explicitly
into account, allows us to detect centrality roles that were completely hidden
in the classical statistical analysis. In particular, we uncover nodes whose
static centrality was considered negligible, but whose temporal role is instead
important to accelerate mobilization flow in the network. We also observe the
reverse behaviour by detecting nodes with high static centrality, whose role as
temporal bridges is instead very low. By revealing important temporal roles,
this study is a first step towards a better understanding of the impact of time
in social networks, and opens the road to further investigation.
</summary>
    <author>
      <name>Amir Afrasiabi Rad</name>
    </author>
    <author>
      <name>Paola Flocchini</name>
    </author>
    <author>
      <name>Joanne Gaudet</name>
    </author>
    <link href="http://arxiv.org/abs/1505.02810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.02810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2312.07169v3</id>
    <updated>2024-04-03T15: 11: 33Z</updated>
    <published>2023-12-12T11: 13: 17Z</published>
    <title>Semi-supervised Active Learning for Video Action Detection</title>
    <summary>  In this work, we focus on label efficient learning for video action
detection. We develop a novel semi-supervised active learning approach which
utilizes both labeled as well as unlabeled data along with informative sample
selection for action detection. Video action detection requires spatio-temporal
localization along with classification, which poses several challenges for both
active learning informative sample selection as well as semi-supervised
learning pseudo label generation. First, we propose NoiseAug, a simple
augmentation strategy which effectively selects informative samples for video
action detection. Next, we propose fft-attention, a novel technique based on
high-pass filtering which enables effective utilization of pseudo label for SSL
in video action detection by emphasizing on relevant activity region within a
video. We evaluate the proposed approach on three different benchmark datasets,
UCF-101-24, JHMDB-21, and Youtube-VOS. First, we demonstrate its effectiveness
on video action detection where the proposed approach outperforms prior works
in semi-supervised and weakly-supervised learning along with several baseline
approaches in both UCF101-24 and JHMDB-21. Next, we also show its effectiveness
on Youtube-VOS for video object segmentation demonstrating its generalization
capability for other dense prediction tasks in videos. The code and models is
publicly available at:
\url{https: //github.com/AKASH2907/semi-sup-active-learning}.
</summary>
    <author>
      <name>Ayush Singh</name>
    </author>
    <author>
      <name>Aayush J Rana</name>
    </author>
    <author>
      <name>Akash Kumar</name>
    </author>
    <author>
      <name>Shruti Vyas</name>
    </author>
    <author>
      <name>Yogesh Singh Rawat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI Conference on Artificial Intelligence, Main Technical Track
  (AAAI),
                        2024, Code: https: //github.com/AKASH2907/semi-sup-active-learning</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.07169v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.07169v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2406.01079v1</id>
    <updated>2024-06-03T07: 58: 40Z</updated>
    <published>2024-06-03T07: 58: 40Z</published>
    <title>Object Aware Egocentric Online Action Detection</title>
    <summary>  Advancements in egocentric video datasets like Ego4D, EPIC-Kitchens, and
Ego-Exo4D have enriched the study of first-person human interactions, which is
crucial for applications in augmented reality and assisted living. Despite
these advancements, current Online Action Detection methods, which efficiently
detect actions in streaming videos, are predominantly designed for exocentric
views and thus fail to capitalize on the unique perspectives inherent to
egocentric videos. To address this gap, we introduce an Object-Aware Module
that integrates egocentric-specific priors into existing OAD frameworks,
enhancing first-person footage interpretation. Utilizing object-specific
details and temporal dynamics, our module improves scene understanding in
detecting actions. Validated extensively on the Epic-Kitchens 100 dataset, our
work can be seamlessly integrated into existing models with minimal overhead
and bring consistent performance enhancements, marking an important step
forward in adapting action detection systems to egocentric video analysis.
</summary>
    <author>
      <name>Joungbin An</name>
    </author>
    <author>
      <name>Yunsu Park</name>
    </author>
    <author>
      <name>Hyolim Kang</name>
    </author>
    <author>
      <name>Seon Joo Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR First Joint Egocentric Vision Workshop 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.01079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.01079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1612.04804v1</id>
    <updated>2016-12-14T20: 50: 48Z</updated>
    <published>2016-12-14T20: 50: 48Z</published>
    <title>Anomaly Detection Using the Knowledge-based Temporal Abstraction Method</title>
    <summary>  The rapid growth in stored time-oriented data necessitates the development of
new methods for handling, processing, and interpreting large amounts of
temporal data. One important example of such processing is detecting anomalies
in time-oriented data. The Knowledge-Based Temporal Abstraction method was
previously proposed for intelligent interpretation of temporal data based on
predefined domain knowledge. In this study we propose a framework that
integrates the KBTA method with a temporal pattern mining process for anomaly
detection. According to the proposed method a temporal pattern mining process
is applied on a dataset of basic temporal abstraction database in order to
extract patterns representing normal behavior. These patterns are then analyzed
in order to identify abnormal time periods characterized by a significantly
small number of normal patterns. The proposed approach was demonstrated using a
dataset collected from a real server.
</summary>
    <author>
      <name>Asaf Shabtai</name>
    </author>
    <link href="http://arxiv.org/abs/1612.04804v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04804v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2307.09720v1</id>
    <updated>2023-07-19T02: 03: 32Z</updated>
    <published>2023-07-19T02: 03: 32Z</published>
    <title>A Computational Topology-based Spatiotemporal Analysis Technique for
  Honeybee Aggregation</title>
    <summary>  A primary challenge in understanding collective behavior is characterizing
the spatiotemporal dynamics of the group. We employ topological data analysis
to explore the structure of honeybee aggregations that form during
trophallaxis, which is the direct exchange of food among nestmates. From the
positions of individual bees, we build topological summaries called CROCKER
matrices to track the morphology of the group as a function of scale and time.
Each column of a CROCKER matrix records the number of topological features,
such as the number of components or holes, that exist in the data for a range
of analysis scales at a given point in time. To detect important changes in the
morphology of the group from this information, we first apply dimensionality
reduction techniques to these matrices and then use classic clustering and
change-point detection algorithms on the resulting scalar data. A test of this
methodology on synthetic data from an agent-based model of honeybees and their
trophallaxis behavior shows two distinct phases: a dispersed phase that occurs
before food is introduced, followed by a food-exchange phase during which
aggregations form. We then move to laboratory data, successfully detecting the
same two phases across multiple experiments. Interestingly, our method reveals
an additional phase change towards the end of the experiments, suggesting the
possibility of another dispersed phase that follows the food-exchange phase.
</summary>
    <author>
      <name>Golnar Gharooni-Fard</name>
    </author>
    <author>
      <name>Morgan Byers</name>
    </author>
    <author>
      <name>Varad Deshmukh</name>
    </author>
    <author>
      <name>Elizabeth Bradley</name>
    </author>
    <author>
      <name>Carissa Mayo</name>
    </author>
    <author>
      <name>Chad Topaz</name>
    </author>
    <author>
      <name>Orit Peleg</name>
    </author>
    <link href="http://arxiv.org/abs/2307.09720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.09720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1604.05633v2</id>
    <updated>2016-07-26T15: 54: 07Z</updated>
    <published>2016-04-19T15: 58: 56Z</published>
    <title>Online Human Action Detection using Joint Classification-Regression
  Recurrent Neural Networks</title>
    <summary>  Human action recognition from well-segmented 3D skeleton data has been
intensively studied and has been attracting an increasing attention. Online
action detection goes one step further and is more challenging, which
identifies the action type and localizes the action positions on the fly from
the untrimmed stream data. In this paper, we study the problem of online action
detection from streaming skeleton data. We propose a multi-task end-to-end
Joint Classification-Regression Recurrent Neural Network to better explore the
action type and temporal localization information. By employing a joint
classification and regression optimization objective, this network is capable
of automatically localizing the start and end points of actions more
accurately. Specifically, by leveraging the merits of the deep Long Short-Term
Memory (LSTM) subnetwork, the proposed model automatically captures the complex
long-range temporal dynamics, which naturally avoids the typical sliding window
design and thus ensures high computational efficiency. Furthermore, the subtask
of regression optimization provides the ability to forecast the action prior to
its occurrence. To evaluate our proposed model, we build a large streaming
video dataset with annotations. Experimental results on our dataset and the
public G3D dataset both demonstrate very promising performance of our scheme.
</summary>
    <author>
      <name>Yanghao Li</name>
    </author>
    <author>
      <name>Cuiling Lan</name>
    </author>
    <author>
      <name>Junliang Xing</name>
    </author>
    <author>
      <name>Wenjun Zeng</name>
    </author>
    <author>
      <name>Chunfeng Yuan</name>
    </author>
    <author>
      <name>Jiaying Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2016 ECCV Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.05633v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05633v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2006.15473v2</id>
    <updated>2021-01-15T02: 13: 45Z</updated>
    <published>2020-06-28T00: 25: 34Z</published>
    <title>Interpretable and Trustworthy Deepfake Detection via Dynamic Prototypes</title>
    <summary>  In this paper we propose a novel human-centered approach for detecting
forgery in face images, using dynamic prototypes as a form of visual
explanations. Currently, most state-of-the-art deepfake detections are based on
black-box models that process videos frame-by-frame for inference, and few
closely examine their temporal inconsistencies. However, the existence of such
temporal artifacts within deepfake videos is key in detecting and explaining
deepfakes to a supervising human. To this end, we propose Dynamic Prototype
Network (DPNet) -- an interpretable and effective solution that utilizes
dynamic representations (i.e., prototypes) to explain deepfake temporal
artifacts. Extensive experimental results show that DPNet achieves competitive
predictive performance, even on unseen testing datasets such as Google's
DeepFakeDetection, DeeperForensics, and Celeb-DF, while providing easy
referential explanations of deepfake dynamics. On top of DPNet's prototypical
framework, we further formulate temporal logic specifications based on these
dynamics to check our model's compliance to desired temporal behaviors, hence
providing trustworthiness for such critical detection systems.
</summary>
    <author>
      <name>Loc Trinh</name>
    </author>
    <author>
      <name>Michael Tsang</name>
    </author>
    <author>
      <name>Sirisha Rambhatla</name>
    </author>
    <author>
      <name>Yan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the 2021 IEEE Winter Conference on Applications of
  Computer Vision (WACV 21')</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.15473v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.15473v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2406.10928v2</id>
    <updated>2024-06-18T11: 52: 26Z</updated>
    <published>2024-06-16T13: 23: 21Z</published>
    <title>Make Your Home Safe: Time-aware Unsupervised User Behavior Anomaly
  Detection in Smart Homes via Loss-guided Mask</title>
    <summary>  Smart homes, powered by the Internet of Things, offer great convenience but
also pose security concerns due to abnormal behaviors, such as improper
operations of users and potential attacks from malicious attackers. Several
behavior modeling methods have been proposed to identify abnormal behaviors and
mitigate potential risks. However, their performance often falls short because
they do not effectively learn less frequent behaviors, consider temporal
context, or account for the impact of noise in human behaviors. In this paper,
we propose SmartGuard, an autoencoder-based unsupervised user behavior anomaly
detection framework. First, we design a Loss-guided Dynamic Mask Strategy
(LDMS) to encourage the model to learn less frequent behaviors, which are often
overlooked during learning. Second, we propose a Three-level Time-aware
Position Embedding (TTPE) to incorporate temporal information into positional
embedding to detect temporal context anomaly. Third, we propose a Noise-aware
Weighted Reconstruction Loss (NWRL) that assigns different weights for routine
behaviors and noise behaviors to mitigate the interference of noise behaviors
during inference. Comprehensive experiments on three datasets with ten types of
anomaly behaviors demonstrates that SmartGuard consistently outperforms
state-of-the-art baselines and also offers highly interpretable results.
</summary>
    <author>
      <name>Jingyu Xiao</name>
    </author>
    <author>
      <name>Zhiyao Xu</name>
    </author>
    <author>
      <name>Qingsong Zou</name>
    </author>
    <author>
      <name>Qing Li</name>
    </author>
    <author>
      <name>Dan Zhao</name>
    </author>
    <author>
      <name>Dong Fang</name>
    </author>
    <author>
      <name>Ruoyu Li</name>
    </author>
    <author>
      <name>Wenxin Tang</name>
    </author>
    <author>
      <name>Kang Li</name>
    </author>
    <author>
      <name>Xudong Zuo</name>
    </author>
    <author>
      <name>Penghui Hu</name>
    </author>
    <author>
      <name>Yong Jiang</name>
    </author>
    <author>
      <name>Zixuan Weng</name>
    </author>
    <author>
      <name>Michael R. Lyv</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3637528.3671708</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3637528.3671708" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">KDD 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.10928v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.10928v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1812.04172v1</id>
    <updated>2018-12-11T01: 06: 56Z</updated>
    <published>2018-12-11T01: 06: 56Z</published>
    <title>Learning Discriminative Motion Features Through Detection</title>
    <summary>  Despite huge success in the image domain, modern detection models such as
Faster R-CNN have not been used nearly as much for video analysis. This is
arguably due to the fact that detection models are designed to operate on
single frames and as a result do not have a mechanism for learning motion
representations directly from video. We propose a learning procedure that
allows detection models such as Faster R-CNN to learn motion features directly
from the RGB video data while being optimized with respect to a pose estimation
task. Given a pair of video frames---Frame A and Frame B---we force our model
to predict human pose in Frame A using the features from Frame B. We do so by
leveraging deformable convolutions across space and time. Our network learns to
spatially sample features from Frame B in order to maximize pose detection
accuracy in Frame A. This naturally encourages our network to learn motion
offsets encoding the spatial correspondences between the two frames. We refer
to these motion offsets as DiMoFs (Discriminative Motion Features).
  In our experiments we show that our training scheme helps learn effective
motion cues, which can be used to estimate and localize salient human motion.
Furthermore, we demonstrate that as a byproduct, our model also learns features
that lead to improved pose detection in still-images, and better keypoint
tracking. Finally, we show how to leverage our learned model for the tasks of
spatiotemporal action localization and fine-grained action recognition.
</summary>
    <author>
      <name>Gedas Bertasius</name>
    </author>
    <author>
      <name>Christoph Feichtenhofer</name>
    </author>
    <author>
      <name>Du Tran</name>
    </author>
    <author>
      <name>Jianbo Shi</name>
    </author>
    <author>
      <name>Lorenzo Torresani</name>
    </author>
    <link href="http://arxiv.org/abs/1812.04172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.04172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2112.07787v1</id>
    <updated>2021-12-14T23: 24: 25Z</updated>
    <published>2021-12-14T23: 24: 25Z</published>
    <title>Revisiting 3D Object Detection From an Egocentric Perspective</title>
    <summary>  3D object detection is a key module for safety-critical robotics applications
such as autonomous driving. For these applications, we care most about how the
detections affect the ego-agent's behavior and safety (the egocentric
perspective). Intuitively, we seek more accurate descriptions of object
geometry when it's more likely to interfere with the ego-agent's motion
trajectory. However, current detection metrics, based on box
Intersection-over-Union (IoU), are object-centric and aren't designed to
capture the spatio-temporal relationship between objects and the ego-agent. To
address this issue, we propose a new egocentric measure to evaluate 3D object
detection, namely Support Distance Error (SDE). Our analysis based on SDE
reveals that the egocentric detection quality is bounded by the coarse geometry
of the bounding boxes. Given the insight that SDE would benefit from more
accurate geometry descriptions, we propose to represent objects as amodal
contours, specifically amodal star-shaped polygons, and devise a simple model,
StarPoly, to predict such contours. Our experiments on the large-scale Waymo
Open Dataset show that SDE better reflects the impact of detection quality on
the ego-agent's safety compared to IoU; and the estimated contours from
StarPoly consistently improve the egocentric detection quality over recent 3D
object detectors.
</summary>
    <author>
      <name>Boyang Deng</name>
    </author>
    <author>
      <name>Charles R. Qi</name>
    </author>
    <author>
      <name>Mahyar Najibi</name>
    </author>
    <author>
      <name>Thomas Funkhouser</name>
    </author>
    <author>
      <name>Yin Zhou</name>
    </author>
    <author>
      <name>Dragomir Anguelov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in NeurIPS 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.07787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.07787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2408.11935v1</id>
    <updated>2024-08-21T18: 38: 59Z</updated>
    <published>2024-08-21T18: 38: 59Z</published>
    <title>Explainable Anomaly Detection: Counterfactual driven What-If Analysis</title>
    <summary>  There exists three main areas of study inside of the field of predictive
maintenance: anomaly detection, fault diagnosis, and remaining useful life
prediction. Notably, anomaly detection alerts the stakeholder that an anomaly
is occurring. This raises two fundamental questions: what is causing the fault
and how can we fix it? Inside of the field of explainable artificial
intelligence, counterfactual explanations can give that information in the form
of what changes to make to put the data point into the opposing class, in this
case "healthy". The suggestions are not always actionable which may raise the
interest in asking "what if we do this instead?" In this work, we provide a
proof of concept for utilizing counterfactual explanations as what-if analysis.
We perform this on the PRONOSTIA dataset with a temporal convolutional network
as the anomaly detector. Our method presents the counterfactuals in the form of
a what-if analysis for this base problem to inspire future work for more
complex systems and scenarios.
</summary>
    <author>
      <name>Logan Cummins</name>
    </author>
    <author>
      <name>Alexander Sommers</name>
    </author>
    <author>
      <name>Sudip Mittal</name>
    </author>
    <author>
      <name>Shahram Rahimi</name>
    </author>
    <author>
      <name>Maria Seale</name>
    </author>
    <author>
      <name>Joseph Jaboure</name>
    </author>
    <author>
      <name>Thomas Arnold</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,
                        6 figures,
                        3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.11935v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.11935v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2103.01302v2</id>
    <updated>2021-04-01T17: 57: 04Z</updated>
    <published>2021-03-01T20: 48: 01Z</published>
    <title>Coarse-Fine Networks for Temporal Activity Detection in Videos</title>
    <summary>  In this paper, we introduce Coarse-Fine Networks, a two-stream architecture
which benefits from different abstractions of temporal resolution to learn
better video representations for long-term motion. Traditional Video models
process inputs at one (or few) fixed temporal resolution without any dynamic
frame selection. However, we argue that, processing multiple temporal
resolutions of the input and doing so dynamically by learning to estimate the
importance of each frame can largely improve video representations, specially
in the domain of temporal activity localization. To this end, we propose (1)
Grid Pool, a learned temporal downsampling layer to extract coarse features,
and, (2) Multi-stage Fusion, a spatio-temporal attention mechanism to fuse a
fine-grained context with the coarse features. We show that our method
outperforms the state-of-the-arts for action detection in public datasets
including Charades with a significantly reduced compute and memory footprint.
The code is available at https: //github.com/kkahatapitiya/Coarse-Fine-Networks
</summary>
    <author>
      <name>Kumara Kahatapitiya</name>
    </author>
    <author>
      <name>Michael S. Ryoo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at CVPR 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.01302v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.01302v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2501.04419v1</id>
    <updated>2025-01-08T11: 07: 54Z</updated>
    <published>2025-01-08T11: 07: 54Z</published>
    <title>Extending the time-over-threshold calibration of Timepix3 for
  spatial-resolved ion spectroscopy</title>
    <summary>  Hybrid pixel detectors have a well-established array of applications ranging
from particle physics to life sciences. The small dimensions of Timepix3 as
well as its relatively low energetic expenses make it an intriguing option
additionally for ion detection in nuclear physics experiments, as it reveals
simultaneously precise temporal, spatial and energetic properties of recorded
events from nuclear reactions. Currently, a limiting factor is the electronics
behavior at high input charge resulting in improper energy determination of
incident heavier ions. While the low-energy per-pixel calibration of Timepix3
is normally performed with the use of photons of up to 60 keV, the
characteristic linear range permits a correct extrapolation up to only 150
keV/pixel. We developed a global per-pixel energy correction method involving
the use of short-ranged accelerated ions and spectroscopic alpha sources, to
suitably extend the energy determination capability of Timepix3 for nuclear ion
spectroscopy experiments, where spatial and temporal precision of recorded
events are equally crucial. It was found that upon applying this correction,
the reliable per-pixel energy range has been increased from the original 150
keV to at least 1.1 MeV, while maintaining the relative energy resolution to
better than 2.5% for stopped protons of up to 1.9 MeV and better than 3.1% for
alpha particles of 5.5 MeV. Furthermore, to demonstrate the spatial resolution
of Timepix3 detectors with silicon sensors, we present alpha-radiography
measurements from which we extract the modulation transfer function (MTF) and
produce real-world biological sample images.
</summary>
    <author>
      <name>Radu-Emanuel Mihai</name>
    </author>
    <author>
      <name>Benedikt Bergmann</name>
    </author>
    <author>
      <name>Petr Smolyanskiy</name>
    </author>
    <link href="http://arxiv.org/abs/2501.04419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.04419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2410.15279v1</id>
    <updated>2024-10-20T04: 28: 19Z</updated>
    <published>2024-10-20T04: 28: 19Z</published>
    <title>ContextDet: Temporal Action Detection with Adaptive Context Aggregation</title>
    <summary>  Temporal action detection (TAD), which locates and recognizes action
segments, remains a challenging task in video understanding due to variable
segment lengths and ambiguous boundaries. Existing methods treat neighboring
contexts of an action segment indiscriminately, leading to imprecise boundary
predictions. We introduce a single-stage ContextDet framework, which makes use
of large-kernel convolutions in TAD for the first time. Our model features a
pyramid adaptive context aggragation (ACA) architecture, capturing long context
and improving action discriminability. Each ACA level consists of two novel
modules. The context attention module (CAM) identifies salient contextual
information, encourages context diversity, and preserves context integrity
through a context gating block (CGB). The long context module (LCM) makes use
of a mixture of large- and small-kernel convolutions to adaptively gather
long-range context and fine-grained local features. Additionally, by varying
the length of these large kernels across the ACA pyramid, our model provides
lightweight yet effective context aggregation and action discrimination. We
conducted extensive experiments and compared our model with a number of
advanced TAD methods on six challenging TAD benchmarks: MultiThumos, Charades,
FineAction, EPIC-Kitchens 100, Thumos14, and HACS, demonstrating superior
accuracy at reduced inference speed.
</summary>
    <author>
      <name>Ning Wang</name>
    </author>
    <author>
      <name>Yun Xiao</name>
    </author>
    <author>
      <name>Xiaopeng Peng</name>
    </author>
    <author>
      <name>Xiaojun Chang</name>
    </author>
    <author>
      <name>Xuanhong Wang</name>
    </author>
    <author>
      <name>Dingyi Fang</name>
    </author>
    <link href="http://arxiv.org/abs/2410.15279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.15279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2211.09950v1</id>
    <updated>2022-11-17T23: 55: 12Z</updated>
    <published>2022-11-17T23: 55: 12Z</published>
    <title>TempNet: Temporal Attention Towards the Detection of Animal Behaviour in
  Videos</title>
    <summary>  Recent advancements in cabled ocean observatories have increased the quality
and prevalence of underwater videos; this data enables the extraction of
high-level biologically relevant information such as species' behaviours.
Despite this increase in capability, most modern methods for the automatic
interpretation of underwater videos focus only on the detection and counting
organisms. We propose an efficient computer vision- and deep learning-based
method for the detection of biological behaviours in videos. TempNet uses an
encoder bridge and residual blocks to maintain model performance with a
two-staged, spatial, then temporal, encoder. TempNet also presents temporal
attention during spatial encoding as well as Wavelet Down-Sampling
pre-processing to improve model accuracy. Although our system is designed for
applications to diverse fish behaviours (i.e, is generic), we demonstrate its
application to the detection of sablefish (Anoplopoma fimbria) startle events.
We compare the proposed approach with a state-of-the-art end-to-end video
detection method (ReMotENet) and a hybrid method previously offered exclusively
for the detection of sablefish's startle events in videos from an existing
dataset. Results show that our novel method comfortably outperforms the
comparison baselines in multiple metrics, reaching a per-clip accuracy and
precision of 80% and 0.81, respectively. This represents a relative improvement
of 31% in accuracy and 27% in precision over the compared methods using this
dataset. Our computational pipeline is also highly efficient, as it can process
each 4-second video clip in only 38ms. Furthermore, since it does not employ
features specific to sablefish startle events, our system can be easily
extended to other behaviours in future works.
</summary>
    <author>
      <name>Declan McIntosh</name>
    </author>
    <author>
      <name>Tunai Porto Marques</name>
    </author>
    <author>
      <name>Alexandra Branzan Albu</name>
    </author>
    <author>
      <name>Rodney Rountree</name>
    </author>
    <author>
      <name>Fabio De Leo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,
                        5 figures,
                        2 tables, International Conference on Pattern
  Recognition, ICPR 2022, ICPR</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.09950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.09950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1409.6813v2</id>
    <updated>2015-09-03T05: 12: 27Z</updated>
    <published>2014-09-24T03: 57: 49Z</published>
    <title>Histogram of Oriented Principal Components for Cross-View Action
  Recognition</title>
    <summary>  Existing techniques for 3D action recognition are sensitive to viewpoint
variations because they extract features from depth images which are viewpoint
dependent. In contrast, we directly process pointclouds for cross-view action
recognition from unknown and unseen views. We propose the Histogram of Oriented
Principal Components (HOPC) descriptor that is robust to noise, viewpoint,
scale and action speed variations. At a 3D point, HOPC is computed by
projecting the three scaled eigenvectors of the pointcloud within its local
spatio-temporal support volume onto the vertices of a regular dodecahedron.
HOPC is also used for the detection of Spatio-Temporal Keypoints (STK) in 3D
pointcloud sequences so that view-invariant STK descriptors (or Local HOPC
descriptors) at these key locations only are used for action recognition. We
also propose a global descriptor computed from the normalized spatio-temporal
distribution of STKs in 4-D, which we refer to as STK-D. We have evaluated the
performance of our proposed descriptors against nine existing techniques on two
cross-view and three single-view human action recognition datasets. The
Experimental results show that our techniques provide significant improvement
over state-of-the-art methods.
</summary>
    <author>
      <name>Hossein Rahmani</name>
    </author>
    <author>
      <name>Arif Mahmood</name>
    </author>
    <author>
      <name>Du Huynh</name>
    </author>
    <author>
      <name>Ajmal Mian</name>
    </author>
    <link href="http://arxiv.org/abs/1409.6813v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.6813v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2103.06669v3</id>
    <updated>2021-03-26T15: 44: 26Z</updated>
    <published>2021-03-11T13: 52: 41Z</published>
    <title>Temporal Action Segmentation from Timestamp Supervision</title>
    <summary>  Temporal action segmentation approaches have been very successful recently.
However, annotating videos with frame-wise labels to train such models is very
expensive and time consuming. While weakly supervised methods trained using
only ordered action lists require less annotation effort, the performance is
still worse than fully supervised approaches. In this paper, we propose to use
timestamp supervision for the temporal action segmentation task. Timestamps
require a comparable annotation effort to weakly supervised approaches, and yet
provide a more supervisory signal. To demonstrate the effectiveness of
timestamp supervision, we propose an approach to train a segmentation model
using only timestamps annotations. Our approach uses the model output and the
annotated timestamps to generate frame-wise labels by detecting the action
changes. We further introduce a confidence loss that forces the predicted
probabilities to monotonically decrease as the distance to the timestamps
increases. This ensures that all and not only the most distinctive frames of an
action are learned during training. The evaluation on four datasets shows that
models trained with timestamps annotations achieve comparable performance to
the fully supervised approaches.
</summary>
    <author>
      <name>Zhe Li</name>
    </author>
    <author>
      <name>Yazan Abu Farha</name>
    </author>
    <author>
      <name>Juergen Gall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.06669v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.06669v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1904.07180v1</id>
    <updated>2019-04-15T16: 53: 36Z</updated>
    <published>2019-04-15T16: 53: 36Z</published>
    <title>Synthetic Neural Vision System Design for Motion Pattern Recognition in
  Dynamic Robot Scenes</title>
    <summary>  Insects have tiny brains but complicated visual systems for motion
perception. A handful of insect visual neurons have been computationally
modeled and successfully applied for robotics. How different neurons
collaborate on motion perception, is an open question to date. In this paper,
we propose a novel embedded vision system in autonomous micro-robots, to
recognize motion patterns in dynamic robot scenes. Here, the basic motion
patterns are categorized into movements of looming (proximity), recession,
translation, and other irrelevant ones. The presented system is a synthetic
neural network, which comprises two complementary sub-systems with four spiking
neurons -- the lobula giant movement detectors (LGMD1 and LGMD2) in locusts for
sensing looming and recession, and the direction selective neurons (DSN-R and
DSN-L) in flies for translational motion extraction. Images are transformed to
spikes via spatiotemporal computations towards a switch function and decision
making mechanisms, in order to invoke proper robot behaviors amongst collision
avoidance, tracking and wandering, in dynamic robot scenes. Our robot
experiments demonstrated two main contributions: (1) This neural vision system
is effective to recognize the basic motion patterns corresponding to timely and
proper robot behaviors in dynamic scenes. (2) The arena tests with multi-robots
demonstrated the effectiveness in recognizing more abundant motion features for
collision detection, which is a great improvement compared with former studies.
</summary>
    <author>
      <name>Qinbing Fu</name>
    </author>
    <author>
      <name>Cheng Hu</name>
    </author>
    <author>
      <name>Pengcheng Liu</name>
    </author>
    <author>
      <name>Shigang Yue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, IEEE format</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.07180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.07180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2209.09236v1</id>
    <updated>2022-09-19T17: 59: 02Z</updated>
    <published>2022-09-19T17: 59: 02Z</published>
    <title>Real-time Online Video Detection with Temporal Smoothing Transformers</title>
    <summary>  Streaming video recognition reasons about objects and their actions in every
frame of a video. A good streaming recognition model captures both long-term
dynamics and short-term changes of video. Unfortunately, in most existing
methods, the computational complexity grows linearly or quadratically with the
length of the considered dynamics. This issue is particularly pronounced in
transformer-based architectures. To address this issue, we reformulate the
cross-attention in a video transformer through the lens of kernel and apply two
kinds of temporal smoothing kernel: A box kernel or a Laplace kernel. The
resulting streaming attention reuses much of the computation from frame to
frame, and only requires a constant time update each frame. Based on this idea,
we build TeSTra, a Temporal Smoothing Transformer, that takes in arbitrarily
long inputs with constant caching and computing overhead. Specifically, it runs
$6\times$ faster than equivalent sliding-window based transformers with 2,
                        048
frames in a streaming setting. Furthermore, thanks to the increased temporal
span, TeSTra achieves state-of-the-art results on THUMOS'14 and
EPIC-Kitchen-100, two standard online action detection and action anticipation
datasets. A real-time version of TeSTra outperforms all but one prior
approaches on the THUMOS'14 dataset.
</summary>
    <author>
      <name>Yue Zhao</name>
    </author>
    <author>
      <name>Philipp Krähenbühl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2022; Code available at
  https: //github.com/zhaoyue-zephyrus/TeSTra</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.09236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.09236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2308.05051v1</id>
    <updated>2023-08-09T16: 29: 31Z</updated>
    <published>2023-08-09T16: 29: 31Z</published>
    <title>PAT: Position-Aware Transformer for Dense Multi-Label Action Detection</title>
    <summary>  We present PAT, a transformer-based network that learns complex temporal
co-occurrence action dependencies in a video by exploiting multi-scale temporal
features. In existing methods, the self-attention mechanism in transformers
loses the temporal positional information, which is essential for robust action
detection. To address this issue, we (i) embed relative positional encoding in
the self-attention mechanism and (ii) exploit multi-scale temporal
relationships by designing a novel non hierarchical network, in contrast to the
recent transformer-based approaches that use a hierarchical structure. We argue
that joining the self-attention mechanism with multiple sub-sampling processes
in the hierarchical approaches results in increased loss of positional
information. We evaluate the performance of our proposed approach on two
challenging dense multi-label benchmark datasets, and show that PAT improves
the current state-of-the-art result by 1.1% and 0.6% mAP on the Charades and
MultiTHUMOS datasets, respectively, thereby achieving the new state-of-the-art
mAP at 26.5% and 44.6%, respectively. We also perform extensive ablation
studies to examine the impact of the different components of our proposed
network.
</summary>
    <author>
      <name>Faegheh Sardari</name>
    </author>
    <author>
      <name>Armin Mustafa</name>
    </author>
    <author>
      <name>Philip J. B. Jackson</name>
    </author>
    <author>
      <name>Adrian Hilton</name>
    </author>
    <link href="http://arxiv.org/abs/2308.05051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.05051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2109.13227v1</id>
    <updated>2021-09-27T17: 59: 39Z</updated>
    <published>2021-09-27T17: 59: 39Z</published>
    <title>TSM: Temporal Shift Module for Efficient and Scalable Video
  Understanding on Edge Device</title>
    <summary>  The explosive growth in video streaming requires video understanding at high
accuracy and low computation cost. Conventional 2D CNNs are computationally
cheap but cannot capture temporal relationships; 3D CNN-based methods can
achieve good performance but are computationally intensive. In this paper, we
propose a generic and effective Temporal Shift Module (TSM) that enjoys both
high efficiency and high performance. The key idea of TSM is to shift part of
the channels along the temporal dimension, thus facilitate information
exchanged among neighboring frames. It can be inserted into 2D CNNs to achieve
temporal modeling at zero computation and zero parameters. TSM offers several
unique advantages. Firstly, TSM has high performance; it ranks the first on the
Something-Something leaderboard upon submission. Secondly, TSM has high
efficiency; it achieves a high frame rate of 74fps and 29fps for online video
recognition on Jetson Nano and Galaxy Note8. Thirdly, TSM has higher
scalability compared to 3D networks, enabling large-scale Kinetics training on
1,
                        536 GPUs in 15 minutes. Lastly, TSM enables action concepts learning, which
2D networks cannot model; we visualize the category attention map and find that
spatial-temporal action detector emerges during the training of classification
tasks. The code is publicly available at
https: //github.com/mit-han-lab/temporal-shift-module.
</summary>
    <author>
      <name>Ji Lin</name>
    </author>
    <author>
      <name>Chuang Gan</name>
    </author>
    <author>
      <name>Kuan Wang</name>
    </author>
    <author>
      <name>Song Han</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Journal preprint of arXiv: 1811.08383 (TPAMI,
                        2020). arXiv admin note:
  substantial text overlap with arXiv: 1910.00932</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.13227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.13227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2304.00439v3</id>
    <updated>2024-11-28T11: 46: 04Z</updated>
    <published>2023-04-02T03: 27: 31Z</published>
    <title>SoftED: Metrics for Soft Evaluation of Time Series Event Detection</title>
    <summary>  Time series event detection methods are evaluated mainly by standard
classification metrics that focus solely on detection accuracy. However,
inaccuracy in detecting an event can often result from its preceding or delayed
effects reflected in neighboring detections. These detections are valuable to
trigger necessary actions or help mitigate unwelcome consequences. In this
context, current metrics are insufficient and inadequate for the context of
event detection. There is a demand for metrics that incorporate both the
concept of time and temporal tolerance for neighboring detections. This paper
introduces SoftED metrics, a new set of metrics designed for soft evaluating
event detection methods. They enable the evaluation of both detection accuracy
and the degree to which their detections represent events. They improved event
detection evaluation by associating events and their representative detections,
incorporating temporal tolerance in over 36\% of experiments compared to the
usual classification metrics. SoftED metrics were validated by domain
specialists that indicated their contribution to detection evaluation and
method selection.
</summary>
    <author>
      <name>Rebecca Salles</name>
    </author>
    <author>
      <name>Janio Lima</name>
    </author>
    <author>
      <name>Michel Reis</name>
    </author>
    <author>
      <name>Rafaelli Coutinho</name>
    </author>
    <author>
      <name>Esther Pacitti</name>
    </author>
    <author>
      <name>Florent Masseglia</name>
    </author>
    <author>
      <name>Reza Akbarinia</name>
    </author>
    <author>
      <name>Chao Chen</name>
    </author>
    <author>
      <name>Jonathan Garibaldi</name>
    </author>
    <author>
      <name>Fabio Porto</name>
    </author>
    <author>
      <name>Eduardo Ogasawara</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cie.2024.110728</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cie.2024.110728" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computers &amp; Industrial Engineering, Volume 198,
                        2024,
                        110728,ISSN
  0360-8352</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2304.00439v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.00439v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1604.01403v1</id>
    <updated>2016-04-05T20: 00: 06Z</updated>
    <published>2016-04-05T20: 00: 06Z</published>
    <title>A study of broadband Faraday rotation and polarization behaviour over
  1.3--10 GHz in 36 discrete radio sources</title>
    <summary>  We present a broadband polarization analysis of 36 discrete polarized radio
sources over a very broad, densely-sampled frequency band. Our sample was
selected on the basis of polarization behaviour apparent in narrowband archival
data at 1.4 GHz: half the sample show complicated frequency-dependent
polarization behaviour (i.e. Faraday complexity) at these frequencies, while
half show comparatively simple behaviour (i.e. they appear Faraday simple). We
re-observed the sample using the Australia Telescope Compact Array (ATCA) in
full polarization, with 6 GHz of densely sampled frequency coverage spanning
1.3 to 10 GHz. We have devised a general polarization modelling technique that
allows us to identify multiple polarized emission components in a source, and
to characterize their properties. We detect Faraday complex behaviour in almost
every source in our sample. Several sources exhibit particularly remarkable
polarization behaviour. By comparing our new and archival data, we have
identified temporal variability in the broadband integrated polarization
spectra of some sources. In a number of cases, the characteristics of the
polarized emission components, including the range of Faraday depths over which
they emit, their temporal variability, spectral index, and the linear extent of
the source, allow us to argue that the spectropolarimetric data encodes
information about the magnetoionic environment of active galactic nuclei
themselves. Furthermore, the data place direct constraints on the geometry and
magnetoionic structure of this material. We discuss the consequences of
restricted frequency bands on the detection and interpretation of polarization
structures, and implications for upcoming spectropolarimetric surveys.
</summary>
    <author>
      <name>C. S. Anderson</name>
    </author>
    <author>
      <name>B. M. Gaensler</name>
    </author>
    <author>
      <name>I. J. Feain</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3847/0004-637X/825/1/59</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3847/0004-637X/825/1/59" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in ApJ on 31 March 2016. 48 pages total (14
  pages text),
                        63 figures,
                        6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.01403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2309.02975v3</id>
    <updated>2023-09-22T03: 58: 24Z</updated>
    <published>2023-09-06T13: 16: 41Z</published>
    <title>FishMOT: A Simple and Effective Method for Fish Tracking Based on IoU
  Matching</title>
    <summary>  Fish tracking plays a vital role in understanding fish behavior and ecology.
However, existing tracking methods face challenges in accuracy and robustness
dues to morphological change of fish, occlusion and complex environment. This
paper proposes FishMOT(Multiple Object Tracking for Fish), a novel fish
tracking approach combining object detection and IoU matching, including basic
module, interaction module and refind module. Wherein, a basic module performs
target association based on IoU of detection boxes between successive frames to
deal with morphological change of fish; an interaction module combines IoU of
detection boxes and IoU of fish entity to handle occlusions; a refind module
use spatio-temporal information uses spatio-temporal information to overcome
the tracking failure resulting from the missed detection by the detector under
complex environment. FishMOT reduces the computational complexity and memory
consumption since it does not require complex feature extraction or identity
assignment per fish, and does not need Kalman filter to predict the detection
boxes of successive frame. Experimental results demonstrate FishMOT outperforms
state-of-the-art multi-object trackers and specialized fish tracking tools in
terms of MOTA, accuracy, computation time, memory consumption, etc..
Furthermore, the method exhibits excellent robustness and generalizability for
varying environments and fish numbers. The simplified workflow and strong
performance make FishMOT as a highly effective fish tracking approach. The
source codes and pre-trained models are available at:
https: //github.com/gakkistar/FishMOT
</summary>
    <author>
      <name>Shuo Liu</name>
    </author>
    <author>
      <name>Lulu Han</name>
    </author>
    <author>
      <name>Xiaoyang Liu</name>
    </author>
    <author>
      <name>Junli Ren</name>
    </author>
    <author>
      <name>Fang Wang</name>
    </author>
    <author>
      <name> YingLiu</name>
    </author>
    <author>
      <name>Yuanshan Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2309.02975v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.02975v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2403.07683v1</id>
    <updated>2024-03-12T14: 20: 11Z</updated>
    <published>2024-03-12T14: 20: 11Z</published>
    <title>Towards a Unified Formalism of Multivariate Coefficients of Variation --
  Application to the Analysis of Polarimetric Speckle Time Series</title>
    <summary>  This article primarily aims to unify the various formalisms of multivariate
coefficients of variation, leveraging advanced concepts of generalized means,
whether weighted or not, applied to the eigenvalues of covariance matrices. We
highlight the existence of an infinite number of these coefficients and
demonstrate that they are bounded. Moreover, we link the various coefficients
of variation identified in the literature to specific instances within our
unified formalism. We illustrate the utility of our method by applying it to a
time series of polarimetric radar imagery. In this context, the coefficient of
variation emerges as a key tool for detecting changes or identifying permanent
scatterers, which are characterized by their remarkable temporal stability. The
multidimensionality arises from the diversity of polarizations. The
introduction of the various possible coefficients demonstrates how their
selection impacts the detection of samples exhibiting specific temporal
behaviors and underscores the contribution of polarimetry to dynamic speckle
analysis.
</summary>
    <author>
      <name>Elise Colin</name>
    </author>
    <author>
      <name>Razvigor Ossikovski</name>
    </author>
    <link href="http://arxiv.org/abs/2403.07683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.07683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2412.18438v1</id>
    <updated>2024-12-24T13: 57: 38Z</updated>
    <published>2024-12-24T13: 57: 38Z</published>
    <title>Heterodyne coherent detection of the electric field temporal trace
  emitted by frequency-modulated comb lasers</title>
    <summary>  Frequency-modulated (FM) combs are produced by mode-locked lasers in which
the electric field has a linearly chirped frequency and nearly constant
amplitude. This regime of operation occurs naturally in certain laser systems
and constitutes a valuable alternative to generate spectra with equidistant
modes. Here, we use a low-noise fs-pulse comb as the local oscillator and
combine dual comb heterodyne detection with time domain analysis of the
multi-heterodyne signal to reveal the temporal trace of both amplitude and
phase quadratures of FM comb lasers' electric field. This technique is applied
to both a dense and a harmonic mid-infrared free-running quantum cascade laser
frequency comb and shows direct evidence of the FM behavior together with the
high degree of coherence of these sources. Our results furnish a deeper insight
on the origin of the FM combs and pave the way to further improvement and
optimization of these devices.
</summary>
    <author>
      <name>Baptiste Chomet</name>
    </author>
    <author>
      <name>Salim Basceken</name>
    </author>
    <author>
      <name>Djamal Gacemi</name>
    </author>
    <author>
      <name>Barbara Schneider</name>
    </author>
    <author>
      <name>Mathias Beck</name>
    </author>
    <author>
      <name>Angela Vasanelli</name>
    </author>
    <author>
      <name>Benoît Darquié</name>
    </author>
    <author>
      <name>Jérôme Faist</name>
    </author>
    <author>
      <name>Carlo Sirtori</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1364/OPTICA.525834</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1364/OPTICA.525834" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,
                        5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Optica 11,
                        1220 (2024)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2412.18438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.18438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2104.08030v1</id>
    <updated>2021-04-16T11: 00: 19Z</updated>
    <published>2021-04-16T11: 00: 19Z</published>
    <title>Temporally smooth online action detection using cycle-consistent future
  anticipation</title>
    <summary>  Many video understanding tasks work in the offline setting by assuming that
the input video is given from the start to the end. However, many real-world
problems require the online setting, making a decision immediately using only
the current and the past frames of videos such as in autonomous driving and
surveillance systems. In this paper, we present a novel solution for online
action detection by using a simple yet effective RNN-based networks called the
Future Anticipation and Temporally Smoothing network (FATSnet). The proposed
network consists of a module for anticipating the future that can be trained in
an unsupervised manner with the cycle-consistency loss, and another component
for aggregating the past and the future for temporally smooth frame-by-frame
predictions. We also propose a solution to relieve the performance loss when
running RNN-based models on very long sequences. Evaluations on TVSeries,
THUMOS14, and BBDB show that our method achieve the state-of-the-art
performances compared to the previous works on online action detection.
</summary>
    <author>
      <name>Young Hwi Kim</name>
    </author>
    <author>
      <name>Seonghyeon Nam</name>
    </author>
    <author>
      <name>Seon Joo Kim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.patcog.2021.107954</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.patcog.2021.107954" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by Pattern Recognition</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Pattern Recognition, Volume 116, August 2021,
                        107954</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.08030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.08030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2206.02997v2</id>
    <updated>2024-02-02T17: 11: 10Z</updated>
    <published>2022-06-07T04: 07: 48Z</published>
    <title>TadML: A fast temporal action detection with Mechanics-MLP</title>
    <summary>  Temporal Action Detection(TAD) is a crucial but challenging task in video
understanding.It is aimed at detecting both the type and start-end frame for
each action instance in a long, untrimmed video.Most current models adopt both
RGB and Optical-Flow streams for the TAD task. Thus, original RGB frames must
be converted manually into Optical-Flow frames with additional computation and
time cost, which is an obstacle to achieve real-time processing. At present,
many models adopt two-stage strategies, which would slow the inference speed
down and complicatedly tuning on proposals generating.By comparison, we propose
a one-stage anchor-free temporal localization method with RGB stream only, in
which a novel Newtonian Mechanics-MLP architecture is established. It has
comparable accuracy with all existing state-of-the-art models, while surpasses
the inference speed of these methods by a large margin. The typical inference
speed in this paper is astounding 4.44 video per second on THUMOS14. In
applications, because there is no need to convert optical flow, the inference
speed will be faster.It also proves that MLP has great potential in downstream
tasks such as TAD. The source code is available at
https: //github.com/BonedDeng/TadML
</summary>
    <author>
      <name>Bowen Deng</name>
    </author>
    <author>
      <name>Dongchang Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,
                        3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.02997v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.02997v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1309.4714v1</id>
    <updated>2013-09-18T17: 29: 03Z</updated>
    <published>2013-09-18T17: 29: 03Z</published>
    <title>Temporal-Difference Learning to Assist Human Decision Making during the
  Control of an Artificial Limb</title>
    <summary>  In this work we explore the use of reinforcement learning (RL) to help with
human decision making, combining state-of-the-art RL algorithms with an
application to prosthetics. Managing human-machine interaction is a problem of
considerable scope, and the simplification of human-robot interfaces is
especially important in the domains of biomedical technology and rehabilitation
medicine. For example, amputees who control artificial limbs are often required
to quickly switch between a number of control actions or modes of operation in
order to operate their devices. We suggest that by learning to anticipate
(predict) a user's behaviour, artificial limbs could take on an active role in
a human's control decisions so as to reduce the burden on their users.
Recently, we showed that RL in the form of general value functions (GVFs) could
be used to accurately detect a user's control intent prior to their explicit
control choices. In the present work, we explore the use of temporal-difference
learning and GVFs to predict when users will switch their control influence
between the different motor functions of a robot arm. Experiments were
performed using a multi-function robot arm that was controlled by muscle
signals from a user's body (similar to conventional artificial limb control).
Our approach was able to acquire and maintain forecasts about a user's
switching decisions in real time. It also provides an intuitive and reward-free
way for users to correct or reinforce the decisions made by the machine
learning system. We expect that when a system is certain enough about its
predictions, it can begin to take over switching decisions from the user to
streamline control and potentially decrease the time and effort needed to
complete tasks. This preliminary study therefore suggests a way to naturally
integrate human- and machine-based decision making systems.
</summary>
    <author>
      <name>Ann L. Edwards</name>
    </author>
    <author>
      <name>Alexandra Kearney</name>
    </author>
    <author>
      <name>Michael Rory Dawson</name>
    </author>
    <author>
      <name>Richard S. Sutton</name>
    </author>
    <author>
      <name>Patrick M. Pilarski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages,
                        4 figures, This version to appear at The 1st
  Multidisciplinary Conference on Reinforcement Learning and Decision Making,
  Princeton, NJ, USA, Oct. 25-27,
                        2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.4714v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.4714v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2412.11436v2</id>
    <updated>2025-04-09T02: 03: 04Z</updated>
    <published>2024-12-16T04: 28: 49Z</published>
    <title>Event-based Detectors for Laser Guide Star Tip-Tilt Sensing</title>
    <summary>  Event-based sensors detect only changes in brightness across a scene, with
each pixel producing an asynchronous stream of spatial-temporal data, rather
than recording frames of overall illumination such as a traditional frame-based
sensor. This is advantageous for implementing into a wavefront sensor, which
benefits from high temporal resolution and high dynamic range. The
determination of tip-tilt in particular is still a problem in laser guide star
(LGS) adaptive optics as there are no current technological capabilities to
measure it. We characterized the behavior of an event-based sensor in the
context of tip-tilt sensing, investigating if the high temporal resolution of
the event streams could address these challenges. Different conditions of
tip-tilt and background illumination levels are explored and found to be a
strong contender for tip-tilt sensing with LGSs.
</summary>
    <author>
      <name>Monique Cockram</name>
    </author>
    <author>
      <name>Noelia Martinez Rey</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/1.OE.64.4.043102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/1.OE.64.4.043102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages,
                        11 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Opt. Eng. 64(4),
                        043102 (2025),</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2412.11436v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.11436v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1601.03260v1</id>
    <updated>2016-01-13T14: 27: 41Z</updated>
    <published>2016-01-13T14: 27: 41Z</published>
    <title>Temporal features of LS I +61$^{\circ
                        }$303 in hard X-rays from the
  Swift/BAT survey data</title>
    <summary>  We study the long-term spectral and timing behaviour of LS I +61$^{\circ
                        }$303
in hard X-rays (15--150 keV) using $\sim$10 years of survey data from the
$Swift$ Burst Alert Telescope (BAT) monitor. We focus on the detection of long
periodicities known to be present in this source in multiple wavelengths. We
clearly detect three periods: the shorter one at 26.48 days is compatible with
the orbital period of the system; the second, longer, periodicity at 26.93
days, is detected for the first time in X-rays and its value is consistent with
an analogous temporal feature recently detected in the radio and in the
gamma-ray waveband, and we associate it with a modulation caused by a
precessing jet in this system. Finally, we find also evidence of the long-term
periodicity at $\sim$1667 d, that results compatible with a beat frequency of
the two close, and shorter, periodicities. We discuss our results in the
context of the multi-band behaviour of the physical processes of this source.
</summary>
    <author>
      <name>A. D'Aì</name>
    </author>
    <author>
      <name>G. Cusumano</name>
    </author>
    <author>
      <name>V. La Parola</name>
    </author>
    <author>
      <name>A. Segreto</name>
    </author>
    <author>
      <name>T. Mineo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/mnras/stv2716</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/mnras/stv2716" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages,
                        8 figures. Published in MNRAS</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">MNRAS,
                        456,
                        1955 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.03260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.03260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1604.06182v1</id>
    <updated>2016-04-21T05: 08: 59Z</updated>
    <published>2016-04-21T05: 08: 59Z</published>
    <title>The THUMOS Challenge on Action Recognition for Videos "in the Wild"</title>
    <summary>  Automatically recognizing and localizing wide ranges of human actions has
crucial importance for video understanding. Towards this goal, the THUMOS
challenge was introduced in 2013 to serve as a benchmark for action
recognition. Until then, video action recognition, including THUMOS challenge,
had focused primarily on the classification of pre-segmented (i.e., trimmed)
videos, which is an artificial task. In THUMOS 2014, we elevated action
recognition to a more practical level by introducing temporally untrimmed
videos. These also include `background videos' which share similar scenes and
backgrounds as action videos, but are devoid of the specific actions. The three
editions of the challenge organized in 2013--2015 have made THUMOS a common
benchmark for action classification and detection and the annual challenge is
widely attended by teams from around the world.
  In this paper we describe the THUMOS benchmark in detail and give an overview
of data collection and annotation procedures. We present the evaluation
protocols used to quantify results in the two THUMOS tasks of action
classification and temporal detection. We also present results of submissions
to the THUMOS 2015 challenge and review the participating approaches.
Additionally, we include a comprehensive empirical study evaluating the
differences in action recognition between trimmed and untrimmed videos, and how
well methods trained on trimmed videos generalize to untrimmed videos. We
conclude by proposing several directions and improvements for future THUMOS
challenges.
</summary>
    <author>
      <name>Haroon Idrees</name>
    </author>
    <author>
      <name>Amir R. Zamir</name>
    </author>
    <author>
      <name>Yu-Gang Jiang</name>
    </author>
    <author>
      <name>Alex Gorban</name>
    </author>
    <author>
      <name>Ivan Laptev</name>
    </author>
    <author>
      <name>Rahul Sukthankar</name>
    </author>
    <author>
      <name>Mubarak Shah</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cviu.2016.10.018</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cviu.2016.10.018" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint submitted to Computer Vision and Image Understanding</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.06182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2301.04288v1</id>
    <updated>2023-01-11T03: 29: 27Z</updated>
    <published>2023-01-11T03: 29: 27Z</published>
    <title>Generic Event Boundary Detection in Video with Pyramid Features</title>
    <summary>  Generic event boundary detection (GEBD) aims to split video into chunks at a
broad and diverse set of actions as humans naturally perceive event boundaries.
In this study, we present an approach that considers the correlation between
neighbor frames with pyramid feature maps in both spatial and temporal
dimensions to construct a framework for localizing generic events in video. The
features at multiple spatial dimensions of a pre-trained ResNet-50 are
exploited with different views in the temporal dimension to form a temporal
pyramid feature map. Based on that, the similarity between neighbor frames is
calculated and projected to build a temporal pyramid similarity feature vector.
A decoder with 1D convolution operations is used to decode these similarities
to a new representation that incorporates their temporal relationship for later
boundary score estimation. Extensive experiments conducted on the GEBD
benchmark dataset show the effectiveness of our system and its variations, in
which we outperformed the state-of-the-art approaches. Additional experiments
on TAPOS dataset, which contains long-form videos with Olympic sport actions,
demonstrated the effectiveness of our study compared to others.
</summary>
    <author>
      <name>Van Thong Huynh</name>
    </author>
    <author>
      <name>Hyung-Jeong Yang</name>
    </author>
    <author>
      <name>Guee-Sang Lee</name>
    </author>
    <author>
      <name>Soo-Hyung Kim</name>
    </author>
    <link href="http://arxiv.org/abs/2301.04288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.04288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2308.10570v1</id>
    <updated>2023-08-21T09: 01: 14Z</updated>
    <published>2023-08-21T09: 01: 14Z</published>
    <title>Self-Feedback DETR for Temporal Action Detection</title>
    <summary>  Temporal Action Detection (TAD) is challenging but fundamental for real-world
video applications. Recently, DETR-based models have been devised for TAD but
have not performed well yet. In this paper, we point out the problem in the
self-attention of DETR for TAD; the attention modules focus on a few key
elements, called temporal collapse problem. It degrades the capability of the
encoder and decoder since their self-attention modules play no role. To solve
the problem, we propose a novel framework, Self-DETR, which utilizes
cross-attention maps of the decoder to reactivate self-attention modules. We
recover the relationship between encoder features by simple matrix
multiplication of the cross-attention map and its transpose. Likewise, we also
get the information within decoder queries. By guiding collapsed self-attention
maps with the guidance map calculated, we settle down the temporal collapse of
self-attention modules in the encoder and decoder. Our extensive experiments
demonstrate that Self-DETR resolves the temporal collapse problem by keeping
high diversity of attention over all layers.
</summary>
    <author>
      <name>Jihwan Kim</name>
    </author>
    <author>
      <name>Miso Lee</name>
    </author>
    <author>
      <name>Jae-Pil Heo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICCV 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.10570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.10570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2409.18478v1</id>
    <updated>2024-09-27T06: 37: 47Z</updated>
    <published>2024-09-27T06: 37: 47Z</published>
    <title>Temporal2Seq: A Unified Framework for Temporal Video Understanding Tasks</title>
    <summary>  With the development of video understanding, there is a proliferation of
tasks for clip-level temporal video analysis, including temporal action
detection (TAD), temporal action segmentation (TAS), and generic event boundary
detection (GEBD). While task-specific video understanding models have exhibited
outstanding performance in each task, there remains a dearth of a unified
framework capable of simultaneously addressing multiple tasks, which is a
promising direction for the next generation of AI. To this end, in this paper,
we propose a single unified framework, coined as Temporal2Seq, to formulate the
output of these temporal video understanding tasks as a sequence of discrete
tokens. With this unified token representation, Temporal2Seq can train a
generalist model within a single architecture on different video understanding
tasks. In the absence of multi-task learning (MTL) benchmarks, we compile a
comprehensive co-training dataset by borrowing the datasets from TAD, TAS, and
GEBD tasks. We evaluate our Temporal2Seq generalist model on the corresponding
test sets of three tasks, demonstrating that Temporal2Seq can produce
reasonable results on various tasks and achieve advantages compared with
single-task training on this framework. We also investigate the generalization
performance of our generalist model on new datasets from different tasks, which
yields superior performance to the specific model.
</summary>
    <author>
      <name>Min Yang</name>
    </author>
    <author>
      <name>Zichen Zhang</name>
    </author>
    <author>
      <name>Limin Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2409.18478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.18478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2201.07131v3</id>
    <updated>2022-10-21T11: 36: 32Z</updated>
    <published>2022-01-18T17: 14: 54Z</published>
    <title>Leveraging Real Talking Faces via Self-Supervision for Robust Forgery
  Detection</title>
    <summary>  One of the most pressing challenges for the detection of face-manipulated
videos is generalising to forgery methods not seen during training while
remaining effective under common corruptions such as compression. In this
paper, we examine whether we can tackle this issue by harnessing videos of real
talking faces, which contain rich information on natural facial appearance and
behaviour and are readily available in large quantities online. Our method,
termed RealForensics, consists of two stages. First, we exploit the natural
correspondence between the visual and auditory modalities in real videos to
learn, in a self-supervised cross-modal manner, temporally dense video
representations that capture factors such as facial movements, expression, and
identity. Second, we use these learned representations as targets to be
predicted by our forgery detector along with the usual binary forgery
classification task; this encourages it to base its real/fake decision on said
factors. We show that our method achieves state-of-the-art performance on
cross-manipulation generalisation and robustness experiments, and examine the
factors that contribute to its performance. Our results suggest that leveraging
natural and unlabelled videos is a promising direction for the development of
more robust face forgery detectors.
</summary>
    <author>
      <name>Alexandros Haliassos</name>
    </author>
    <author>
      <name>Rodrigo Mira</name>
    </author>
    <author>
      <name>Stavros Petridis</name>
    </author>
    <author>
      <name>Maja Pantic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2022. Code: https: //github.com/ahaliassos/RealForensics</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.07131v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.07131v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1508.03755v1</id>
    <updated>2015-08-15T17: 04: 50Z</updated>
    <published>2015-08-15T17: 04: 50Z</published>
    <title>Beat-Event Detection in Action Movie Franchises</title>
    <summary>  While important advances were recently made towards temporally localizing and
recognizing specific human actions or activities in videos, efficient detection
and classification of long video chunks belonging to semantically defined
categories such as "pursuit" or "romance" remains challenging.We introduce a
new dataset, Action Movie Franchises, consisting of a collection of Hollywood
action movie franchises. We define 11 non-exclusive semantic categories -
called beat-categories - that are broad enough to cover most of the movie
footage. The corresponding beat-events are annotated as groups of video shots,
possibly overlapping.We propose an approach for localizing beat-events based on
classifying shots into beat-categories and learning the temporal constraints
between shots. We show that temporal constraints significantly improve the
classification performance. We set up an evaluation protocol for beat-event
localization as well as for shot classification, depending on whether movies
from the same franchise are present or not in the training data.
</summary>
    <author>
      <name>Danila Potapov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LEAR</arxiv:affiliation>
    </author>
    <author>
      <name>Matthijs Douze</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LEAR</arxiv:affiliation>
    </author>
    <author>
      <name>Jerome Revaud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LEAR</arxiv:affiliation>
    </author>
    <author>
      <name>Zaid Harchaoui</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LEAR, CIMS</arxiv:affiliation>
    </author>
    <author>
      <name>Cordelia Schmid</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LEAR</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1508.03755v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.03755v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2307.16453v1</id>
    <updated>2023-07-31T07: 20: 31Z</updated>
    <published>2023-07-31T07: 20: 31Z</published>
    <title>Every Mistake Counts in Assembly</title>
    <summary>  One promising use case of AI assistants is to help with complex procedures
like cooking, home repair, and assembly tasks. Can we teach the assistant to
interject after the user makes a mistake? This paper targets the problem of
identifying ordering mistakes in assembly procedures. We propose a system that
can detect ordering mistakes by utilizing a learned knowledge base. Our
framework constructs a knowledge base with spatial and temporal beliefs based
on observed mistakes. Spatial beliefs depict the topological relationship of
the assembling components, while temporal beliefs aggregate prerequisite
actions as ordering constraints. With an episodic memory design, our algorithm
can dynamically update and construct the belief sets as more actions are
observed, all in an online fashion. We demonstrate experimentally that our
inferred spatial and temporal beliefs are capable of identifying incorrect
orderings in real-world action sequences. To construct the spatial beliefs, we
collect a new set of coarse-level action annotations for Assembly101 based on
the positioning of the toy parts. Finally, we demonstrate the superior
performance of our belief inference algorithm in detecting ordering mistakes on
the Assembly101 dataset.
</summary>
    <author>
      <name>Guodong Ding</name>
    </author>
    <author>
      <name>Fadime Sener</name>
    </author>
    <author>
      <name>Shugao Ma</name>
    </author>
    <author>
      <name>Angela Yao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,
                        5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.16453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.16453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2309.06276v1</id>
    <updated>2023-09-12T14: 37: 41Z</updated>
    <published>2023-09-12T14: 37: 41Z</published>
    <title>OTAS: Unsupervised Boundary Detection for Object-Centric Temporal Action
  Segmentation</title>
    <summary>  Temporal action segmentation is typically achieved by discovering the
dramatic variances in global visual descriptors. In this paper, we explore the
merits of local features by proposing the unsupervised framework of
Object-centric Temporal Action Segmentation (OTAS). Broadly speaking, OTAS
consists of self-supervised global and local feature extraction modules as well
as a boundary selection module that fuses the features and detects salient
boundaries for action segmentation. As a second contribution, we discuss the
pros and cons of existing frame-level and boundary-level evaluation metrics.
Through extensive experiments, we find OTAS is superior to the previous
state-of-the-art method by $41\%$ on average in terms of our recommended F1
score. Surprisingly, OTAS even outperforms the ground-truth human annotations
in the user study. Moreover, OTAS is efficient enough to allow real-time
inference.
</summary>
    <author>
      <name>Yuerong Li</name>
    </author>
    <author>
      <name>Zhengrong Xue</name>
    </author>
    <author>
      <name>Huazhe Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to WACV 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.06276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.06276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2302.10813v1</id>
    <updated>2023-02-21T16: 42: 52Z</updated>
    <published>2023-02-21T16: 42: 52Z</published>
    <title>Tracking Objects and Activities with Attention for Temporal Sentence
  Grounding</title>
    <summary>  Temporal sentence grounding (TSG) aims to localize the temporal segment which
is semantically aligned with a natural language query in an untrimmed
video.Most existing methods extract frame-grained features or object-grained
features by 3D ConvNet or detection network under a conventional TSG framework,
failing to capture the subtle differences between frames or to model the
spatio-temporal behavior of core persons/objects. In this paper, we introduce a
new perspective to address the TSG task by tracking pivotal objects and
activities to learn more fine-grained spatio-temporal behaviors. Specifically,
we propose a novel Temporal Sentence Tracking Network (TSTNet), which contains
(A) a Cross-modal Targets Generator to generate multi-modal templates and
search space, filtering objects and activities, and (B) a Temporal Sentence
Tracker to track multi-modal targets for modeling the targets' behavior and to
predict query-related segment. Extensive experiments and comparisons with
state-of-the-arts are conducted on challenging benchmarks: Charades-STA and
TACoS. And our TSTNet achieves the leading performance with a considerable
real-time speed.
</summary>
    <author>
      <name>Zeyu Xiong</name>
    </author>
    <author>
      <name>Daizong Liu</name>
    </author>
    <author>
      <name>Pan Zhou</name>
    </author>
    <author>
      <name>Jiahao Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by ICASSP2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.10813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.10813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2110.13110v1</id>
    <updated>2021-10-25T17: 04: 08Z</updated>
    <published>2021-10-25T17: 04: 08Z</published>
    <title>Diagnosing Errors in Video Relation Detectors</title>
    <summary>  Video relation detection forms a new and challenging problem in computer
vision, where subjects and objects need to be localized spatio-temporally and a
predicate label needs to be assigned if and only if there is an interaction
between the two. Despite recent progress in video relation detection, overall
performance is still marginal and it remains unclear what the key factors are
towards solving the problem. Following examples set in the object detection and
action localization literature, we perform a deep dive into the error diagnosis
of current video relation detection approaches. We introduce a diagnostic tool
for analyzing the sources of detection errors. Our tool evaluates and compares
current approaches beyond the single scalar metric of mean Average Precision by
defining different error types specific to video relation detection, used for
false positive analyses. Moreover, we examine different factors of influence on
the performance in a false negative analysis, including relation length, number
of subject/object/predicate instances, and subject/object size. Finally, we
present the effect on video relation performance when considering an oracle fix
for each error type. On two video relation benchmarks, we show where current
approaches excel and fall short, allowing us to pinpoint the most important
future directions in the field. The tool is available at
\url{https: //github.com/shanshuo/DiagnoseVRD}.
</summary>
    <author>
      <name>Shuo Chen</name>
    </author>
    <author>
      <name>Pascal Mettes</name>
    </author>
    <author>
      <name>Cees G. M. Snoek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BMVC 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.13110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.13110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2105.00067v1</id>
    <updated>2021-04-30T20: 07: 27Z</updated>
    <published>2021-04-30T20: 07: 27Z</published>
    <title>Unsupervised Discriminative Embedding for Sub-Action Learning in Complex
  Activities</title>
    <summary>  Action recognition and detection in the context of long untrimmed video
sequences has seen an increased attention from the research community. However,
annotation of complex activities is usually time consuming and challenging in
practice. Therefore, recent works started to tackle the problem of unsupervised
learning of sub-actions in complex activities. This paper proposes a novel
approach for unsupervised sub-action learning in complex activities. The
proposed method maps both visual and temporal representations to a latent space
where the sub-actions are learnt discriminatively in an end-to-end fashion. To
this end, we propose to learn sub-actions as latent concepts and a novel
discriminative latent concept learning (DLCL) module aids in learning
sub-actions. The proposed DLCL module lends on the idea of latent concepts to
learn compact representations in the latent embedding space in an unsupervised
way. The result is a set of latent vectors that can be interpreted as cluster
centers in the embedding space. The latent space itself is formed by a joint
visual and temporal embedding capturing the visual similarity and temporal
ordering of the data. Our joint learning with discriminative latent concept
module is novel which eliminates the need for explicit clustering. We validate
our approach on three benchmark datasets and show that the proposed combination
of visual-temporal embedding and discriminative latent concepts allow to learn
robust action representations in an unsupervised setting.
</summary>
    <author>
      <name>Sirnam Swetha</name>
    </author>
    <author>
      <name>Hilde Kuehne</name>
    </author>
    <author>
      <name>Yogesh S Rawat</name>
    </author>
    <author>
      <name>Mubarak Shah</name>
    </author>
    <link href="http://arxiv.org/abs/2105.00067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.00067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2209.10361v1</id>
    <updated>2022-09-21T13: 56: 12Z</updated>
    <published>2022-09-21T13: 56: 12Z</published>
    <title>MulBot: Unsupervised Bot Detection Based on Multivariate Time Series</title>
    <summary>  Online social networks are actively involved in the removal of malicious
social bots due to their role in the spread of low quality information.
However, most of the existing bot detectors are supervised classifiers
incapable of capturing the evolving behavior of sophisticated bots. Here we
propose MulBot, an unsupervised bot detector based on multivariate time series
(MTS). For the first time, we exploit multidimensional temporal features
extracted from user timelines. We manage the multidimensionality with an LSTM
autoencoder, which projects the MTS in a suitable latent space. Then, we
perform a clustering step on this encoded representation to identify dense
groups of very similar users -- a known sign of automation. Finally, we perform
a binary classification task achieving f1-score $= 0.99$, outperforming
state-of-the-art methods (f1-score $\le 0.97$). Not only does MulBot achieve
excellent results in the binary classification task, but we also demonstrate
its strengths in a novel and practically-relevant task: detecting and
separating different botnets. In this multi-class classification task we
achieve f1-score $= 0.96$. We conclude by estimating the importance of the
different features used in our model and by evaluating MulBot's capability to
generalize to new unseen bots, thus proposing a solution to the generalization
deficiencies of supervised bot detectors.
</summary>
    <author>
      <name>Lorenzo Mannocci</name>
    </author>
    <author>
      <name>Stefano Cresci</name>
    </author>
    <author>
      <name>Anna Monreale</name>
    </author>
    <author>
      <name>Athina Vakali</name>
    </author>
    <author>
      <name>Maurizio Tesconi</name>
    </author>
    <link href="http://arxiv.org/abs/2209.10361v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.10361v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1601.03273v1</id>
    <updated>2016-01-12T10: 02: 00Z</updated>
    <published>2016-01-12T10: 02: 00Z</published>
    <title>Non-invasive detection of animal nerve impulses with an atomic
  magnetometer operating near quantum limited sensitivity</title>
    <summary>  Magnetic fields generated by human and animal organs, such as the heart,
brain and nervous system carry information useful for biological and medical
purposes. These magnetic fields are most commonly detected using
cryogenically-cooled superconducting magnetometers. Here we present the frst
detection of action potentials from an animal nerve using an optical atomic
magnetometer. Using an optimal design we are able to achieve the sensitivity
dominated by the quantum shot noise of light and quantum projection noise of
atomic spins. Such sensitivity allows us to measure the nerve impulse with a
miniature room-temperature sensor which is a critical advantage for biomedical
applications. Positioning the sensor at a distance of a few millimeters from
the nerve, corresponding to the distance between the skin and nerves in
biological studies, we detect the magnetic field generated by an action
potential of a frog sciatic nerve. From the magnetic field measurements we
determine the activity of the nerve and the temporal shape of the nerve
impulse. This work opens new ways towards implementing optical magnetometers as
practical devices for medical diagnostics.
</summary>
    <author>
      <name>Kasper Jensen</name>
    </author>
    <author>
      <name>Rima Budvytyte</name>
    </author>
    <author>
      <name>Rodrigo A. Thomas</name>
    </author>
    <author>
      <name>Tian Wang</name>
    </author>
    <author>
      <name>Annette Fuchs</name>
    </author>
    <author>
      <name>Mikhail V. Balabas</name>
    </author>
    <author>
      <name>Georgios Vasilakis</name>
    </author>
    <author>
      <name>Lars Mosgaard</name>
    </author>
    <author>
      <name>Thomas Heimburg</name>
    </author>
    <author>
      <name>Søren-Peter Olesen</name>
    </author>
    <author>
      <name>Eugene S. Polzik</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/srep29638</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/srep29638" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Main text with figures, and methods and supplementary information</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Sci. Rep. 6,
                            29638 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.03273v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.03273v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2304.04688v4</id>
    <updated>2023-09-20T14: 14: 16Z</updated>
    <published>2023-04-10T16: 08: 59Z</published>
    <title>Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action
  Detection</title>
    <summary>  The goal of spatial-temporal action detection is to determine the time and
place where each person's action occurs in a video and classify the
corresponding action category. Most of the existing methods adopt
fully-supervised learning, which requires a large amount of training data,
making it very difficult to achieve zero-shot learning. In this paper, we
propose to utilize a pre-trained visual-language model to extract the
representative image and text features, and model the relationship between
these features through different interaction modules to obtain the interaction
feature. In addition, we use this feature to prompt each label to obtain more
appropriate text features. Finally, we calculate the similarity between the
interaction feature and the text feature for each label to determine the action
category. Our experiments on J-HMDB and UCF101-24 datasets demonstrate that the
proposed interaction module and prompting make the visual-language features
better aligned, thus achieving excellent accuracy for zero-shot spatio-temporal
action detection. The code will be available at
https: //github.com/webber2933/iCLIP.
</summary>
    <author>
      <name>Wei-Jhe Huang</name>
    </author>
    <author>
      <name>Jheng-Hsien Yeh</name>
    </author>
    <author>
      <name>Min-Hung Chen</name>
    </author>
    <author>
      <name>Gueter Josmy Faure</name>
    </author>
    <author>
      <name>Shang-Hong Lai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICCV Workshop 2023 (What is Next in Multimodal Foundation
  Models?)</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.04688v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.04688v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1712.01111v1</id>
    <updated>2017-11-30T19: 26: 49Z</updated>
    <published>2017-11-30T19: 26: 49Z</published>
    <title>An End-to-end 3D Convolutional Neural Network for Action Detection and
  Segmentation in Videos</title>
    <summary>  In this paper, we propose an end-to-end 3D CNN for action detection and
segmentation in videos. The proposed architecture is a unified deep network
that is able to recognize and localize action based on 3D convolution features.
A video is first divided into equal length clips and next for each clip a set
of tube proposals are generated based on 3D CNN features. Finally, the tube
proposals of different clips are linked together and spatio-temporal action
detection is performed using these linked video proposals. This top-down action
detection approach explicitly relies on a set of good tube proposals to perform
well and training the bounding box regression usually requires a large number
of annotated samples. To remedy this, we further extend the 3D CNN to an
encoder-decoder structure and formulate the localization problem as action
segmentation. The foreground regions (i.e. action regions) for each frame are
segmented first then the segmented foreground maps are used to generate the
bounding boxes. This bottom-up approach effectively avoids tube proposal
generation by leveraging the pixel-wise annotations of segmentation. The
segmentation framework also can be readily applied to a general problem of
video object segmentation. Extensive experiments on several video datasets
demonstrate the superior performance of our approach for action detection and
video object segmentation compared to the state-of-the-arts.
</summary>
    <author>
      <name>Rui Hou</name>
    </author>
    <author>
      <name>Chen Chen</name>
    </author>
    <author>
      <name>Mubarak Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv: 1703.10664</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.01111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.01111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2411.07510v1</id>
    <updated>2024-11-12T03: 09: 14Z</updated>
    <published>2024-11-12T03: 09: 14Z</published>
    <title>An Attack Traffic Identification Method Based on Temporal Spectrum</title>
    <summary>  To address the issues of insufficient robustness, unstable features, and data
noise interference in existing network attack detection and identification
models, this paper proposes an attack traffic detection and identification
method based on temporal spectrum. First, traffic data is segmented by a
sliding window to construct a feature sequence and a corresponding label
sequence for network traffic. Next, the proposed spectral label generation
methods, SSPE and COAP, are applied to transform the label sequence into
spectral labels and the feature sequence into temporal features. Spectral
labels and temporal features are used to capture and represent behavioral
patterns of attacks. Finally, the constructed temporal features and spectral
labels are used to train models, which subsequently detects and identifies
network attack behaviors. Experimental results demonstrate that compared to
traditional methods, models trained with the SSPE or COAP method improve
identification accuracy by 10%, and exhibit strong robustness, particularly in
noisy environments.
</summary>
    <author>
      <name>Wenwei Xie</name>
    </author>
    <author>
      <name>Jie Yin</name>
    </author>
    <author>
      <name>Zihao Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages,
                            7 figures,
                            7 tables,
                            8 formulas</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.07510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.07510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2304.11513v1</id>
    <updated>2023-04-23T01: 32: 47Z</updated>
    <published>2023-04-23T01: 32: 47Z</published>
    <title>Detecting Socially Abnormal Highway Driving Behaviors via Recurrent
  Graph Attention Networks</title>
    <summary>  With the rapid development of Internet of Things technologies, the next
generation traffic monitoring infrastructures are connected via the web, to aid
traffic data collection and intelligent traffic management. One of the most
important tasks in traffic is anomaly detection, since abnormal drivers can
reduce traffic efficiency and cause safety issues. This work focuses on
detecting abnormal driving behaviors from trajectories produced by highway
video surveillance systems. Most of the current abnormal driving behavior
detection methods focus on a limited category of abnormal behaviors that deal
with a single vehicle without considering vehicular interactions. In this work,
we consider the problem of detecting a variety of socially abnormal driving
behaviors, i.e., behaviors that do not conform to the behavior of other nearby
drivers. This task is complicated by the variety of vehicular interactions and
the spatial-temporal varying nature of highway traffic. To solve this problem,
we propose an autoencoder with a Recurrent Graph Attention Network that can
capture the highway driving behaviors contextualized on the surrounding cars,
and detect anomalies that deviate from learned patterns. Our model is scalable
to large freeways with thousands of cars. Experiments on data generated from
traffic simulation software show that our model is the only one that can spot
the exact vehicle conducting socially abnormal behaviors, among the
state-of-the-art anomaly detection models. We further show the performance on
real world HighD traffic dataset, where our model detects vehicles that violate
the local driving norms.
</summary>
    <author>
      <name>Yue Hu</name>
    </author>
    <author>
      <name>Yuhang Zhang</name>
    </author>
    <author>
      <name>Yanbing Wang</name>
    </author>
    <author>
      <name>Daniel Work</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3543507.3583452.</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3543507.3583452." rel="related"/>
    <link href="http://arxiv.org/abs/2304.11513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.11513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1512.04208v1</id>
    <updated>2015-12-14T07: 50: 22Z</updated>
    <published>2015-12-14T07: 50: 22Z</published>
    <title>Watch-Bot: Unsupervised Learning for Reminding Humans of Forgotten
  Actions</title>
    <summary>  We present a robotic system that watches a human using a Kinect v2 RGB-D
sensor, detects what he forgot to do while performing an activity, and if
necessary reminds the person using a laser pointer to point out the related
object. Our simple setup can be easily deployed on any assistive robot.
  Our approach is based on a learning algorithm trained in a purely
unsupervised setting, which does not require any human annotations. This makes
our approach scalable and applicable to variant scenarios. Our model learns the
action/object co-occurrence and action temporal relations in the activity, and
uses the learned rich relationships to infer the forgotten action and the
related object. We show that our approach not only improves the unsupervised
action segmentation and action cluster assignment performance, but also
effectively detects the forgotten actions on a challenging human activity RGB-D
video dataset. In robotic experiments, we show that our robot is able to remind
people of forgotten actions successfully.
</summary>
    <author>
      <name>Chenxia Wu</name>
    </author>
    <author>
      <name>Jiemi Zhang</name>
    </author>
    <author>
      <name>Bart Selman</name>
    </author>
    <author>
      <name>Silvio Savarese</name>
    </author>
    <author>
      <name>Ashutosh Saxena</name>
    </author>
    <link href="http://arxiv.org/abs/1512.04208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.04208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2101.08567v1</id>
    <updated>2021-01-21T11: 59: 47Z</updated>
    <published>2021-01-21T11: 59: 47Z</published>
    <title>Discovering Multi-Label Actor-Action Association in a Weakly Supervised
  Setting</title>
    <summary>  Since collecting and annotating data for spatio-temporal action detection is
very expensive, there is a need to learn approaches with less supervision.
Weakly supervised approaches do not require any bounding box annotations and
can be trained only from labels that indicate whether an action occurs in a
video clip. Current approaches, however, cannot handle the case when there are
multiple persons in a video that perform multiple actions at the same time. In
this work, we address this very challenging task for the first time. We propose
a baseline based on multi-instance and multi-label learning. Furthermore, we
propose a novel approach that uses sets of actions as representation instead of
modeling individual action classes. Since computing, the probabilities for the
full power set becomes intractable as the number of action classes increases,
we assign an action set to each detected person under the constraint that the
assignment is consistent with the annotation of the video clip. We evaluate the
proposed approach on the challenging AVA dataset where the proposed approach
outperforms the MIML baseline and is competitive to fully supervised
approaches.
</summary>
    <author>
      <name>Sovan Biswas</name>
    </author>
    <author>
      <name>Juergen Gall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in ACCV 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.08567v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.08567v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2012.08236v1</id>
    <updated>2020-12-15T12: 11: 48Z</updated>
    <published>2020-12-15T12: 11: 48Z</published>
    <title>Point-Level Temporal Action Localization: Bridging Fully-supervised
  Proposals to Weakly-supervised Losses</title>
    <summary>  Point-Level temporal action localization (PTAL) aims to localize actions in
untrimmed videos with only one timestamp annotation for each action instance.
Existing methods adopt the frame-level prediction paradigm to learn from the
sparse single-frame labels. However, such a framework inevitably suffers from a
large solution space. This paper attempts to explore the proposal-based
prediction paradigm for point-level annotations, which has the advantage of
more constrained solution space and consistent predictions among neighboring
frames. The point-level annotations are first used as the keypoint supervision
to train a keypoint detector. At the location prediction stage, a simple but
effective mapper module, which enables back-propagation of training errors, is
then introduced to bridge the fully-supervised framework with weak supervision.
To our best of knowledge, this is the first work to leverage the
fully-supervised paradigm for the point-level setting. Experiments on THUMOS14,
BEOID, and GTEA verify the effectiveness of our proposed method both
quantitatively and qualitatively, and demonstrate that our method outperforms
state-of-the-art methods.
</summary>
    <author>
      <name>Chen Ju</name>
    </author>
    <author>
      <name>Peisen Zhao</name>
    </author>
    <author>
      <name>Ya Zhang</name>
    </author>
    <author>
      <name>Yanfeng Wang</name>
    </author>
    <author>
      <name>Qi Tian</name>
    </author>
    <link href="http://arxiv.org/abs/2012.08236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.08236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2010.15680v1</id>
    <updated>2020-10-29T15: 26: 08Z</updated>
    <published>2020-10-29T15: 26: 08Z</published>
    <title>LSTM for Model-Based Anomaly Detection in Cyber-Physical Systems</title>
    <summary>  Anomaly detection is the task of detecting data which differs from the normal
behaviour of a system in a given context. In order to approach this problem,
data-driven models can be learned to predict current or future observations.
Oftentimes, anomalous behaviour depends on the internal dynamics of the system
and looks normal in a static context. To address this problem, the model should
also operate depending on state. Long Short-Term Memory (LSTM) neural networks
have been shown to be particularly useful to learn time sequences with varying
length of temporal dependencies and are therefore an interesting general
purpose approach to learn the behaviour of arbitrarily complex Cyber-Physical
Systems. In order to perform anomaly detection, we slightly modify the standard
norm 2 error to incorporate an estimate of model uncertainty. We analyse the
approach on artificial and real data.
</summary>
    <author>
      <name>Benedikt Eiteneuer</name>
    </author>
    <author>
      <name>Oliver Niggemann</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 29th International Workshop on Principles of
  Diagnosis DX-2018, Warsaw, Poland</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.15680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.15680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2205.14136v1</id>
    <updated>2022-05-27T17: 55: 54Z</updated>
    <published>2022-05-27T17: 55: 54Z</published>
    <title>PSL is Dead. Long Live PSL</title>
    <summary>  Property Specification Language (PSL) is a form of temporal logic that has
been mainly used in discrete domains (e.g. formal hardware verification). In
this paper, we show that by merging machine learning techniques with PSL
monitors, we can extend PSL to work on continuous domains. We apply this
technique in machine learning-based anomaly detection to analyze scenarios of
real-time streaming events from continuous variables in order to detect
abnormal behaviors of a system. By using machine learning with formal models,
we leverage the strengths of both machine learning methods and formal semantics
of time. On one hand, machine learning techniques can produce distributions on
continuous variables, where abnormalities can be captured as deviations from
the distributions. On the other hand, formal methods can characterize discrete
temporal behaviors and relations that cannot be easily learned by machine
learning techniques. Interestingly, the anomalies detected by machine learning
and the underlying time representation used are discrete events. We implemented
a temporal monitoring package (TEF) that operates in conjunction with normal
data science packages for anomaly detection machine learning systems, and we
show that TEF can be used to perform accurate interpretation of temporal
correlation between events.
</summary>
    <author>
      <name>Kevin Smith</name>
    </author>
    <author>
      <name>Hai Lin</name>
    </author>
    <author>
      <name>Praveen Tiwari</name>
    </author>
    <author>
      <name>Marjorie Sayer</name>
    </author>
    <author>
      <name>Claudionor Coelho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages,
                            16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.14136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.14136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.14818v1</id>
    <updated>2025-03-19T01: 23: 26Z</updated>
    <published>2025-03-19T01: 23: 26Z</published>
    <title>Bidirectional quantitative scattering microscopy</title>
    <summary>  Quantitative phase microscopy (QPM) and interferometric scattering (iSCAT)
microscopy are powerful label-free imaging techniques and are widely used for
biomedical applications. Each method, however, possesses distinct limitations:
QPM, which measures forward scattering (FS), excels at imaging microscale
structures but struggles with rapidly moving nanoscale objects, while iSCAT,
based on backward scattering (BS), is highly sensitive to nanoscale dynamics
but lacks the ability to image microscale structures comprehensively. Here, we
introduce bidirectional quantitative scattering microscopy (BiQSM), an
innovative approach that integrates FS and BS detection using off-axis digital
holography with bidirectional illumination and spatial-frequency multiplexing.
BiQSM achieves spatiotemporal consistency and a dynamic range 14 times wider
than QPM, enabling simultaneous imaging of nanoscale and microscale cellular
components. We demonstrate BiQSM's ability to reveal spatiotemporal behaviors
of intracellular structures, with FS-BS correlation analysis providing insights
into proteins, lipids, and membranes. Time-lapse imaging of dying cells further
highlights BiQSM's potential as a label-free tool for monitoring cellular vital
states through structural and motion-related changes. By bridging the strengths
of QPM and iSCAT, BiQSM advances quantitative cellular imaging and opens new
avenues for studying dynamic biological processes.
</summary>
    <author>
      <name>Kohki Horie</name>
    </author>
    <author>
      <name>Keiichiro Toda</name>
    </author>
    <author>
      <name>Takuma Nakamura</name>
    </author>
    <author>
      <name>Takuro Ideguchi</name>
    </author>
    <link href="http://arxiv.org/abs/2503.14818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.14818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2308.01618v1</id>
    <updated>2023-08-03T08: 48: 14Z</updated>
    <published>2023-08-03T08: 48: 14Z</published>
    <title>A Survey on Deep Learning-based Spatio-temporal Action Detection</title>
    <summary>  Spatio-temporal action detection (STAD) aims to classify the actions present
in a video and localize them in space and time. It has become a particularly
active area of research in computer vision because of its explosively emerging
real-world applications, such as autonomous driving, visual surveillance,
entertainment, etc. Many efforts have been devoted in recent years to building
a robust and effective framework for STAD. This paper provides a comprehensive
review of the state-of-the-art deep learning-based methods for STAD. Firstly, a
taxonomy is developed to organize these methods. Next, the linking algorithms,
which aim to associate the frame- or clip-level detection results together to
form action tubes, are reviewed. Then, the commonly used benchmark datasets and
evaluation metrics are introduced, and the performance of state-of-the-art
models is compared. At last, this paper is concluded, and a set of potential
research directions of STAD are discussed.
</summary>
    <author>
      <name>Peng Wang</name>
    </author>
    <author>
      <name>Fanwei Zeng</name>
    </author>
    <author>
      <name>Yuntao Qian</name>
    </author>
    <link href="http://arxiv.org/abs/2308.01618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.01618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1608.00911v1</id>
    <updated>2016-08-02T17: 37: 38Z</updated>
    <published>2016-08-02T17: 37: 38Z</published>
    <title>Modeling Spatial and Temporal Cues for Multi-label Facial Action Unit
  Detection</title>
    <summary>  Facial action units (AUs) are essential to decode human facial expressions.
Researchers have focused on training AU detectors with a variety of features
and classifiers. However, several issues remain. These are spatial
representation, temporal modeling, and AU correlation. Unlike most studies that
tackle these issues separately, we propose a hybrid network architecture to
jointly address them. Specifically, spatial representations are extracted by a
Convolutional Neural Network (CNN), which, as analyzed in this paper, is able
to reduce person-specific biases caused by hand-crafted features (eg, SIFT and
Gabor). To model temporal dependencies, Long Short-Term Memory (LSTMs) are
stacked on top of these representations, regardless of the lengths of input
videos. The outputs of CNNs and LSTMs are further aggregated into a fusion
network to produce per-frame predictions of 12 AUs. Our network naturally
addresses the three issues, and leads to superior performance compared to
existing methods that consider these issues independently. Extensive
experiments were conducted on two large spontaneous datasets, GFT and BP4D,
containing more than 400,
                            000 frames coded with 12 AUs. On both datasets, we
report significant improvement over a standard multi-label CNN and
feature-based state-of-the-art. Finally, we provide visualization of the
learned AU models, which, to our best knowledge, reveal how machines see facial
AUs for the first time.
</summary>
    <author>
      <name>Wen-Sheng Chu</name>
    </author>
    <author>
      <name>Fernando De la Torre</name>
    </author>
    <author>
      <name>Jeffrey F. Cohn</name>
    </author>
    <link href="http://arxiv.org/abs/1608.00911v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00911v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1606.00195v2</id>
    <updated>2016-12-06T09: 18: 57Z</updated>
    <published>2016-06-01T09: 44: 57Z</published>
    <title>Self-stabilizing Reconfiguration</title>
    <summary>  Current reconfiguration techniques are based on starting the system in a
consistent configuration, in which all participating entities are in their
initial state. Starting from that state, the system must preserve consistency
as long as a predefined churn rate of processors joins and leaves is not
violated, and unbounded storage is available. Many working systems cannot
control this churn rate and do not have access to unbounded storage. System
designers that neglect the outcome of violating the above assumptions may doom
the system to exhibit illegal behaviors. We present the first automatically
recovering reconfiguration scheme that recovers from transient faults, such as
temporal violations of the above assumptions. Our self-stabilizing solutions
regain safety automatically by assuming temporal access to reliable failure
detectors. Once safety is re-established, the failure detector reliability is
no longer needed. Still, liveness is conditioned by the failure detector's
unreliable signals. We show that our self-stabilizing reconfiguration
techniques can serve as the basis for the implementation of several dynamic
services over message passing systems. Examples include self-stabilizing
reconfigurable virtual synchrony, which, in turn, can be used for implementing
a self-stabilizing reconfigurable state-machine replication and
self-stabilizing reconfigurable emulation of shared memory.
</summary>
    <author>
      <name>Shlomi Dolev</name>
    </author>
    <author>
      <name>Chryssis Georgiou</name>
    </author>
    <author>
      <name>Ioannis Marcoullis</name>
    </author>
    <author>
      <name>Elad M. Schiller</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00195v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00195v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2401.16280v1</id>
    <updated>2024-01-29T16: 37: 00Z</updated>
    <published>2024-01-29T16: 37: 00Z</published>
    <title>Cutup and Detect: Human Fall Detection on Cutup Untrimmed Videos Using a
  Large Foundational Video Understanding Model</title>
    <summary>  This work explores the performance of a large video understanding foundation
model on the downstream task of human fall detection on untrimmed video and
leverages a pretrained vision transformer for multi-class action detection,
with classes: "Fall",
                            "Lying" and "Other/Activities of daily living (ADL)". A
method for temporal action localization that relies on a simple cutup of
untrimmed videos is demonstrated. The methodology includes a preprocessing
pipeline that converts datasets with timestamp action annotations into labeled
datasets of short action clips. Simple and effective clip-sampling strategies
are introduced. The effectiveness of the proposed method has been empirically
evaluated on the publicly available High-Quality Fall Simulation Dataset
(HQFSD). The experimental results validate the performance of the proposed
pipeline. The results are promising for real-time application, and the falls
are detected on video level with a state-of-the-art 0.96 F1 score on the HQFSD
dataset under the given experimental settings. The source code will be made
available on GitHub.
</summary>
    <author>
      <name>Till Grutschus</name>
    </author>
    <author>
      <name>Ola Karrar</name>
    </author>
    <author>
      <name>Emir Esenov</name>
    </author>
    <author>
      <name>Ekta Vats</name>
    </author>
    <link href="http://arxiv.org/abs/2401.16280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.16280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1707.09145v1</id>
    <updated>2017-07-28T08: 21: 11Z</updated>
    <published>2017-07-28T08: 21: 11Z</published>
    <title>Spatial-Aware Object Embeddings for Zero-Shot Localization and
  Classification of Actions</title>
    <summary>  We aim for zero-shot localization and classification of human actions in
video. Where traditional approaches rely on global attribute or object
classification scores for their zero-shot knowledge transfer, our main
contribution is a spatial-aware object embedding. To arrive at spatial
awareness, we build our embedding on top of freely available actor and object
detectors. Relevance of objects is determined in a word embedding space and
further enforced with estimated spatial preferences. Besides local object
awareness, we also embed global object awareness into our embedding to maximize
actor and object interaction. Finally, we exploit the object positions and
sizes in the spatial-aware embedding to demonstrate a new spatio-temporal
action retrieval scenario with composite queries. Action localization and
classification experiments on four contemporary action video datasets support
our proposal. Apart from state-of-the-art results in the zero-shot localization
and classification settings, our spatial-aware embedding is even competitive
with recent supervised action localization alternatives.
</summary>
    <author>
      <name>Pascal Mettes</name>
    </author>
    <author>
      <name>Cees G. M. Snoek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1910.06497v2</id>
    <updated>2021-02-21T22: 21: 48Z</updated>
    <published>2019-10-15T02: 57: 30Z</published>
    <title>The Value of Summary Statistics for Anomaly Detection in
  Temporally-Evolving Networks: A Performance Evaluation Study</title>
    <summary>  Network data has emerged as an active research area in statistics. Much of
the focus of ongoing research has been on static networks that represent a
single snapshot or aggregated historical data unchanging over time. However,
most networks result from temporally-evolving systems that exhibit intrinsic
dynamic behavior. Monitoring such temporally-varying networks to detect
anomalous changes has applications in both social and physical sciences. In
this work, we perform an evaluation study of summary statistics for anomaly
detection in temporally-evolving networks by incorporating principles from
statistical process monitoring. In contrast to previous studies, we
deliberately incorporate temporal auto-correlation in our study. Other
considerations in our comprehensive assessment include types and duration of
anomaly, model type, and sparsity in temporally-evolving networks. We conclude
that summary statistics can be valuable tools for network monitoring and often
perform better than more complicated statistics.
</summary>
    <author>
      <name>Lata Kodali</name>
    </author>
    <author>
      <name>Srijan Sengupta</name>
    </author>
    <author>
      <name>Leanna House</name>
    </author>
    <author>
      <name>William H. Woodall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">47 pages,
                            9 main figures,
                            17 main tables,
                            12 figures and tables in
  appendix</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Applied Stochastic Models in Business and Industry 36.6 (2020): 980-1013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.06497v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06497v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2305.10953v1</id>
    <updated>2023-05-18T13: 23: 05Z</updated>
    <published>2023-05-18T13: 23: 05Z</published>
    <title>Detecting the driver nodes of temporal networks</title>
    <summary>  Detecting the driver nodes of complex networks has garnered significant
attention recently to control complex systems to desired behaviors, where nodes
represent system components and edges encode their interactions. Driver nodes,
which are directly controlled by external inputs, play a crucial role in
controlling all network nodes. While many approaches have been proposed to
identify driver nodes of static networks, we still lack an effective algorithm
to control ubiquitous temporal networks, where network structures evolve over
time. Here we propose an effective online time-accelerated heuristic algorithm
(OTaHa) to detect driver nodes of temporal networks. Together with theoretical
analysis and numerical simulations on synthetic and empirical temporal
networks, we show that OTaHa offers multiple sets of driver nodes, and
noticeably outperforms existing methods in terms of accuracy and execution
time. We further report that most edges are redundant in controlling temporal
networks although the complete instantaneous signal-carrying edges cannot be
guaranteed. Moreover, removing edges with high edge betweenness (the number of
all-pairs shortest paths passing through the edge) significantly impedes the
overall controllability. Our work provides an effective algorithm and paves the
way for subsequent explorations on achieving the ultimate control of temporal
networks.
</summary>
    <author>
      <name>Tingting Qin</name>
    </author>
    <author>
      <name>Gaopeng Duan</name>
    </author>
    <author>
      <name>Aming Li</name>
    </author>
    <link href="http://arxiv.org/abs/2305.10953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.10953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2210.16863v2</id>
    <updated>2024-04-01T09: 13: 22Z</updated>
    <published>2022-10-30T15: 31: 19Z</published>
    <title>Time-aware Metapath Feature Augmentation for Ponzi Detection in Ethereum</title>
    <summary>  With the development of Web 3.0 which emphasizes decentralization, blockchain
technology ushers in its revolution and also brings numerous challenges,
particularly in the field of cryptocurrency. Recently, a large number of
criminal behaviors continuously emerge on blockchain, such as Ponzi schemes and
phishing scams, which severely endanger decentralized finance. Existing
graph-based abnormal behavior detection methods on blockchain usually focus on
constructing homogeneous transaction graphs without distinguishing the
heterogeneity of nodes and edges, resulting in partial loss of transaction
pattern information. Although existing heterogeneous modeling methods can
depict richer information through metapaths, the extracted metapaths generally
neglect temporal dependencies between entities and do not reflect real
behavior. In this paper, we introduce Time-aware Metapath Feature Augmentation
(TMFAug) as a plug-and-play module to capture the real metapath-based
transaction patterns during Ponzi scheme detection on Ethereum. The proposed
module can be adaptively combined with existing graph-based Ponzi detection
methods. Extensive experimental results show that our TMFAug can help existing
Ponzi detection methods achieve significant performance improvements on the
Ethereum dataset, indicating the effectiveness of heterogeneous temporal
information for Ponzi scheme detection.
</summary>
    <author>
      <name>Chengxiang Jin</name>
    </author>
    <author>
      <name>Jiajun Zhou</name>
    </author>
    <author>
      <name>Jie Jin</name>
    </author>
    <author>
      <name>Jiajing Wu</name>
    </author>
    <author>
      <name>Qi Xuan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TNSE.2024.3384499</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TNSE.2024.3384499" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IEEE Transactions on Network Science and Engineering</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.16863v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.16863v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1909.09269v1</id>
    <updated>2019-09-20T00: 38: 05Z</updated>
    <published>2019-09-20T00: 38: 05Z</published>
    <title>Fine-grained Action Segmentation using the Semi-Supervised Action GAN</title>
    <summary>  In this paper we address the problem of continuous fine-grained action
segmentation, in which multiple actions are present in an unsegmented video
stream. The challenge for this task lies in the need to represent the
hierarchical nature of the actions and to detect the transitions between
actions, allowing us to localise the actions within the video effectively. We
propose a novel recurrent semi-supervised Generative Adversarial Network (GAN)
model for continuous fine-grained human action segmentation. Temporal context
information is captured via a novel Gated Context Extractor (GCE) module,
composed of gated attention units, that directs the queued context information
through the generator model, for enhanced action segmentation. The GAN is made
to learn features in a semi-supervised manner, enabling the model to perform
action classification jointly with the standard, unsupervised, GAN learning
procedure. We perform extensive evaluations on different architectural variants
to demonstrate the importance of the proposed network architecture, and show
that it is capable of outperforming current state-of-the-art on three
challenging datasets: 50 Salads, MERL Shopping and Georgia Tech Egocentric
Activities dataset.
</summary>
    <author>
      <name>Harshala Gammulle</name>
    </author>
    <author>
      <name>Simon Denman</name>
    </author>
    <author>
      <name>Sridha Sridharan</name>
    </author>
    <author>
      <name>Clinton Fookes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Pattern Recognition Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.09269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.09269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1510.04437v1</id>
    <updated>2015-10-15T08: 10: 42Z</updated>
    <published>2015-10-15T08: 10: 42Z</published>
    <title>A Novel Approach for Human Action Recognition from Silhouette Images</title>
    <summary>  In this paper, a novel human action recognition technique from video is
presented. Any action of human is a combination of several micro action
sequences performed by one or more body parts of the human. The proposed
approach uses spatio-temporal body parts movement (STBPM) features extracted
from foreground silhouette of the human objects. The newly proposed STBPM
feature estimates the movements of different body parts for any given time
segment to classify actions. We also proposed a rule based logic named rule
action classifier (RAC), which uses a series of condition action rules based on
prior knowledge and hence does not required training to classify any action.
Since we don't require training to classify actions, the proposed approach is
view independent. The experimental results on publicly available Wizeman and
MuHVAi datasets are compared with that of the related research work in terms of
accuracy in the human action detection, and proposed technique outperforms the
others.
</summary>
    <author>
      <name>Satyabrata Maity</name>
    </author>
    <author>
      <name>Debotosh Bhattacharjee</name>
    </author>
    <author>
      <name>Amlan Chakrabarti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Manuscript</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2202.04132v1</id>
    <updated>2022-02-08T20: 20: 08Z</updated>
    <published>2022-02-08T20: 20: 08Z</published>
    <title>Untrimmed Action Anticipation</title>
    <summary>  Egocentric action anticipation consists in predicting a future action the
camera wearer will perform from egocentric video. While the task has recently
attracted the attention of the research community, current approaches assume
that the input videos are "trimmed", meaning that a short video sequence is
sampled a fixed time before the beginning of the action. We argue that, despite
the recent advances in the field, trimmed action anticipation has a limited
applicability in real-world scenarios where it is important to deal with
"untrimmed" video inputs and it cannot be assumed that the exact moment in
which the action will begin is known at test time. To overcome such
limitations, we propose an untrimmed action anticipation task, which, similarly
to temporal action detection, assumes that the input video is untrimmed at test
time, while still requiring predictions to be made before the actions actually
take place. We design an evaluation procedure for methods designed to address
this novel task, and compare several baselines on the EPIC-KITCHENS-100
dataset. Experiments show that the performance of current models designed for
trimmed action anticipation is very limited and more research on this task is
required.
</summary>
    <author>
      <name>Ivan Rodin</name>
    </author>
    <author>
      <name>Antonino Furnari</name>
    </author>
    <author>
      <name>Dimitrios Mavroeidis</name>
    </author>
    <author>
      <name>Giovanni Maria Farinella</name>
    </author>
    <link href="http://arxiv.org/abs/2202.04132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.04132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1411.3749v1</id>
    <updated>2014-11-13T21: 41: 55Z</updated>
    <published>2014-11-13T21: 41: 55Z</published>
    <title>Anomaly Detection in Dynamic Networks of Varying Size</title>
    <summary>  Dynamic networks, also called network streams, are an important data
representation that applies to many real-world domains. Many sets of network
data such as e-mail networks, social networks, or internet traffic networks are
best represented by a dynamic network due to the temporal component of the
data. One important application in the domain of dynamic network analysis is
anomaly detection. Here the task is to identify points in time where the
network exhibits behavior radically different from a typical time, either due
to some event (like the failure of machines in a computer network) or a shift
in the network properties. This problem is made more difficult by the fluid
nature of what is considered "normal" network behavior. The volume of traffic
on a network, for example, can change over the course of a month or even vary
based on the time of the day without being considered unusual. Anomaly
detection tests using traditional network statistics have difficulty in these
scenarios due to their Density Dependence: as the volume of edges changes the
value of the statistics changes as well making it difficult to determine if the
change in signal is due to the traffic volume or due to some fundamental shift
in the behavior of the network. To more accurately detect anomalies in dynamic
networks, we introduce the concept of Density-Consistent network statistics. On
synthetically generated graphs anomaly detectors using these statistics show a
a 20-400% improvement in the recall when distinguishing graphs drawn from
different distributions. When applied to several real datasets
Density-Consistent statistics recover multiple network events which standard
statistics failed to find.
</summary>
    <author>
      <name>Timothy La Fond</name>
    </author>
    <author>
      <name>Jennifer Neville</name>
    </author>
    <author>
      <name>Brian Gallagher</name>
    </author>
    <link href="http://arxiv.org/abs/1411.3749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.3749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2108.00079v2</id>
    <updated>2021-08-05T18: 57: 34Z</updated>
    <published>2021-07-29T00: 54: 02Z</published>
    <title>Zooming Into the Darknet: Characterizing Internet Background Radiation
  and its Structural Changes</title>
    <summary>  Network telescopes or "Darknets" provide a unique window into Internet-wide
malicious activities associated with malware propagation, denial of service
attacks, scanning performed for network reconnaissance, and others. Analyses of
the resulting data can provide actionable insights to security analysts that
can be used to prevent or mitigate cyber-threats. Large Darknets, however,
observe millions of nefarious events on a daily basis which makes the
transformation of the captured information into meaningful insights
challenging. We present a novel framework for characterizing Darknet behavior
and its temporal evolution aiming to address this challenge. The proposed
framework: (i) Extracts a high dimensional representation of Darknet events
composed of features distilled from Darknet data and other external sources;
(ii) Learns, in an unsupervised fashion, an information-preserving
low-dimensional representation of these events (using deep representation
learning) that is amenable to clustering; (iv) Performs clustering of the
scanner data in the resulting representation space and provides interpretable
insights using optimal decision trees; and (v) Utilizes the clustering outcomes
as "signatures" that can be used to detect structural changes in the Darknet
activities. We evaluate the proposed system on a large operational Network
Telescope and demonstrate its ability to detect real-world, high-impact
cybersecurity incidents.
</summary>
    <author>
      <name>Michalis Kallitsis</name>
    </author>
    <author>
      <name>Vasant Honavar</name>
    </author>
    <author>
      <name>Rupesh Prajapati</name>
    </author>
    <author>
      <name>Dinghao Wu</name>
    </author>
    <author>
      <name>John Yen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages,
                            14 tables,
                            12 figures, arXiv.org perpetual, non-exclusive
  license</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.00079v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.00079v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2203.11493v1</id>
    <updated>2022-03-22T07: 05: 57Z</updated>
    <published>2022-03-22T07: 05: 57Z</published>
    <title>FrameHopper: Selective Processing of Video Frames in Detection-driven
  Real-Time Video Analytics</title>
    <summary>  Detection-driven real-time video analytics require continuous detection of
objects contained in the video frames using deep learning models like YOLOV3,
EfficientDet. However, running these detectors on each and every frame in
resource-constrained edge devices is computationally intensive. By taking the
temporal correlation between consecutive video frames into account, we note
that detection outputs tend to be overlapping in successive frames. Elimination
of similar consecutive frames will lead to a negligible drop in performance
while offering significant performance benefits by reducing overall computation
and communication costs. The key technical questions are, therefore, (a) how to
identify which frames to be processed by the object detector, and (b) how many
successive frames can be skipped (called skip-length) once a frame is selected
to be processed. The overall goal of the process is to keep the error due to
skipping frames as small as possible. We introduce a novel error vs processing
rate optimization problem with respect to the object detection task that
balances between the error rate and the fraction of frames filtering.
Subsequently, we propose an off-line Reinforcement Learning (RL)-based
algorithm to determine these skip-lengths as a state-action policy of the RL
agent from a recorded video and then deploy the agent online for live video
streams. To this end, we develop FrameHopper, an edge-cloud collaborative video
analytics framework, that runs a lightweight trained RL agent on the camera and
passes filtered frames to the server where the object detection model runs for
a set of applications. We have tested our approach on a number of live videos
captured from real-life scenarios and show that FrameHopper processes only a
handful of frames but produces detection results closer to the oracle solution
and outperforms recent state-of-the-art solutions in most cases.
</summary>
    <author>
      <name>Md Adnan Arefeen</name>
    </author>
    <author>
      <name>Sumaiya Tabassum Nimi</name>
    </author>
    <author>
      <name>Md Yusuf Sarwar Uddin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in The 18th International Conference on Distributed
  Computing in Sensor Systems (DCOSS 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.11493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.11493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1704.00616v2</id>
    <updated>2017-05-26T18: 40: 14Z</updated>
    <published>2017-04-03T14: 29: 40Z</published>
    <title>Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance
  for Action Classification and Detection</title>
    <summary>  General human action recognition requires understanding of various visual
cues. In this paper, we propose a network architecture that computes and
integrates the most important visual cues for action recognition: pose, motion,
and the raw images. For the integration, we introduce a Markov chain model
which adds cues successively. The resulting approach is efficient and
applicable to action classification as well as to spatial and temporal action
localization. The two contributions clearly improve the performance over
respective baselines. The overall approach achieves state-of-the-art action
classification performance on HMDB51, J-HMDB and NTU RGB+D datasets. Moreover,
it yields state-of-the-art spatio-temporal action localization results on
UCF101 and J-HMDB.
</summary>
    <author>
      <name>Mohammadreza Zolfaghari</name>
    </author>
    <author>
      <name>Gabriel L. Oliveira</name>
    </author>
    <author>
      <name>Nima Sedaghat</name>
    </author>
    <author>
      <name>Thomas Brox</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,
                            7 figures, ICCV 2017 submission</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00616v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00616v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2408.05955v1</id>
    <updated>2024-08-12T07: 09: 12Z</updated>
    <published>2024-08-12T07: 09: 12Z</published>
    <title>Probabilistic Vision-Language Representation for Weakly Supervised
  Temporal Action Localization</title>
    <summary>  Weakly supervised temporal action localization (WTAL) aims to detect action
instances in untrimmed videos using only video-level annotations. Since many
existing works optimize WTAL models based on action classification labels, they
encounter the task discrepancy problem (i.e., localization-by-classification).
To tackle this issue, recent studies have attempted to utilize action category
names as auxiliary semantic knowledge through vision-language pre-training
(VLP). However, there are still areas where existing research falls short.
Previous approaches primarily focused on leveraging textual information from
language models but overlooked the alignment of dynamic human action and VLP
knowledge in a joint space. Furthermore, the deterministic representation
employed in previous studies struggles to capture fine-grained human motions.
To address these problems, we propose a novel framework that aligns human
action knowledge and VLP knowledge in a probabilistic embedding space.
Moreover, we propose intra- and inter-distribution contrastive learning to
enhance the probabilistic embedding space based on statistical similarities.
Extensive experiments and ablation studies reveal that our method significantly
outperforms all previous state-of-the-art methods. Code is available at
https: //github.com/sejong-rcv/PVLR.
</summary>
    <author>
      <name>Geuntaek Lim</name>
    </author>
    <author>
      <name>Hyunwoo Kim</name>
    </author>
    <author>
      <name>Joonsoo Kim</name>
    </author>
    <author>
      <name>Yukyung Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACM MM 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.05955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.05955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2112.11721v1</id>
    <updated>2021-12-22T08: 11: 58Z</updated>
    <published>2021-12-22T08: 11: 58Z</published>
    <title>Towards Malicious address identification in Bitcoin</title>
    <summary>  The temporal aspect of blockchain transactions enables us to study the
address's behavior and detect if it is involved in any illicit activity.
However, due to the concept of change addresses (used to thwart replay
attacks), temporal aspects are not directly applicable in the Bitcoin
blockchain. Several pre-processing steps should be performed before such
temporal aspects are utilized. We are motivated to study the Bitcoin
transaction network and use the temporal features such as burst,
attractiveness, and inter-event time along with several graph-based properties
such as the degree of node and clustering coefficient to validate the
applicability of already existing approaches known for other cryptocurrency
blockchains on the Bitcoin blockchain. We generate the temporal and
non-temporal feature set and train the Machine Learning (ML) algorithm over
different temporal granularities to validate the state-of-the-art methods. We
study the behavior of the addresses over different time granularities of the
dataset. We identify that after applying change-address clustering, in Bitcoin,
existing temporal features can be extracted and ML approaches can be applied. A
comparative analysis of results show that the behavior of addresses in Ethereum
and Bitcoin is similar with respect to in-degree, out-degree and inter-event
time. Further, we identify 3 suspects that showed malicious behavior across
different temporal granularities. These suspects are not marked as malicious in
Bitcoin.
</summary>
    <author>
      <name>Deepesh Chaudhari</name>
    </author>
    <author>
      <name>Rachit Agarwal</name>
    </author>
    <author>
      <name>Sandeep Kumar Shukla</name>
    </author>
    <link href="http://arxiv.org/abs/2112.11721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.11721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1812.06082v1</id>
    <updated>2018-12-03T02: 15: 26Z</updated>
    <published>2018-12-03T02: 15: 26Z</published>
    <title>Modeling Temporal Evidence from External Collections</title>
    <summary>  Newsworthy events are broadcast through multiple mediums and prompt the
crowds to produce comments on social media. In this paper, we propose to
leverage on this behavioral dynamics to estimate the most relevant time periods
for an event (i.e., query). Recent advances have shown how to improve the
estimation of the temporal relevance of such topics. In this approach, we build
on two major novelties. First, we mine temporal evidences from hundreds of
external sources into topic-based external collections to improve the
robustness of the detection of relevant time periods. Second, we propose a
formal retrieval model that generalizes the use of the temporal dimension
across different aspects of the retrieval process. In particular, we show that
temporal evidence of external collections can be used to (i) infer a topic's
temporal relevance, (ii) select the query expansion terms, and (iii) re-rank
the final results for improved precision. Experiments with TREC Microblog
collections show that the proposed time-aware retrieval model makes an
effective and extensive use of the temporal dimension to improve search results
over the most recent temporal models. Interestingly, we observe a strong
correlation between precision and the temporal distribution of retrieved and
relevant documents.
</summary>
    <author>
      <name>Flávio Martins</name>
    </author>
    <author>
      <name>João Magalhães</name>
    </author>
    <author>
      <name>Jamie Callan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3289600.3290966</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3289600.3290966" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in WSDM 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.06082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.06082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1604.08916v2</id>
    <updated>2016-05-02T19: 29: 39Z</updated>
    <published>2016-04-29T17: 07: 47Z</published>
    <title>Inferring models of bacterial dynamics toward point sources</title>
    <summary>  Experiments have shown that bacteria can be sensitive to small variations in
chemoattractant (CA) concentrations. Motivated by these findings, our focus
here is on a regime rarely studied in experiments: bacteria tracking point CA
sources (such as food patches or even prey). In tracking point sources, the CA
detected by bacteria may show very large spatiotemporal fluctuations which vary
with distance from the source. We present a general statistical model to
describe how bacteria locate point sources of food on the basis of stochastic
event detection, rather than CA gradient information. We show how all model
parameters can be directly inferred from single cell tracking data even in the
limit of high detection noise. Once parameterized, our model recapitulates
bacterial behavior around point sources such as the "volcano effect". In
addition, while the search by bacteria for point sources such as prey may
appear random, our model identifies key statistical signatures of a targeted
search for a point source given any arbitrary source configuration.
</summary>
    <author>
      <name>Hossein Jashnsaz</name>
    </author>
    <author>
      <name>Tyler Nguyen</name>
    </author>
    <author>
      <name>Horia I. Petrache</name>
    </author>
    <author>
      <name>Steve Pressé</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0140428</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0140428" rel="related"/>
    <link href="http://arxiv.org/abs/1604.08916v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08916v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.CB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.CB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2208.06411v2</id>
    <updated>2023-03-09T02: 16: 14Z</updated>
    <published>2022-08-12T01: 20: 51Z</published>
    <title>SFF-DA: Sptialtemporal Feature Fusion for Detecting Anxiety
  Nonintrusively</title>
    <summary>  Early detection of anxiety is crucial for reducing the suffering of
individuals with mental disorders and improving treatment outcomes. Utilizing
an mHealth platform for anxiety screening can be particularly practical in
improving screening efficiency and reducing costs. However, the effectiveness
of existing methods has been hindered by differences in mobile devices used to
capture subjects' physical and mental evaluations, as well as by the
variability in data quality and small sample size problems encountered in
real-world settings. To address these issues, we propose a framework with
spatiotemporal feature fusion for detecting anxiety nonintrusively. We use a
feature extraction network based on a 3D convolutional network and long
short-term memory ("3DCNN+LSTM") to fuse the spatiotemporal features of facial
behavior and noncontact physiology, which reduces the impact of uneven data
quality. Additionally, we design a similarity assessment strategy to address
the issue of deteriorating model accuracy due to small sample sizes. Our
framework is validated with a crew dataset from the real world and two public
datasets: the University of Burgundy Franche-Comt\'e Psychophysiological
(UBFC-Phys) dataset and the Smart Reasoning for Well-being at Home and at Work
for Knowledge Work (SWELL-KW) dataset. The experimental results indicate that
our framework outperforms the comparison methods.
</summary>
    <author>
      <name>Haimiao Mo</name>
    </author>
    <author>
      <name>Yuchen Li</name>
    </author>
    <author>
      <name>Shanlin Yang</name>
    </author>
    <author>
      <name>Wei Zhang</name>
    </author>
    <author>
      <name>Shuai Ding</name>
    </author>
    <link href="http://arxiv.org/abs/2208.06411v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.06411v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1203.5126v1</id>
    <updated>2012-03-22T21: 03: 28Z</updated>
    <published>2012-03-22T21: 03: 28Z</published>
    <title>Online detection of temporal communities in evolving networks by
  estrangement confinement</title>
    <summary>  Temporal communities result from a consistent partitioning of nodes across
multiple snapshots of an evolving complex network that can help uncover how
dense clusters in a network emerge, combine, split and decay with time. Current
methods for finding communities in a single snapshot are not straightforwardly
generalizable to finding temporal communities since the quality functions used
for finding static communities have highly degenerate landscapes, and the
eventual partition chosen among the many partitions of similar quality is
highly sensitive to small changes in the network. To reliably detect temporal
communities we need not only to find a good community partition in a given
snapshot but also ensure that it bears some similarity to the partition(s)
found in immediately preceding snapshots. We present a new measure of partition
distance called "estrangement" motivated by the inertia of inter-node
relationships which, when incorporated into the measurement of partition
quality, facilitates the detection of meaningful temporal communities.
Specifically, we propose the estrangement confinement method, which postulates
that neighboring nodes in a community prefer to continue to share community
affiliation as the network evolves. Constraining estrangement enables us to
find meaningful temporal communities at various degrees of temporal smoothness
in diverse real-world datasets. Specifically, we study the evolution of voting
behavior of senators in the United States Congress, the evolution of proximity
in human mobility datasets, and the detection of evolving communities in
synthetic networks that are otherwise hard to find. Estrangement confinement
thus provides a principled approach to uncovering temporal communities in
evolving networks.
</summary>
    <author>
      <name>Vikas Kawadia</name>
    </author>
    <author>
      <name>Sameet Sreenivasan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/srep00794</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/srep00794" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scientific Reports 2, Article number: 794, Mar 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.5126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.5126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.10450v1</id>
    <updated>2025-03-13T15: 15: 54Z</updated>
    <published>2025-03-13T15: 15: 54Z</published>
    <title>Consistent multi-animal pose estimation in cattle using dynamic Kalman
  filter based tracking</title>
    <summary>  Over the past decade, studying animal behaviour with the help of computer
vision has become more popular. Replacing human observers by computer vision
lowers the cost of data collection and therefore allows to collect more
extensive datasets. However, the majority of available computer vision
algorithms to study animal behaviour is highly tailored towards a single
research objective, limiting possibilities for data reuse. In this perspective,
pose-estimation in combination with animal tracking offers opportunities to
yield a higher level representation capturing both the spatial and temporal
component of animal behaviour. Such a higher level representation allows to
answer a wide variety of research questions simultaneously, without the need to
develop repeatedly tailored computer vision algorithms. In this paper, we
therefore first cope with several weaknesses of current pose-estimation
algorithms and thereafter introduce KeySORT (Keypoint Simple and Online
Realtime Tracking). KeySORT deploys an adaptive Kalman filter to construct
tracklets in a bounding-box free manner, significantly improving the temporal
consistency of detected keypoints. In this paper, we focus on pose estimation
in cattle, but our methodology can easily be generalised to any other animal
species. Our test results indicate our algorithm is able to detect up to 80% of
the ground truth keypoints with high accuracy, with only a limited drop in
performance when daylight recordings are compared to nightvision recordings.
Moreover, by using KeySORT to construct skeletons, the temporal consistency of
generated keypoint coordinates was largely improved, offering opportunities
with regard to automated behaviour monitoring of animals.
</summary>
    <author>
      <name>Maarten Perneel</name>
    </author>
    <author>
      <name>Ines Adriaens</name>
    </author>
    <author>
      <name>Ben Aernouts</name>
    </author>
    <author>
      <name>Jan Verwaeren</name>
    </author>
    <link href="http://arxiv.org/abs/2503.10450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.10450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2411.03210v1</id>
    <updated>2024-11-05T15: 59: 30Z</updated>
    <published>2024-11-05T15: 59: 30Z</published>
    <title>bursty_dynamics: A Python Package for Exploring the Temporal Properties
  of Longitudinal Data</title>
    <summary>  Understanding the temporal properties of longitudinal data is critical for
identifying trends, predicting future events, and making informed decisions in
any field where temporal data is analysed, including health and epidemiology,
finance, geosciences, and social sciences. Traditional time-series analysis
techniques often fail to capture the complexity of irregular temporal patterns
present in such data. To address this gap, we introduce bursty_dynamics, a
Python package that enables the quantification of bursty dynamics through the
calculation of the Burstiness Parameter (BP) and Memory Coefficient (MC). In
temporal data, BP and MC provide insights into the irregularity and temporal
dependencies within event sequences, shedding light on complex patterns of
disease aetiology, human behaviour, or other information diffusion over time.
An event train detection method is also implemented to identify clustered
events occurring within a specified time interval, allowing for more focused
analysis with reduced noise. With built-in visualisation tools, bursty_dynamics
provides an accessible yet powerful platform for researchers to explore and
interpret the temporal dynamics of longitudinal data. This paper outlines the
core functionalities of the package, demonstrates its applications in diverse
research domains, and discusses the advantages of using BP, MC, and event train
detection for enhanced temporal data analysis.
</summary>
    <author>
      <name>Alisha Angdembe</name>
    </author>
    <author>
      <name>Wasim A Iqbal</name>
    </author>
    <author>
      <name>Rebeen Ali Hamad</name>
    </author>
    <author>
      <name>John Casement</name>
    </author>
    <author>
      <name>AI-Multiply Consortium</name>
    </author>
    <author>
      <name>Paolo Missier</name>
    </author>
    <author>
      <name>Nick Reynolds</name>
    </author>
    <author>
      <name>Rafael Henkin</name>
    </author>
    <author>
      <name>Michael R Barnes</name>
    </author>
    <link href="http://arxiv.org/abs/2411.03210v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.03210v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.12857v1</id>
    <updated>2024-04-19T12: 50: 51Z</updated>
    <published>2024-04-19T12: 50: 51Z</published>
    <title>Development of Two-Dimensional Neutron Imager with a Sandwich
  Configuration</title>
    <summary>  We have developed a two-dimensional neutron imager based on a semiconductor
pixelated sensor, especially designed for experiments measuring of a spatial
and a temporal behavior of quantum bound states of ultra-cold neutrons. Through
these measurements, we expect to measure the ratio between the inertial and
gravitational masses of neutrons and to test the equivalence principle in the
quantum regime. As one of the principal neutron imagers, we fabricated a sensor
with a sandwich configuration, named 10B-INTPIX4-sw, and tested its response to
ultra-cold neutrons at the Los Alamos Neutron Science Center (LANSCE). We
observed simultaneous events on both sandwiching sensors without significant
loss of detection efficiency. The efficiency was evaluated to be about 16%,
relative to the 10B/ZnS reference detector. The coincidence condition reduces
its efficiency by a factor of about 3.
</summary>
    <author>
      <name>Y. Kamiya</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UTokyo</arxiv:affiliation>
    </author>
    <author>
      <name>R. Nishimura</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">KEK</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SOKENDAI</arxiv:affiliation>
    </author>
    <author>
      <name>S. Mitsui</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shiga Univ</arxiv:affiliation>
    </author>
    <author>
      <name>Z. Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LANL</arxiv:affiliation>
    </author>
    <author>
      <name>C. L. Morris</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LANL</arxiv:affiliation>
    </author>
    <author>
      <name>M. Makela</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LANL</arxiv:affiliation>
    </author>
    <author>
      <name>S. M. Clayton</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LANL</arxiv:affiliation>
    </author>
    <author>
      <name>J. K. Baldwin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LANL</arxiv:affiliation>
    </author>
    <author>
      <name>T. M. Ito</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LANL</arxiv:affiliation>
    </author>
    <author>
      <name>S. Akamatsu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Rikkyo Univ</arxiv:affiliation>
    </author>
    <author>
      <name>H. Iwase</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">KEK</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SOKENDAI</arxiv:affiliation>
    </author>
    <author>
      <name>Y. Arai</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">KEK</arxiv:affiliation>
    </author>
    <author>
      <name>J. Murata</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Rikkyo Univ</arxiv:affiliation>
    </author>
    <author>
      <name>S. Asai</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UTokyo</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.nima.2024.169390</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.nima.2024.169390" rel="related"/>
    <link href="http://arxiv.org/abs/2404.12857v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.12857v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2401.10300v2</id>
    <updated>2024-10-28T03: 33: 04Z</updated>
    <published>2024-01-18T08: 55: 05Z</published>
    <title>A Hierarchical Framework with Spatio-Temporal Consistency Learning for
  Emergence Detection in Complex Adaptive Systems</title>
    <summary>  Emergence, a global property of complex adaptive systems (CASs) constituted
by interactive agents, is prevalent in real-world dynamic systems, e.g.,
network-level traffic congestions. Detecting its formation and evaporation
helps to monitor the state of a system, allowing to issue a warning signal for
harmful emergent phenomena. Since there is no centralized controller of CAS,
detecting emergence based on each agent's local observation is desirable but
challenging. Existing works are unable to capture emergence-related spatial
patterns, and fail to model the nonlinear relationships among agents. This
paper proposes a hierarchical framework with spatio-temporal consistency
learning to solve these two problems by learning the system representation and
agent representations, respectively. Spatio-temporal encoders composed of
spatial and temporal transformers are designed to capture agents' nonlinear
relationships and the system's complex evolution. Agents' and the system's
representations are learned to preserve the spatio-temporal consistency by
minimizing the spatial and temporal dissimilarities in a self-supervised manner
in the latent space. Our method achieves more accurate detection than
traditional methods and deep learning methods on three datasets with well-known
yet hard-to-detect emergent behaviors. Notably, our hierarchical framework is
generic in incorporating other deep learning methods for agent-level and
system-level detection.
</summary>
    <author>
      <name>Siyuan Chen</name>
    </author>
    <author>
      <name>Xin Du</name>
    </author>
    <author>
      <name>Jiahai Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TNNLS.2024.3477320</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TNNLS.2024.3477320" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, accepted by IEEE TNNLS</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.10300v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.10300v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1611.05365v4</id>
    <updated>2017-08-02T15: 10: 27Z</updated>
    <published>2016-11-16T17: 01: 27Z</published>
    <title>Am I a Baller? Basketball Performance Assessment from First-Person
  Videos</title>
    <summary>  This paper presents a method to assess a basketball player's performance from
his/her first-person video. A key challenge lies in the fact that the
evaluation metric is highly subjective and specific to a particular evaluator.
We leverage the first-person camera to address this challenge. The
spatiotemporal visual semantics provided by a first-person view allows us to
reason about the camera wearer's actions while he/she is participating in an
unscripted basketball game. Our method takes a player's first-person video and
provides a player's performance measure that is specific to an evaluator's
preference.
  To achieve this goal, we first use a convolutional LSTM network to detect
atomic basketball events from first-person videos. Our network's ability to
zoom-in to the salient regions addresses the issue of a severe camera wearer's
head movement in first-person videos. The detected atomic events are then
passed through the Gaussian mixtures to construct a highly non-linear visual
spatiotemporal basketball assessment feature. Finally, we use this feature to
learn a basketball assessment model from pairs of labeled first-person
basketball videos, for which a basketball expert indicates, which of the two
players is better.
  We demonstrate that despite not knowing the basketball evaluator's criterion,
our model learns to accurately assess the players in real-world games.
Furthermore, our model can also discover basketball events that contribute
positively and negatively to a player's performance.
</summary>
    <author>
      <name>Gedas Bertasius</name>
    </author>
    <author>
      <name>Hyun Soo Park</name>
    </author>
    <author>
      <name>Stella X. Yu</name>
    </author>
    <author>
      <name>Jianbo Shi</name>
    </author>
    <link href="http://arxiv.org/abs/1611.05365v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.05365v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1807.03269v1</id>
    <updated>2018-07-03T09: 26: 28Z</updated>
    <published>2018-07-03T09: 26: 28Z</published>
    <title>Full-Field Interferometric Imaging of Propagating Action Potentials</title>
    <summary>  Currently, cellular action potentials are detected using either electrical
recordings or exogenous fluorescent probes sensing calcium concentration or
transmembrane voltage. Ca imaging has low temporal resolution, while voltage
indicators are vulnerable to phototoxicity, photobleaching and heating. Here we
report full-field interferometric imaging of individual action potentials by
detecting the movement across the entire cell membrane. Using spike-triggered
averaging of the movies synchronized to electrical recording, we demonstrate
deformations of up to 3 nm (0.9 mrad) during the action potential in spiking
HEK-293 cells, with a rise time of 4 ms. The time course of the
optically-recorded spikes matches electrical waveforms. Since the shot noise
limit of the camera (~2 mrad/pix) precludes detection of the action potential
in a single frame, for all-optical spike detection, images are acquired at 50
kHz, and 50 frames are binned into 1 ms steps to achieve a sensitivity of 0.3
mrad in a single pixel. Using self-reinforcing sensitivity enhancement
algorithm based on iteratively expanding the region of interest for spatial
averaging, individual spikes can be detected by matching the previously
extracted template of the action potential with the optical recording. This
allows all-optical full-field imaging of the propagating action potentials
without exogeneous labels or electrodes.
</summary>
    <author>
      <name>Tong Ling</name>
    </author>
    <author>
      <name>Kevin C. Boyle</name>
    </author>
    <author>
      <name>Georges Goetz</name>
    </author>
    <author>
      <name>Peng Zhou</name>
    </author>
    <author>
      <name>Yi Quan</name>
    </author>
    <author>
      <name>Felix S. Alfonso</name>
    </author>
    <author>
      <name>Tiffany W. Huang</name>
    </author>
    <author>
      <name>Daniel Palanker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s41377-018-0107-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s41377-018-0107-9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages,
                            5 figures, supplementary information available as
  'Ancillary Files'; videos available upon request</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Light: Science &amp; Applications 7,
                            107 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.03269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2410.01011v2</id>
    <updated>2024-10-03T22: 08: 22Z</updated>
    <published>2024-10-01T19: 02: 06Z</published>
    <title>Back to Bayesics: Uncovering Human Mobility Distributions and Anomalies
  with an Integrated Statistical and Neural Framework</title>
    <summary>  Existing methods for anomaly detection often fall short due to their
inability to handle the complexity, heterogeneity, and high dimensionality
inherent in real-world mobility data. In this paper, we propose DeepBayesic, a
novel framework that integrates Bayesian principles with deep neural networks
to model the underlying multivariate distributions from sparse and complex
datasets. Unlike traditional models, DeepBayesic is designed to manage
heterogeneous inputs, accommodating both continuous and categorical data to
provide a more comprehensive understanding of mobility patterns. The framework
features customized neural density estimators and hybrid architectures,
allowing for flexibility in modeling diverse feature distributions and enabling
the use of specialized neural networks tailored to different data types. Our
approach also leverages agent embeddings for personalized anomaly detection,
enhancing its ability to distinguish between normal and anomalous behaviors for
individual agents. We evaluate our approach on several mobility datasets,
demonstrating significant improvements over state-of-the-art anomaly detection
methods. Our results indicate that incorporating personalization and advanced
sequence modeling techniques can substantially enhance the ability to detect
subtle and complex anomalies in spatiotemporal event sequences.
</summary>
    <author>
      <name>Minxuan Duan</name>
    </author>
    <author>
      <name>Yinlong Qian</name>
    </author>
    <author>
      <name>Lingyi Zhao</name>
    </author>
    <author>
      <name>Zihao Zhou</name>
    </author>
    <author>
      <name>Zeeshan Rasheed</name>
    </author>
    <author>
      <name>Rose Yu</name>
    </author>
    <author>
      <name>Khurram Shafique</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.01011v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.01011v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1711.06427v1</id>
    <updated>2017-11-17T06: 32: 09Z</updated>
    <published>2017-11-17T06: 32: 09Z</published>
    <title>Action-Attending Graphic Neural Network</title>
    <summary>  The motion analysis of human skeletons is crucial for human action
recognition, which is one of the most active topics in computer vision. In this
paper, we propose a fully end-to-end action-attending graphic neural network
(A$^2$GNN) for skeleton-based action recognition, in which each irregular
skeleton is structured as an undirected attribute graph. To extract high-level
semantic representation from skeletons, we perform the local spectral graph
filtering on the constructed attribute graphs like the standard image
convolution operation. Considering not all joints are informative for action
analysis, we design an action-attending layer to detect those salient action
units (AUs) by adaptively weighting skeletal joints. Herein the filtering
responses are parameterized into a weighting function irrelevant to the order
of input nodes. To further encode continuous motion variations, the deep
features learnt from skeletal graphs are gathered along consecutive temporal
slices and then fed into a recurrent gated network. Finally, the spectral graph
filtering, action-attending and recurrent temporal encoding are integrated
together to jointly train for the sake of robust action recognition as well as
the intelligibility of human actions. To evaluate our A$^2$GNN, we conduct
extensive experiments on four benchmark skeleton-based action datasets,
including the large-scale challenging NTU RGB+D dataset. The experimental
results demonstrate that our network achieves the state-of-the-art
performances.
</summary>
    <author>
      <name>Chaolong Li</name>
    </author>
    <author>
      <name>Zhen Cui</name>
    </author>
    <author>
      <name>Wenming Zheng</name>
    </author>
    <author>
      <name>Chunyan Xu</name>
    </author>
    <author>
      <name>Rongrong Ji</name>
    </author>
    <author>
      <name>Jian Yang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2018.2815744</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2018.2815744" rel="related"/>
    <link href="http://arxiv.org/abs/1711.06427v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.06427v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1702.08091v1</id>
    <updated>2017-02-26T21: 52: 43Z</updated>
    <published>2017-02-26T21: 52: 43Z</published>
    <title>Modeling mechanical interactions in growing populations of rod-shaped
  bacteria</title>
    <summary>  Advances in synthetic biology allow us to engineer bacterial collectives with
pre-specified characteristics. However, the behavior of these collectives is
difficult to understand, as cellular growth and division as well as
extra-cellular fluid flow lead to complex, changing arrangements of cells
within the population. To rationally engineer and control the behavior of cell
collectives we need theoretical and computational tools to understand their
emergent spatiotemporal dynamics. Here, we present an agent-based model that
allows growing cells to detect and respond to mechanical interactions.
Crucially, our model couples the dynamics of cell growth to the cell's
environment: Mechanical constraints can affect cellular growth rate and a cell
may alter its behavior in response to these constraints. This coupling links
the mechanical forces that influence cell growth and emergent behaviors in cell
assemblies. We illustrate our approach by showing how mechanical interactions
can impact the dynamics of bacterial collectives growing in microfluidic traps.
</summary>
    <author>
      <name>James J. Winkle</name>
    </author>
    <author>
      <name>Oleg Igoshin</name>
    </author>
    <author>
      <name>Matthew R. Bennett</name>
    </author>
    <author>
      <name>Krešimir Josić</name>
    </author>
    <author>
      <name>William Ott</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1478-3975/aa7bae</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1478-3975/aa7bae" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages,
                            5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.08091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.08091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.CB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.CB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92C" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1612.06570v1</id>
    <updated>2016-12-20T09: 49: 50Z</updated>
    <published>2016-12-20T09: 49: 50Z</published>
    <title>Coronal activity cycles in action - X-rays from alpha Centauri A/B</title>
    <summary>  We report on the coronal activity cycles of our stellar neighbors alpha
Centauri A/B. The binary has been monitored with XMM-Newton since 2002 to study
the long-term evolution of coronal activity evolution in X-rays. The solar
analog alpha Cen A was clearly detected early in the program, but virtually
faded away from XMM's detectors view around 2005. After remaining nearly a
decade in a state of coronal weakness, we now detect a clear re-brightening of
its corona. The secondary alpha Cen B dominates the X-ray emission at most
times and more than a full cycle is covered for this star. A new X-ray maximum
was observed around 2012 that is again followed by gentle dimming over the
recent years. The temporal evolution of the X-ray emission can be well
understood, in analogy to the 11 year solar-cycle, by coronal activity cycles
with different amplitudes and periods operating in both stars.
</summary>
    <author>
      <name>J. Robrade</name>
    </author>
    <author>
      <name>J. H. M. M. Schmitt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Cool Stars 19 (The 19th Cambridge Workshop on Cool
  Stars, Stellar Systems, and the Sun, edited by G.A. Feiden); 3 pages,
                            5
  figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.06570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1704.03067v1</id>
    <updated>2017-04-10T21: 58: 56Z</updated>
    <published>2017-04-10T21: 58: 56Z</published>
    <title>Action Unit Detection with Region Adaptation, Multi-labeling Learning
  and Optimal Temporal Fusing</title>
    <summary>  Action Unit (AU) detection becomes essential for facial analysis. Many
proposed approaches face challenging problems in dealing with the alignments of
different face regions, in the effective fusion of temporal information, and in
training a model for multiple AU labels. To better address these problems, we
propose a deep learning framework for AU detection with region of interest
(ROI) adaptation, integrated multi-label learning, and optimal LSTM-based
temporal fusing. First, ROI cropping nets (ROI Nets) are designed to make sure
specifically interested regions of faces are learned independently; each
sub-region has a local convolutional neural network (CNN) - an ROI Net, whose
convolutional filters will only be trained for the corresponding region.
Second, multi-label learning is employed to integrate the outputs of those
individual ROI cropping nets, which learns the inter-relationships of various
AUs and acquires global features across sub-regions for AU detection. Finally,
the optimal selection of multiple LSTM layers to form the best LSTM Net is
carried out to best fuse temporal features, in order to make the AU prediction
the most accurate. The proposed approach is evaluated on two popular AU
detection datasets, BP4D and DISFA, outperforming the state of the art
significantly, with an average improvement of around 13% on BP4D and 25% on
DISFA, respectively.
</summary>
    <author>
      <name>Wei Li</name>
    </author>
    <author>
      <name>Farnaz Abitahi</name>
    </author>
    <author>
      <name>Zhigang Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper is accepted to CVPR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.03067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2205.02717v3</id>
    <updated>2023-04-10T14: 57: 34Z</updated>
    <published>2022-05-05T15: 42: 56Z</published>
    <title>BasicTAD: an Astounding RGB-Only Baseline for Temporal Action Detection</title>
    <summary>  Temporal action detection (TAD) is extensively studied in the video
understanding community by generally following the object detection pipeline in
images. However, complex designs are not uncommon in TAD, such as two-stream
feature extraction, multi-stage training, complex temporal modeling, and global
context fusion. In this paper, we do not aim to introduce any novel technique
for TAD. Instead, we study a simple, straightforward, yet must-known baseline
given the current status of complex design and low detection efficiency in TAD.
In our simple baseline (termed BasicTAD), we decompose the TAD pipeline into
several essential components: data sampling, backbone design, neck
construction, and detection head. We extensively investigate the existing
techniques in each component for this baseline, and more importantly, perform
end-to-end training over the entire pipeline thanks to the simplicity of
design. As a result, this simple BasicTAD yields an astounding and real-time
RGB-Only baseline very close to the state-of-the-art methods with two-stream
inputs. In addition, we further improve the BasicTAD by preserving more
temporal and spatial information in network representation (termed as PlusTAD).
Empirical results demonstrate that our PlusTAD is very efficient and
significantly outperforms the previous methods on the datasets of THUMOS14 and
FineAction. Meanwhile, we also perform in-depth visualization and error
analysis on our proposed method and try to provide more insights on the TAD
problem. Our approach can serve as a strong baseline for future TAD research.
The code and model will be released at https: //github.com/MCG-NJU/BasicTAD.
</summary>
    <author>
      <name>Min Yang</name>
    </author>
    <author>
      <name>Guo Chen</name>
    </author>
    <author>
      <name>Yin-Dong Zheng</name>
    </author>
    <author>
      <name>Tong Lu</name>
    </author>
    <author>
      <name>Limin Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVIU</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.02717v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.02717v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2412.09832v1</id>
    <updated>2024-12-13T03: 51: 39Z</updated>
    <published>2024-12-13T03: 51: 39Z</published>
    <title>Multivariate Time Series Clustering for Environmental State
  Characterization of Ground-Based Gravitational-Wave Detectors</title>
    <summary>  Gravitational-wave observatories like LIGO are large-scale, terrestrial
instruments housed in infrastructure that spans a multi-kilometer geographic
area and which must be actively controlled to maintain operational stability
for long observation periods. Despite exquisite seismic isolation, they remain
susceptible to seismic noise and other terrestrial disturbances that can couple
undesirable vibrations into the instrumental infrastructure, potentially
leading to control instabilities or noise artifacts in the detector output. It
is, therefore, critical to characterize the seismic state of these
observatories to identify a set of temporal patterns that can inform the
detector operators in day-to-day monitoring and diagnostics. On a day-to-day
basis, the operators monitor several seismically relevant data streams to
diagnose operational instabilities and sources of noise using some simple
empirically-determined thresholds. It can be untenable for a human operator to
monitor multiple data streams in this manual fashion and thus a distillation of
these data-streams into a more human-friendly format is sought. In this paper,
we present an end-to-end machine learning pipeline for features-based
multivariate time series clustering to achieve this goal and to provide
actionable insights to the detector operators by correlating found clusters
with events of interest in the detector.
</summary>
    <author>
      <name>Rutuja Gurav</name>
    </author>
    <author>
      <name>Isaac Kelly</name>
    </author>
    <author>
      <name>Pooyan Goodarzi</name>
    </author>
    <author>
      <name>Anamaria Effler</name>
    </author>
    <author>
      <name>Barry Barish</name>
    </author>
    <author>
      <name>Evangelos Papalexakis</name>
    </author>
    <author>
      <name>Jonathan Richardson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,
                            6 figures, Accepted to The 5th International Workshop on Big
  Data &amp; AI Tools, Methods, and Use Cases for Innovative Scientific Discovery
  (BTSD 2024)</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.09832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.09832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1908.05877v1</id>
    <updated>2019-08-16T08: 02: 48Z</updated>
    <published>2019-08-16T08: 02: 48Z</published>
    <title>Zero-Shot Crowd Behavior Recognition</title>
    <summary>  Understanding crowd behavior in video is challenging for computer vision.
There have been increasing attempts on modeling crowded scenes by introducing
ever larger property ontologies (attributes) and annotating ever larger
training datasets. However, in contrast to still images, manually annotating
video attributes needs to consider spatiotemporal evolution which is inherently
much harder and more costly. Critically, the most interesting crowd behaviors
captured in surveillance videos (e.g., street fighting, flash mobs) are either
rare, thus have few examples for model training, or unseen previously. Existing
crowd analysis techniques are not readily scalable to recognize novel (unseen)
crowd behaviors. To address this problem, we investigate and develop methods
for recognizing visual crowd behavioral attributes without any training
samples, i.e., zero-shot learning crowd behavior recognition. To that end, we
relax the common assumption that each individual crowd video instance is only
associated with a single crowd attribute. Instead, our model learns to jointly
recognize multiple crowd behavioral attributes in each video instance by
exploring multiattribute cooccurrence as contextual knowledge for optimizing
individual crowd attribute recognition. Joint multilabel attribute prediction
in zero-shot learning is inherently nontrivial because cooccurrence statistics
does not exist for unseen attributes. To solve this problem, we learn to
predict cross-attribute cooccurrence from both online text corpus and
multilabel annotation of videos with known attributes. Our experiments show
that this approach to modeling multiattribute context not only improves
zero-shot crowd behavior recognition on the WWW crowd video dataset, but also
generalizes to novel behavior (violence) detection cross-domain in the Violence
Flow video dataset.
</summary>
    <author>
      <name>Xun Xu</name>
    </author>
    <author>
      <name>Shaogang Gong</name>
    </author>
    <author>
      <name>Timothy Hospedales</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/B978-0-12-809276-7.00018-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/B978-0-12-809276-7.00018-7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Group and Crowd Behavior for Computer Vision 2017, Pages 341-369</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.05877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.05877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1606.07525v1</id>
    <updated>2016-06-24T00: 32: 41Z</updated>
    <published>2016-06-24T00: 32: 41Z</published>
    <title>Relating Knowledge and Coordinated Action: The Knowledge of
  Preconditions Principle</title>
    <summary>  The Knowledge of Preconditions principle (KoP) is proposed as a widely
applicable connection between knowledge and action in multi-agent systems.
Roughly speaking, it asserts that if some condition is a necessary condition
for performing a given action A, then knowing that this condition holds is also
a necessary condition for performing A. Since the specifications of tasks often
involve necessary conditions for actions, the KoP principle shows that such
specifications induce knowledge preconditions for the actions. Distributed
protocols or multi-agent plans that satisfy the specifications must ensure that
this knowledge be attained, and that it is detected by the agents as a
condition for action. The knowledge of preconditions principle is formalised in
the runs and systems framework, and is proven to hold in a wide class of
settings. Well-known connections between knowledge and coordinated action are
extended and shown to derive directly from the KoP principle: a "common
knowledge of preconditions" principle is established showing that common
knowledge is a necessary condition for performing simultaneous actions, and a
"nested knowledge of preconditions" principle is proven, showing that
coordinating actions to be performed in linear temporal order requires a
corresponding form of nested knowledge.
</summary>
    <author>
      <name>Yoram Moses</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.215.17</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.215.17" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings TARK 2015, arXiv: 1606.07295</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 215,
                            2016, pp. 231-245</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.07525v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07525v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1804.06055v1</id>
    <updated>2018-04-17T06: 00: 13Z</updated>
    <published>2018-04-17T06: 00: 13Z</published>
    <title>Co-occurrence Feature Learning from Skeleton Data for Action Recognition
  and Detection with Hierarchical Aggregation</title>
    <summary>  Skeleton-based human action recognition has recently drawn increasing
attentions with the availability of large-scale skeleton datasets. The most
crucial factors for this task lie in two aspects: the intra-frame
representation for joint co-occurrences and the inter-frame representation for
skeletons' temporal evolutions. In this paper we propose an end-to-end
convolutional co-occurrence feature learning framework. The co-occurrence
features are learned with a hierarchical methodology, in which different levels
of contextual information are aggregated gradually. Firstly point-level
information of each joint is encoded independently. Then they are assembled
into semantic representation in both spatial and temporal domains.
Specifically, we introduce a global spatial aggregation scheme, which is able
to learn superior joint co-occurrence features over local aggregation. Besides,
raw skeleton coordinates as well as their temporal difference are integrated
with a two-stream paradigm. Experiments show that our approach consistently
outperforms other state-of-the-arts on action recognition and detection
benchmarks like NTU RGB+D, SBU Kinect Interaction and PKU-MMD.
</summary>
    <author>
      <name>Chao Li</name>
    </author>
    <author>
      <name>Qiaoyong Zhong</name>
    </author>
    <author>
      <name>Di Xie</name>
    </author>
    <author>
      <name>Shiliang Pu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI18 oral</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.06055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2106.11811v1</id>
    <updated>2021-06-20T02: 58: 45Z</updated>
    <published>2021-06-20T02: 58: 45Z</published>
    <title>Weakly-Supervised Temporal Action Localization Through Local-Global
  Background Modeling</title>
    <summary>  Weakly-Supervised Temporal Action Localization (WS-TAL) task aims to
recognize and localize temporal starts and ends of action instances in an
untrimmed video with only video-level label supervision. Due to lack of
negative samples of background category, it is difficult for the network to
separate foreground and background, resulting in poor detection performance. In
this report, we present our 2021 HACS Challenge - Weakly-supervised Learning
Track solution that based on BaSNet to address above problem. Specifically, we
first adopt pre-trained CSN, Slowfast, TDN, and ViViT as feature extractors to
get feature sequences. Then our proposed Local-Global Background Modeling
Network (LGBM-Net) is trained to localize instances by using only video-level
labels based on Multi-Instance Learning (MIL). Finally, we ensemble multiple
models to get the final detection results and reach 22.45% mAP on the test set
</summary>
    <author>
      <name>Xiang Wang</name>
    </author>
    <author>
      <name>Zhiwu Qing</name>
    </author>
    <author>
      <name>Ziyuan Huang</name>
    </author>
    <author>
      <name>Yutong Feng</name>
    </author>
    <author>
      <name>Shiwei Zhang</name>
    </author>
    <author>
      <name>Jianwen Jiang</name>
    </author>
    <author>
      <name>Mingqian Tang</name>
    </author>
    <author>
      <name>Yuanjie Shao</name>
    </author>
    <author>
      <name>Nong Sang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR-2021 HACS Challenge - Weakly-supervised Learning Track champion
  solution (1st Place)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CVPRW-2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2106.11811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.11811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2211.06870v2</id>
    <updated>2023-02-04T21: 44: 34Z</updated>
    <published>2022-11-13T10: 29: 25Z</published>
    <title>Detecting Disengagement in Virtual Learning as an Anomaly using Temporal
  Convolutional Network Autoencoder</title>
    <summary>  Student engagement is an important factor in meeting the goals of virtual
learning programs. Automatic measurement of student engagement provides helpful
information for instructors to meet learning program objectives and
individualize program delivery. Many existing approaches solve video-based
engagement measurement using the traditional frameworks of binary
classification (classifying video snippets into engaged or disengaged classes),
multi-class classification (classifying video snippets into multiple classes
corresponding to different levels of engagement), or regression (estimating a
continuous value corresponding to the level of engagement). However, we observe
that while the engagement behaviour is mostly well-defined (e.g., focused, not
distracted), disengagement can be expressed in various ways. In addition, in
some cases, the data for disengaged classes may not be sufficient to train
generalizable binary or multi-class classifiers. To handle this situation, in
this paper, for the first time, we formulate detecting disengagement in virtual
learning as an anomaly detection problem. We design various autoencoders,
including temporal convolutional network autoencoder, long-short-term memory
autoencoder, and feedforward autoencoder using different behavioral and affect
features for video-based student disengagement detection. The result of our
experiments on two publicly available student engagement datasets, DAiSEE and
EmotiW, shows the superiority of the proposed approach for disengagement
detection as an anomaly compared to binary classifiers for classifying videos
into engaged versus disengaged classes (with an average improvement of 9% on
the area under the curve of the receiver operating characteristic curve and 22%
on the area under the curve of the precision-recall curve).
</summary>
    <author>
      <name>Ali Abedi</name>
    </author>
    <author>
      <name>Shehroz S. Khan</name>
    </author>
    <link href="http://arxiv.org/abs/2211.06870v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.06870v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2301.07051v1</id>
    <updated>2023-01-17T18: 06: 00Z</updated>
    <published>2023-01-17T18: 06: 00Z</published>
    <title>ActSafe: Predicting Violations of Medical Temporal Constraints for
  Medication Adherence</title>
    <summary>  Prescription medications often impose temporal constraints on regular health
behaviors (RHBs) of patients, e.g., eating before taking medication. Violations
of such medical temporal constraints (MTCs) can result in adverse effects.
Detecting and predicting such violations before they occur can help alert the
patient. We formulate the problem of modeling MTCs and develop a
proof-of-concept solution, ActSafe, to predict violations of MTCs well ahead of
time. ActSafe utilizes a context-free grammar based approach for extracting and
mapping MTCs from patient education materials. It also addresses the challenges
of accurately predicting RHBs central to MTCs (e.g., medication intake). Our
novel behavior prediction model, HERBERT , utilizes a basis vectorization of
time series that is generalizable across temporal scale and duration of
behaviors, explicitly capturing the dependency between temporally collocated
behaviors. Based on evaluation using a real-world RHB dataset collected from 28
patients in uncontrolled environments, HERBERT outperforms baseline models with
an average of 51% reduction in root mean square error. Based on an evaluation
involving patients with chronic conditions, ActSafe can predict MTC violations
a day ahead of time with an average F1 score of 0.86.
</summary>
    <author>
      <name>Parker Seegmiller</name>
    </author>
    <author>
      <name>Joseph Gatto</name>
    </author>
    <author>
      <name>Abdullah Mamun</name>
    </author>
    <author>
      <name>Hassan Ghasemzadeh</name>
    </author>
    <author>
      <name>Diane Cook</name>
    </author>
    <author>
      <name>John Stankovic</name>
    </author>
    <author>
      <name>Sarah Masud Preum</name>
    </author>
    <link href="http://arxiv.org/abs/2301.07051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.07051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2111.03319v1</id>
    <updated>2021-11-05T08: 39: 36Z</updated>
    <published>2021-11-05T08: 39: 36Z</published>
    <title>KORSAL: Key-point Detection based Online Real-Time Spatio-Temporal
  Action Localization</title>
    <summary>  Real-time and online action localization in a video is a critical yet highly
challenging problem. Accurate action localization requires the utilization of
both temporal and spatial information. Recent attempts achieve this by using
computationally intensive 3D CNN architectures or highly redundant two-stream
architectures with optical flow, making them both unsuitable for real-time,
online applications. To accomplish activity localization under highly
challenging real-time constraints, we propose utilizing fast and efficient
key-point based bounding box prediction to spatially localize actions. We then
introduce a tube-linking algorithm that maintains the continuity of action
tubes temporally in the presence of occlusions. Further, we eliminate the need
for a two-stream architecture by combining temporal and spatial information
into a cascaded input to a single network, allowing the network to learn from
both types of information. Temporal information is efficiently extracted using
a structural similarity index map as opposed to computationally intensive
optical flow. Despite the simplicity of our approach, our lightweight
end-to-end architecture achieves state-of-the-art frame-mAP of 74.7% on the
challenging UCF101-24 dataset, demonstrating a performance gain of 6.4% over
the previous best online methods. We also achieve state-of-the-art video-mAP
results compared to both online and offline methods. Moreover, our model
achieves a frame rate of 41.8 FPS, which is a 10.7% improvement over
contemporary real-time methods.
</summary>
    <author>
      <name>Kalana Abeywardena</name>
    </author>
    <author>
      <name>Shechem Sumanthiran</name>
    </author>
    <author>
      <name>Sakuna Jayasundara</name>
    </author>
    <author>
      <name>Sachira Karunasena</name>
    </author>
    <author>
      <name>Ranga Rodrigo</name>
    </author>
    <author>
      <name>Peshala Jayasekara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.03319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.03319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/hep-ph/0301132v1</id>
    <updated>2003-01-17T12: 24: 36Z</updated>
    <published>2003-01-17T12: 24: 36Z</published>
    <title>Temporal distortion of annual modulation at low recoil energies</title>
    <summary>  We show that the main features of the annual modulation of the signal
expected in a WIMP direct detection experiment, i.e. its sinusoidal dependence
with time, the occurrence of its maxima and minima during the year and (under
some circumstances) even the one-year period, may be affected by relaxing the
isothermal sphere hypothesis in the description of the WIMP velocity phase
space. The most relevant effect is a distortion of the time-behaviour at low
recoil energies for anisotropic galactic halos. While some of these effects
turn out to be relevant at recoil energies below the current detector
thresholds, some others could already be measurable, although some degree of
tuning between the WIMP mass and the experimental parameters would be required.
Either the observation or non-observation of these effects could provide clues
on the phase space distribution of our galactic halo.
</summary>
    <author>
      <name>N. Fornengo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">U. of Torino and INFN-Torino</arxiv:affiliation>
    </author>
    <author>
      <name>S. Scopel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">U. of Torino and INFN-Torino</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physletb.2003.09.077</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physletb.2003.09.077" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages,
                            4 figures, typeset with ReVTeX4. The paper may also be found
  at http: //www.to.infn.it/~fornengo/papers/distortion.ps.gz</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys.Lett. B576 (2003) 189-194</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/hep-ph/0301132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/hep-ph/0301132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/0710.0335v1</id>
    <updated>2007-10-01T16: 55: 00Z</updated>
    <published>2007-10-01T16: 55: 00Z</published>
    <title>Long term monitoring of the BHC 1E 1740.7-2942</title>
    <summary>  The microquasar 1E 1740.7-2942 is one of the most appealing source of the
Galactic Centre region. The high energy feature detected once by SIGMA has been
searched in the last years by INTEGRAL, but never confirmed. Classified as a
persistent source, on 2004 it showed a quiescent-like state. In fact for few
month 1E 1740.7-2942 was below the detector sensitivity level. We present the
long term temporal behaviour of 1E 1740.7-2942 observed by INTEGRAL and RXTE in
2004 and 2005, as well as preliminary results on possible spectral transitions.
</summary>
    <author>
      <name>M. Del Santo</name>
    </author>
    <author>
      <name>A. Bazzano</name>
    </author>
    <author>
      <name>N. Bezayiff</name>
    </author>
    <author>
      <name>D. M. Smith</name>
    </author>
    <author>
      <name>P. Ubertini</name>
    </author>
    <author>
      <name>G. De Cesare</name>
    </author>
    <author>
      <name>M. Federici</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, proceedings of the 6th INTEGRAL Workshop 'The Obscured
  Universe', July 2-8 2006, ESA SP-622</arxiv:comment>
    <link href="http://arxiv.org/abs/0710.0335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.0335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2104.06607v1</id>
    <updated>2021-04-14T03: 14: 58Z</updated>
    <published>2021-04-14T03: 14: 58Z</published>
    <title>Revisiting the Onsets and Frames Model with Additive Attention</title>
    <summary>  Recent advances in automatic music transcription (AMT) have achieved highly
accurate polyphonic piano transcription results by incorporating onset and
offset detection. The existing literature, however, focuses mainly on the
leverage of deep and complex models to achieve state-of-the-art (SOTA)
accuracy, without understanding model behaviour. In this paper, we conduct a
comprehensive examination of the Onsets-and-Frames AMT model, and pinpoint the
essential components contributing to a strong AMT performance. This is achieved
through exploitation of a modified additive attention mechanism. The
experimental results suggest that the attention mechanism beyond a moderate
temporal context does not benefit the model, and that rule-based
post-processing is largely responsible for the SOTA performance. We also
demonstrate that the onsets are the most significant attentive feature
regardless of model complexity. The findings encourage AMT research to weigh
more on both a robust onset detector and an effective post-processor.
</summary>
    <author>
      <name>Kin Wai Cheuk</name>
    </author>
    <author>
      <name>Yin-Jyun Luo</name>
    </author>
    <author>
      <name>Emmanouil Benetos</name>
    </author>
    <author>
      <name>Dorien Herremans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in IJCNN 2021 Special Session S04.
  https: //dr-costas.github.io/rlasmp2021-website/</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.06607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.06607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2412.02930v2</id>
    <updated>2025-03-09T07: 25: 51Z</updated>
    <published>2024-12-04T00: 50: 33Z</published>
    <title>Video LLMs for Temporal Reasoning in Long Videos</title>
    <summary>  This paper introduces TemporalVLM, a video large language model (video LLM)
capable of effective temporal reasoning and fine-grained understanding in long
videos. At the core, our approach includes a visual encoder for mapping a
long-term input video into features which are time-aware and contain both local
and global cues. In particular, it first divides the input video into
short-term clips, which are jointly encoded with their timestamps into
time-sensitive local features. Next, the local features are passed through a
bidirectional long short-term memory (BiLSTM) module for global feature
aggregation. The extracted time-aware and multi-level features are important
for accurate temporal reasoning and fine-grained understanding in long videos.
Moreover, to facilitate the evaluation of TemporalVLM, we present a large-scale
long video dataset of industry assembly processes, namely IndustryASM, which
consists of videos recorded on factory floors with actions and timestamps
annotated by industrial engineers for time and motion studies and temporal
action segmentation evaluation. Finally, extensive experiments on datasets of
long videos, including TimeIT and IndustryASM, show that TemporalVLM achieves
superior performance than previous methods across temporal reasoning and
fine-grained understanding tasks, namely dense video captioning, temporal video
grounding, video highlight detection, and temporal action segmentation. To the
best of our knowledge, our work is the first to incorporate LSTMs into video
LLMs.
</summary>
    <author>
      <name>Fawad Javed Fateh</name>
    </author>
    <author>
      <name>Umer Ahmed</name>
    </author>
    <author>
      <name>Hamza Khan</name>
    </author>
    <author>
      <name>M. Zeeshan Zia</name>
    </author>
    <author>
      <name>Quoc-Huy Tran</name>
    </author>
    <link href="http://arxiv.org/abs/2412.02930v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.02930v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2401.08381v2</id>
    <updated>2024-06-03T13: 40: 44Z</updated>
    <published>2024-01-16T14: 11: 54Z</published>
    <title>Robotic Imitation of Human Actions</title>
    <summary>  Imitation can allow us to quickly gain an understanding of a new task.
Through a demonstration, we can gain direct knowledge about which actions need
to be performed and which goals they have. In this paper, we introduce a new
approach to imitation learning that tackles the challenges of a robot imitating
a human, such as the change in perspective and body schema. Our approach can
use a single human demonstration to abstract information about the demonstrated
task, and use that information to generalise and replicate it. We facilitate
this ability by a new integration of two state-of-the-art methods: a diffusion
action segmentation model to abstract temporal information from the
demonstration and an open vocabulary object detector for spatial information.
Furthermore, we refine the abstracted information and use symbolic reasoning to
create an action plan utilising inverse kinematics, to allow the robot to
imitate the demonstrated action.
</summary>
    <author>
      <name>Josua Spisak</name>
    </author>
    <author>
      <name>Matthias Kerzel</name>
    </author>
    <author>
      <name>Stefan Wermter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the ICDL 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.08381v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.08381v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2103.10502v3</id>
    <updated>2022-11-16T08: 50: 27Z</updated>
    <published>2021-03-18T20: 08: 53Z</published>
    <title>Ano-Graph: Learning Normal Scene Contextual Graphs to Detect Video
  Anomalies</title>
    <summary>  Video anomaly detection has proved to be a challenging task owing to its
unsupervised training procedure and high spatio-temporal complexity existing in
real-world scenarios. In the absence of anomalous training samples,
state-of-the-art methods try to extract features that fully grasp normal
behaviors in both space and time domains using different approaches such as
autoencoders, or generative adversarial networks. However, these approaches
completely ignore or, by using the ability of deep networks in the hierarchical
modeling, poorly model the spatio-temporal interactions that exist between
objects. To address this issue, we propose a novel yet efficient method named
Ano-Graph for learning and modeling the interaction of normal objects. Towards
this end, a Spatio-Temporal Graph (STG) is made by considering each node as an
object's feature extracted from a real-time off-the-shelf object detector, and
edges are made based on their interactions. After that, a self-supervised
learning method is employed on the STG in such a way that encapsulates
interactions in a semantic space. Our method is data-efficient, significantly
more robust against common real-world variations such as illumination, and
passes SOTA by a large margin on the challenging datasets ADOC and Street Scene
while stays competitive on Avenue, ShanghaiTech, and UCSD.
</summary>
    <author>
      <name>Masoud Pourreza</name>
    </author>
    <author>
      <name>Mohammadreza Salehi</name>
    </author>
    <author>
      <name>Mohammad Sabokrou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Inconsistencies in the results</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.10502v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.10502v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1909.04974v1</id>
    <updated>2019-09-11T11: 15: 10Z</updated>
    <published>2019-09-11T11: 15: 10Z</published>
    <title>Computer-Aided Automated Detection of Gene-Controlled Social Actions of
  Drosophila</title>
    <summary>  Gene expression of social actions in Drosophilae has been attracting wide
interest from biologists, medical scientists and psychologists. Gene-edited
Drosophilae have been used as a test platform for experimental investigation.
For example, Parkinson's genes can be embedded into a group of newly bred
Drosophilae for research purpose. However, human observation of numerous tiny
Drosophilae for a long term is an arduous work, and the dependence on human's
acute perception is highly unreliable. As a result, an automated system of
social action detection using machine learning has been highly demanded. In
this study, we propose to automate the detection and classification of two
innate aggressive actions demonstrated by Drosophilae. Robust keypoint
detection is achieved using selective spatio-temporal interest points (sSTIP)
which are then described using the 3D Scale Invariant Feature Transform
(3D-SIFT) descriptors. Dimensionality reduction is performed using Spectral
Regression Kernel Discriminant Analysis (SR-KDA) and classification is done
using the nearest centre rule. The classification accuracy shown demonstrates
the feasibility of the proposed system.
</summary>
    <author>
      <name>Khan Faraz</name>
    </author>
    <author>
      <name>Ahmed Bouridane</name>
    </author>
    <author>
      <name>Richard Jiang</name>
    </author>
    <author>
      <name>Tiancheng Xia</name>
    </author>
    <author>
      <name>Paul Chazot</name>
    </author>
    <author>
      <name>Abdel Ennaceur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">published on International Conference on Smart Cities at Cambridge
  2018</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Smart Cities at Cambridge 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1909.04974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.04974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2205.05895v1</id>
    <updated>2022-05-12T06: 33: 24Z</updated>
    <published>2022-05-12T06: 33: 24Z</published>
    <title>Weakly-Supervised Action Detection Guided by Audio Narration</title>
    <summary>  Videos are more well-organized curated data sources for visual concept
learning than images. Unlike the 2-dimensional images which only involve the
spatial information, the additional temporal dimension bridges and synchronizes
multiple modalities. However, in most video detection benchmarks, these
additional modalities are not fully utilized. For example, EPIC Kitchens is the
largest dataset in first-person (egocentric) vision, yet it still relies on
crowdsourced information to refine the action boundaries to provide
instance-level action annotations.
  We explored how to eliminate the expensive annotations in video detection
data which provide refined boundaries. We propose a model to learn from the
narration supervision and utilize multimodal features, including RGB, motion
flow, and ambient sound. Our model learns to attend to the frames related to
the narration label while suppressing the irrelevant frames from being used.
Our experiments show that noisy audio narration suffices to learn a good action
detection model, thus reducing annotation expenses.
</summary>
    <author>
      <name>Keren Ye</name>
    </author>
    <author>
      <name>Adriana Kovashka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear, in Joint 1st Ego4D and 10th EPIC Workshop, held in
  conjunction with the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.05895v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.05895v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1712.08002v1</id>
    <updated>2017-12-20T12: 58: 46Z</updated>
    <published>2017-12-20T12: 58: 46Z</published>
    <title>Human Action Recognition: Pose-based Attention draws focus to Hands</title>
    <summary>  We propose a new spatio-temporal attention based mechanism for human action
recognition able to automatically attend to the hands most involved into the
studied action and detect the most discriminative moments in an action.
Attention is handled in a recurrent manner employing Recurrent Neural Network
(RNN) and is fully-differentiable. In contrast to standard soft-attention based
mechanisms, our approach does not use the hidden RNN state as input to the
attention model. Instead, attention distributions are extracted using external
information: human articulated pose. We performed an extensive ablation study
to show the strengths of this approach and we particularly studied the
conditioning aspect of the attention mechanism. We evaluate the method on the
largest currently available human action recognition dataset, NTU-RGB+D, and
report state-of-the-art results. Other advantages of our model are certain
aspects of explanability, as the spatial and temporal attention distributions
at test time allow to study and verify on which parts of the input data the
method focuses.
</summary>
    <author>
      <name>Fabien Baradel</name>
    </author>
    <author>
      <name>Christian Wolf</name>
    </author>
    <author>
      <name>Julien Mille</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV 2017 Workshop "Hands in action". arXiv admin note: text overlap
  with arXiv: 1703.10106</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.08002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1912.05534v1</id>
    <updated>2019-12-11T18: 59: 15Z</updated>
    <published>2019-12-11T18: 59: 15Z</published>
    <title>Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action
  Recognition</title>
    <summary>  Human activities often occur in specific scene contexts, e.g., playing
basketball on a basketball court. Training a model using existing video
datasets thus inevitably captures and leverages such bias (instead of using the
actual discriminative cues). The learned representation may not generalize well
to new action classes or different tasks. In this paper, we propose to mitigate
scene bias for video representation learning. Specifically, we augment the
standard cross-entropy loss for action classification with 1) an adversarial
loss for scene types and 2) a human mask confusion loss for videos where the
human actors are masked out. These two losses encourage learning
representations that are unable to predict the scene types and the correct
actions when there is no evidence. We validate the effectiveness of our method
by transferring our pre-trained model to three different tasks, including
action classification, temporal localization, and spatio-temporal action
detection. Our results show consistent improvement over the baseline model
without debiasing.
</summary>
    <author>
      <name>Jinwoo Choi</name>
    </author>
    <author>
      <name>Chen Gao</name>
    </author>
    <author>
      <name>Joseph C. E. Messou</name>
    </author>
    <author>
      <name>Jia-Bin Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS 2019. Project webpage: http: //chengao.vision/SDN/ Code:
  https: //github.com/vt-vl-lab/SDN</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.05534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.05534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2311.15924v1</id>
    <updated>2023-11-27T15: 34: 40Z</updated>
    <published>2023-11-27T15: 34: 40Z</published>
    <title>Diagnosis driven Anomaly Detection for CPS</title>
    <summary>  In Cyber-Physical Systems (CPS) research, anomaly detection (detecting
abnormal behavior) and diagnosis (identifying the underlying root cause) are
often treated as distinct, isolated tasks. However, diagnosis algorithms
require symptoms, i.e. temporally and spatially isolated anomalies, as input.
Thus, anomaly detection and diagnosis must be developed together to provide a
holistic solution for diagnosis in CPS. We therefore propose a method for
utilizing deep learning-based anomaly detection to generate inputs for
Consistency-Based Diagnosis (CBD). We evaluate our approach on a simulated and
a real-world CPS dataset, where our model demonstrates strong performance
relative to other state-of-the-art models.
</summary>
    <author>
      <name>Henrik S. Steude</name>
    </author>
    <author>
      <name>Lukas Moddemann</name>
    </author>
    <author>
      <name>Alexander Diedrich</name>
    </author>
    <author>
      <name>Jonas Ehrhardt</name>
    </author>
    <author>
      <name>Oliver Niggemann</name>
    </author>
    <link href="http://arxiv.org/abs/2311.15924v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.15924v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1903.07256v1</id>
    <updated>2019-03-18T05: 07: 09Z</updated>
    <published>2019-03-18T05: 07: 09Z</published>
    <title>Graph Convolutional Label Noise Cleaner: Train a Plug-and-play Action
  Classifier for Anomaly Detection</title>
    <summary>  Video anomaly detection under weak labels is formulated as a typical
multiple-instance learning problem in previous works. In this paper, we provide
a new perspective, i.e., a supervised learning task under noisy labels. In such
a viewpoint, as long as cleaning away label noise, we can directly apply fully
supervised action classifiers to weakly supervised anomaly detection, and take
maximum advantage of these well-developed classifiers. For this purpose, we
devise a graph convolutional network to correct noisy labels. Based upon
feature similarity and temporal consistency, our network propagates supervisory
signals from high-confidence snippets to low-confidence ones. In this manner,
the network is capable of providing cleaned supervision for action classifiers.
During the test phase, we only need to obtain snippet-wise predictions from the
action classifier without any extra post-processing. Extensive experiments on 3
datasets at different scales with 2 types of action classifiers demonstrate the
efficacy of our method. Remarkably, we obtain the frame-level AUC score of
82.12% on UCF-Crime.
</summary>
    <author>
      <name>Jia-Xing Zhong</name>
    </author>
    <author>
      <name>Nannan Li</name>
    </author>
    <author>
      <name>Weijie Kong</name>
    </author>
    <author>
      <name>Shan Liu</name>
    </author>
    <author>
      <name>Thomas H. Li</name>
    </author>
    <author>
      <name>Ge Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in CVPR 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.07256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.07256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/0909.2488v1</id>
    <updated>2009-09-14T08: 35: 22Z</updated>
    <published>2009-09-14T08: 35: 22Z</published>
    <title>S2DFS: Analysis of temporal changes of drifting subpulses</title>
    <summary>  We introduce a new technique, called the Sliding Two-Dimensional Fluctuation
Spectrum, used for detecting and characterising the temporal changes of
drifting subpulses from radio pulsars. The method was tested using simulated
data as well as archived observations made with the WSRT at wavelengths of 92
and 21 cm. The drifting subpulse phenomenon is a well known property of radio
pulsars. However the properties of the temporal behaviour of drifting subpulses
are not fully explored. The drifting can also be non-coherent and the presence
of effects like nulling or drift rate changing can mask the drifting behaviour
of the pulsar. The S2DFS is a robust method for investigating this phenomenon
and by introducing it we aim to expand our knowledge of the temporal drifting
subpulse properties. Our new analysis method uses horizonally collapsed
fluctuation spectra obtained with the Two-Dimensional Fluctuation Spectrum
method. Stacking the collapsed spectra obtained in a 256 pulse window which
slides by a pulse at a time produces a map of the collapsed fluctuation
spectrum. By analysing the maps one can easily determine the presence of any
temporal drift changes. Simulated data showed that the technique can reveal the
presence of any temporal changes in drift behaviour like mode changing or
nulling. We have also analysed data of three pulsars, PSRs B0031-07, B1819-22
and B1944+17, which were selected based on the quality of the data and their
known drift properties. All three sources are known to exhibit mode changes
which could easily be seen in the S2DFS. The results from the analysis of the
data sets used in this paper have shown that the S2DFS method is robust and
complimentary to the 2DFS method in detecting and characterising the temporal
changes in drifting subpulses from radio pulsars.
</summary>
    <author>
      <name>M. Serylak</name>
    </author>
    <author>
      <name>B. W. Stappers</name>
    </author>
    <author>
      <name>P. Weltevrede</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1051/0004-6361/200912453</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1051/0004-6361/200912453" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for publication in A&amp;A,
                            10 pages,
                            7 figures,
                            1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.2488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.2488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2302.00646v2</id>
    <updated>2024-09-28T13: 35: 40Z</updated>
    <published>2023-02-01T18: 19: 37Z</published>
    <title>Epic-Sounds: A Large-scale Dataset of Actions That Sound</title>
    <summary>  We introduce Epic-Sounds, a large-scale dataset of audio annotations
capturing temporal extents and class labels within the audio stream of the
egocentric videos. We propose an annotation pipeline where annotators
temporally label distinguishable audio segments and describe the action that
could have caused this sound. We identify actions that can be discriminated
purely from audio, through grouping these free-form descriptions of audio into
classes. For actions that involve objects colliding, we collect human
annotations of the materials of these objects (e.g. a glass object being placed
on a wooden surface), which we verify from video, discarding ambiguities.
Overall, Epic-Sounds includes 78.4k categorised segments of audible events and
actions, distributed across 44 classes as well as 39.2k non-categorised
segments. We train and evaluate state-of-the-art audio recognition and
detection models on our dataset, for both audio-only and audio-visual methods.
We also conduct analysis on: the temporal overlap between audio events, the
temporal and label correlations between audio and visual modalities, the
ambiguities in annotating materials from audio-only input, the importance of
audio-only labels and the limitations of current models to understand actions
that sound. Project page : https: //epic-kitchens.github.io/epic-sounds/
</summary>
    <author>
      <name>Jaesung Huh</name>
    </author>
    <author>
      <name>Jacob Chalk</name>
    </author>
    <author>
      <name>Evangelos Kazakos</name>
    </author>
    <author>
      <name>Dima Damen</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages,
                            12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.00646v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.00646v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2107.03377v3</id>
    <updated>2021-12-22T18: 01: 50Z</updated>
    <published>2021-07-07T17: 49: 51Z</published>
    <title>Long Short-Term Transformer for Online Action Detection</title>
    <summary>  We present Long Short-term TRansformer (LSTR), a temporal modeling algorithm
for online action detection, which employs a long- and short-term memory
mechanism to model prolonged sequence data. It consists of an LSTR encoder that
dynamically leverages coarse-scale historical information from an extended
temporal window (e.g.,
                            2048 frames spanning of up to 8 minutes), together with
an LSTR decoder that focuses on a short time window (e.g.,
                            32 frames spanning 8
seconds) to model the fine-scale characteristics of the data. Compared to prior
work, LSTR provides an effective and efficient method to model long videos with
fewer heuristics, which is validated by extensive empirical analysis. LSTR
achieves state-of-the-art performance on three standard online action detection
benchmarks, THUMOS'14, TVSeries, and HACS Segment. Code has been made available
at: https: //xumingze0308.github.io/projects/lstr
</summary>
    <author>
      <name>Mingze Xu</name>
    </author>
    <author>
      <name>Yuanjun Xiong</name>
    </author>
    <author>
      <name>Hao Chen</name>
    </author>
    <author>
      <name>Xinyu Li</name>
    </author>
    <author>
      <name>Wei Xia</name>
    </author>
    <author>
      <name>Zhuowen Tu</name>
    </author>
    <author>
      <name>Stefano Soatto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS 2021 Spotlight</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.03377v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.03377v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1904.06183v1</id>
    <updated>2019-04-03T16: 50: 29Z</updated>
    <published>2019-04-03T16: 50: 29Z</published>
    <title>Angular Velocity Estimation of Image Motion Mimicking the Honeybee
  Tunnel Centring Behaviour</title>
    <summary>  Insects use visual information to estimate angular velocity of retinal image
motion, which determines a variety of flight behaviours including speed
regulation, tunnel centring and visual navigation. For angular velocity
estimation, honeybees show large spatial-independence against visual stimuli,
whereas the previous models have not fulfilled such an ability. To address this
issue, we propose a bio-plausible model for estimating the image motion
velocity based on behavioural experiments of the honeybee flying through
patterned tunnels. The proposed model contains mainly three parts, the texture
estimation layer for spatial information extraction, the delay-and-correlate
layer for temporal information extraction and the decoding layer for angular
velocity estimation. This model produces responses that are largely independent
of the spatial frequency in grating experiments. And the model has been
implemented in a virtual bee for tunnel centring simulations. The results
coincide with both electro-physiological neuron spike and behavioural path
recordings, which indicates our proposed method provides a better explanation
of the honeybee's image motion detection mechanism guiding the tunnel centring
behaviour.
</summary>
    <author>
      <name>Huatian Wang</name>
    </author>
    <author>
      <name>Qinbing Fu</name>
    </author>
    <author>
      <name>Hongxin Wang</name>
    </author>
    <author>
      <name>Jigen Peng</name>
    </author>
    <author>
      <name>Paul Baxter</name>
    </author>
    <author>
      <name>Cheng Hu</name>
    </author>
    <author>
      <name>Shigang Yue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages,
                            8 figures, conference, IEEE format. arXiv admin note: text
  overlap with arXiv: 1904.02356</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.06183v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.06183v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1904.00696v3</id>
    <updated>2019-06-11T11: 29: 06Z</updated>
    <published>2019-04-01T11: 09: 03Z</published>
    <title>Dance with Flow: Two-in-One Stream Action Detection</title>
    <summary>  The goal of this paper is to detect the spatio-temporal extent of an action.
The two-stream detection network based on RGB and flow provides
state-of-the-art accuracy at the expense of a large model-size and heavy
computation. We propose to embed RGB and optical-flow into a single two-in-one
stream network with new layers. A motion condition layer extracts motion
information from flow images, which is leveraged by the motion modulation layer
to generate transformation parameters for modulating the low-level RGB
features. The method is easily embedded in existing appearance- or two-stream
action detection networks, and trained end-to-end. Experiments demonstrate that
leveraging the motion condition to modulate RGB features improves detection
accuracy. With only half the computation and parameters of the state-of-the-art
two-stream methods, our two-in-one stream still achieves impressive results on
UCF101-24, UCFSports and J-HMDB.
</summary>
    <author>
      <name>Jiaojiao Zhao</name>
    </author>
    <author>
      <name>Cees G. M. Snoek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVPR2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.00696v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.00696v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2207.07059v1</id>
    <updated>2022-07-14T16: 58: 47Z</updated>
    <published>2022-07-14T16: 58: 47Z</published>
    <title>Semi-Supervised Temporal Action Detection with Proposal-Free Masking</title>
    <summary>  Existing temporal action detection (TAD) methods rely on a large number of
training data with segment-level annotations. Collecting and annotating such a
training set is thus highly expensive and unscalable. Semi-supervised TAD
(SS-TAD) alleviates this problem by leveraging unlabeled videos freely
available at scale. However, SS-TAD is also a much more challenging problem
than supervised TAD, and consequently much under-studied. Prior SS-TAD methods
directly combine an existing proposal-based TAD method and a SSL method. Due to
their sequential localization (e.g, proposal generation) and classification
design, they are prone to proposal error propagation. To overcome this
limitation, in this work we propose a novel Semi-supervised Temporal action
detection model based on PropOsal-free Temporal mask (SPOT) with a parallel
localization (mask generation) and classification architecture. Such a novel
design effectively eliminates the dependence between localization and
classification by cutting off the route for error propagation in-between. We
further introduce an interaction mechanism between classification and
localization for prediction refinement, and a new pretext task for
self-supervised model pre-training. Extensive experiments on two standard
benchmarks show that our SPOT outperforms state-of-the-art alternatives, often
by a large margin. The PyTorch implementation of SPOT is available at
https: //github.com/sauradip/SPOT
</summary>
    <author>
      <name>Sauradip Nag</name>
    </author>
    <author>
      <name>Xiatian Zhu</name>
    </author>
    <author>
      <name>Yi-Zhe Song</name>
    </author>
    <author>
      <name>Tao Xiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ECCV 2022; Code available at https: //github.com/sauradip/SPOT</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.07059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.07059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2009.05224v2</id>
    <updated>2021-08-16T16: 59: 58Z</updated>
    <published>2020-09-11T04: 18: 41Z</published>
    <title>HAA500: Human-Centric Atomic Action Dataset with Curated Videos</title>
    <summary>  We contribute HAA500, a manually annotated human-centric atomic action
dataset for action recognition on 500 classes with over 591K labeled frames. To
minimize ambiguities in action classification, HAA500 consists of highly
diversified classes of fine-grained atomic actions, where only consistent
actions fall under the same label, e.g.,
                            "Baseball Pitching" vs "Free Throw in
Basketball". Thus HAA500 is different from existing atomic action datasets,
where coarse-grained atomic actions were labeled with coarse action-verbs such
as "Throw". HAA500 has been carefully curated to capture the precise movement
of human figures with little class-irrelevant motions or spatio-temporal label
noises. The advantages of HAA500 are fourfold: 1) human-centric actions with a
high average of 69.7% detectable joints for the relevant human poses; 2) high
scalability since adding a new class can be done under 20-60 minutes; 3)
curated videos capturing essential elements of an atomic action without
irrelevant frames; 4) fine-grained atomic action classes. Our extensive
experiments including cross-data validation using datasets collected in the
wild demonstrate the clear benefits of human-centric and atomic characteristics
of HAA500, which enable training even a baseline deep learning model to improve
prediction by attending to atomic human poses. We detail the HAA500 dataset
statistics and collection methodology and compare quantitatively with existing
action recognition datasets.
</summary>
    <author>
      <name>Jihoon Chung</name>
    </author>
    <author>
      <name>Cheng-hsin Wuu</name>
    </author>
    <author>
      <name>Hsuan-ru Yang</name>
    </author>
    <author>
      <name>Yu-Wing Tai</name>
    </author>
    <author>
      <name>Chi-Keung Tang</name>
    </author>
    <link href="http://arxiv.org/abs/2009.05224v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.05224v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1206.1865v1</id>
    <updated>2012-06-08T20: 09: 44Z</updated>
    <published>2012-06-08T20: 09: 44Z</published>
    <title>Frequency decoding of periodically timed action potentials through
  distinct activity patterns in a random neural network</title>
    <summary>  Frequency discrimination is a fundamental task of the auditory system. The
mammalian inner ear, or cochlea, provides a place code in which different
frequencies are detected at different spatial locations. However, a temporal
code based on spike timing is also available: action potentials evoked in an
auditory-nerve fiber by a low-frequency tone occur at a preferred phase of the
stimulus-they exhibit phase locking-and thus provide temporal information about
the tone's frequency. In an accompanying psychoacoustic study, and in agreement
with previous experiments, we show that humans employ this temporal information
for discrimination of low frequencies. How might such temporal information be
read out in the brain? Here we demonstrate that recurrent random neural
networks in which connections between neurons introduce characteristic time
delays, and in which neurons require temporally coinciding inputs for spike
initiation, can perform sharp frequency discrimination when stimulated with
phase-locked inputs. Although the frequency resolution achieved by such
networks is limited by the noise in phase locking, the resolution for realistic
values reaches the tiny frequency difference of 0.2% that has been measured in
humans.
</summary>
    <author>
      <name>Tobias Reichenbach</name>
    </author>
    <author>
      <name>A. J. Hudspeth</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1367-2630/14/11/113022</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1367-2630/14/11/113022" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages,
                            5 figures, and supplementary information</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.1865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.1865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2408.13643v1</id>
    <updated>2024-08-24T18: 12: 58Z</updated>
    <published>2024-08-24T18: 12: 58Z</published>
    <title>Temporal Divide-and-Conquer Anomaly Actions Localization in
  Semi-Supervised Videos with Hierarchical Transformer</title>
    <summary>  Anomaly action detection and localization play an essential role in security
and advanced surveillance systems. However, due to the tremendous amount of
surveillance videos, most of the available data for the task is unlabeled or
semi-labeled with the video class known, but the location of the anomaly event
is unknown. In this work, we target anomaly localization in semi-supervised
videos. While the mainstream direction in addressing this task is focused on
segment-level multi-instance learning and the generation of pseudo labels, we
aim to explore a promising yet unfulfilled direction to solve the problem by
learning the temporal relations within videos in order to locate anomaly
events. To this end, we propose a hierarchical transformer model designed to
evaluate the significance of observed actions in anomalous videos with a
divide-and-conquer strategy along the temporal axis. Our approach segments a
parent video hierarchically into multiple temporal children instances and
measures the influence of the children nodes in classifying the abnormality of
the parent video. Evaluating our model on two well-known anomaly detection
datasets, UCF-crime and ShanghaiTech, proves its ability to interpret the
observed actions within videos and localize the anomalous ones. Our proposed
approach outperforms previous works relying on segment-level multiple-instance
learning approaches while reaching a promising performance compared to the more
recent pseudo-labeling-based approaches.
</summary>
    <author>
      <name>Nada Osman</name>
    </author>
    <author>
      <name>Marwan Torki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the 27th International Conference on Pattern Recognition
  (ICPR-2024)</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.13643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.13643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2010.08278v1</id>
    <updated>2020-10-16T10: 02: 41Z</updated>
    <published>2020-10-16T10: 02: 41Z</published>
    <title>Real-Time Face &amp; Eye Tracking and Blink Detection using Event Cameras</title>
    <summary>  Event cameras contain emerging, neuromorphic vision sensors that capture
local light intensity changes at each pixel, generating a stream of
asynchronous events. This way of acquiring visual information constitutes a
departure from traditional frame based cameras and offers several significant
advantages: low energy consumption, high temporal resolution, high dynamic
range and low latency. Driver monitoring systems (DMS) are in-cabin safety
systems designed to sense and understand a drivers physical and cognitive
state. Event cameras are particularly suited to DMS due to their inherent
advantages. This paper proposes a novel method to simultaneously detect and
track faces and eyes for driver monitoring. A unique, fully convolutional
recurrent neural network architecture is presented. To train this network, a
synthetic event-based dataset is simulated with accurate bounding box
annotations, called Neuromorphic HELEN. Additionally, a method to detect and
analyse drivers eye blinks is proposed, exploiting the high temporal resolution
of event cameras. Behaviour of blinking provides greater insights into a driver
level of fatigue or drowsiness. We show that blinks have a unique temporal
signature that can be better captured by event cameras.
</summary>
    <author>
      <name>Cian Ryan</name>
    </author>
    <author>
      <name>Brian O Sullivan</name>
    </author>
    <author>
      <name>Amr Elrasad</name>
    </author>
    <author>
      <name>Joe Lemley</name>
    </author>
    <author>
      <name>Paul Kielty</name>
    </author>
    <author>
      <name>Christoph Posch</name>
    </author>
    <author>
      <name>Etienne Perot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 Pages,
                            8 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.08278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.08278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2210.15177v1</id>
    <updated>2022-10-27T05: 09: 13Z</updated>
    <published>2022-10-27T05: 09: 13Z</published>
    <title>Spatial-Temporal Recurrent Graph Neural Networks for Fault Diagnostics
  in Power Distribution Systems</title>
    <summary>  Fault diagnostics are extremely important to decide proper actions toward
fault isolation and system restoration. The growing integration of
inverter-based distributed energy resources imposes strong influences on fault
detection using traditional overcurrent relays. This paper utilizes emerging
graph learning techniques to build a new temporal recurrent graph neural
network models for fault diagnostics. The temporal recurrent graph neural
network structures can extract the spatial-temporal features from data of
voltage measurement units installed at the critical buses. From these features,
fault event detection, fault type/phase classification, and fault location are
performed. Compared with previous works, the proposed temporal recurrent graph
neural networks provide a better generalization for fault diagnostics.
Moreover, the proposed scheme retrieves the voltage signals instead of current
signals so that there is no need to install relays at all lines of the
distribution system. Therefore, the proposed scheme is generalizable and not
limited by the number of relays installed. The effectiveness of the proposed
method is comprehensively evaluated on the Potsdam microgrid and IEEE 123-node
system in comparison with other neural network structures.
</summary>
    <author>
      <name>Bang Nguyen</name>
    </author>
    <author>
      <name>Tuyen Vu</name>
    </author>
    <author>
      <name>Thai-Thanh Nguyen</name>
    </author>
    <author>
      <name>Mayank Panwar</name>
    </author>
    <author>
      <name>Rob Hovsapian</name>
    </author>
    <link href="http://arxiv.org/abs/2210.15177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.15177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1609.00866v2</id>
    <updated>2017-04-30T20: 37: 05Z</updated>
    <published>2016-09-03T21: 31: 45Z</published>
    <title>Deep-Anomaly: Fully Convolutional Neural Network for Fast Anomaly
  Detection in Crowded Scenes</title>
    <summary>  The detection of abnormal behaviours in crowded scenes has to deal with many
challenges. This paper presents an efficient method for detection and
localization of anomalies in videos. Using fully convolutional neural networks
(FCNs) and temporal data, a pre-trained supervised FCN is transferred into an
unsupervised FCN ensuring the detection of (global) anomalies in scenes. High
performance in terms of speed and accuracy is achieved by investigating the
cascaded detection as a result of reducing computation complexities. This
FCN-based architecture addresses two main tasks, feature representation and
cascaded outlier detection. Experimental results on two benchmarks suggest that
detection and localization of the proposed method outperforms existing methods
in terms of accuracy.
</summary>
    <author>
      <name>Mohammad Sabokrou</name>
    </author>
    <author>
      <name>Mohsen Fayyaz</name>
    </author>
    <author>
      <name>Mahmood Fathy</name>
    </author>
    <author>
      <name>Zahra Moayedd</name>
    </author>
    <author>
      <name>Reinhard klette</name>
    </author>
    <link href="http://arxiv.org/abs/1609.00866v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00866v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2303.15270v1</id>
    <updated>2023-03-27T14: 59: 08Z</updated>
    <published>2023-03-27T14: 59: 08Z</published>
    <title>Unified Keypoint-based Action Recognition Framework via Structured
  Keypoint Pooling</title>
    <summary>  This paper simultaneously addresses three limitations associated with
conventional skeleton-based action recognition; skeleton detection and tracking
errors, poor variety of the targeted actions, as well as person-wise and
frame-wise action recognition. A point cloud deep-learning paradigm is
introduced to the action recognition, and a unified framework along with a
novel deep neural network architecture called Structured Keypoint Pooling is
proposed. The proposed method sparsely aggregates keypoint features in a
cascaded manner based on prior knowledge of the data structure (which is
inherent in skeletons), such as the instances and frames to which each keypoint
belongs, and achieves robustness against input errors. Its less constrained and
tracking-free architecture enables time-series keypoints consisting of human
skeletons and nonhuman object contours to be efficiently treated as an input 3D
point cloud and extends the variety of the targeted action. Furthermore, we
propose a Pooling-Switching Trick inspired by Structured Keypoint Pooling. This
trick switches the pooling kernels between the training and inference phases to
detect person-wise and frame-wise actions in a weakly supervised manner using
only video-level action labels. This trick enables our training scheme to
naturally introduce novel data augmentation, which mixes multiple point clouds
extracted from different videos. In the experiments, we comprehensively verify
the effectiveness of the proposed method against the limitations, and the
method outperforms state-of-the-art skeleton-based action recognition and
spatio-temporal action localization methods.
</summary>
    <author>
      <name>Ryo Hachiuma</name>
    </author>
    <author>
      <name>Fumiaki Sato</name>
    </author>
    <author>
      <name>Taiki Sekii</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.15270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.15270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2211.04114v1</id>
    <updated>2022-11-08T09: 24: 18Z</updated>
    <published>2022-11-08T09: 24: 18Z</published>
    <title>Two-photon real-time device for single-particle holographic tracking
  (red shot)</title>
    <summary>  Three-dimension real-time tracking of single emitters is an emerging tool for
assessment of biological behavior as intraneuronal transport, for which
spatiotemporal resolution is crucial to understand the microscopic interactions
between molecular motors. We report the use of second harmonic signal from
nonlinear nanoparticles to localize them in a super-localization regime, down
to 15 nm precision, and at high refreshing rates, up to 1.1 kHz, allowing us to
track the particles in real-time. Holograms dynamically displayed on a digital
micro-mirror device are used to steer the excitation laser focus in 3D around
the particle on a specific pattern. The particle position is inferred from the
collected intensities using a maximum likelihood approach. The holograms are
also used to compensate for optical aberrations of the optical system. We
report tracking of particles moving faster than 30 $\mu$m/s with an uncertainty
on the localization around 40 nm. We have been able to track freely moving
particles over tens of micrometers, and directional intracellular transport in
neurites.
</summary>
    <author>
      <name>Florian Semmer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LuMIn</arxiv:affiliation>
    </author>
    <author>
      <name>Marie-Charlotte Emperauger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LuMIn</arxiv:affiliation>
    </author>
    <author>
      <name>Colin Lopez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LuMIn</arxiv:affiliation>
    </author>
    <author>
      <name>Christine Bogicevic</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SPMS</arxiv:affiliation>
    </author>
    <author>
      <name>François Treussart</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LuMIn</arxiv:affiliation>
    </author>
    <author>
      <name>Karen Perronet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LuMIn</arxiv:affiliation>
    </author>
    <author>
      <name>François Marquier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LuMIn</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2211.04114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.04114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2010.08180v1</id>
    <updated>2020-10-16T05: 59: 12Z</updated>
    <published>2020-10-16T05: 59: 12Z</published>
    <title>Who's in the Gang? Revealing Coordinating Communities in Social Media</title>
    <summary>  Political astroturfing and organised trolling are online malicious behaviours
with significant real-world effects. Common approaches examining these
phenomena focus on broad campaigns rather than the small groups responsible. To
reveal latent networks of cooperating accounts, we propose a novel temporal
window approach that relies on account interactions and metadata alone. It
detects groups of accounts engaging in behaviours that, in concert, execute
different goal-based strategies, which we describe. Our approach is validated
against two relevant datasets with ground truth data.
</summary>
    <author>
      <name>Derek Weber</name>
    </author>
    <author>
      <name>Frank Neumann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/asonam49781.2020.9381418</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/asonam49781.2020.9381418" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages,
                            10 images, accepted by The 2020 IEEE/ACM International
  Conference on Advances in Social Networks Analysis and Mining (ASONAM'20)</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.08180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.08180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2209.12074v1</id>
    <updated>2022-09-24T19: 06: 46Z</updated>
    <published>2022-09-24T19: 06: 46Z</published>
    <title>Self-supervised Learning for Unintentional Action Prediction</title>
    <summary>  Distinguishing if an action is performed as intended or if an intended action
fails is an important skill that not only humans have, but that is also
important for intelligent systems that operate in human environments.
Recognizing if an action is unintentional or anticipating if an action will
fail, however, is not straightforward due to lack of annotated data. While
videos of unintentional or failed actions can be found in the Internet in
abundance, high annotation costs are a major bottleneck for learning networks
for these tasks. In this work, we thus study the problem of self-supervised
representation learning for unintentional action prediction. While previous
works learn the representation based on a local temporal neighborhood, we show
that the global context of a video is needed to learn a good representation for
the three downstream tasks: unintentional action classification, localization
and anticipation. In the supplementary material, we show that the learned
representation can be used for detecting anomalies in videos as well.
</summary>
    <author>
      <name>Olga Zatsarynna</name>
    </author>
    <author>
      <name>Yazan Abu Farha</name>
    </author>
    <author>
      <name>Juergen Gall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to GCPR 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.12074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.12074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1908.10700v1</id>
    <updated>2019-08-28T13: 04: 28Z</updated>
    <published>2019-08-28T13: 04: 28Z</published>
    <title>Explainable Video Action Reasoning via Prior Knowledge and State
  Transitions</title>
    <summary>  Human action analysis and understanding in videos is an important and
challenging task. Although substantial progress has been made in past years,
the explainability of existing methods is still limited. In this work, we
propose a novel action reasoning framework that uses prior knowledge to explain
semantic-level observations of video state changes. Our method takes advantage
of both classical reasoning and modern deep learning approaches. Specifically,
prior knowledge is defined as the information of a target video domain,
including a set of objects, attributes and relationships in the target video
domain, as well as relevant actions defined by the temporal attribute and
relationship changes (i.e. state transitions). Given a video sequence, we first
generate a scene graph on each frame to represent concerned objects, attributes
and relationships. Then those scene graphs are linked by tracking objects
across frames to form a spatio-temporal graph (also called video graph), which
represents semantic-level video states. Finally, by sequentially examining each
state transition in the video graph, our method can detect and explain how
those actions are executed with prior knowledge, just like the logical manner
of thinking by humans. Compared to previous works, the action reasoning results
of our method can be explained by both logical rules and semantic-level
observations of video content changes. Besides, the proposed method can be used
to detect multiple concurrent actions with detailed information, such as who
(particular objects), when (time), where (object locations) and how (what kind
of changes). Experiments on a re-annotated dataset CAD-120 show the
effectiveness of our method.
</summary>
    <author>
      <name>Tao Zhuo</name>
    </author>
    <author>
      <name>Zhiyong Cheng</name>
    </author>
    <author>
      <name>Peng Zhang</name>
    </author>
    <author>
      <name>Yongkang Wong</name>
    </author>
    <author>
      <name>Mohan Kankanhalli</name>
    </author>
    <link href="http://arxiv.org/abs/1908.10700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2112.07998v1</id>
    <updated>2021-12-15T09: 50: 11Z</updated>
    <published>2021-12-15T09: 50: 11Z</published>
    <title>Multi-modal Networks Reveal Patterns of Operational Similarity of
  Terrorist Organizations</title>
    <summary>  Capturing dynamics of operational similarity among terrorist groups is
critical to provide actionable insights for counter-terrorism and intelligence
monitoring. Yet, in spite of its theoretical and practical relevance, research
addressing this problem is currently lacking. We tackle this problem proposing
a novel computational framework for detecting clusters of terrorist groups
sharing similar behaviors, focusing on groups' yearly repertoire of deployed
tactics, attacked targets, and utilized weapons. Specifically considering those
organizations that have plotted at least 50 attacks from 1997 to 2018,
accounting for a total of 105 groups responsible for more than 42,
                            000 events
worldwide, we offer three sets of results. First, we show that over the years
global terrorism has been characterized by increasing operational cohesiveness.
Second, we highlight that year-to-year stability in co-clustering among groups
has been particularly high from 2009 to 2018, indicating temporal consistency
of similarity patterns in the last decade. Third, we demonstrate that
operational similarity between two organizations is driven by three factors:
(a) their overall activity; (b) the difference in the diversity of their
operational repertoires; (c) the difference in a combined measure of diversity
and activity. Groups' operational preferences, geographical homophily and
ideological affinity have no consistent role in determining operational
similarity.
</summary>
    <author>
      <name>Gian Maria Campedelli</name>
    </author>
    <author>
      <name>Iain J. Cruickshank</name>
    </author>
    <author>
      <name>Kathleen M. Carley</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/09546553.2021.2003785</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/09546553.2021.2003785" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages,
                            19 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Terrorism and Political Violence,
                            0(0),
                            1-20 (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2112.07998v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.07998v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2302.13877v1</id>
    <updated>2023-01-24T18: 22: 19Z</updated>
    <published>2023-01-24T18: 22: 19Z</published>
    <title>DeepADMR: A Deep Learning based Anomaly Detection for MANET Routing</title>
    <summary>  We developed DeepADMR, a novel neural anomaly detector for the deep
reinforcement learning (DRL)-based DeepCQ+ MANET routing policy. The
performance of DRL-based algorithms such as DeepCQ+ is only verified within the
trained and tested environments, hence their deployment in the tactical domain
induces high risks. DeepADMR monitors unexpected behavior of the DeepCQ+ policy
based on the temporal difference errors (TD-errors) in real-time and detects
anomaly scenarios with empirical and non-parametric cumulative-sum statistics.
The DeepCQ+ design via multi-agent weight-sharing proximal policy optimization
(PPO) is slightly modified to enable the real-time estimation of the TD-errors.
We report the DeepADMR performance in the presence of channel disruptions, high
mobility levels, and network sizes beyond the training environments, which
shows its effectiveness.
</summary>
    <author>
      <name>Alex Yahja</name>
    </author>
    <author>
      <name>Saeed Kaviani</name>
    </author>
    <author>
      <name>Bo Ryu</name>
    </author>
    <author>
      <name>Jae H. Kim</name>
    </author>
    <author>
      <name>Kevin A. Larson</name>
    </author>
    <link href="http://arxiv.org/abs/2302.13877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.13877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2311.02280v1</id>
    <updated>2023-11-04T00: 01: 38Z</updated>
    <published>2023-11-04T00: 01: 38Z</published>
    <title>Criticality and Chaos in Auditory and Vestibular Sensing</title>
    <summary>  The auditory and vestibular systems exhibit remarkable sensitivity of
detection, responding to deflections on the order of Angstroms, even in the
presence of biological noise. Further, these complex systems exhibit high
temporal acuity and frequency selectivity, allowing us to make sense of the
acoustic world around us. As this acoustic environment of interest spans
several orders of magnitude in both amplitude and frequency, these systems rely
heavily on nonlinearities and power-law scaling. The behavior of these sensory
systems has been extensively studied in the context of dynamical systems
theory, with many empirical phenomena described by critical dynamics. Other
phenomena have been explained by systems in the chaotic regime, where weak
perturbations drastically impact the future state of the system. We first
review the conceptual framework behind these two types of detectors, as well as
the detection features that they can capture. We then explore the intersection
of the two types of systems and propose ideal parameter regimes for auditory
and vestibular systems.
</summary>
    <author>
      <name>Justin Faber</name>
    </author>
    <author>
      <name>Dolores Bozovic</name>
    </author>
    <link href="http://arxiv.org/abs/2311.02280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.02280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2206.08672v1</id>
    <updated>2022-06-17T10: 17: 24Z</updated>
    <published>2022-06-17T10: 17: 24Z</published>
    <title>A Deep Learning Approach for the Segmentation of Electroencephalography
  Data in Eye Tracking Applications</title>
    <summary>  The collection of eye gaze information provides a window into many critical
aspects of human cognition, health and behaviour. Additionally, many
neuroscientific studies complement the behavioural information gained from eye
tracking with the high temporal resolution and neurophysiological markers
provided by electroencephalography (EEG). One of the essential eye-tracking
software processing steps is the segmentation of the continuous data stream
into events relevant to eye-tracking applications, such as saccades, fixations,
and blinks.
  Here, we introduce DETRtime, a novel framework for time-series segmentation
that creates ocular event detectors that do not require additionally recorded
eye-tracking modality and rely solely on EEG data. Our end-to-end deep
learning-based framework brings recent advances in Computer Vision to the
forefront of the times series segmentation of EEG data. DETRtime achieves
state-of-the-art performance in ocular event detection across diverse
eye-tracking experiment paradigms. In addition to that, we provide evidence
that our model generalizes well in the task of EEG sleep stage segmentation.
</summary>
    <author>
      <name>Lukas Wolf</name>
    </author>
    <author>
      <name>Ard Kastrati</name>
    </author>
    <author>
      <name>Martyna Beata Płomecka</name>
    </author>
    <author>
      <name>Jie-Ming Li</name>
    </author>
    <author>
      <name>Dustin Klebe</name>
    </author>
    <author>
      <name>Alexander Veicht</name>
    </author>
    <author>
      <name>Roger Wattenhofer</name>
    </author>
    <author>
      <name>Nicolas Langer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, Published at the Proceedings of the 39th International
  Conference on Machine Learning (ICML) 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.08672v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.08672v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1607.02737v3</id>
    <updated>2017-03-31T15: 39: 45Z</updated>
    <published>2016-07-10T12: 05: 41Z</published>
    <title>Transition Forests: Learning Discriminative Temporal Transitions for
  Action Recognition and Detection</title>
    <summary>  A human action can be seen as transitions between one's body poses over time,
where the transition depicts a temporal relation between two poses. Recognizing
actions thus involves learning a classifier sensitive to these pose transitions
as well as to static poses. In this paper, we introduce a novel method called
transitions forests, an ensemble of decision trees that both learn to
discriminate static poses and transitions between pairs of two independent
frames. During training, node splitting is driven by alternating two criteria:
the standard classification objective that maximizes the discrimination power
in individual frames, and the proposed one in pairwise frame transitions.
Growing the trees tends to group frames that have similar associated
transitions and share same action label incorporating temporal information that
was not available otherwise. Unlike conventional decision trees where the best
split in a node is determined independently of other nodes, the transition
forests try to find the best split of nodes jointly (within a layer) for
incorporating distant node transitions. When inferring the class label of a new
frame, it is passed down the trees and the prediction is made based on previous
frame predictions and the current one in an efficient and online manner. We
apply our method on varied skeleton action recognition and online detection
datasets showing its suitability over several baselines and state-of-the-art
approaches.
</summary>
    <author>
      <name>Guillermo Garcia-Hernando</name>
    </author>
    <author>
      <name>Tae-Kyun Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in CVPR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.02737v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02737v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.10468v2</id>
    <updated>2024-11-27T16: 20: 36Z</updated>
    <published>2024-04-16T11: 10: 11Z</published>
    <title>Community detection and anomaly prediction in dynamic networks</title>
    <summary>  Anomaly detection is an essential task in the analysis of dynamic networks,
offering early warnings of abnormal behavior. We present a principled approach
to detect anomalies in dynamic networks that integrates community structure as
a foundational model for regular behavior. Our model identifies anomalies as
irregular edges while capturing structural changes. Our approach leverages a
Markovian framework for temporal transitions and latent variables for community
and anomaly detection, inferring hidden parameters to detect unusual
interactions. Evaluations on synthetic and real-world datasets show strong
anomaly detection across various scenarios. In a case study on professional
football player transfers, we detect patterns influenced by club wealth and
country, as well as unexpected transactions both within and across community
boundaries. This work provides a framework for adaptable anomaly detection,
highlighting the value of integrating domain knowledge with data-driven
techniques for improved interpretability and robustness in complex networks.
</summary>
    <author>
      <name>Hadiseh Safdari</name>
    </author>
    <author>
      <name>Caterina De Bacco</name>
    </author>
    <link href="http://arxiv.org/abs/2404.10468v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.10468v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68-XX" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4; E.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2304.04220v1</id>
    <updated>2023-04-09T11: 50: 41Z</updated>
    <published>2023-04-09T11: 50: 41Z</published>
    <title>Towards Active Learning for Action Spotting in Association Football
  Videos</title>
    <summary>  Association football is a complex and dynamic sport, with numerous actions
occurring simultaneously in each game. Analyzing football videos is challenging
and requires identifying subtle and diverse spatio-temporal patterns. Despite
recent advances in computer vision, current algorithms still face significant
challenges when learning from limited annotated data, lowering their
performance in detecting these patterns. In this paper, we propose an active
learning framework that selects the most informative video samples to be
annotated next, thus drastically reducing the annotation effort and
accelerating the training of action spotting models to reach the highest
accuracy at a faster pace. Our approach leverages the notion of uncertainty
sampling to select the most challenging video clips to train on next, hastening
the learning process of the algorithm. We demonstrate that our proposed active
learning framework effectively reduces the required training data for accurate
action spotting in football videos. We achieve similar performances for action
spotting with NetVLAD++ on SoccerNet-v2, using only one-third of the dataset,
indicating significant capabilities for reducing annotation time and improving
data efficiency. We further validate our approach on two new datasets that
focus on temporally localizing actions of headers and passes, proving its
effectiveness across different action semantics in football. We believe our
active learning framework for action spotting would support further
applications of action spotting algorithms and accelerate annotation campaigns
in the sports domain.
</summary>
    <author>
      <name>Silvio Giancola</name>
    </author>
    <author>
      <name>Anthony Cioppa</name>
    </author>
    <author>
      <name>Julia Georgieva</name>
    </author>
    <author>
      <name>Johsan Billingham</name>
    </author>
    <author>
      <name>Andreas Serner</name>
    </author>
    <author>
      <name>Kerry Peek</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <author>
      <name>Marc Van Droogenbroeck</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/cvprw59228.2023.00538</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/cvprw59228.2023.00538" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CVSports'23</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.04220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.04220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1101.0234v1</id>
    <updated>2010-12-31T12: 06: 11Z</updated>
    <published>2010-12-31T12: 06: 11Z</published>
    <title>Dynamic Feature Description in Human Action Recognition</title>
    <summary>  This work aims to present novel description methods for human action
recognition. Generally, a video sequence can be represented as a collection of
spatial temporal words by detecting space-time interest points and describing
the unique features around the detected points (Bag of Words representation).
Interest points as well as the cuboids around them are considered informative
for feature description in terms of both the structural distribution of
interest points and the information content inside the cuboids. Our proposed
description approaches are based on this idea and making the feature
descriptors more discriminative.
</summary>
    <author>
      <name>Ruoyun Gao</name>
    </author>
    <author>
      <name>Michael S. Lew</name>
    </author>
    <author>
      <name>Ling Shao</name>
    </author>
    <link href="http://arxiv.org/abs/1101.0234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.0234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2410.00054v2</id>
    <updated>2024-10-11T19: 59: 35Z</updated>
    <published>2024-09-28T22: 31: 00Z</published>
    <title>Transferable Unsupervised Outlier Detection Framework for Human Semantic
  Trajectories</title>
    <summary>  Semantic trajectories, which enrich spatial-temporal data with textual
information such as trip purposes or location activities, are key for
identifying outlier behaviors critical to healthcare, social security, and
urban planning. Traditional outlier detection relies on heuristic rules, which
requires domain knowledge and limits its ability to identify unseen outliers.
Besides, there lacks a comprehensive approach that can jointly consider
multi-modal data across spatial, temporal, and textual dimensions. Addressing
the need for a domain-agnostic model, we propose the Transferable Outlier
Detection for Human Semantic Trajectories (TOD4Traj) framework.TOD4Traj first
introduces a modality feature unification module to align diverse data feature
representations, enabling the integration of multi-modal information and
enhancing transferability across different datasets. A contrastive learning
module is further pro-posed for identifying regular mobility patterns both
temporally and across populations, allowing for a joint detection of outliers
based on individual consistency and group majority patterns. Our experimental
results have shown TOD4Traj's superior performance over existing models,
demonstrating its effectiveness and adaptability in detecting human trajectory
outliers across various datasets.
</summary>
    <author>
      <name>Zheng Zhang</name>
    </author>
    <author>
      <name>Hossein Amiri</name>
    </author>
    <author>
      <name>Dazhou Yu</name>
    </author>
    <author>
      <name>Yuntong Hu</name>
    </author>
    <author>
      <name>Liang Zhao</name>
    </author>
    <author>
      <name>Andreas Zufle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an accepted paper on
  https: //sigspatial2024.sigspatial.org/accepted-papers/</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.00054v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.00054v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2502.19307v1</id>
    <updated>2025-02-26T17: 06: 13Z</updated>
    <published>2025-02-26T17: 06: 13Z</published>
    <title>Anomaly Detection in Complex Dynamical Systems: A Systematic Framework
  Using Embedding Theory and Physics-Inspired Consistency</title>
    <summary>  Anomaly detection in complex dynamical systems is essential for ensuring
reliability, safety, and efficiency in industrial and cyber-physical
infrastructures. Predictive maintenance helps prevent costly failures, while
cybersecurity monitoring has become critical as digitized systems face growing
threats. Many of these systems exhibit oscillatory behaviors and bounded
motion, requiring anomaly detection methods that capture structured temporal
dependencies while adhering to physical consistency principles. In this work,
we propose a system-theoretic approach to anomaly detection, grounded in
classical embedding theory and physics-inspired consistency principles. We
build upon the Fractal Whitney Embedding Prevalence Theorem, extending
traditional embedding techniques to complex system dynamics. Additionally, we
introduce state-derivative pairs as an embedding strategy to capture system
evolution. To enforce temporal coherence, we develop a Temporal Differential
Consistency Autoencoder (TDC-AE), incorporating a TDC-Loss that aligns the
approximated derivatives of latent variables with their dynamic
representations. We evaluate our method on the C-MAPSS dataset, a benchmark for
turbofan aeroengine degradation. TDC-AE outperforms LSTMs and Transformers
while achieving a 200x reduction in MAC operations, making it particularly
suited for lightweight edge computing. Our findings support the hypothesis that
anomalies disrupt stable system dynamics, providing a robust, interpretable
signal for anomaly detection.
</summary>
    <author>
      <name>Michael Somma</name>
    </author>
    <author>
      <name>Thomas Gallien</name>
    </author>
    <author>
      <name>Branka Stojanovic</name>
    </author>
    <link href="http://arxiv.org/abs/2502.19307v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.19307v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2203.06610v1</id>
    <updated>2022-03-13T09: 43: 27Z</updated>
    <published>2022-03-13T09: 43: 27Z</published>
    <title>Context-LSTM: a robust classifier for video detection on UCF101</title>
    <summary>  Video detection and human action recognition may be computationally
expensive, and need a long time to train models. In this paper, we were
intended to reduce the training time and the GPU memory usage of video
detection, and achieved a competitive detection accuracy. Other research works
such as Two-stream, C3D, TSN have shown excellent performance on UCF101. Here,
we used a LSTM structure simply for video detection. We used a simple structure
to perform a competitive top-1 accuracy on the entire validation dataset of
UCF101. The LSTM structure is named Context-LSTM, since it may process the deep
temporal features. The Context-LSTM may simulate the human recognition system.
We cascaded the LSTM blocks in PyTorch and connected the cell state flow and
hidden output flow. At the connection of the blocks, we used ReLU, Batch
Normalization, and MaxPooling functions. The Context-LSTM could reduce the
training time and the GPU memory usage, while keeping a state-of-the-art top-1
accuracy on UCF101 entire validation dataset, show a robust performance on
video action detection.
</summary>
    <author>
      <name>Dengshan Li</name>
    </author>
    <author>
      <name>Rujing Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages,
                            6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.06610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.06610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2303.10561v1</id>
    <updated>2023-03-19T04: 34: 17Z</updated>
    <published>2023-03-19T04: 34: 17Z</published>
    <title>Spatial-temporal Transformer for Affective Behavior Analysis</title>
    <summary>  The in-the-wild affective behavior analysis has been an important study. In
this paper, we submit our solutions for the 5th Workshop and Competition on
Affective Behavior Analysis in-the-wild (ABAW), which includes V-A Estimation,
Facial Expression Classification and AU Detection Sub-challenges. We propose a
Transformer Encoder with Multi-Head Attention framework to learn the
distribution of both the spatial and temporal features. Besides, there are
virious effective data augmentation strategies employed to alleviate the
problems of sample imbalance during model training. The results fully
demonstrate the effectiveness of our proposed model based on the Aff-Wild2
dataset.
</summary>
    <author>
      <name>Peng Zou</name>
    </author>
    <author>
      <name>Rui Wang</name>
    </author>
    <author>
      <name>Kehua Wen</name>
    </author>
    <author>
      <name>Yasi Peng</name>
    </author>
    <author>
      <name>Xiao Sun</name>
    </author>
    <link href="http://arxiv.org/abs/2303.10561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.10561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2501.06997v1</id>
    <updated>2025-01-13T01: 08: 06Z</updated>
    <published>2025-01-13T01: 08: 06Z</published>
    <title>TFLAG:Towards Practical APT Detection via Deviation-Aware Learning on
  Temporal Provenance Graph</title>
    <summary>  Advanced Persistent Threat (APT) have grown increasingly complex and
concealed, posing formidable challenges to existing Intrusion Detection Systems
in identifying and mitigating these attacks. Recent studies have incorporated
graph learning techniques to extract detailed information from provenance
graphs, enabling the detection of attacks with greater granularity.
Nevertheless, existing studies have largely overlooked the continuous yet
subtle temporal variations in the structure of provenance graphs, which may
correspond to surreptitious perturbation anomalies in ongoing APT attacks.
Therefore, we introduce TFLAG, an advanced anomaly detection framework that for
the first time integrates the structural dynamic extraction capabilities of
temporal graph model with the anomaly delineation abilities of deviation
networks to pinpoint covert attack activities in provenance graphs. This
self-supervised integration framework leverages the graph model to extract
neighbor interaction data under continuous temporal changes from historical
benign behaviors within provenance graphs, while simultaneously utilizing
deviation networks to accurately distinguish authentic attack activities from
false positive deviations due to unexpected subtle perturbations. The
experimental results indicate that, through a comprehensive design that
utilizes both attribute and temporal information, it can accurately identify
the time windows associated with APT attack behaviors without prior knowledge
(e.g., labeled data samples), demonstrating superior accuracy compared to
current state-of-the-art methods in differentiating between attack events and
system false positive events.
</summary>
    <author>
      <name>Wenhan Jiang</name>
    </author>
    <author>
      <name>Tingting Chai</name>
    </author>
    <author>
      <name>Hongri Liu</name>
    </author>
    <author>
      <name>Kai Wang</name>
    </author>
    <author>
      <name>Hongke Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2501.06997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.06997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1603.03541v1</id>
    <updated>2016-03-11T07: 13: 59Z</updated>
    <published>2016-03-11T07: 13: 59Z</published>
    <title>Watch-n-Patch: Unsupervised Learning of Actions and Relations</title>
    <summary>  There is a large variation in the activities that humans perform in their
everyday lives. We consider modeling these composite human activities which
comprises multiple basic level actions in a completely unsupervised setting.
Our model learns high-level co-occurrence and temporal relations between the
actions. We consider the video as a sequence of short-term action clips, which
contains human-words and object-words. An activity is about a set of
action-topics and object-topics indicating which actions are present and which
objects are interacting with. We then propose a new probabilistic model
relating the words and the topics. It allows us to model long-range action
relations that commonly exist in the composite activities, which is challenging
in previous works. We apply our model to the unsupervised action segmentation
and clustering, and to a novel application that detects forgotten actions,
which we call action patching. For evaluation, we contribute a new challenging
RGB-D activity video dataset recorded by the new Kinect v2, which contains
several human daily activities as compositions of multiple actions interacting
with different objects. Moreover, we develop a robotic system that watches
people and reminds people by applying our action patching algorithm. Our
robotic setup can be easily deployed on any assistive robot.
</summary>
    <author>
      <name>Chenxia Wu</name>
    </author>
    <author>
      <name>Jiemi Zhang</name>
    </author>
    <author>
      <name>Ozan Sener</name>
    </author>
    <author>
      <name>Bart Selman</name>
    </author>
    <author>
      <name>Silvio Savarese</name>
    </author>
    <author>
      <name>Ashutosh Saxena</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv: 1512.04208</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.03541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2007.05169v2</id>
    <updated>2021-01-28T10: 58: 41Z</updated>
    <published>2020-07-10T05: 15: 26Z</published>
    <title>Detecting Malicious Accounts in Permissionless Blockchains using
  Temporal Graph Properties</title>
    <summary>  The temporal nature of modeling accounts as nodes and transactions as
directed edges in a directed graph -- for a blockchain, enables us to
understand the behavior (malicious or benign) of the accounts. Predictive
classification of accounts as malicious or benign could help users of the
permissionless blockchain platforms to operate in a secure manner. Motivated by
this, we introduce temporal features such as burst and attractiveness on top of
several already used graph properties such as the node degree and clustering
coefficient. Using identified features, we train various Machine Learning (ML)
algorithms and identify the algorithm that performs the best in detecting which
accounts are malicious. We then study the behavior of the accounts over
different temporal granularities of the dataset before assigning them malicious
tags. For Ethereum blockchain, we identify that for the entire dataset - the
ExtraTreesClassifier performs the best among supervised ML algorithms. On the
other hand, using cosine similarity on top of the results provided by
unsupervised ML algorithms such as K-Means on the entire dataset, we were able
to detect 554 more suspicious accounts. Further, using behavior change analysis
for accounts, we identify 814 unique suspicious accounts across different
temporal granularities.
</summary>
    <author>
      <name>Rachit Agarwal</name>
    </author>
    <author>
      <name>Shikhar Barve</name>
    </author>
    <author>
      <name>Sandeep K. Shukla</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Springer Applied Network Science Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.05169v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.05169v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2008.01002v1</id>
    <updated>2020-08-03T16: 40: 44Z</updated>
    <published>2020-08-03T16: 40: 44Z</published>
    <title>Detection of biological signals from a live mammalian muscle using a
  diamond quantum sensor</title>
    <summary>  The ability to perform noninvasive, non-contact measurements of electric
signals produced by action potentials is essential in biomedicine. A key method
to do this is to remotely sense signals by the magnetic field they induce.
Existing methods for magnetic field sensing of mammalian tissue, used in
techniques such as magnetoencephalography of the brain, require cryogenically
cooled superconducting detectors. These have many disadvantages in terms of
high cost, flexibility and limited portability as well as poor spatial and
temporal resolution. In this work we demonstrate an alternative technique for
detecting magnetic fields generated by the current from action potentials in
living tissue using nitrogen vacancy centres in diamond. With 50pT/$\sqrt{Hz
                            }$
sensitivity, we show the first measurements of sensing from mammalian tissue
with a diamond sensor using mouse muscle optogenetically activated with blue
light. We show these measurements can be performed in an ordinary, unshielded
lab environment and that the signal can be easily recovered by digital signal
processing techniques.
</summary>
    <author>
      <name>James Luke Webb</name>
    </author>
    <author>
      <name>Luca Troise</name>
    </author>
    <author>
      <name>Nikolaj Winther Hansen</name>
    </author>
    <author>
      <name>Christoffer Olsson</name>
    </author>
    <author>
      <name>Adam Wojciechowski</name>
    </author>
    <author>
      <name>Jocelyn Achard</name>
    </author>
    <author>
      <name>Ovidiu Brinza</name>
    </author>
    <author>
      <name>Robert Staacke</name>
    </author>
    <author>
      <name>Michael Kieschnick</name>
    </author>
    <author>
      <name>Jan Meijer</name>
    </author>
    <author>
      <name>Axel Thielscher</name>
    </author>
    <author>
      <name>Jean-Francois Perrier</name>
    </author>
    <author>
      <name>Kirstine Berg-Sorensen</name>
    </author>
    <author>
      <name>Alexander Huck</name>
    </author>
    <author>
      <name>Ulrik Lund Andersen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s41598-021-81828-x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s41598-021-81828-x" rel="related"/>
    <link href="http://arxiv.org/abs/2008.01002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.01002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2212.10682v2</id>
    <updated>2023-01-18T00: 06: 28Z</updated>
    <published>2022-12-20T22: 55: 46Z</published>
    <title>Privacy-Protecting Behaviours of Risk Detection in People with Dementia
  using Videos</title>
    <summary>  People living with dementia often exhibit behavioural and psychological
symptoms of dementia that can put their and others' safety at risk. Existing
video surveillance systems in long-term care facilities can be used to monitor
such behaviours of risk to alert the staff to prevent potential injuries or
death in some cases. However, these behaviours of risk events are heterogeneous
and infrequent in comparison to normal events. Moreover, analyzing raw videos
can also raise privacy concerns. In this paper, we present two novel
privacy-protecting video-based anomaly detection approaches to detect
behaviours of risks in people with dementia. We either extracted body pose
information as skeletons or used semantic segmentation masks to replace
multiple humans in the scene with their semantic boundaries. Our work differs
from most existing approaches for video anomaly detection that focus on
appearance-based features, which can put the privacy of a person at risk and is
also susceptible to pixel-based noise, including illumination and viewing
direction. We used anonymized videos of normal activities to train customized
spatio-temporal convolutional autoencoders and identify behaviours of risk as
anomalies. We showed our results on a real-world study conducted in a dementia
care unit with patients with dementia, containing approximately 21 hours of
normal activities data for training and 9 hours of data containing normal and
behaviours of risk events for testing. We compared our approaches with the
original RGB videos and obtained a similar area under the receiver operating
characteristic curve performance of 0.807 for the skeleton-based approach and
0.823 for the segmentation mask-based approach.
</summary>
    <author>
      <name>Pratik K. Mishra</name>
    </author>
    <author>
      <name>Andrea Iaboni</name>
    </author>
    <author>
      <name>Bing Ye</name>
    </author>
    <author>
      <name>Kristine Newman</name>
    </author>
    <author>
      <name>Alex Mihailidis</name>
    </author>
    <author>
      <name>Shehroz S. Khan</name>
    </author>
    <link href="http://arxiv.org/abs/2212.10682v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.10682v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1508.00188v2</id>
    <updated>2016-04-13T15: 24: 10Z</updated>
    <published>2015-08-02T04: 16: 44Z</published>
    <title>Explore Spatiotemporal and Demographic Characteristics of Human Mobility
  via Twitter: A Case Study of Chicago</title>
    <summary>  Characterizing human mobility patterns is essential for understanding human
behaviors and the interactions with socioeconomic and natural environment. With
the continuing advancement of location and Web 2.0 technologies, location-based
social media (LBSM) have been gaining widespread popularity in the past few
years. With an access to locations of users, profiles and the contents of the
social media posts, the LBSM data provided a novel modality of data source for
human mobility study. By exploiting the explicit location footprints and mining
the latent demographic information implied in the LBSM data, the purpose of
this paper is to investigate the spatiotemporal characteristics of human
mobility with a particular focus on the impact of demography. We first collect
geo-tagged Twitter feeds posted in the conterminous United States area, and
organize the collection of feeds using the concept of space-time trajectory
corresponding to each Twitter user. Commonly human mobility measures, including
detected home and activity centers, are derived for each user trajectory. We
then select a subset of Twitter users that have detected home locations in the
city of Chicago as a case study, and apply name analysis to the names provided
in user profiles to learn the implicit demographic information of Twitter
users, including race/ethnicity, gender and age. Finally we explore the
spatiotemporal distribution and mobility characteristics of Chicago Twitter
users, and investigate the demographic impact by comparing the differences
across three demographic dimensions (race/ethnicity, gender and age). We found
that, although the human mobility measures of different demographic groups
generally follow the generic laws (e.g., power law distribution), the
demographic information, particular the race/ethnicity group, significantly
affects the urban human mobility patterns.
</summary>
    <author>
      <name>Feixiong Luo</name>
    </author>
    <author>
      <name>Guofeng Cao</name>
    </author>
    <author>
      <name>Kevin Mulligan</name>
    </author>
    <author>
      <name>Xiang Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.apgeog.2016.03.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.apgeog.2016.03.001" rel="related"/>
    <link href="http://arxiv.org/abs/1508.00188v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.00188v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2302.09657v1</id>
    <updated>2023-02-19T19: 13: 24Z</updated>
    <published>2023-02-19T19: 13: 24Z</published>
    <title>Table Tennis Stroke Detection and Recognition Using Ball Trajectory Data</title>
    <summary>  In this work, the novel task of detecting and classifying table tennis
strokes solely using the ball trajectory has been explored. A single camera
setup positioned in the umpire's view has been employed to procure a dataset
consisting of six stroke classes executed by four professional table tennis
players. Ball tracking using YOLOv4, a traditional object detection model, and
TrackNetv2, a temporal heatmap based model, have been implemented on our
dataset and their performances have been benchmarked. A mathematical approach
developed to extract temporal boundaries of strokes using the ball trajectory
data yielded a total of 2023 valid strokes in our dataset, while also detecting
services and missed strokes successfully. The temporal convolutional network
developed performed stroke recognition on completely unseen data with an
accuracy of 87.155%. Several machine learning and deep learning based model
architectures have been trained for stroke recognition using ball trajectory
input and benchmarked based on their performances. While stroke recognition in
the field of table tennis has been extensively explored based on human action
recognition using video data focused on the player's actions, the use of ball
trajectory data for the same is an unexplored characteristic of the sport.
Hence, the motivation behind the work is to demonstrate that meaningful
inferences such as stroke detection and recognition can be drawn using minimal
input information.
</summary>
    <author>
      <name>Kaustubh Milind Kulkarni</name>
    </author>
    <author>
      <name>Rohan S Jamadagni</name>
    </author>
    <author>
      <name>Jeffrey Aaron Paul</name>
    </author>
    <author>
      <name>Sucheth Shenoy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages,
                            5 figures,
                            6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.09657v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.09657v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.11455v1</id>
    <updated>2025-03-14T14: 44: 39Z</updated>
    <published>2025-03-14T14: 44: 39Z</published>
    <title>Demography-independent behavioural dynamics influenced the spread of
  COVID-19 in Denmark</title>
    <summary>  Understanding the factors that impact how a communicable disease like
COVID-19 spreads is of central importance to mitigate future outbreaks.
Traditionally, epidemic surveillance and forecasting analyses have focused on
epidemiological data but recent advancements have demonstrated that monitoring
behavioural changes may be equally important. Prior studies have shown that
high-frequency survey data on social contact behaviour were able to improve
predictions of epidemiological observables during the COVID-19 pandemic. Yet,
the full potential of such highly granular survey data remains debated. Here,
we utilise daily nationally representative survey data from Denmark collected
during 23 months of the COVID-19 pandemic to demonstrate two central use-cases
for such high-frequency survey data. First, we show that complex behavioural
patterns across demographics collapse to a small number of universal key
features, greatly simplifying the monitoring and analysis of adherence to
outbreak-mitigation measures. Notably, the temporal evolution of the
self-reported median number of face-to-face contacts follows a universal
behavioural pattern across age groups, with potential to simplify analysis
efforts for future outbreaks. Second, we show that these key features can be
leveraged to improve deep-learning-based predictions of daily reported new
infections. In particular, our models detect a strong link between aggregated
self-reported social distancing and hygiene behaviours and the number of new
cases in the subsequent days. Taken together, our results highlight the value
of high-frequency surveys to improve our understanding of population behaviour
in an ongoing public health crisis and its potential use for prediction of
central epidemiological observables.
</summary>
    <author>
      <name>Léo Meynent</name>
    </author>
    <author>
      <name>Michael Bang Petersen</name>
    </author>
    <author>
      <name>Sune Lehmann</name>
    </author>
    <author>
      <name>Benjamin F. Maier</name>
    </author>
    <link href="http://arxiv.org/abs/2503.11455v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.11455v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2401.07890v2</id>
    <updated>2024-01-20T14: 18: 49Z</updated>
    <published>2024-01-15T18: 43: 48Z</published>
    <title>A Strategy for Implementing description Temporal Dynamic Algorithms in
  Dynamic Knowledge Graphs by SPIN</title>
    <summary>  Planning and reasoning about actions and processes, in addition to reasoning
about propositions, are important issues in recent logical and computer science
studies. The widespread use of actions in everyday life such as IoT, semantic
web services, etc., and the limitations and issues in the action formalisms are
two factors that lead us to study how actions are represented.
  Since 2007, there have been some ideas to integrate Description Logic (DL)
and action formalisms for representing both static and dynamic knowledge.
Meanwhile, time is an important factor in dynamic situations, and actions
change states over time. In this study, on the one hand, we examined related
logical structures such as extensions of description logics (DLs), temporal
formalisms, and action formalisms. On the other hand, we analyzed possible
tools for designing and developing the Knowledge and Action Base (KAB).
  For representation and reasoning about actions, we embedded actions into DLs
(such as Dynamic-ALC and its extensions). We propose a terminable algorithm for
action projection, planning, checking the satisfiability, consistency,
realizability, and executability, and also querying from KAB. Actions in this
framework were modeled with SPIN and added to state space. This framework has
also been implemented as a plugin for the Prot\'eg\'e ontology editor.
  During the last two decades, various algorithms have been presented, but due
to the high computational complexity, we face many problems in implementing
dynamic ontologies. In addition, an algorithm to detect the inconsistency of
actions' effects was not explicitly stated. In the proposed strategy, the
interactions of actions with other parts of modeled knowledge, and a method to
check consistency between the effects of actions are presented. With this
framework, the ramification problem can be well handled in future works.
</summary>
    <author>
      <name>Alireza Shahbazi</name>
    </author>
    <author>
      <name>Seyyed Ahmad Mirsanei</name>
    </author>
    <author>
      <name>Malikeh Haj Khan Mirzaye Sarraf</name>
    </author>
    <author>
      <name>Behrouz Minaei Bidgoli</name>
    </author>
    <link href="http://arxiv.org/abs/2401.07890v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.07890v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2405.07864v1</id>
    <updated>2024-05-13T15: 52: 30Z</updated>
    <published>2024-05-13T15: 52: 30Z</published>
    <title>Sub-percent Characterization and Polarimetric Performance Analysis of
  Commercial Micro-polarizer Array Detectors</title>
    <summary>  Polarization imaging can yield crucial information in multiple applications
of remote sensing, such as characterization of clouds, aerosols, and the Aurora
Borealis. Some applications require sub-percent polarimetric sensitivity and
accuracy in determining the Stokes parameters, which can be a challenge to
attain. In 2018, Sony released a low-cost CMOS-based imaging chip with
integrated micro-polarizer array for general polarization measurements. We
implement the calibration steps required for these Sony chips to reach
sub-percent polarimetric accuracies. To analyze their performances, we have
compared the characteristics of four different detector packages by three
manufacturers housing either the monochromatic version or the RGB color
variant. We present a comprehensive overview of the effects that these
characteristics have on the polarimetric performance of the camera. They
include dark noise, behavior over different gain settings, detector/pixel
artifacts, and polarimetric effects determined by polarizer extinction ratios,
polarizer orientations, and accuracy of polarimetric zero points due to
differential pixel gains. In addition to calibrations using unpolarized light
and fully linearly polarized light, we assess the polarimetric sensitivity
within a tilting and rotating glass-plate set-up. We discuss the benefits of
adding a rotating half-wave plate as an additional temporal modulator to
generically mitigate some of the detector effects, and achieve better
polarimetric sensitivity/accuracy albeit at the expense of lower temporal
resolution. We conclude by presenting and discussing the polarimetric limits to
which we were able to calibrate the detector effects for practical purposes. By
reaching a compound absolute polarimetric uncertainty of less than a percent,
these very compact, low-cost detectors are enabled for a multitude of
scientific goals.
</summary>
    <author>
      <name>Thijs Stockmans</name>
    </author>
    <author>
      <name>Naor Scheinowitz</name>
    </author>
    <author>
      <name>Erwoud van der Linden</name>
    </author>
    <author>
      <name>Irina Malysheva</name>
    </author>
    <author>
      <name>Kira Strelow</name>
    </author>
    <author>
      <name>Martijn Smit</name>
    </author>
    <author>
      <name>Frans Snik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conference proceeding for SPIE Polarization: Measurement, Analysis,
  and Remote Sensing XVI,
                            31 pages,
                            21 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.07864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.07864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2203.01057v2</id>
    <updated>2022-03-22T13: 31: 53Z</updated>
    <published>2022-03-02T12: 13: 08Z</published>
    <title>Colar: Effective and Efficient Online Action Detection by Consulting
  Exemplars</title>
    <summary>  Online action detection has attracted increasing research interests in recent
years. Current works model historical dependencies and anticipate the future to
perceive the action evolution within a video segment and improve the detection
accuracy. However, the existing paradigm ignores category-level modeling and
does not pay sufficient attention to efficiency. Considering a category, its
representative frames exhibit various characteristics. Thus, the category-level
modeling can provide complimentary guidance to the temporal dependencies
modeling. This paper develops an effective exemplar-consultation mechanism that
first measures the similarity between a frame and exemplary frames, and then
aggregates exemplary features based on the similarity weights. This is also an
efficient mechanism, as both similarity measurement and feature aggregation
require limited computations. Based on the exemplar-consultation mechanism, the
long-term dependencies can be captured by regarding historical frames as
exemplars, while the category-level modeling can be achieved by regarding
representative frames from a category as exemplars. Due to the complementarity
from the category-level modeling, our method employs a lightweight architecture
but achieves new high performance on three benchmarks. In addition, using a
spatio-temporal network to tackle video frames, our method makes a good
trade-off between effectiveness and efficiency. Code is available at
https: //github.com/VividLe/Online-Action-Detection.
</summary>
    <author>
      <name>Le Yang</name>
    </author>
    <author>
      <name>Junwei Han</name>
    </author>
    <author>
      <name>Dingwen Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.01057v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.01057v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2412.11555v1</id>
    <updated>2024-12-16T08: 40: 12Z</updated>
    <published>2024-12-16T08: 40: 12Z</published>
    <title>TS-SatFire: A Multi-Task Satellite Image Time-Series Dataset for
  Wildfire Detection and Prediction</title>
    <summary>  Wildfire monitoring and prediction are essential for understanding wildfire
behaviour. With extensive Earth observation data, these tasks can be integrated
and enhanced through multi-task deep learning models. We present a
comprehensive multi-temporal remote sensing dataset for active fire detection,
daily wildfire monitoring, and next-day wildfire prediction. Covering wildfire
events in the contiguous U.S. from January 2017 to October 2021, the dataset
includes 3552 surface reflectance images and auxiliary data such as weather,
topography, land cover, and fuel information, totalling 71 GB. The lifecycle of
each wildfire is documented, with labels for active fires (AF) and burned areas
(BA), supported by manual quality assurance of AF and BA test labels. The
dataset supports three tasks: a) active fire detection, b) daily burned area
mapping, and c) wildfire progression prediction. Detection tasks use pixel-wise
classification of multi-spectral, multi-temporal images, while prediction tasks
integrate satellite and auxiliary data to model fire dynamics. This dataset and
its benchmarks provide a foundation for advancing wildfire research using deep
learning.
</summary>
    <author>
      <name>Yu Zhao</name>
    </author>
    <author>
      <name>Sebastian Gerard</name>
    </author>
    <author>
      <name>Yifang Ban</name>
    </author>
    <link href="http://arxiv.org/abs/2412.11555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.11555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2306.17778v3</id>
    <updated>2024-01-22T00: 54: 30Z</updated>
    <published>2023-06-30T16: 31: 14Z</published>
    <title>Look, Remember and Reason: Grounded reasoning in videos with language
  models</title>
    <summary>  Multi-modal language models (LM) have recently shown promising performance in
high-level reasoning tasks on videos. However, existing methods still fall
short in tasks like causal or compositional spatiotemporal reasoning over
actions, in which model predictions need to be grounded in fine-grained
low-level details, such as object motions and object interactions. In this
work, we propose training an LM end-to-end on low-level surrogate tasks,
including object detection, re-identification, and tracking, to endow the model
with the required low-level visual capabilities. We show that a two-stream
video encoder with spatiotemporal attention is effective at capturing the
required static and motion-based cues in the video. By leveraging the LM's
ability to perform the low-level surrogate tasks, we can cast reasoning in
videos as the three-step process of Look, Remember, Reason wherein visual
information is extracted using low-level visual skills step-by-step and then
integrated to arrive at a final answer. We demonstrate the effectiveness of our
framework on diverse visual reasoning tasks from the ACRE, CATER,
Something-Else and STAR datasets. Our approach is trainable end-to-end and
surpasses state-of-the-art task-specific methods across these tasks by a large
margin.
</summary>
    <author>
      <name>Apratim Bhattacharyya</name>
    </author>
    <author>
      <name>Sunny Panchal</name>
    </author>
    <author>
      <name>Mingu Lee</name>
    </author>
    <author>
      <name>Reza Pourreza</name>
    </author>
    <author>
      <name>Pulkit Madan</name>
    </author>
    <author>
      <name>Roland Memisevic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at ICLR 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.17778v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.17778v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2301.06774v2</id>
    <updated>2024-05-09T10: 15: 28Z</updated>
    <published>2023-01-17T09: 52: 54Z</published>
    <title>Temporal Dynamics of Coordinated Online Behavior: Stability, Archetypes,
  and Influence</title>
    <summary>  Large-scale online campaigns, malicious or otherwise, require a significant
degree of coordination among participants, which sparked interest in the study
of coordinated online behavior. State-of-the-art methods for detecting
coordinated behavior perform static analyses, disregarding the temporal
dynamics of coordination. Here, we carry out the first dynamic analysis of
coordinated behavior. To reach our goal we build a multiplex temporal network
and we perform dynamic community detection to identify groups of users that
exhibited coordinated behaviors in time. Thanks to our novel approach we find
that: (i) coordinated communities feature variable degrees of temporal
instability; (ii) dynamic analyses are needed to account for such instability,
and results of static analyses can be unreliable and scarcely representative of
unstable communities; (iii) some users exhibit distinct archetypal behaviors
that have important practical implications; (iv) content and network
characteristics contribute to explaining why users leave and join coordinated
communities. Our results demonstrate the advantages of dynamic analyses and
open up new directions of research on the unfolding of online debates, on the
strategies of coordinated communities, and on the patterns of online influence.
</summary>
    <author>
      <name>Serena Tardelli</name>
    </author>
    <author>
      <name>Leonardo Nizzoli</name>
    </author>
    <author>
      <name>Maurizio Tesconi</name>
    </author>
    <author>
      <name>Mauro Conti</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <author>
      <name>Giovanni Da San Martino</name>
    </author>
    <author>
      <name>Stefano Cresci</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1073/pnas.2307038121</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1073/pnas.2307038121" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Article published in PNAS 121 (20). Please, cite the published
  version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the National Academy of Sciences 121 (20),
  e2307038121,
                            2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2301.06774v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.06774v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1812.10748v1</id>
    <updated>2018-12-27T15: 56: 07Z</updated>
    <published>2018-12-27T15: 56: 07Z</published>
    <title>Malicious Software Detection and Classification utilizing
  Temporal-Graphs of System-call Group Relations</title>
    <summary>  In this work we propose a graph-based model that, utilizing relations between
groups of System-calls, distinguishes malicious from benign software samples
and classifies the detected malicious samples to one of a set of known malware
families. More precisely, given a System-call Dependency Graph (ScDG) that
depicts the malware's behavior, we first transform it to a more abstract
representation, utilizing the indexing of System-calls to a set of groups of
similar functionality, constructing thus an abstract and mutation-tolerant
graph that we call Group Relation Graph (GrG); then, we construct another graph
representation, which we call Coverage Graph (CvG), that depicts the dominating
relations between the nodes of a GrG graph. Based on the research so far in the
field, we pointed out that behavior-based graph representations had not
leveraged the aspect of the temporal evolution of the graph. Hence, the novelty
of our work is that, preserving the initial representations of GrG and CvG
graphs, we focus on augmenting the potentials of theses graphs by adding
further features that enhance its abilities on detecting and further
classifying to a known malware family an unknown malware sample. To that end,
we construct periodical instances of the graph that represent its temporal
evolution concerning its structural modifications, creating another graph
representation that we call Temporal Graphs. In this paper, we present the
theoretical background behind our approach, discuss the current technological
status on malware detection and classification and demonstrate the overall
architecture of our proposed detection and classification model alongside with
its underlying main principles and its structural key-components.
</summary>
    <author>
      <name>Anna Mpanti</name>
    </author>
    <author>
      <name>Stavros D. Nikolopoulos</name>
    </author>
    <author>
      <name>Iosif Polenakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages,
                            15 figures,
                            1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.10748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.10748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6; D.4.8; K.6.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1703.07855v1</id>
    <updated>2017-03-22T21: 12: 30Z</updated>
    <published>2017-03-22T21: 12: 30Z</published>
    <title>The Chandra Dust Scattering Halo of Galactic Center transient Swift
  J174540.7-290015</title>
    <summary>  We report the detection of a dust scattering halo around a recently
discovered X-ray transient, Swift J174540.7-290015, which in early February of
2016 underwent one of the brightest outbursts (F_X ~ 5e-10 erg/cm^2/s) observed
from a compact object in the Galactic Center field. We analyze four Chandra
images that were taken as follow-up observations to Swift discoveries of new
Galactic Center transients. After adjusting our spectral extraction for the
effects of detector pileup, we construct a point spread function for each
observation and compare it to the GC field before the outburst. We find
residual surface brightness around Swift J174540.7-290015, which has a shape
and temporal evolution consistent with the behavior expected from X-rays
scattered by foreground dust. We examine the spectral properties of the source,
which shows evidence that the object transitioned from a soft to hard spectral
state as it faded below L_X ~ 1e36 erg/s. This behavior is consistent with the
hypothesis that the object is a low mass X-ray binary in the Galactic Center.
</summary>
    <author>
      <name>L. Corrales</name>
    </author>
    <author>
      <name>B. Mon</name>
    </author>
    <author>
      <name>D. Haggard</name>
    </author>
    <author>
      <name>F. Baganoff</name>
    </author>
    <author>
      <name>G. Garmire</name>
    </author>
    <author>
      <name>N. Degenaar</name>
    </author>
    <author>
      <name>M. Reynolds</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3847/1538-4357/aa68dd</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3847/1538-4357/aa68dd" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in ApJ</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.07855v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07855v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2303.16943v1</id>
    <updated>2023-03-29T18: 05: 45Z</updated>
    <published>2023-03-29T18: 05: 45Z</published>
    <title>Soft Gamma-Ray Spectral and Time evolution of the GRB 221009A: prompt
  and afterglow emission with INTEGRAL/IBIS-PICsIT</title>
    <summary>  The gamma-ray burst (GRB) 221009A, with its extreme brightness, has provided
the opportunity to explore GRB prompt and afterglow emission behavior on short
time scales with high statistics. In conjunction with detection up to very
high-energy gamma-rays, studies of this event shed light on the emission
processes at work in the initial phases of GRBs emission. Using INTEGRAL/IBIS's
soft gamma-ray detector, PICsIT (200-2600 keV), we studied the temporal and
spectral evolution during the prompt phase and the early afterglow period. We
found a "flux-tracking" behavior with the source spectrum "softer" when
brighter. However the relationship between the spectral index and the flux
changes during the burst. The PICsIT light curve shows afterglow emission
begins to dominate at ~ T0 + 630s and decays with a slope of 1.6 +/- 0.2,
consistent with the slopes reported at soft X-rays.
</summary>
    <author>
      <name>James Rodi</name>
    </author>
    <author>
      <name>Pietro Ubertini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1051/0004-6361/202346373</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1051/0004-6361/202346373" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,
                            4 figures, A&amp;A Accepted</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">A&amp;A 677, L3 (2023)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2303.16943v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.16943v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2202.04250v1</id>
    <updated>2022-02-09T03: 15: 33Z</updated>
    <published>2022-02-09T03: 15: 33Z</published>
    <title>GenAD: General Representations of Multivariate Time Seriesfor Anomaly
  Detection</title>
    <summary>  The reliability of wireless base stations in China Mobile is of vital
importance, because the cell phone users are connected to the stations and the
behaviors of the stations are directly related to user experience. Although the
monitoring of the station behaviors can be realized by anomaly detection on
multivariate time series, due to complex correlations and various temporal
patterns of multivariate series in large-scale stations, building a general
unsupervised anomaly detection model with a higher F1-score remains a
challenging task. In this paper, we propose a General representation of
multivariate time series for Anomaly Detection(GenAD). First, we pre-train a
general model on large-scale wireless base stations with self-supervision,
which can be easily transferred to a specific station anomaly detection with a
small amount of training data. Second, we employ Multi-Correlation Attention
and Time-Series Attention to represent the correlations and temporal patterns
of the stations. With the above innovations, GenAD increases F1-score by total
9% on real-world datasets in China Mobile, while the performance does not
significantly degrade on public datasets with only 10% of the training data.
</summary>
    <author>
      <name>Xiaolei Hua</name>
    </author>
    <author>
      <name>Lin Zhu</name>
    </author>
    <author>
      <name>Shenglin Zhang</name>
    </author>
    <author>
      <name>Zeyan Li</name>
    </author>
    <author>
      <name>Su Wang</name>
    </author>
    <author>
      <name>Dong Zhou</name>
    </author>
    <author>
      <name>Shuo Wang</name>
    </author>
    <author>
      <name>Chao Deng</name>
    </author>
    <link href="http://arxiv.org/abs/2202.04250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.04250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/astro-ph/0609341v1</id>
    <updated>2006-09-13T01: 19: 34Z</updated>
    <published>2006-09-13T01: 19: 34Z</published>
    <title>GEO debris and interplanetary dust: fluxes and charging behavior</title>
    <summary>  In September 1996, a dust/debris detector: GORID was launched into the
geostationary (GEO) region as a piggyback instrument on the Russian Express-2
telecommunications spacecraft. The instrument began its normal operation in
April 1997 and ended its mission in July 2002. The goal of this work was to use
GORID's particle data to identify and separate the space debris to
interplanetary dust particles (IDPs) in GEO, to more finely determine the
instrument's measurement characteristics and to derive impact fluxes. While the
physical characteristics of the GORID impacts alone are insufficient for a
reliable distinction between debris and interplanetary dust, the temporal
behavior of the impacts are strong enough indicators to separate the
populations based on clustering. Non-cluster events are predominantly
interplanetary, while cluster events are debris. The GORID mean flux
distributions (at mass thresholds which are impact speed dependent) for IDPs,
corrected for dead time, are 1.35x10^{
                                -4
                            } m^{
                                -2
                            } s^{
                                -1
                            } using a mean detection
rate: 0.54 d^{
                                -1
                            }, and for space debris are 6.1x10^{
                                -4
                            } m^{
                                -2
                            } s^{
                                -1
                            } using a
mean detection rate: 2.5 d^{
                                -1
                            }. Beta-meteoroids were not detected. Clusters
could be a closely-packed debris cloud or a particle breaking up due to
electrostatic fragmentation after high charging.
</summary>
    <author>
      <name>Amara L. Graps</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INAF-IFSI, Italy</arxiv:affiliation>
    </author>
    <author>
      <name>Simon F. Green</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PSSRI, The Open University, U.K.</arxiv:affiliation>
    </author>
    <author>
      <name>Neil McBride</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PSSRI, The Open University, U.K.</arxiv:affiliation>
    </author>
    <author>
      <name>J. A. M. McDonnell</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Unispace Kent, U.K.</arxiv:affiliation>
    </author>
    <author>
      <name>Kalle Bunte</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">eta max space GmbH, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Hakan Svedhem</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ESA/ESTEC, The Netherlands</arxiv:affiliation>
    </author>
    <author>
      <name>Gerhard Drolshagen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ESA/ESTEC, The Netherlands</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">* Comments: 6 pages,
                            4 postscript figures, in Dust in Planetary
  Systems 2005, Krueger, H. and Graps, A. eds., ESA Publications, SP in press
  (2006). For high resolution version, see:
  http: //www.mpi-hd.mpg.de/dustgroup/~graps/dips2005/GrapsetalDIPS2005.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/astro-ph/0609341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/0609341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1703.09026v2</id>
    <updated>2017-07-26T14: 13: 23Z</updated>
    <published>2017-03-27T12: 14: 07Z</published>
    <title>Trespassing the Boundaries: Labeling Temporal Bounds for Object
  Interactions in Egocentric Video</title>
    <summary>  Manual annotations of temporal bounds for object interactions (i.e. start and
end times) are typical training input to recognition, localization and
detection algorithms. For three publicly available egocentric datasets, we
uncover inconsistencies in ground truth temporal bounds within and across
annotators and datasets. We systematically assess the robustness of
state-of-the-art approaches to changes in labeled temporal bounds, for object
interaction recognition. As boundaries are trespassed, a drop of up to 10% is
observed for both Improved Dense Trajectories and Two-Stream Convolutional
Neural Network.
  We demonstrate that such disagreement stems from a limited understanding of
the distinct phases of an action, and propose annotating based on the Rubicon
Boundaries, inspired by a similarly named cognitive model, for consistent
temporal bounds of object interactions. Evaluated on a public dataset, we
report a 4% increase in overall accuracy, and an increase in accuracy for 55%
of classes when Rubicon Boundaries are used for temporal annotations.
</summary>
    <author>
      <name>Davide Moltisanti</name>
    </author>
    <author>
      <name>Michael Wray</name>
    </author>
    <author>
      <name>Walterio Mayol-Cuevas</name>
    </author>
    <author>
      <name>Dima Damen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.09026v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09026v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2206.04153v2</id>
    <updated>2022-07-03T19: 52: 08Z</updated>
    <published>2022-06-08T20: 31: 02Z</published>
    <title>Unsupervised Key Event Detection from Massive Text Corpora</title>
    <summary>  Automated event detection from news corpora is a crucial task towards mining
fast-evolving structured knowledge. As real-world events have different
granularities, from the top-level themes to key events and then to event
mentions corresponding to concrete actions, there are generally two lines of
research: (1) theme detection identifies from a news corpus major themes (e.g.,
                            "2019 Hong Kong Protests" vs. "2020 U.S. Presidential Election") that have very
distinct semantics; and (2) action extraction extracts from one document
mention-level actions (e.g.,
                            "the police hit the left arm of the protester")
that are too fine-grained for comprehending the event. In this paper, we
propose a new task, key event detection at the intermediate level, aiming to
detect from a news corpus key events (e.g., "HK Airport Protest on Aug.12-14"), each happening at a particular time/location and focusing on the same
topic. This task can bridge event understanding and structuring and is
inherently challenging because of the thematic and temporal closeness of key
events and the scarcity of labeled data due to the fast-evolving nature of news
articles. To address these challenges, we develop an unsupervised key event
detection framework, EvMine, that (1) extracts temporally frequent peak phrases
using a novel ttf-itf score, (2) merges peak phrases into event-indicative
feature sets by detecting communities from our designed peak phrase graph that
captures document co-occurrences, semantic similarities, and temporal closeness
signals, and (3) iteratively retrieves documents related to each key event by
training a classifier with automatically generated pseudo labels from the
event-indicative feature sets and refining the detected key events using the
retrieved documents. Extensive experiments and case studies show EvMine
outperforms all the baseline methods and its ablations on two real-world news
corpora.
</summary>
    <author>
      <name>Yunyi Zhang</name>
    </author>
    <author>
      <name>Fang Guo</name>
    </author>
    <author>
      <name>Jiaming Shen</name>
    </author>
    <author>
      <name>Jiawei Han</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3534678.3539395</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3534678.3539395" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to KDD 2022 Research Track</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.04153v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.04153v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2107.02588v2</id>
    <updated>2022-08-12T05: 45: 56Z</updated>
    <published>2021-07-06T13: 05: 12Z</published>
    <title>Temporal Nuances of Coordination Network Semantics</title>
    <summary>  Current network-based methods for detecting coordinated inauthentic behaviour
on social media focus primarily on inferring links between accounts based on
common "behavioural traces"[
                                19
                            ], such as retweeting the same tweet or posting
the same URL. Assuming the goal of coordination is amplification, boosting a
message within a constrained period, most approaches use a temporal window to
ensure the co-activity occurs within a specific timeframe [
                                9,
                                14,
                                19,
                                24
                            ].
Real-world application requires considering near real-time processing, creating
performance requirements, which also highlight gaps in the semantics of
coordination in the literature. These methods could all exploit temporal
elements of coordinated activity. We describe preliminary research regarding
coordination network semantics, coordination network construction, relevant
observations in three political Twitter datasets and the role of cheerleaders
in revealing social bots.
</summary>
    <author>
      <name>Derek Weber</name>
    </author>
    <author>
      <name>Lucia Falzon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages,
                            10 figures,
                            2 tables, prepared for the 4th
  Multidisciplinary International Symposium on Disinformation in Open Online
  Media (MISDOOM'22)</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.02588v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.02588v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2109.13572v1</id>
    <updated>2021-09-28T09: 02: 15Z</updated>
    <published>2021-09-28T09: 02: 15Z</published>
    <title>Information Elevation Network for Fast Online Action Detection</title>
    <summary>  Online action detection (OAD) is a task that receives video segments within a
streaming video as inputs and identifies ongoing actions within them. It is
important to retain past information associated with a current action. However,
long short-term memory (LSTM), a popular recurrent unit for modeling temporal
information from videos, accumulates past information from the previous hidden
and cell states and the extracted visual features at each timestep without
considering the relationships between the past and current information.
Consequently, the forget gate of the original LSTM can lose the accumulated
information relevant to the current action because it determines which
information to forget without considering the current action. We introduce a
novel information elevation unit (IEU) that lifts up and accumulate the past
information relevant to the current action in order to model the past
information that is especially relevant to the current action. To the best of
our knowledge, our IEN is the first attempt that considers the computational
overhead for the practical use of OAD. Through ablation studies, we design an
efficient and effective OAD network using IEUs, called an information elevation
network (IEN). Our IEN uses visual features extracted by a fast action
recognition network taking only RGB frames because extracting optical flows
requires heavy computation overhead. On two OAD benchmark datasets, THUMOS-14
and TVSeries, our IEN outperforms state-of-the-art OAD methods using only RGB
frames. Furthermore, on the THUMOS-14 dataset, our IEN outperforms the
state-of-the-art OAD methods using two-stream features based on RGB frames and
optical flows.
</summary>
    <author>
      <name>Sunah Min</name>
    </author>
    <author>
      <name>Jinyoung Moon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to the IEEE for possible publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.13572v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.13572v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2409.13091v1</id>
    <updated>2024-09-19T21: 23: 44Z</updated>
    <published>2024-09-19T21: 23: 44Z</published>
    <title>Interpretable Action Recognition on Hard to Classify Actions</title>
    <summary>  We investigate a human-like interpretable model of video understanding.
Humans recognise complex activities in video by recognising critical
spatio-temporal relations among explicitly recognised objects and parts, for
example, an object entering the aperture of a container. To mimic this we build
on a model which uses positions of objects and hands, and their motions, to
recognise the activity taking place. To improve this model we focussed on three
of the most confused classes (for this model) and identified that the lack of
3D information was the major problem. To address this we extended our basic
model by adding 3D awareness in two ways: (1) A state-of-the-art object
detection model was fine-tuned to determine the difference between "Container"
and "NotContainer" in order to integrate object shape information into the
existing object features. (2) A state-of-the-art depth estimation model was
used to extract depth values for individual objects and calculate depth
relations to expand the existing relations used our interpretable model. These
3D extensions to our basic model were evaluated on a subset of three
superficially similar "Putting" actions from the Something-Something-v2
dataset. The results showed that the container detector did not improve
performance, but the addition of depth relations made a significant improvement
to performance.
</summary>
    <author>
      <name>Anastasia Anichenko</name>
    </author>
    <author>
      <name>Frank Guerin</name>
    </author>
    <author>
      <name>Andrew Gilbert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, This manuscript has been accepted at the Human-inspired
  Computer Vision (HCV) ECCV 2024 Workshop. arXiv admin note: text overlap with
  arXiv: 2107.05319</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.13091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.13091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1902.07438v1</id>
    <updated>2019-02-20T07: 39: 45Z</updated>
    <published>2019-02-20T07: 39: 45Z</published>
    <title>Dynamic Matrix Decomposition for Action Recognition</title>
    <summary>  Designing a technique for the automatic analysis of different actions in
videos in order to detect the presence of interested activities is of high
significance nowadays. In this paper, we explore a robust and dynamic
appearance technique for the purpose of identifying different action
activities. We also exploit a low-rank and structured sparse matrix
decomposition (LSMD) method to better model these activities.. Our method is
effective in encoding localized spatio-temporal features which enables the
analysis of local motion taking place in the video. Our proposed model use
adjacent frame differences as the input to the method thereby forcing it to
capture the changes occurring in the video. The performance of our model is
tested on a benchmark dataset in terms of detection accuracy. Results achieved
with our model showed the promising capability of our model in detecting action
activities.
</summary>
    <author>
      <name>Abdul Basit</name>
    </author>
    <link href="http://arxiv.org/abs/1902.07438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.07438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2311.17822v1</id>
    <updated>2023-11-29T17: 22: 28Z</updated>
    <published>2023-11-29T17: 22: 28Z</published>
    <title>Anomalous Behavior Detection in Trajectory Data of Older Drivers</title>
    <summary>  Given a road network and a set of trajectory data, the anomalous behavior
detection (ABD) problem is to identify drivers that show significant
directional deviations, hardbrakings, and accelerations in their trips. The ABD
problem is important in many societal applications, including Mild Cognitive
Impairment (MCI) detection and safe route recommendations for older drivers.
The ABD problem is computationally challenging due to the large size of
temporally-detailed trajectories dataset. In this paper, we propose an
Edge-Attributed Matrix that can represent the key properties of
temporally-detailed trajectory datasets and identify abnormal driving
behaviors. Experiments using real-world datasets demonstrated that our approach
identifies abnormal driving behaviors.
</summary>
    <author>
      <name>Seyedeh Gol Ara Ghoreishi</name>
    </author>
    <author>
      <name>Sonia Moshfeghi</name>
    </author>
    <author>
      <name>Muhammad Tanveer Jan</name>
    </author>
    <author>
      <name>Joshua Conniff</name>
    </author>
    <author>
      <name>KwangSoo Yang</name>
    </author>
    <author>
      <name>Jinwoo Jang</name>
    </author>
    <author>
      <name>Borko Furht</name>
    </author>
    <author>
      <name>Ruth Tappen</name>
    </author>
    <author>
      <name>David Newman</name>
    </author>
    <author>
      <name>Monica Rosselli</name>
    </author>
    <author>
      <name>Jiannan Zhai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE HONET 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.17822v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.17822v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2504.16655v1</id>
    <updated>2025-04-23T12: 22: 24Z</updated>
    <published>2025-04-23T12: 22: 24Z</published>
    <title>WiFi based Human Fall and Activity Recognition using Transformer based
  Encoder Decoder and Graph Neural Networks</title>
    <summary>  Human pose estimation and action recognition have received attention due to
their critical roles in healthcare monitoring, rehabilitation, and assistive
technologies. In this study, we proposed a novel architecture named Transformer
based Encoder Decoder Network (TED Net) designed for estimating human skeleton
poses from WiFi Channel State Information (CSI). TED Net integrates
convolutional encoders with transformer based attention mechanisms to capture
spatiotemporal features from CSI signals. The estimated skeleton poses were
used as input to a customized Directed Graph Neural Network (DGNN) for action
recognition. We validated our model on two datasets: a publicly available multi
modal dataset for assessing general pose estimation, and a newly collected
dataset focused on fall related scenarios involving 20 participants.
Experimental results demonstrated that TED Net outperformed existing approaches
in pose estimation, and that the DGNN achieves reliable action classification
using CSI based skeletons, with performance comparable to RGB based systems.
Notably, TED Net maintains robust performance across both fall and non fall
cases. These findings highlight the potential of CSI driven human skeleton
estimation for effective action recognition, particularly in home environments
such as elderly fall detection. In such settings, WiFi signals are often
readily available, offering a privacy preserving alternative to vision based
methods, which may raise concerns about continuous camera monitoring.
</summary>
    <author>
      <name>Younggeol Cho</name>
    </author>
    <author>
      <name>Elisa Motta</name>
    </author>
    <author>
      <name>Olivia Nocentini</name>
    </author>
    <author>
      <name>Marta Lagomarsino</name>
    </author>
    <author>
      <name>Andrea Merello</name>
    </author>
    <author>
      <name>Marco Crepaldi</name>
    </author>
    <author>
      <name>Arash Ajoudani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,
                            4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.16655v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.16655v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2311.16138v1</id>
    <updated>2023-11-03T16: 56: 02Z</updated>
    <published>2023-11-03T16: 56: 02Z</published>
    <title>After-Stroke Arm Paresis Detection using Kinematic Data</title>
    <summary>  This paper presents an approach for detecting unilateral arm
paralysis/weakness using kinematic data. Our method employs temporal
convolution networks and recurrent neural networks, guided by knowledge
distillation, where we use inertial measurement units attached to the body to
capture kinematic information such as acceleration, rotation, and flexion of
body joints during an action. This information is then analyzed to recognize
body actions and patterns. Our proposed network achieves a high paretic
detection accuracy of 97.99\%, with an action classification accuracy of
77.69\%, through knowledge sharing. Furthermore, by incorporating causal
reasoning, we can gain additional insights into the patient's condition, such
as their Fugl-Meyer assessment score or impairment level based on the machine
learning result. Overall, our approach demonstrates the potential of using
kinematic data and machine learning for detecting arm paralysis/weakness. The
results suggest that our method could be a useful tool for clinicians and
healthcare professionals working with patients with this condition.
</summary>
    <author>
      <name>Kenneth Lai</name>
    </author>
    <author>
      <name>Mohammed Almekhlafi</name>
    </author>
    <author>
      <name>Svetlana Yanushkevich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to IEEE Symposium Series on Computational Intelligence</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.16138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.16138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1303.6021v1</id>
    <updated>2013-03-25T03: 16: 08Z</updated>
    <published>2013-03-25T03: 16: 08Z</published>
    <title>Spatio-Temporal Covariance Descriptors for Action and Gesture
  Recognition</title>
    <summary>  We propose a new action and gesture recognition method based on
spatio-temporal covariance descriptors and a weighted Riemannian locality
preserving projection approach that takes into account the curved space formed
by the descriptors. The weighted projection is then exploited during boosting
to create a final multiclass classification algorithm that employs the most
useful spatio-temporal regions. We also show how the descriptors can be
computed quickly through the use of integral video representations. Experiments
on the UCF sport, CK+ facial expression and Cambridge hand gesture datasets
indicate superior performance of the proposed method compared to several recent
state-of-the-art techniques. The proposed method is robust and does not require
additional processing of the videos, such as foreground detection,
interest-point detection or tracking.
</summary>
    <author>
      <name>Andres Sanin</name>
    </author>
    <author>
      <name>Conrad Sanderson</name>
    </author>
    <author>
      <name>Mehrtash T. Harandi</name>
    </author>
    <author>
      <name>Brian C. Lovell</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/WACV.2013.6475006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/WACV.2013.6475006" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Workshop on Applications of Computer Vision, pp. 103-110,
                            2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1303.6021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.6021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; I.5.4; I.4.7; I.4.8; I.2.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2005.08701v1</id>
    <updated>2020-05-18T13: 31: 12Z</updated>
    <published>2020-05-18T13: 31: 12Z</published>
    <title>Machine learning for the diagnosis of early stage diabetes using
  temporal glucose profiles</title>
    <summary>  Machine learning shows remarkable success for recognizing patterns in data.
Here we apply the machine learning (ML) for the diagnosis of early stage
diabetes, which is known as a challenging task in medicine. Blood glucose
levels are tightly regulated by two counter-regulatory hormones, insulin and
glucagon, and the failure of the glucose homeostasis leads to the common
metabolic disease, diabetes mellitus. It is a chronic disease that has a long
latent period the complicates detection of the disease at an early stage. The
vast majority of diabetics result from that diminished effectiveness of insulin
action. The insulin resistance must modify the temporal profile of blood
glucose. Thus we propose to use ML to detect the subtle change in the temporal
pattern of glucose concentration. Time series data of blood glucose with
sufficient resolution is currently unavailable, so we confirm the proposal
using synthetic data of glucose profiles produced by a biophysical model that
considers the glucose regulation and hormone action. Multi-layered perceptrons,
convolutional neural networks, and recurrent neural networks all identified the
degree of insulin resistance with high accuracy above $85\%$.
</summary>
    <author>
      <name>Woo Seok Lee</name>
    </author>
    <author>
      <name>Junghyo Jo</name>
    </author>
    <author>
      <name>Taegeun Song</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s40042-021-00056-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s40042-021-00056-8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages,
                            2 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.08701v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.08701v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1907.08483v1</id>
    <updated>2019-07-19T12: 23: 40Z</updated>
    <published>2019-07-19T12: 23: 40Z</published>
    <title>Inferring Accurate Bus Trajectories from Noisy Estimated Arrival Time
  Records</title>
    <summary>  Urban commuting data has long been a vital source of understanding population
mobility behaviour and has been widely adopted for various applications such as
transport infrastructure planning and urban anomaly detection. While
individual-specific transaction records (such as smart card (tap-in, tap-out)
data or taxi trip records) hold a wealth of information, these are often
private data available only to the service provider (e.g., taxicab operator).
In this work, we explore the utility in harnessing publicly available, albeit
noisy, transportation datasets, such as noisy "Estimated Time of Arrival" (ETA)
records (commonly available to commuters through transit Apps or electronic
signages). We first propose a framework to extract accurate individual bus
trajectories from such ETA records, and present results from both a primary
city (Singapore) and a secondary city (London) to validate the techniques.
Finally, we quantify the upper bound on the spatiotemporal resolution, of the
reconstructed trajectory outputs, achieved by our proposed technique.
</summary>
    <author>
      <name>Lakmal Meegahapola</name>
    </author>
    <author>
      <name>Noel Athaide</name>
    </author>
    <author>
      <name>Kasthuri Jayarajah</name>
    </author>
    <author>
      <name>Shili Xiang</name>
    </author>
    <author>
      <name>Archan Misra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ITSC.2019.8916939</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ITSC.2019.8916939" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in 22nd IEEE Intelligent Transportation Systems Conference
  (ITSC) 2019</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Intelligent Transportation Systems Conference (ITSC),
  Auckland, New Zealand,
                            2019, pp. 4517-4524</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1907.08483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.08483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2208.06128v1</id>
    <updated>2022-08-12T06: 14: 36Z</updated>
    <published>2022-08-12T06: 14: 36Z</published>
    <title>Online Discovery of Evolving Groups over Massive-Scale Trajectory
  Streams</title>
    <summary>  The increasing pervasiveness of object tracking technologies leads to huge
volumes of spatiotemporal data collected in the form of trajectory streams. The
discovery of useful group patterns from moving objects' movement behaviours in
trajectory streams is critical for real-time applications ranging from
transportation management to military surveillance. Motivated by this, we first
propose a novel pattern, called evolving group, which models the unusual group
events of moving objects that travel together within density connected clusters
in evolving streaming trajectories. Our theoretical analysis and empirical
study on the Osaka Pedestrian data and Beijing Taxi data demonstrate its
effectiveness in capturing the development, evolution, and trend of group
events of moving objects in streaming context. Moreover, we propose a discovery
method that efficiently supports online detection of evolving groups over
massive-scale trajectory streams using a sliding window. It contains three
phases along with a set of novel optimization techniques designed to minimize
the computation costs. Furthermore, to scale to huge workloads over evolving
streams, we extend our discovery method to a parallel framework by using a
sector-based partition. Our comprehensive empirical study demonstrates that our
online discovery framework is effective and efficient on real-world high-volume
trajectory streams.
</summary>
    <author>
      <name>Yanwei Yu</name>
    </author>
    <author>
      <name>Ruoshan Lan</name>
    </author>
    <author>
      <name>Lei Cao</name>
    </author>
    <author>
      <name>Peng Song</name>
    </author>
    <author>
      <name>Yingjie Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2208.06128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.06128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2010.12171v1</id>
    <updated>2020-10-23T05: 32: 21Z</updated>
    <published>2020-10-23T05: 32: 21Z</published>
    <title>DualNet: Locate Then Detect Effective Payload with Deep Attention
  Network</title>
    <summary>  Network intrusion detection (NID) is an essential defense strategy that is
used to discover the trace of suspicious user behaviour in large-scale
cyberspace, and machine learning (ML), due to its capability of automation and
intelligence, has been gradually adopted as a mainstream hunting method in
recent years. However, traditional ML based network intrusion detection systems
(NIDSs) are not effective to recognize unknown threats and their high detection
rate often comes with the cost of high false alarms, which leads to the problem
of alarm fatigue. To address the above problems, in this paper, we propose a
novel neural network based detection system, DualNet, which is constructed with
a general feature extraction stage and a crucial feature learning stage.
DualNet can rapidly reuse the spatial-temporal features in accordance with
their importance to facilitate the entire learning process and simultaneously
mitigate several optimization problems occurred in deep learning (DL). We
evaluate the DualNet on two benchmark cyber attack datasets, NSL-KDD and
UNSW-NB15. Our experiment shows that DualNet outperforms classical ML based
NIDSs and is more effective than existing DL methods for NID in terms of
accuracy, detection rate and false alarm rate.
</summary>
    <author>
      <name>Shiyi Yang</name>
    </author>
    <author>
      <name>Peilun Wu</name>
    </author>
    <author>
      <name>Hui Guo</name>
    </author>
    <link href="http://arxiv.org/abs/2010.12171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.12171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2001.01596v2</id>
    <updated>2020-05-15T15: 57: 48Z</updated>
    <published>2020-01-02T15: 54: 20Z</published>
    <title>Deep learning for brake squeal: vibration detection, characterization
  and prediction</title>
    <summary>  Despite significant advances in modeling of friction-induced vibrations and
brake squeal, the majority of industrial research and design is still conducted
experimentally, since many aspects of squeal and its mechanisms involved remain
unknown. We report here for the first time on novel strategies for handling
data-intensive vibration testings to gain better insights into friction brake
system vibrations and noise generation mechanisms. Machine learning-based
methods to detect and characterize vibrations, to understand sensitivities and
to predict brake squeal are applied with the aim to illustrate how
interdisciplinary approaches can leverage the potential of data science
techniques for classical mechanical engineering challenges. In the first part,
a deep learning brake squeal detector is developed to identify several classes
of typical friction noise recordings. The detection method is rooted in recent
computer vision techniques for object detection based on convolutional neural
networks. It allows to overcome limitations of classical approaches that solely
rely on instantaneous spectral properties of the recorded noise. Results
indicate superior detection and characterization quality when compared to a
state-of-the-art brake squeal detector. In the second part, a recurrent neural
network is employed to learn the parametric patterns that determine the dynamic
stability of an operating brake system. Given a set of multivariate loading
conditions, the RNN learns to predict the noise generation of the structure.
The validated RNN represents a virtual twin model for the squeal behavior of a
specific brake system. It is found that this model can predict the occurrence
and the onset of brake squeal with high accuracy and that it can identify the
complicated patterns and temporal dependencies in the loading conditions that
drive the dynamical structure into regimes of instability.
</summary>
    <author>
      <name>Merten Stender</name>
    </author>
    <author>
      <name>Merten Tiedemann</name>
    </author>
    <author>
      <name>David Spieler</name>
    </author>
    <author>
      <name>Daniel Schoepflin</name>
    </author>
    <author>
      <name>Norbert Hofffmann</name>
    </author>
    <author>
      <name>Sebastian Oberst</name>
    </author>
    <link href="http://arxiv.org/abs/2001.01596v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01596v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2308.09296v4</id>
    <updated>2024-08-08T03: 06: 37Z</updated>
    <published>2023-08-18T04: 45: 56Z</published>
    <title>CARLA: Self-supervised Contrastive Representation Learning for Time
  Series Anomaly Detection</title>
    <summary>  One main challenge in time series anomaly detection (TSAD) is the lack of
labelled data in many real-life scenarios. Most of the existing anomaly
detection methods focus on learning the normal behaviour of unlabelled time
series in an unsupervised manner. The normal boundary is often defined tightly,
resulting in slight deviations being classified as anomalies, consequently
leading to a high false positive rate and a limited ability to generalise
normal patterns. To address this, we introduce a novel end-to-end
self-supervised ContrAstive Representation Learning approach for time series
Anomaly detection (CARLA). While existing contrastive learning methods assume
that augmented time series windows are positive samples and temporally distant
windows are negative samples, we argue that these assumptions are limited as
augmentation of time series can transform them to negative samples, and a
temporally distant window can represent a positive sample. Our contrastive
approach leverages existing generic knowledge about time series anomalies and
injects various types of anomalies as negative samples. Therefore, CARLA not
only learns normal behaviour but also learns deviations indicating anomalies.
It creates similar representations for temporally closed windows and distinct
ones for anomalies. Additionally, it leverages the information about
representations' neighbours through a self-supervised approach to classify
windows based on their nearest/furthest neighbours to further enhance the
performance of anomaly detection. In extensive tests on seven major real-world
time series anomaly detection datasets, CARLA shows superior performance over
state-of-the-art self-supervised and unsupervised TSAD methods. Our research
shows the potential of contrastive representation learning to advance time
series anomaly detection.
</summary>
    <author>
      <name>Zahra Zamanzadeh Darban</name>
    </author>
    <author>
      <name>Geoffrey I. Webb</name>
    </author>
    <author>
      <name>Shirui Pan</name>
    </author>
    <author>
      <name>Charu C. Aggarwal</name>
    </author>
    <author>
      <name>Mahsa Salehi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.patcog.2024.110874</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.patcog.2024.110874" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages,
                            9 figures,
                            9 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.09296v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.09296v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1707.01900v1</id>
    <updated>2017-07-06T07: 08: 46Z</updated>
    <published>2017-07-06T07: 08: 46Z</published>
    <title>On the Distinct Periodicities of Sunspot Counts in Flaring and
  Non-flaring Active Regions</title>
    <summary>  In a recent work, Kilcik et al. (2017), have detected the temporal and
periodic behavior of sunspot counts (SSC) in flaring (i.e. C, M, or X class
flares), and non-flaring active regions for the last two solar cycles, covering
the period: 1996 - 2016. The main results obtained are: 1) The temporal
behavior of monthly means of daily total SSC in flaring and non-flaring active
regions are different and these differences are also varying from cycle to
cycle; 2) The periodicities detected in SSC of flaring and non-flaring active
regions are quite different and these variations are also different from one
cycle to another; the highest detected period in the flaring active regions is
113 days, while there are much higher periodicities (327,
                            312, and 256 days) in
non-flaring regions. The detection of typical different periodicities in
flaring and non-flaring regions can suggests both important differences and
physical interpretation in the magneto-hydrodynamic behavior of the Sun. For
this reason in the present paper we show a further periodicity analysis of the
sunspot counts in flaring and in non-flaring active regions using the same data
source of that used by the above cited authors and applying a powerful wavelet
analysis tool which is particularly useful to detect multiscale features of
complex unsteady and unevenly sampled time series. In order to futher support
the differences and similarities found in the time behavior of SSC in flaring
and non-flaring regions, we also computed the behavior of the wavelet entropy,
a proper time function which allow us to measure the degree of complexity in
the dynamics of the related time series.
</summary>
    <author>
      <name>Stefano Sello</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages,
                            3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.01900v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01900v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1706.08592v1</id>
    <updated>2017-06-23T13: 56: 22Z</updated>
    <published>2017-06-23T13: 56: 22Z</published>
    <title>Profilometry with digital fringe-projection at the spatial and temporal
  Nyquist frequencies</title>
    <summary>  A phase-demodulation method for digital fringe-projection profilometry using
the spatial and temporal Nyquist frequencies is presented. It allows us to
digitize tridimensional surfaces using the highest spatial frequency ({\pi
                            }
radians per pixel) and consequently with the highest sensitivity for a given
digital fringe projector. Working with the highest temporal frequency ({\pi
                            }
radians per temporal sample), the proposed method rejects the DC component and
all even-order distorting harmonics using a bare-minimum 2-step phase shift.
The proposed method is suitable for digitization of piece-wise continuous
surfaces because it does not require spatial low-pass filtering. Gamma
calibration is unnecessary because the projected fringes are binary, and the
harmonics produced by the binary profile can be easily attenuated with a slight
defocusing on the digital projector. Viability of the proposed method is
supported by experimental results showing complete agreement with the predicted
behavior.
</summary>
    <author>
      <name>Moises Padilla</name>
    </author>
    <author>
      <name>Manuel Servin</name>
    </author>
    <author>
      <name>Guillermo Garnica</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1364/OE.25.022292</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1364/OE.25.022292" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,
                            11 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Opt. Express 25(19),
                            22292-22302 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.08592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2208.04950v2</id>
    <updated>2022-10-12T04: 34: 41Z</updated>
    <published>2022-08-09T07: 38: 36Z</published>
    <title>BabyNet: A Lightweight Network for Infant Reaching Action Recognition in
  Unconstrained Environments to Support Future Pediatric Rehabilitation
  Applications</title>
    <summary>  Action recognition is an important component to improve autonomy of physical
rehabilitation devices, such as wearable robotic exoskeletons. Existing human
action recognition algorithms focus on adult applications rather than pediatric
ones. In this paper, we introduce BabyNet, a light-weight (in terms of
trainable parameters) network structure to recognize infant reaching action
from off-body stationary cameras. We develop an annotated dataset that includes
diverse reaches performed while in a sitting posture by different infants in
unconstrained environments (e.g., in home settings, etc.). Our approach uses
the spatial and temporal connection of annotated bounding boxes to interpret
onset and offset of reaching, and to detect a complete reaching action. We
evaluate the efficiency of our proposed approach and compare its performance
against other learning-based network structures in terms of capability of
capturing temporal inter-dependencies and accuracy of detection of reaching
onset and offset. Results indicate our BabyNet can attain solid performance in
terms of (average) testing accuracy that exceeds that of other larger networks,
and can hence serve as a light-weight data-driven framework for video-based
infant reaching action recognition.
</summary>
    <author>
      <name>Amel Dechemi</name>
    </author>
    <author>
      <name>Vikarn Bhakri</name>
    </author>
    <author>
      <name>Ipsita Sahin</name>
    </author>
    <author>
      <name>Arjun Modi</name>
    </author>
    <author>
      <name>Julya Mestas</name>
    </author>
    <author>
      <name>Pamodya Peiris</name>
    </author>
    <author>
      <name>Dannya Enriquez Barrundia</name>
    </author>
    <author>
      <name>Elena Kokkoni</name>
    </author>
    <author>
      <name>Konstantinos Karydis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/RO-MAN50785.2021.9515507</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/RO-MAN50785.2021.9515507" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to RO-MAN 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.04950v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.04950v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1706.00077v1</id>
    <updated>2017-05-31T20: 26: 21Z</updated>
    <published>2017-05-31T20: 26: 21Z</published>
    <title>Collective behaviour of social bots is encoded in their temporal Twitter
  activity</title>
    <summary>  Computational propaganda deploys social or political bots to try to shape,
steer and manipulate online public discussions and influence decisions.
Collective behaviour of populations of social bots has not been yet widely
studied, though understanding of collective patterns arising from interactions
between bots would aid social bot detection. Here we show that there are
significant differences in collective behaviour between population of bots and
population of humans as detected from their Twitter activity. Using a large
dataset of tweets we have collected during the UK EU referendum campaign, we
separated users into population of bots and population of humans based on the
length of sequences of their high-frequency tweeting activity. We show that
while pairwise correlations between users are weak they co-exist with
collective correlated states, however the statistics of correlations and
co-spiking probability differ in both populations. Our results demonstrate that
populations of social bots and human users in social media exhibit collective
properties similar to the ones found in social and biological systems placed
near a critical point.
</summary>
    <author>
      <name>Andrej Duh</name>
    </author>
    <author>
      <name>Marjan Slak Rupnik</name>
    </author>
    <author>
      <name>Dean Korošak</name>
    </author>
    <link href="http://arxiv.org/abs/1706.00077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1605.00902v2</id>
    <updated>2016-09-07T09: 41: 03Z</updated>
    <published>2016-05-03T13: 38: 19Z</published>
    <title>Bayesian parameter estimation by continuous homodyne detection</title>
    <summary>  We simulate the process of continuous homodyne detection of the radiative
emission from a quantum system, and we investigate how a Bayesian analysis can
be employed to determine unknown parameters that govern the system evolution.
Measurement back-action quenches the system dynamics at all times and we show
that the ensuing transient evolution is more sensitive to system parameters
than the steady state of the system. The parameter sensitivity can be
quantified by the Fisher information, and we investigate numerically and
analytically how the temporal noise correlations in the measurement signal
contribute to the ultimate sensitivity limit of homodyne detection.
</summary>
    <author>
      <name>Alexander Holm Kiilerich</name>
    </author>
    <author>
      <name>Klaus Mølmer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevA.94.032103</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevA.94.032103" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,
                            4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. A 94,
                            032103 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1605.00902v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00902v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1607.03385v2</id>
    <updated>2016-07-15T19: 37: 18Z</updated>
    <published>2016-07-12T14: 56: 21Z</published>
    <title>Compiling Stateful Network Properties for Runtime Verification</title>
    <summary>  Networks are difficult to configure correctly, and tricky to debug. These
problems are accentuated by temporal and stateful behavior. Static
verification, while useful, is ineffectual for detecting behavioral deviations
induced by hardware faults, security failures, and so on, so dynamic property
monitoring is also valuable. Unfortunately, existing monitoring and runtime
verification for networks largely focuses on properties about individual
packets (such as connectivity) or requires a digest of all network events be
sent to a server, incurring enormous cost.
  We present a network monitoring system that avoids these problems. Because
traces of network events correspond well to temporal logic, we use a subset of
Metric First-Order Temporal Logic as the query language. These queries are
compiled down to execute completely on the network switches. This vastly
reduces network load, improves the precision of queries, and decreases
detection latency. We show the practical feasibility of our work by extending a
widely-used software switch and deploying it on networks. Our work also
suggests improvements to network instruction sets to better support temporal
monitoring.
</summary>
    <author>
      <name>Tim Nelson</name>
    </author>
    <author>
      <name>Nicholas DeMarinis</name>
    </author>
    <author>
      <name>Timothy Adam Hoff</name>
    </author>
    <author>
      <name>Rodrigo Fonseca</name>
    </author>
    <author>
      <name>Shriram Krishnamurthi</name>
    </author>
    <link href="http://arxiv.org/abs/1607.03385v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03385v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2211.14285v1</id>
    <updated>2022-11-25T18: 40: 00Z</updated>
    <published>2022-11-25T18: 40: 00Z</published>
    <title>Geo-Spatial Cluster based Hybrid Spatio-Temporal Copula Interpolation</title>
    <summary>  In the absence of Gaussianity assumptions without disturbing spatial
continuity interpolating along the whole spatial surface for different time
lags is challenging. The past researchers pay enough attention to
Spatio-temporal interpolation ignoring the dynamic behavior of a spatial mean
function, threshold distance, and direction of maintaining spatial continuity.
Therefore, we employ hierarchical spatial clustering (HSC) to preserve local
spatial stationarity. This research work introduces a hybrid extreme valued
copula-based Spatio-temporal interpolation algorithm. Spatial dependence is
captured by a blended extreme valued probability distribution (BEVD). Temporal
dependency is modeled by the Bi-directional long short-time memory (BLSTM) at
different temporal granularities,
                            1 month,
                            2 months, and 3 months.
Spatio-temporal dependence is modeled by the Gumbel-Hougaard copula (GH). We
apply the proposed Spatio-temporal interpolation approach to the air pollution
data (Outdoor Particulate Matter (PM) concentration) of Delhi, collected from
the website of the Central Pollution Control Board, India as a crucial
circumstantial study. This article describes a probabilistic-recurrent neural
networking algorithm for Spatio-temporal interpolation. This Spatio-temporal
hybrid copula interpolation algorithm outperforms and is efficient enough to
detect spatial trends and temporal influence. From the entire research, we
notice that PM concentration in a year reaches a maximum, generally in November
and December. The northern and central part of Del-hi is the most sensitive
regarding air pollution.
</summary>
    <author>
      <name>Debjoy Thakur</name>
    </author>
    <author>
      <name>Ishapathik Das</name>
    </author>
    <link href="http://arxiv.org/abs/2211.14285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.14285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2208.12673v1</id>
    <updated>2022-08-25T13: 41: 03Z</updated>
    <published>2022-08-25T13: 41: 03Z</published>
    <title>Enabling Weakly-Supervised Temporal Action Localization from On-Device
  Learning of the Video Stream</title>
    <summary>  Detecting actions in videos have been widely applied in on-device
applications. Practical on-device videos are always untrimmed with both action
and background. It is desirable for a model to both recognize the class of
action and localize the temporal position where the action happens. Such a task
is called temporal action location (TAL), which is always trained on the cloud
where multiple untrimmed videos are collected and labeled. It is desirable for
a TAL model to continuously and locally learn from new data, which can directly
improve the action detection precision while protecting customers' privacy.
However, it is non-trivial to train a TAL model, since tremendous video samples
with temporal annotations are required. However, annotating videos frame by
frame is exorbitantly time-consuming and expensive. Although weakly-supervised
TAL (W-TAL) has been proposed to learn from untrimmed videos with only
video-level labels, such an approach is also not suitable for on-device
learning scenarios. In practical on-device learning applications, data are
collected in streaming. Dividing such a long video stream into multiple video
segments requires lots of human effort, which hinders the exploration of
applying the TAL tasks to realistic on-device learning applications. To enable
W-TAL models to learn from a long, untrimmed streaming video, we propose an
efficient video learning approach that can directly adapt to new environments.
We first propose a self-adaptive video dividing approach with a contrast
score-based segment merging approach to convert the video stream into multiple
segments. Then, we explore different sampling strategies on the TAL tasks to
request as few labels as possible. To the best of our knowledge, we are the
first attempt to directly learn from the on-device, long video stream.
</summary>
    <author>
      <name>Yue Tang</name>
    </author>
    <author>
      <name>Yawen Wu</name>
    </author>
    <author>
      <name>Peipei Zhou</name>
    </author>
    <author>
      <name>Jingtong Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Manuscript received April 07,
                            2022; revised June 11,
                            2022; accepted
  July 05, 2022. This article was presented in the International Conference on
  2022 and appears as part of the ESWEEK-TCAD special issue</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.12673v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.12673v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1708.07590v2</id>
    <updated>2017-08-28T05: 23: 58Z</updated>
    <published>2017-08-25T01: 08: 10Z</published>
    <title>Hierarchical Multi-scale Attention Networks for Action Recognition</title>
    <summary>  Recurrent Neural Networks (RNNs) have been widely used in natural language
processing and computer vision. Among them, the Hierarchical Multi-scale RNN
(HM-RNN), a kind of multi-scale hierarchical RNN proposed recently, can learn
the hierarchical temporal structure from data automatically. In this paper, we
extend the work to solve the computer vision task of action recognition.
However, in sequence-to-sequence models like RNN, it is normally very hard to
discover the relationships between inputs and outputs given static inputs. As a
solution, attention mechanism could be applied to extract the relevant
information from input thus facilitating the modeling of input-output
relationships. Based on these considerations, we propose a novel attention
network, namely Hierarchical Multi-scale Attention Network (HM-AN), by
combining the HM-RNN and the attention mechanism and apply it to action
recognition. A newly proposed gradient estimation method for stochastic
neurons, namely Gumbel-softmax, is exploited to implement the temporal boundary
detectors and the stochastic hard attention mechanism. To amealiate the
negative effect of sensitive temperature of the Gumbel-softmax, an adaptive
temperature training method is applied to better the system performance. The
experimental results demonstrate the improved effect of HM-AN over LSTM with
attention on the vision task. Through visualization of what have been learnt by
the networks, it can be observed that both the attention regions of images and
the hierarchical temporal structure can be captured by HM-AN.
</summary>
    <author>
      <name>Shiyang Yan</name>
    </author>
    <author>
      <name>Jeremy S. Smith</name>
    </author>
    <author>
      <name>Wenjin Lu</name>
    </author>
    <author>
      <name>Bailing Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1708.07590v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.07590v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2410.15909v3</id>
    <updated>2024-11-29T16: 32: 45Z</updated>
    <published>2024-10-21T11: 32: 46Z</published>
    <title>Hybrid Architecture for Real-Time Video Anomaly Detection: Integrating
  Spatial and Temporal Analysis</title>
    <summary>  In this paper, we propose a new architecture for real-time anomaly detection
in video data, inspired by human behavior combining spatial and temporal
analyses. This approach uses two distinct models: (i) for temporal analysis, a
recurrent convolutional network (CNN + RNN) is employed, associating VGG19 and
a GRU to process video sequences; (ii) regarding spatial analysis, it is
performed using YOLOv7 to analyze individual images. These two analyses can be
carried out either in parallel, with a final prediction that combines the
results of both analysis, or in series, where the spatial analysis enriches the
data before the temporal analysis. Some experimentations are been made to
compare these two architectural configurations with each other, and evaluate
the effectiveness of our hybrid approach in video anomaly detection.
</summary>
    <author>
      <name>Fabien Poirier</name>
    </author>
    <link href="http://arxiv.org/abs/2410.15909v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.15909v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2202.10001v1</id>
    <updated>2022-02-21T05: 47: 22Z</updated>
    <published>2022-02-21T05: 47: 22Z</published>
    <title>Recurrent Auto-Encoder With Multi-Resolution Ensemble and Predictive
  Coding for Multivariate Time-Series Anomaly Detection</title>
    <summary>  As large-scale time-series data can easily be found in real-world
applications, multivariate time-series anomaly detection has played an
essential role in diverse industries. It enables productivity improvement and
maintenance cost reduction by preventing malfunctions and detecting anomalies
based on time-series data. However, multivariate time-series anomaly detection
is challenging because real-world time-series data exhibit complex temporal
dependencies. For this task, it is crucial to learn a rich representation that
effectively contains the nonlinear temporal dynamics of normal behavior. In
this study, we propose an unsupervised multivariate time-series anomaly
detection model named RAE-MEPC which learns informative normal representations
based on multi-resolution ensemble and predictive coding. We introduce
multi-resolution ensemble encoding to capture the multi-scale dependency from
the input time series. The encoder hierarchically aggregates the temporal
features extracted from the sub-encoders with different encoding lengths. From
these encoded features, the reconstruction decoder reconstructs the input time
series based on multi-resolution ensemble decoding where lower-resolution
information helps to decode sub-decoders with higher-resolution outputs.
Predictive coding is further introduced to encourage the model to learn the
temporal dependencies of the time series. Experiments on real-world benchmark
datasets show that the proposed model outperforms the benchmark models for
multivariate time-series anomaly detection.
</summary>
    <author>
      <name>Heejeong Choi</name>
    </author>
    <author>
      <name>Subin Kim</name>
    </author>
    <author>
      <name>Pilsung Kang</name>
    </author>
    <link href="http://arxiv.org/abs/2202.10001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.10001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2202.13096v1</id>
    <updated>2022-02-26T09: 25: 44Z</updated>
    <published>2022-02-26T09: 25: 44Z</published>
    <title>Continuous Human Action Recognition for Human-Machine Interaction: A
  Review</title>
    <summary>  With advances in data-driven machine learning research, a wide variety of
prediction models have been proposed to capture spatio-temporal features for
the analysis of video streams. Recognising actions and detecting action
transitions within an input video are challenging but necessary tasks for
applications that require real-time human-machine interaction. By reviewing a
large body of recent related work in the literature, we thoroughly analyse,
explain and compare action segmentation methods and provide details on the
feature extraction and learning strategies that are used on most
state-of-the-art methods. We cover the impact of the performance of object
detection and tracking techniques on human action segmentation methodologies.
We investigate the application of such models to real-world scenarios and
discuss several limitations and key research directions towards improving
interpretability, generalisation, optimisation and deployment.
</summary>
    <author>
      <name>Harshala Gammulle</name>
    </author>
    <author>
      <name>David Ahmedt-Aristizabal</name>
    </author>
    <author>
      <name>Simon Denman</name>
    </author>
    <author>
      <name>Lachlan Tychsen-Smith</name>
    </author>
    <author>
      <name>Lars Petersson</name>
    </author>
    <author>
      <name>Clinton Fookes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3587931</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3587931" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint submitted to ACM Computing Surveys</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2023, Volume 55, Issue 13s</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2202.13096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.13096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2003.09087v1</id>
    <updated>2020-03-20T03: 18: 15Z</updated>
    <published>2020-03-20T03: 18: 15Z</published>
    <title>Fully Automated Hand Hygiene Monitoring\\in Operating Room using 3D
  Convolutional Neural Network</title>
    <summary>  Hand hygiene is one of the most significant factors in preventing hospital
acquired infections (HAI) which often be transmitted by medical staffs in
contact with patients in the operating room (OR). Hand hygiene monitoring could
be important to investigate and reduce the outbreak of infections within the
OR. However, an effective monitoring tool for hand hygiene compliance is
difficult to develop due to the visual complexity of the OR scene. Recent
progress in video understanding with convolutional neural net (CNN) has
increased the application of recognition and detection of human actions.
Leveraging this progress, we proposed a fully automated hand hygiene monitoring
tool of the alcohol-based hand rubbing action of anesthesiologists on OR video
using spatio-temporal features with 3D CNN. First, the region of interest (ROI)
of anesthesiologists' upper body were detected and cropped. A temporal
smoothing filter was applied to the ROIs. Then, the ROIs were given to a 3D CNN
and classified into two classes: rubbing hands or other actions. We observed
that a transfer learning from Kinetics-400 is beneficial and the optical flow
stream was not helpful in our dataset. The final accuracy, precision, recall
and F1 score in testing is 0.76,
                            0.85,
                            0.65 and 0.74, respectively.
</summary>
    <author>
      <name>Minjee Kim</name>
    </author>
    <author>
      <name>Joonmyeong Choi</name>
    </author>
    <author>
      <name>Namkug Kim</name>
    </author>
    <link href="http://arxiv.org/abs/2003.09087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.09087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2311.17241v2</id>
    <updated>2024-04-20T19: 30: 38Z</updated>
    <published>2023-11-28T21: 31: 04Z</published>
    <title>End-to-End Temporal Action Detection with 1B Parameters Across 1000
  Frames</title>
    <summary>  Recently, temporal action detection (TAD) has seen significant performance
improvement with end-to-end training. However, due to the memory bottleneck,
only models with limited scales and limited data volumes can afford end-to-end
training, which inevitably restricts TAD performance. In this paper, we reduce
the memory consumption for end-to-end training, and manage to scale up the TAD
backbone to 1 billion parameters and the input video to 1,
                            536 frames, leading
to significant detection performance. The key to our approach lies in our
proposed temporal-informative adapter (TIA), which is a novel lightweight
module that reduces training memory. Using TIA, we free the humongous backbone
from learning to adapt to the TAD task by only updating the parameters in TIA.
TIA also leads to better TAD representation by temporally aggregating context
from adjacent frames throughout the backbone. We evaluate our model across four
representative datasets. Owing to our efficient design, we are able to train
end-to-end on VideoMAEv2-giant and achieve 75.4% mAP on THUMOS14, being the
first end-to-end model to outperform the best feature-based methods. Code is
available at https: //github.com/sming256/AdaTAD.
</summary>
    <author>
      <name>Shuming Liu</name>
    </author>
    <author>
      <name>Chen-Lin Zhang</name>
    </author>
    <author>
      <name>Chen Zhao</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CVPR 2024. Camera-Ready Version</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.17241v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.17241v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2502.20361v1</id>
    <updated>2025-02-27T18: 32: 27Z</updated>
    <published>2025-02-27T18: 32: 27Z</published>
    <title>OpenTAD: A Unified Framework and Comprehensive Study of Temporal Action
  Detection</title>
    <summary>  Temporal action detection (TAD) is a fundamental video understanding task
that aims to identify human actions and localize their temporal boundaries in
videos. Although this field has achieved remarkable progress in recent years,
further progress and real-world applications are impeded by the absence of a
standardized framework. Currently, different methods are compared under
different implementation settings, evaluation protocols, etc., making it
difficult to assess the real effectiveness of a specific technique. To address
this issue, we propose \textbf{OpenTAD
                            }, a unified TAD framework consolidating
16 different TAD methods and 9 standard datasets into a modular codebase. In
OpenTAD, minimal effort is required to replace one module with a different
design, train a feature-based TAD model in end-to-end mode, or switch between
the two. OpenTAD also facilitates straightforward benchmarking across various
datasets and enables fair and in-depth comparisons among different methods.
With OpenTAD, we comprehensively study how innovations in different network
components affect detection performance and identify the most effective design
choices through extensive experiments. This study has led to a new
state-of-the-art TAD method built upon existing techniques for each component.
We have made our code and models available at
https: //github.com/sming256/OpenTAD.
</summary>
    <author>
      <name>Shuming Liu</name>
    </author>
    <author>
      <name>Chen Zhao</name>
    </author>
    <author>
      <name>Fatimah Zohra</name>
    </author>
    <author>
      <name>Mattia Soldan</name>
    </author>
    <author>
      <name>Alejandro Pardo</name>
    </author>
    <author>
      <name>Mengmeng Xu</name>
    </author>
    <author>
      <name>Lama Alssum</name>
    </author>
    <author>
      <name>Merey Ramazanova</name>
    </author>
    <author>
      <name>Juan León Alcázar</name>
    </author>
    <author>
      <name>Anthony Cioppa</name>
    </author>
    <author>
      <name>Silvio Giancola</name>
    </author>
    <author>
      <name>Carlos Hinojosa</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <link href="http://arxiv.org/abs/2502.20361v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.20361v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2004.00222v3</id>
    <updated>2020-04-11T19: 07: 05Z</updated>
    <published>2020-04-01T04: 13: 55Z</published>
    <title>Video Anomaly Detection for Smart Surveillance</title>
    <summary>  In modern intelligent video surveillance systems, automatic anomaly detection
through computer vision analytics plays a pivotal role which not only
significantly increases monitoring efficiency but also reduces the burden on
live monitoring. Anomalies in videos are broadly defined as events or
activities that are unusual and signify irregular behavior. The goal of anomaly
detection is to temporally or spatially localize the anomaly events in video
sequences. Temporal localization (i.e. indicating the start and end frames of
the anomaly event in a video) is referred to as frame-level detection. Spatial
localization, which is more challenging, means to identify the pixels within
each anomaly frame that correspond to the anomaly event. This setting is
usually referred to as pixel-level detection. In this paper, we provide a brief
overview of the recent research progress on video anomaly detection and
highlight a few future research directions.
</summary>
    <author>
      <name>Sijie Zhu</name>
    </author>
    <author>
      <name>Chen Chen</name>
    </author>
    <author>
      <name>Waqas Sultani</name>
    </author>
    <link href="http://arxiv.org/abs/2004.00222v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.00222v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2209.06828v1</id>
    <updated>2022-09-14T14: 33: 07Z</updated>
    <published>2022-09-14T14: 33: 07Z</published>
    <title>A Temporal Anomaly Detection System for Vehicles utilizing Functional
  Working Groups and Sensor Channels</title>
    <summary>  A modern vehicle fitted with sensors, actuators, and Electronic Control Units
(ECUs) can be divided into several operational subsystems called Functional
Working Groups (FWGs). Examples of these FWGs include the engine system,
transmission, fuel system, brakes, etc. Each FWG has associated sensor-channels
that gauge vehicular operating conditions. This data rich environment is
conducive to the development of Predictive Maintenance (PdM) technologies.
Undercutting various PdM technologies is the need for robust anomaly detection
models that can identify events or observations which deviate significantly
from the majority of the data and do not conform to a well defined notion of
normal vehicular operational behavior. In this paper, we introduce the Vehicle
Performance, Reliability, and Operations (VePRO) dataset and use it to create a
multi-phased approach to anomaly detection. Utilizing Temporal Convolution
Networks (TCN), our anomaly detection system can achieve 96% detection accuracy
and accurately predicts 91% of true anomalies. The performance of our anomaly
detection system improves when sensor channels from multiple FWGs are utilized.
</summary>
    <author>
      <name>Subash Neupane</name>
    </author>
    <author>
      <name>Ivan A. Fernandez</name>
    </author>
    <author>
      <name>Wilson Patterson</name>
    </author>
    <author>
      <name>Sudip Mittal</name>
    </author>
    <author>
      <name>Shahram Rahimi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Collaboration and Internet
  Computing 2022 (IEEE CIC 2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2209.06828v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.06828v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2308.15624v1</id>
    <updated>2023-08-29T20: 45: 41Z</updated>
    <published>2023-08-29T20: 45: 41Z</published>
    <title>Detection of Mild Cognitive Impairment Using Facial Features in Video
  Conversations</title>
    <summary>  Early detection of Mild Cognitive Impairment (MCI) leads to early
interventions to slow the progression from MCI into dementia. Deep Learning
(DL) algorithms could help achieve early non-invasive, low-cost detection of
MCI. This paper presents the detection of MCI in older adults using DL models
based only on facial features extracted from video-recorded conversations at
home. We used the data collected from the I-CONECT behavioral intervention
study (NCT02871921), where several sessions of semi-structured interviews
between socially isolated older individuals and interviewers were video
recorded. We develop a framework that extracts spatial holistic facial features
using a convolutional autoencoder and temporal information using transformers.
Our proposed DL model was able to detect the I-CONECT study participants'
cognitive conditions (MCI vs. those with normal cognition (NC)) using facial
features. The segments and sequence information of the facial features improved
the prediction performance compared with the non-temporal features. The
detection accuracy using this combined method reached 88% whereas 84% is the
accuracy without applying the segments and sequences information of the facial
features within a video on a certain theme.
</summary>
    <author>
      <name>Muath Alsuhaibani</name>
    </author>
    <author>
      <name>Hiroko H. Dodge</name>
    </author>
    <author>
      <name>Mohammad H. Mahoor</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.eswa.2024.124185</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.eswa.2024.124185" rel="related"/>
    <link href="http://arxiv.org/abs/2308.15624v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.15624v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2209.02899v1</id>
    <updated>2022-09-07T03: 12: 02Z</updated>
    <published>2022-09-07T03: 12: 02Z</published>
    <title>Context Recovery and Knowledge Retrieval: A Novel Two-Stream Framework
  for Video Anomaly Detection</title>
    <summary>  Video anomaly detection aims to find the events in a video that do not
conform to the expected behavior. The prevalent methods mainly detect anomalies
by snippet reconstruction or future frame prediction error. However, the error
is highly dependent on the local context of the current snippet and lacks the
understanding of normality. To address this issue, we propose to detect
anomalous events not only by the local context, but also according to the
consistency between the testing event and the knowledge about normality from
the training data. Concretely, we propose a novel two-stream framework based on
context recovery and knowledge retrieval, where the two streams can complement
each other. For the context recovery stream, we propose a spatiotemporal U-Net
which can fully utilize the motion information to predict the future frame.
Furthermore, we propose a maximum local error mechanism to alleviate the
problem of large recovery errors caused by complex foreground objects. For the
knowledge retrieval stream, we propose an improved learnable locality-sensitive
hashing, which optimizes hash functions via a Siamese network and a mutual
difference loss. The knowledge about normality is encoded and stored in hash
tables, and the distance between the testing event and the knowledge
representation is used to reveal the probability of anomaly. Finally, we fuse
the anomaly scores from the two streams to detect anomalies. Extensive
experiments demonstrate the effectiveness and complementarity of the two
streams, whereby the proposed two-stream framework achieves state-of-the-art
performance on four datasets.
</summary>
    <author>
      <name>Congqi Cao</name>
    </author>
    <author>
      <name>Yue Lu</name>
    </author>
    <author>
      <name>Yanning Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages,
                            10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.02899v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.02899v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2111.06783v1</id>
    <updated>2021-11-12T15: 49: 34Z</updated>
    <published>2021-11-12T15: 49: 34Z</published>
    <title>Can neural networks predict dynamics they have never seen?</title>
    <summary>  Neural networks have proven to be remarkably successful for a wide range of
complicated tasks, from image recognition and object detection to speech
recognition and machine translation. One of their successes is the skill in
prediction of future dynamics given a suitable training set of data. Previous
studies have shown how Echo State Networks (ESNs), a subset of Recurrent Neural
Networks, can successfully predict even chaotic systems for times longer than
the Lyapunov time. This study shows that, remarkably, ESNs can successfully
predict dynamical behavior that is qualitatively different from any behavior
contained in the training set. Evidence is provided for a fluid dynamics
problem where the flow can transition between laminar (ordered) and turbulent
(disordered) regimes. Despite being trained on the turbulent regime only, ESNs
are found to predict laminar behavior. Moreover, the statistics of
turbulent-to-laminar and laminar-to-turbulent transitions are also predicted
successfully, and the utility of ESNs in acting as an early-warning system for
transition is discussed. These results are expected to be widely applicable to
data-driven modelling of temporal behaviour in a range of physical, climate,
biological, ecological and finance models characterized by the presence of
tipping points and sudden transitions between several competing states.
</summary>
    <author>
      <name>Anton Pershin</name>
    </author>
    <author>
      <name>Cedric Beaume</name>
    </author>
    <author>
      <name>Kuan Li</name>
    </author>
    <author>
      <name>Steven M. Tobias</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages,
                            5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.06783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.06783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2108.08996v1</id>
    <updated>2021-08-20T04: 31: 07Z</updated>
    <published>2021-08-20T04: 31: 07Z</published>
    <title>Weakly-supervised Joint Anomaly Detection and Classification</title>
    <summary>  Anomaly activities such as robbery, explosion, accidents, etc. need immediate
actions for preventing loss of human life and property in real world
surveillance systems. Although the recent automation in surveillance systems
are capable of detecting the anomalies, but they still need human efforts for
categorizing the anomalies and taking necessary preventive actions. This is due
to the lack of methodology performing both anomaly detection and classification
for real world scenarios. Thinking of a fully automatized surveillance system,
which is capable of both detecting and classifying the anomalies that need
immediate actions, a joint anomaly detection and classification method is a
pressing need. The task of joint detection and classification of anomalies
becomes challenging due to the unavailability of dense annotated videos
pertaining to anomalous classes, which is a crucial factor for training modern
deep architecture. Furthermore, doing it through manual human effort seems
impossible. Thus, we propose a method that jointly handles the anomaly
detection and classification in a single framework by adopting a
weakly-supervised learning paradigm. In weakly-supervised learning instead of
dense temporal annotations, only video-level labels are sufficient for
learning. The proposed model is validated on a large-scale publicly available
UCF-Crime dataset, achieving state-of-the-art results.
</summary>
    <author>
      <name>Snehashis Majhi</name>
    </author>
    <author>
      <name>Srijan Das</name>
    </author>
    <author>
      <name>Francois Bremond</name>
    </author>
    <author>
      <name>Ratnakar Dash</name>
    </author>
    <author>
      <name>Pankaj Kumar Sa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Provisionally accepted in the first round of FG 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.08996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.08996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1512.04115v1</id>
    <updated>2015-12-13T20: 08: 43Z</updated>
    <published>2015-12-13T20: 08: 43Z</published>
    <title>Unsupervised Temporal Segmentation of Repetitive Human Actions Based on
  Kinematic Modeling and Frequency Analysis</title>
    <summary>  In this paper, we propose a method for temporal segmentation of human
repetitive actions based on frequency analysis of kinematic parameters,
zero-velocity crossing detection, and adaptive k-means clustering. Since the
human motion data may be captured with different modalities which have
different temporal sampling rate and accuracy (e.g., optical motion capture
systems vs. Microsoft Kinect), we first apply a generic full-body kinematic
model with an unscented Kalman filter to convert the motion data into a unified
representation that is robust to noise. Furthermore, we extract the most
representative kinematic parameters via the primary frequency analysis. The
sequences are segmented based on zero-velocity crossing of the selected
parameters followed by an adaptive k-means clustering to identify the
repetition segments. Experimental results demonstrate that for the motion data
captured by both the motion capture system and the Microsoft Kinect, our
proposed algorithm obtains robust segmentation of repetitive action sequences.
</summary>
    <author>
      <name>Qifei Wang</name>
    </author>
    <author>
      <name>Gregorij Kurillo</name>
    </author>
    <author>
      <name>Ferda Ofli</name>
    </author>
    <author>
      <name>Ruzena Bajcsy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, International Conference on 3D Vision 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.04115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.04115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2202.04947v3</id>
    <updated>2022-10-26T13: 24: 39Z</updated>
    <published>2022-02-10T10: 50: 52Z</published>
    <title>OWL (Observe, Watch, Listen): Audiovisual Temporal Context for
  Localizing Actions in Egocentric Videos</title>
    <summary>  Egocentric videos capture sequences of human activities from a first-person
perspective and can provide rich multimodal signals. However, most current
localization methods use third-person videos and only incorporate visual
information. In this work, we take a deep look into the effectiveness of
audiovisual context in detecting actions in egocentric videos and introduce a
simple-yet-effective approach via Observing, Watching, and Listening (OWL). OWL
leverages audiovisual information and context for egocentric temporal action
localization (TAL). We validate our approach in two large-scale datasets,
EPIC-Kitchens, and HOMAGE. Extensive experiments demonstrate the relevance of
the audiovisual temporal context. Namely, we boost the localization performance
(mAP) over visual-only models by +2.23% and +3.35% in the above datasets.
</summary>
    <author>
      <name>Merey Ramazanova</name>
    </author>
    <author>
      <name>Victor Escorcia</name>
    </author>
    <author>
      <name>Fabian Caba Heilbron</name>
    </author>
    <author>
      <name>Chen Zhao</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <link href="http://arxiv.org/abs/2202.04947v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.04947v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2407.11468v1</id>
    <updated>2024-07-16T08: 07: 47Z</updated>
    <published>2024-07-16T08: 07: 47Z</published>
    <title>AU-vMAE: Knowledge-Guide Action Units Detection via Video Masked
  Autoencoder</title>
    <summary>  Current Facial Action Unit (FAU) detection methods generally encounter
difficulties due to the scarcity of labeled video training data and the limited
number of training face IDs, which renders the trained feature extractor
insufficient coverage for modeling the large diversity of inter-person facial
structures and movements. To explicitly address the above challenges, we
propose a novel video-level pre-training scheme by fully exploring the
multi-label property of FAUs in the video as well as the temporal label
consistency. At the heart of our design is a pre-trained video feature
extractor based on the video-masked autoencoder together with a fine-tuning
network that jointly completes the multi-level video FAUs analysis tasks,
\emph{i.e.
                            } integrating both video-level and frame-level FAU detections, thus
dramatically expanding the supervision set from sparse FAUs annotations to ALL
video frames including masked ones. Moreover, we utilize inter-frame and
intra-frame AU pair state matrices as prior knowledge to guide network training
instead of traditional Graph Neural Networks, for better temporal supervision.
Our approach demonstrates substantial enhancement in performance compared to
the existing state-of-the-art methods used in BP4D and DISFA FAUs datasets.
</summary>
    <author>
      <name>Qiaoqiao Jin</name>
    </author>
    <author>
      <name>Rui Shi</name>
    </author>
    <author>
      <name>Yishun Dou</name>
    </author>
    <author>
      <name>Bingbing Ni</name>
    </author>
    <link href="http://arxiv.org/abs/2407.11468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.11468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2310.01904v1</id>
    <updated>2023-10-03T09: 22: 06Z</updated>
    <published>2023-10-03T09: 22: 06Z</published>
    <title>Beyond the Benchmark: Detecting Diverse Anomalies in Videos</title>
    <summary>  Video Anomaly Detection (VAD) plays a crucial role in modern surveillance
systems, aiming to identify various anomalies in real-world situations.
However, current benchmark datasets predominantly emphasize simple,
single-frame anomalies such as novel object detection. This narrow focus
restricts the advancement of VAD models. In this research, we advocate for an
expansion of VAD investigations to encompass intricate anomalies that extend
beyond conventional benchmark boundaries. To facilitate this, we introduce two
datasets, HMDB-AD and HMDB-Violence, to challenge models with diverse
action-based anomalies. These datasets are derived from the HMDB51 action
recognition dataset. We further present Multi-Frame Anomaly Detection (MFAD), a
novel method built upon the AI-VAD framework. AI-VAD utilizes single-frame
features such as pose estimation and deep image encoding, and two-frame
features such as object velocity. They then apply a density estimation
algorithm to compute anomaly scores. To address complex multi-frame anomalies,
we add a deep video encoding features capturing long-range temporal
dependencies, and logistic regression to enhance final score calculation.
Experimental results confirm our assumptions, highlighting existing models
limitations with new anomaly types. MFAD excels in both simple and complex
anomaly detection scenarios.
</summary>
    <author>
      <name>Yoav Arad</name>
    </author>
    <author>
      <name>Michael Werman</name>
    </author>
    <link href="http://arxiv.org/abs/2310.01904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.01904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1502.06235v1</id>
    <updated>2015-02-22T14: 54: 02Z</updated>
    <published>2015-02-22T14: 54: 02Z</published>
    <title>Spatio-temporal Video Parsing for Abnormality Detection</title>
    <summary>  Abnormality detection in video poses particular challenges due to the
infinite size of the class of all irregular objects and behaviors. Thus no (or
by far not enough) abnormal training samples are available and we need to find
abnormalities in test data without actually knowing what they are.
Nevertheless, the prevailing concept of the field is to directly search for
individual abnormal local patches or image regions independent of another. To
address this problem, we propose a method for joint detection of abnormalities
in videos by spatio-temporal video parsing. The goal of video parsing is to
find a set of indispensable normal spatio-temporal object hypotheses that
jointly explain all the foreground of a video, while, at the same time, being
supported by normal training samples. Consequently, we avoid a direct detection
of abnormalities and discover them indirectly as those hypotheses which are
needed for covering the foreground without finding an explanation for
themselves by normal samples. Abnormalities are localized by MAP inference in a
graphical model and we solve it efficiently by formulating it as a convex
optimization problem. We experimentally evaluate our approach on several
challenging benchmark sets, improving over the state-of-the-art on all standard
benchmarks both in terms of abnormality classification and localization.
</summary>
    <author>
      <name>Borislav Antić</name>
    </author>
    <author>
      <name>Björn Ommer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages,
                            12 figures,
                            3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.06235v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.06235v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1806.01018v2</id>
    <updated>2018-11-27T03: 07: 37Z</updated>
    <published>2018-06-04T09: 05: 24Z</published>
    <title>A 2.5D Cascaded Convolutional Neural Network with Temporal Information
  for Automatic Mitotic Cell Detection in 4D Microscopic Images</title>
    <summary>  In recent years, intravital skin imaging has been increasingly used in
mammalian skin research to investigate cell behaviors. A fundamental step of
the investigation is mitotic cell (cell division) detection. Because of the
complex backgrounds (normal cells), the majority of the existing methods cause
several false positives. In this paper, we proposed a 2.5D cascaded end-to-end
convolutional neural network (CasDetNet) with temporal information to
accurately detect automatic mitotic cell in 4D microscopic images with few
training data. The CasDetNet consists of two 2.5D networks. The first one is
used for detecting candidate cells with only volume information and the second
one, containing temporal information, for reducing false positive and adding
mitotic cells that were missed in the first step. The experimental results show
that our CasDetNet can achieve higher precision and recall compared to other
state-of-the-art methods.
</summary>
    <author>
      <name>Titinunt Kitrungrotsakul</name>
    </author>
    <author>
      <name>Xian-Hau Han</name>
    </author>
    <author>
      <name>Yutaro Iwamoto</name>
    </author>
    <author>
      <name>Satoko Takemoto</name>
    </author>
    <author>
      <name>Hideo Yokota</name>
    </author>
    <author>
      <name>Sari Ipponjima</name>
    </author>
    <author>
      <name>Tomomi Nemoto</name>
    </author>
    <author>
      <name>Xiong Wei</name>
    </author>
    <author>
      <name>Yen-Wei Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages,
                            4 figures, conference paper (submitted to arxiv then update
  version and submitted to conference. Finally update in arxiv for newest
  version)</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01018v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01018v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.07653v1</id>
    <updated>2025-03-06T23: 08: 08Z</updated>
    <published>2025-03-06T23: 08: 08Z</published>
    <title>Early Detection of Mental Health Issues Using Social Media Posts</title>
    <summary>  The increasing prevalence of mental health disorders, such as depression,
anxiety, and bipolar disorder, calls for immediate need in developing tools for
early detection and intervention. Social media platforms, like Reddit,
represent a rich source of user-generated content, reflecting emotional and
behavioral patterns. In this work, we propose a multi-modal deep learning
framework that integrates linguistic and temporal features for early detection
of mental health crises. Our approach is based on the method that utilizes a
BiLSTM network both for text and temporal feature analysis, modeling sequential
dependencies in a different manner, capturing contextual patterns quite well.
This work includes a cross-modal attention approach that allows fusion of such
outputs into context-aware classification of mental health conditions. The
model was then trained and evaluated on a dataset of labeled Reddit posts
preprocessed using text preprocessing, scaling of temporal features, and
encoding of labels. Experimental results indicate that the proposed
architecture performs better compared to traditional models with a validation
accuracy of 74.55% and F1-Score of 0.7376. This study presents the importance
of multi-modal learning for mental health detection and provides a baseline for
further improvements by using more advanced attention mechanisms and other data
modalities.
</summary>
    <author>
      <name>Qasim Bin Saeed</name>
    </author>
    <author>
      <name>Ijaz Ahmed</name>
    </author>
    <link href="http://arxiv.org/abs/2503.07653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.07653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1811.04549v1</id>
    <updated>2018-11-12T04: 09: 21Z</updated>
    <published>2018-11-12T04: 09: 21Z</published>
    <title>Temporal Stable Community in Time-Varying Networks</title>
    <summary>  Identifying community structure of a complex network provides insight to the
interdependence between the network topology and emergent collective behaviors
of networks, while detecting such invariant communities in a time-varying
network is more challenging. In this paper, we define the temporal stable
community and newly propose the concept of dynamic modularity to evaluate the
stable community structures in time-varying networks, which is robust against
small changes as verified by several empirical time-varying network datasets.
Besides, using the volatility features of temporal stable communities in
functional brain networks, we successfully differentiate the ADHD (Attention
Deficit Hyperactivity Disorder) patients and healthy controls efficiently.
</summary>
    <author>
      <name>Wenjing Wang</name>
    </author>
    <author>
      <name>Xiang Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,
                            5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.04549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.04549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1907.10211v1</id>
    <updated>2019-07-24T02: 36: 28Z</updated>
    <published>2019-07-24T02: 36: 28Z</published>
    <title>Motion-Aware Feature for Improved Video Anomaly Detection</title>
    <summary>  Motivated by our observation that motion information is the key to good
anomaly detection performance in video, we propose a temporal augmented network
to learn a motion-aware feature. This feature alone can achieve competitive
performance with previous state-of-the-art methods, and when combined with
them, can achieve significant performance improvements. Furthermore, we
incorporate temporal context into the Multiple Instance Learning (MIL) ranking
model by using an attention block. The learned attention weights can help to
differentiate between anomalous and normal video segments better. With the
proposed motion-aware feature and the temporal MIL ranking model, we outperform
previous approaches by a large margin on both anomaly detection and anomalous
action recognition tasks in the UCF Crime dataset.
</summary>
    <author>
      <name>Yi Zhu</name>
    </author>
    <author>
      <name>Shawn Newsam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BMVC 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.10211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.10211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/astro-ph/0403144v1</id>
    <updated>2004-03-05T10: 27: 27Z</updated>
    <published>2004-03-05T10: 27: 27Z</published>
    <title>The chaotic behavior of the black hole system GRS 1915+105</title>
    <summary>  A modified non-linear time series analysis technique, which computes the
correlation dimension $D_2$, is used to analyze the X-ray light curves of the
black hole system GRS 1915+105 in all twelve temporal classes. For four of
these temporal classes $D_2 $ saturates to $\approx 4-5$ which indicates that
the underlying dynamical mechanism is a low dimensional chaotic system. Of the
other eight classes, three show stochastic behavior while five show deviation
from randomness. The light curves for four classes which depict chaotic
behavior have the smallest ratio of the expected Poisson noise to the
variability ($ &lt; 0.05$) while those for the three classes which depict
stochastic behavior is the highest ($ &gt; 0.2$). This suggests that the temporal
behavior of the black hole system is governed by a low dimensional chaotic
system, whose nature is detectable only when the Poisson fluctuations are much
smaller than the variability.
</summary>
    <author>
      <name>R. Misra</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IUCAA</arxiv:affiliation>
    </author>
    <author>
      <name>K. P. Harikrishnan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The Cochin College</arxiv:affiliation>
    </author>
    <author>
      <name>B. Mukhopadhyay</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Oulu</arxiv:affiliation>
    </author>
    <author>
      <name>G. Ambika</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Maharajas College</arxiv:affiliation>
    </author>
    <author>
      <name>A. K. Kembhavi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IUCAA</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1086/421005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1086/421005" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Astrophysical Journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Astrophys.J. 609 (2004) 313-316</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/astro-ph/0403144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/0403144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2004.10299v1</id>
    <updated>2020-04-21T21: 11: 30Z</updated>
    <published>2020-04-21T21: 11: 30Z</published>
    <title>Group Activity Detection from Trajectory and Video Data in Soccer</title>
    <summary>  Group activity detection in soccer can be done by using either video data or
player and ball trajectory data. In current soccer activity datasets,
activities are labelled as atomic events without a duration. Given that the
state-of-the-art activity detection methods are not well-defined for atomic
actions, these methods cannot be used. In this work, we evaluated the
effectiveness of activity recognition models for detecting such events, by
using an intuitive non-maximum suppression process and evaluation metrics. We
also considered the problem of explicitly modeling interactions between players
and ball. For this, we propose self-attention models to learn and extract
relevant information from a group of soccer players for activity detection from
both trajectory and video data. We conducted an extensive study on the use of
visual features and trajectory data for group activity detection in sports
using a large scale soccer dataset provided by Sportlogiq. Our results show
that most events can be detected using either vision or trajectory-based
approaches with a temporal resolution of less than 0.5 seconds, and that each
approach has unique challenges.
</summary>
    <author>
      <name>Ryan Sanford</name>
    </author>
    <author>
      <name>Siavash Gorji</name>
    </author>
    <author>
      <name>Luiz G. Hafemann</name>
    </author>
    <author>
      <name>Bahareh Pourbabaee</name>
    </author>
    <author>
      <name>Mehrsan Javan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the 6th International Workshop on Computer Vision in
  Sports (CVsports) at CVPR 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.10299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.10299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2208.01954v1</id>
    <updated>2022-08-03T10: 00: 49Z</updated>
    <published>2022-08-03T10: 00: 49Z</published>
    <title>Dilated Context Integrated Network with Cross-Modal Consensus for
  Temporal Emotion Localization in Videos</title>
    <summary>  Understanding human emotions is a crucial ability for intelligent robots to
provide better human-robot interactions. The existing works are limited to
trimmed video-level emotion classification, failing to locate the temporal
window corresponding to the emotion. In this paper, we introduce a new task,
named Temporal Emotion Localization in videos~(TEL), which aims to detect human
emotions and localize their corresponding temporal boundaries in untrimmed
videos with aligned subtitles. TEL presents three unique challenges compared to
temporal action localization: 1) The emotions have extremely varied temporal
dynamics; 2) The emotion cues are embedded in both appearances and complex
plots; 3) The fine-grained temporal annotations are complicated and
labor-intensive. To address the first two challenges, we propose a novel
dilated context integrated network with a coarse-fine two-stream architecture.
The coarse stream captures varied temporal dynamics by modeling
multi-granularity temporal contexts. The fine stream achieves complex plots
understanding by reasoning the dependency between the multi-granularity
temporal contexts from the coarse stream and adaptively integrates them into
fine-grained video segment features. To address the third challenge, we
introduce a cross-modal consensus learning paradigm, which leverages the
inherent semantic consensus between the aligned video and subtitle to achieve
weakly-supervised learning. We contribute a new testing set with 3,
                            000
manually-annotated temporal boundaries so that future research on the TEL
problem can be quantitatively evaluated. Extensive experiments show the
effectiveness of our approach on temporal emotion localization. The repository
of this work is at
https: //github.com/YYJMJC/Temporal-Emotion-Localization-in-Videos.
</summary>
    <author>
      <name>Juncheng Li</name>
    </author>
    <author>
      <name>Junlin Xie</name>
    </author>
    <author>
      <name>Linchao Zhu</name>
    </author>
    <author>
      <name>Long Qian</name>
    </author>
    <author>
      <name>Siliang Tang</name>
    </author>
    <author>
      <name>Wenqiao Zhang</name>
    </author>
    <author>
      <name>Haochen Shi</name>
    </author>
    <author>
      <name>Shengyu Zhang</name>
    </author>
    <author>
      <name>Longhui Wei</name>
    </author>
    <author>
      <name>Qi Tian</name>
    </author>
    <author>
      <name>Yueting Zhuang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACM Multimedia 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.01954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.01954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1906.02549v1</id>
    <updated>2019-06-06T12: 32: 55Z</updated>
    <published>2019-06-06T12: 32: 55Z</published>
    <title>Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video</title>
    <summary>  In this paper, we address a novel task, namely weakly-supervised
spatio-temporally grounding natural sentence in video. Specifically, given a
natural sentence and a video, we localize a spatio-temporal tube in the video
that semantically corresponds to the given sentence, with no reliance on any
spatio-temporal annotations during training. First, a set of spatio-temporal
tubes, referred to as instances, are extracted from the video. We then encode
these instances and the sentence using our proposed attentive interactor which
can exploit their fine-grained relationships to characterize their matching
behaviors. Besides a ranking loss, a novel diversity loss is introduced to
train the proposed attentive interactor to strengthen the matching behaviors of
reliable instance-sentence pairs and penalize the unreliable ones. Moreover, we
also contribute a dataset, called VID-sentence, based on the ImageNet video
object detection dataset, to serve as a benchmark for our task. Extensive
experimental results demonstrate the superiority of our model over the baseline
approaches.
</summary>
    <author>
      <name>Zhenfang Chen</name>
    </author>
    <author>
      <name>Lin Ma</name>
    </author>
    <author>
      <name>Wenhan Luo</name>
    </author>
    <author>
      <name>Kwan-Yee K. Wong</name>
    </author>
    <link href="http://arxiv.org/abs/1906.02549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.02549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2205.04685v1</id>
    <updated>2022-05-10T05: 40: 17Z</updated>
    <published>2022-05-10T05: 40: 17Z</published>
    <title>DNS based In-Browser Cryptojacking Detection</title>
    <summary>  The metadata aspect of Domain Names (DNs) enables us to perform a behavioral
study of DNs and detect if a DN is involved in in-browser cryptojacking. Thus,
we are motivated to study different temporal and behavioral aspects of DNs
involved in cryptojacking. We use temporal features such as query frequency and
query burst along with graph-based features such as degree and diameter, and
non-temporal features such as the string-based to detect if a DNs is suspect to
be involved in the in-browser cryptojacking. Then, we use them to train the
Machine Learning (ML) algorithms over different temporal granularities such as
2 hours datasets and complete dataset. Our results show DecisionTrees
classifier performs the best with 59.5% Recall on cryptojacked DN, while for
unsupervised learning, K-Means with K=2 perform the best. Similarity analysis
of the features reveals a minimal divergence between the cryptojacking DNs and
other already known malicious DNs. It also reveals the need for improvements in
the feature set of state-of-the-art methods to improve their accuracy in
detecting in-browser cryptojacking. As added analysis, our signature-based
analysis identifies that none-of-the Indian Government websites were involved
in cryptojacking during October-December 2021. However, based on the resource
utilization, we identify 10 DNs with different properties than others.
</summary>
    <author>
      <name>Rohit Kumar Sachan</name>
    </author>
    <author>
      <name>Rachit Agarwal</name>
    </author>
    <author>
      <name>Sandeep Kumar Shukla</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.04685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.04685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2103.15708v2</id>
    <updated>2022-06-28T10: 54: 13Z</updated>
    <published>2021-03-29T15: 45: 38Z</published>
    <title>Dynamically Modelling Heterogeneous Higher-Order Interactions for
  Malicious Behavior Detection in Event Logs</title>
    <summary>  Anomaly detection in event logs is a promising approach for intrusion
detection in enterprise networks. By building a statistical model of usual
activity, it aims to detect multiple kinds of malicious behavior, including
stealthy tactics, techniques and procedures (TTPs) designed to evade
signature-based detection systems. However, finding suitable anomaly detection
methods for event logs remains an important challenge. This results from the
very complex, multi-faceted nature of the data: event logs are not only
combinatorial, but also temporal and heterogeneous data, thus they fit poorly
in most theoretical frameworks for anomaly detection. Most previous research
focuses on either one of these three aspects, building a simplified
representation of the data that can be fed to standard anomaly detection
algorithms. In contrast, we propose to simultaneously address all three of
these characteristics through a specifically tailored statistical model. We
introduce \textsc{Decades
                            }, a \underline{d
                            }ynamic, h\underline{e
                            }terogeneous
and \underline{c
                            }ombinatorial model for \underline{a
                            }nomaly
\underline{d
                            }etection in \underline{e
                            }vent \underline{s
                            }treams, and we
demonstrate its effectiveness at detecting malicious behavior through
experiments on a real dataset containing labelled red team activity. In
particular, we empirically highlight the importance of handling the multiple
characteristics of the data by comparing our model with state-of-the-art
baselines relying on various data representations.
</summary>
    <author>
      <name>Corentin Larroche</name>
    </author>
    <author>
      <name>Johan Mazel</name>
    </author>
    <author>
      <name>Stephan Clémençon</name>
    </author>
    <link href="http://arxiv.org/abs/2103.15708v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.15708v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/nlin/0508001v1</id>
    <updated>2005-07-29T20: 24: 09Z</updated>
    <published>2005-07-29T20: 24: 09Z</published>
    <title>Automatic Filters for the Detection of Coherent Structure in
  Spatiotemporal Systems</title>
    <summary>  Most current methods for identifying coherent structures in
spatially-extended systems rely on prior information about the form which those
structures take. Here we present two new approaches to automatically filter the
changing configurations of spatial dynamical systems and extract coherent
structures. One, local sensitivity filtering, is a modification of the local
Lyapunov exponent approach suitable to cellular automata and other discrete
spatial systems. The other, local statistical complexity filtering, calculates
the amount of information needed for optimal prediction of the system's
behavior in the vicinity of a given point. By examining the changing
spatiotemporal distributions of these quantities, we can find the coherent
structures in a variety of pattern-forming cellular automata, without needing
to guess or postulate the form of that structure. We apply both filters to
elementary and cyclical cellular automata (ECA and CCA) and find that they
readily identify particles, domains and other more complicated structures. We
compare the results from ECA with earlier ones based upon the theory of formal
languages, and the results from CCA with a more traditional approach based on
an order parameter and free energy. While sensitivity and statistical
complexity are equally adept at uncovering structure, they are based on
different system properties (dynamical and probabilistic, respectively), and
provide complementary information.
</summary>
    <author>
      <name>Cosma Rohilla Shalizi</name>
    </author>
    <author>
      <name>Robert Haslinger</name>
    </author>
    <author>
      <name>Jean-Baptiste Rouquier</name>
    </author>
    <author>
      <name>Kristina Lisa Klinkner</name>
    </author>
    <author>
      <name>Cristopher Moore</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.73.036104</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.73.036104" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages,
                            21 figures. Figures considerably compressed to fit arxiv
  requirements; write first author for higher-resolution versions</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physical Review E 73 (2006): 036104</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/nlin/0508001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0508001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2003.01351v1</id>
    <updated>2020-03-03T06: 09: 15Z</updated>
    <published>2020-03-03T06: 09: 15Z</published>
    <title>DETECT: Deep Trajectory Clustering for Mobility-Behavior Analysis</title>
    <summary>  Identifying mobility behaviors in rich trajectory data is of great economic
and social interest to various applications including urban planning, marketing
and intelligence. Existing work on trajectory clustering often relies on
similarity measurements that utilize raw spatial and/or temporal information of
trajectories. These measures are incapable of identifying similar moving
behaviors that exhibit varying spatio-temporal scales of movement. In addition,
the expense of labeling massive trajectory data is a barrier to supervised
learning models. To address these challenges, we propose an unsupervised neural
approach for mobility behavior clustering, called the Deep Embedded TrajEctory
ClusTering network (DETECT). DETECT operates in three parts: first it
transforms the trajectories by summarizing their critical parts and augmenting
them with context derived from their geographical locality (e.g., using POIs
from gazetteers). In the second part, it learns a powerful representation of
trajectories in the latent space of behaviors, thus enabling a clustering
function (such as $k$-means) to be applied. Finally, a clustering oriented loss
is directly built on the embedded features to jointly perform feature
refinement and cluster assignment, thus improving separability between mobility
behaviors. Exhaustive quantitative and qualitative experiments on two
real-world datasets demonstrate the effectiveness of our approach for mobility
behavior analyses.
</summary>
    <author>
      <name>Mingxuan Yue</name>
    </author>
    <author>
      <name>Yaguang Li</name>
    </author>
    <author>
      <name>Haoze Yang</name>
    </author>
    <author>
      <name>Ritesh Ahuja</name>
    </author>
    <author>
      <name>Yao-Yi Chiang</name>
    </author>
    <author>
      <name>Cyrus Shahabi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at BigData 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.01351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.01351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1909.09946v1</id>
    <updated>2019-09-22T06: 22: 36Z</updated>
    <published>2019-09-22T06: 22: 36Z</published>
    <title>Semi-supervised estimation of event temporal length for cell event
  detection</title>
    <summary>  Cell event detection in cell videos is essential for monitoring of cellular
behavior over extended time periods. Deep learning methods have shown great
success in the detection of cell events for their ability to capture more
discriminative features of cellular processes compared to traditional methods.
In particular, convolutional long short-term memory (LSTM) models, which
exploits the changes in cell events observable in video sequences, is the
state-of-the-art for mitosis detection in cell videos. However, their
limitations are the determination of the input sequence length, which is often
performed empirically, and the need for a large annotated training dataset
which is expensive to prepare. We propose a novel semi-supervised method of
optimal length detection for mitosis detection with two key contributions: (i)
an unsupervised step for learning the spatial and temporal locations of cells
in their normal stage and approximating the distribution of temporal lengths of
cell events and, (ii) a step of inferring, from that distribution, an optimal
input sequence length and a minimal number of annotated frames for training a
LSTM model for each particular video. We evaluated our method in detecting
mitosis in densely packed stem cells in a phase-contrast microscopy videos. Our
experimental data prove that increasing the input sequence length of LSTM can
lead to a decrease in performance. Our results also show that by approximating
the optimal input sequence length of the tested video, a model trained with
only 18 annotated frames achieved F1-scores of 0.880-0.907, which are 10%
higher than those of other published methods with a full set of 110 training
annotated frames.
</summary>
    <author>
      <name>Ha Tran Hong Phan</name>
    </author>
    <author>
      <name>Ashnil Kumar</name>
    </author>
    <author>
      <name>David Feng</name>
    </author>
    <author>
      <name>Michael Fulham</name>
    </author>
    <author>
      <name>Jinman Kim</name>
    </author>
    <link href="http://arxiv.org/abs/1909.09946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.09946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1403.7432v2</id>
    <updated>2014-04-04T15: 47: 05Z</updated>
    <published>2014-03-28T16: 16: 50Z</published>
    <title>Measuring Temporal Photon Bunching in Blackbody Radiation</title>
    <summary>  Light from thermal black body radiators such as stars exhibits photon
bunching behaviour at sufficiently short timescales. However, with available
detector bandwidths, this bunching signal is difficult to be directly used for
intensity interferometry with sufficient statistics in astronomy. Here we
present an experimental technique to increase the photon bunching signal in
blackbody radiation via spectral filtering of the light source. Our
measurements reveal strong temporal photon bunching in light from blackbody
radiation, including the Sun. Such filtering techniques may revive the interest
in intensity interferometry as a tool in astronomy.
</summary>
    <author>
      <name>Peng Kian Tan</name>
    </author>
    <author>
      <name>Guang Hui Yeo</name>
    </author>
    <author>
      <name>Hou Shun Poh</name>
    </author>
    <author>
      <name>Aik Hui Chan</name>
    </author>
    <author>
      <name>Christian Kurtsiefer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/2041-8205/789/1/L10</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/2041-8205/789/1/L10" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages,
                            5 figures (minor bugfixes, reference updates)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Astrophys. J. Lett. 789, L10 (2014)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1403.7432v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.7432v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2402.06107v1</id>
    <updated>2024-02-09T00: 01: 42Z</updated>
    <published>2024-02-09T00: 01: 42Z</published>
    <title>Multiple Instance Learning for Cheating Detection and Localization in
  Online Examinations</title>
    <summary>  The spread of the Coronavirus disease-2019 epidemic has caused many courses
and exams to be conducted online. The cheating behavior detection model in
examination invigilation systems plays a pivotal role in guaranteeing the
equality of long-distance examinations. However, cheating behavior is rare, and
most researchers do not comprehensively take into account features such as head
posture, gaze angle, body posture, and background information in the task of
cheating behavior detection. In this paper, we develop and present CHEESE, a
CHEating detection framework via multiplE inStancE learning. The framework
consists of a label generator that implements weak supervision and a feature
encoder to learn discriminative features. In addition, the framework combines
body posture and background features extracted by 3D convolution with eye gaze,
head posture and facial features captured by OpenFace 2.0. These features are
fed into the spatio-temporal graph module by stitching to analyze the
spatio-temporal changes in video clips to detect the cheating behaviors. Our
experiments on three datasets, UCF-Crime, ShanghaiTech and Online Exam
Proctoring (OEP), prove the effectiveness of our method as compared to the
state-of-the-art approaches, and obtain the frame-level AUC score of 87.58% on
the OEP dataset.
</summary>
    <author>
      <name>Yemeng Liu</name>
    </author>
    <author>
      <name>Jing Ren</name>
    </author>
    <author>
      <name>Jianshuo Xu</name>
    </author>
    <author>
      <name>Xiaomei Bai</name>
    </author>
    <author>
      <name>Roopdeep Kaur</name>
    </author>
    <author>
      <name>Feng Xia</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCDS.2024.3349705</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCDS.2024.3349705" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages,
                            7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Cognitive and Developmental Systems 2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2402.06107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.06107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T40, 68T45" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2405.17488v1</id>
    <updated>2024-05-24T20: 27: 45Z</updated>
    <published>2024-05-24T20: 27: 45Z</published>
    <title>Pattern-Based Time-Series Risk Scoring for Anomaly Detection and Alert
  Filtering -- A Predictive Maintenance Case Study</title>
    <summary>  Fault detection is a key challenge in the management of complex systems. In
the context of SparkCognition's efforts towards predictive maintenance in large
scale industrial systems, this problem is often framed in terms of anomaly
detection - identifying patterns of behavior in the data which deviate from
normal. Patterns of normal behavior aren't captured simply in the coarse
statistics of measured signals. Rather, the multivariate sequential pattern
itself can be indicative of normal vs. abnormal behavior. For this reason,
normal behavior modeling that relies on snapshots of the data without taking
into account temporal relationships as they evolve would be lacking. However,
common strategies for dealing with temporal dependence, such as Recurrent
Neural Networks or attention mechanisms are oftentimes computationally
expensive and difficult to train. In this paper, we propose a fast and
efficient approach to anomaly detection and alert filtering based on sequential
pattern similarities. In our empirical analysis section, we show how this
approach can be leveraged for a variety of purposes involving anomaly detection
on a large scale real-world industrial system. Subsequently, we test our
approach on a publicly-available dataset in order to establish its general
applicability and robustness compared to a state-of-the-art baseline. We also
demonstrate an efficient way of optimizing the framework based on an alert
recall objective function.
</summary>
    <author>
      <name>Elad Liebman</name>
    </author>
    <link href="http://arxiv.org/abs/2405.17488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.17488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.05732v1</id>
    <updated>2025-02-19T06: 51: 57Z</updated>
    <published>2025-02-19T06: 51: 57Z</published>
    <title>Collaborative design of fault diagnosis and fault tolerance control
  under nested signal temporal logic specifications</title>
    <summary>  Signal Temporal Logic (STL) specifications play a crucial role in defining
complex temporal properties and behaviors in safety-critical cyber-physical
systems (CPS). However, fault diagnosis (FD) and fault-tolerant control (FTC)
for CPS with nonlinear dynamics remain significant challenges, particularly
when dealing with nested signal temporal logic (NSTL) specifications. This
paper introduces a novel framework for the collaborative design of FD and FTC,
aimed at optimizing fault diagnostic performance while ensuring fault tolerance
under NSTL specifications. The proposed framework consists of four key steps:
(1) construction of the Signal Temporal Logic Tree (STLT), (2) fault detection
via the construction of fault-tolerant feasible sets, (3) evaluation of fault
detection performance, and (4) synthesis of fault-tolerant control. Initially,
a controller for nonlinear systems is designed to satisfy NSTL specifications,
and a fault detection observer is developed alongside fault-tolerant feasible
sets. To address the challenge of maintaining solution feasibility in dynamic
optimization control problems, the concept of fault-tolerant control recursive
feasibility is introduced. Subsequently, suboptimal controller gains are
derived through a quadratic programming approach to ensure fault tolerance. The
collaborative design framework enables more rapid and accurate fault detection
while preserving FTC performance. A simulation study is presented to
demonstrate the effectiveness of the proposed framework.
</summary>
    <author>
      <name>Penghong Lu</name>
    </author>
    <author>
      <name>Gang Chen</name>
    </author>
    <author>
      <name>Rong Su</name>
    </author>
    <link href="http://arxiv.org/abs/2503.05732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.05732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1811.09986v3</id>
    <updated>2018-12-05T09: 35: 07Z</updated>
    <published>2018-11-25T10: 59: 19Z</published>
    <title>Learning Conditional Random Fields with Augmented Observations for
  Partially Observed Action Recognition</title>
    <summary>  This paper aims at recognizing partially observed human actions in videos.
Action videos acquired in uncontrolled environments often contain corrupt
frames, which make actions partially observed. Furthermore, these frames can
last for arbitrary lengths of time and appear irregularly. They are
inconsistent with training data and degrade the performance of pre-trained
action recognition systems. We present an approach to address this issue. For
each training and testing actions, we divide it into segments and explore the
mutual dependency between temporal segments. This property states that the
similarity of two actions at one segment often implies their similarity at
another. We augment each segment with extra alternatives retrieved from
training data. The augmentation algorithm is designed in a way where a few
alternatives are good enough to replace the original segment where corrupt
frames occur. Our approach is developed upon hidden conditional random fields
and leverages the flexibility of hidden variables for uncertainty handling. It
turns out that our approach integrates corrupt segment detection and
alternative selection into the process of prediction, and can recognize
partially observed actions more accurately. It is evaluated on both fully
observed actions and partially observed ones with either synthetic or real
corrupt frames. The experimental results manifest its general applicability and
superior performance, especially when corrupt frames are present in the action
videos.
</summary>
    <author>
      <name>Shih-Yao Lin</name>
    </author>
    <author>
      <name>Yen-Yu Lin</name>
    </author>
    <author>
      <name>Chu-Song Chen</name>
    </author>
    <author>
      <name>Yi-Ping Hung</name>
    </author>
    <link href="http://arxiv.org/abs/1811.09986v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.09986v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2003.06838v1</id>
    <updated>2020-03-15T14: 21: 18Z</updated>
    <published>2020-03-15T14: 21: 18Z</published>
    <title>Energy-based Periodicity Mining with Deep Features for Action Repetition
  Counting in Unconstrained Videos</title>
    <summary>  Action repetition counting is to estimate the occurrence times of the
repetitive motion in one action, which is a relatively new, important but
challenging measurement problem. To solve this problem, we propose a new method
superior to the traditional ways in two aspects, without preprocessing and
applicable for arbitrary periodicity actions. Without preprocessing, the
proposed model makes our method convenient for real applications; processing
the arbitrary periodicity action makes our model more suitable for the actual
circumstance. In terms of methodology, firstly, we analyze the movement
patterns of the repetitive actions based on the spatial and temporal features
of actions extracted by deep ConvNets; Secondly, the Principal Component
Analysis algorithm is used to generate the intuitive periodic information from
the chaotic high-dimensional deep features; Thirdly, the periodicity is mined
based on the high-energy rule using Fourier transform; Finally, the inverse
Fourier transform with a multi-stage threshold filter is proposed to improve
the quality of the mined periodicity, and peak detection is introduced to
finish the repetition counting. Our work features two-fold: 1) An important
insight that deep features extracted for action recognition can well model the
self-similarity periodicity of the repetitive action is presented. 2) A
high-energy based periodicity mining rule using deep features is presented,
which can process arbitrary actions without preprocessing. Experimental results
show that our method achieves comparable results on the public datasets YT
Segments and QUVA.
</summary>
    <author>
      <name>Jianqin Yin</name>
    </author>
    <author>
      <name>Yanchun Wu</name>
    </author>
    <author>
      <name>Huaping Liu</name>
    </author>
    <author>
      <name>Yonghao Dang</name>
    </author>
    <author>
      <name>Zhiyi Liu</name>
    </author>
    <author>
      <name>Jun Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2003.06838v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.06838v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2104.00125v1</id>
    <updated>2021-03-31T21: 15: 29Z</updated>
    <published>2021-03-31T21: 15: 29Z</published>
    <title>Drowsiness Detection Based On Driver Temporal Behavior Using a New
  Developed Dataset</title>
    <summary>  Driver drowsiness detection has been the subject of many researches in the
past few decades and various methods have been developed to detect it. In this
study, as an image-based approach with adequate accuracy, along with the
expedite process, we applied YOLOv3 (You Look Only Once-version3) CNN
(Convolutional Neural Network) for extracting facial features automatically.
Then, LSTM (Long-Short Term Memory) neural network is employed to learn driver
temporal behaviors including yawning and blinking time period as well as
sequence classification. To train YOLOv3, we utilized our collected dataset
alongside the transfer learning method. Moreover, the dataset for the LSTM
training process is produced by the mentioned CNN and is formatted as a
two-dimensional sequence comprised of eye blinking and yawning time durations.
The developed dataset considers both disturbances such as illumination and
drivers' head posture. To have real-time experiments a multi-thread framework
is developed to run both CNN and LSTM in parallel. Finally, results indicate
the hybrid of CNN and LSTM ability in drowsiness detection and the
effectiveness of the proposed method.
</summary>
    <author>
      <name>Farnoosh Faraji</name>
    </author>
    <author>
      <name>Faraz Lotfi</name>
    </author>
    <author>
      <name>Javad Khorramdel</name>
    </author>
    <author>
      <name>Ali Najafi</name>
    </author>
    <author>
      <name>Ali Ghaffari</name>
    </author>
    <link href="http://arxiv.org/abs/2104.00125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.00125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1905.02171v1</id>
    <updated>2019-05-06T17: 39: 09Z</updated>
    <published>2019-05-06T17: 39: 09Z</published>
    <title>Spatio-Temporal Action Localization in a Weakly Supervised Setting</title>
    <summary>  Enabling computational systems with the ability to localize actions in
video-based content has manifold applications. Traditionally, such a problem is
approached in a fully-supervised setting where video-clips with complete
frame-by-frame annotations around the actions of interest are provided for
training. However, the data requirements needed to achieve adequate
generalization in this setting is prohibitive. In this work, we circumvent this
issue by casting the problem in a weakly supervised setting, i.e., by
considering videos as labelled `sets' of unlabelled video segments. Firstly, we
apply unsupervised segmentation to take advantage of the elementary structure
of each video. Subsequently, a convolutional neural network is used to extract
RGB features from the resulting video segments. Finally, Multiple Instance
Learning (MIL) is employed to predict labels at the video segment level, thus
inherently performing spatio-temporal action detection. In contrast to previous
work, we make use of a different MIL formulation in which the label of each
video segment is continuous rather then discrete, making the resulting
optimization function tractable. Additionally, we utilize a set splitting
technique for regularization. Experimental results considering multiple
performance indicators on the UCF-Sports data-set support the effectiveness of
our approach.
</summary>
    <author>
      <name>Kurt Degiorgio</name>
    </author>
    <author>
      <name>Fabio Cuzzolin</name>
    </author>
    <link href="http://arxiv.org/abs/1905.02171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.02171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1404.3312v1</id>
    <updated>2014-04-12T19: 01: 36Z</updated>
    <published>2014-04-12T19: 01: 36Z</published>
    <title>Shrinkage Optimized Directed Information using Pictorial Structures for
  Action Recognition</title>
    <summary>  In this paper, we propose a novel action recognition framework. The method
uses pictorial structures and shrinkage optimized directed information
assessment (SODA) coupled with Markov Random Fields called SODA+MRF to model
the directional temporal dependency and bidirectional spatial dependency. As a
variant of mutual information, directional information captures the directional
information flow and temporal structure of video sequences across frames.
Meanwhile, within each frame, Markov random fields are utilized to model the
spatial relations among different parts of a human body and the body parts of
different people. The proposed SODA+MRF model is robust to view point
transformations and detect complex interactions accurately. We compare the
proposed method against several baseline methods to highlight the effectiveness
of the SODA+MRF model. We demonstrate that our algorithm has superior action
recognition performance on the UCF action recognition dataset, the Olympic
sports dataset and the collective activity dataset over several
state-of-the-art methods.
</summary>
    <author>
      <name>Xu Chen</name>
    </author>
    <author>
      <name>Alfred Hero</name>
    </author>
    <author>
      <name>Silvio Savarese</name>
    </author>
    <link href="http://arxiv.org/abs/1404.3312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.3312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1604.06979v1</id>
    <updated>2016-04-24T03: 35: 15Z</updated>
    <published>2016-04-24T03: 35: 15Z</published>
    <title>Cardiac Motion Analysis by Temporal Flow Graphs</title>
    <summary>  Cardiac motion analysis from B-mode ultrasound sequence is a key task in
assessing the health of the heart. The paper proposes a new methodology for
cardiac motion analysis based on the temporal behaviour of points of interest
on the myocardium. We define a new signal called the Temporal Flow Graph (TFG)
which depicts the movement of a point of interest over time. It is a graphical
representation derived from a flow field and describes the temporal evolution
of a point. We prove that TFG for an object undergoing periodic motion is also
periodic. This principle can be utilized to derive both global and local
information from a given sequence. We demonstrate this for detecting motion
irregularities at the sequence, as well as regional levels on real and
synthetic data. A coarse localisation of anatomical landmarks such as centres
of left/right cavities and valve points is also demonstrated using TFGs.
</summary>
    <author>
      <name>V S R Veeravasarapu</name>
    </author>
    <author>
      <name>Jayanthi Sivaswamy</name>
    </author>
    <author>
      <name>Vishanji Karani</name>
    </author>
    <link href="http://arxiv.org/abs/1604.06979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2407.05610v1</id>
    <updated>2024-07-08T04: 54: 39Z</updated>
    <published>2024-07-08T04: 54: 39Z</published>
    <title>Described Spatial-Temporal Video Detection</title>
    <summary>  Detecting visual content on language expression has become an emerging topic
in the community. However, in the video domain, the existing setting, i.e.,
spatial-temporal video grounding (STVG), is formulated to only detect one
pre-existing object in each frame, ignoring the fact that language descriptions
can involve none or multiple entities within a video. In this work, we advance
the STVG to a more practical setting called described spatial-temporal video
detection (DSTVD) by overcoming the above limitation. To facilitate the
exploration of DSTVD, we first introduce a new benchmark, namely DVD-ST.
Notably, DVD-ST supports grounding from none to many objects onto the video in
response to queries and encompasses a diverse range of over 150 entities,
including appearance, actions, locations, and interactions. The extensive
breadth and diversity of the DVD-ST dataset make it an exemplary testbed for
the investigation of DSTVD. In addition to the new benchmark, we further
present two baseline methods for our proposed DSTVD task by extending two
representative STVG models, i.e., TubeDETR, and STCAT. These extended models
capitalize on tubelet queries to localize and track referred objects across the
video sequence. Besides, we adjust the training objectives of these models to
optimize spatial and temporal localization accuracy and multi-class
classification capabilities. Furthermore, we benchmark the baselines on the
introduced DVD-ST dataset and conduct extensive experimental analysis to guide
future investigation. Our code and benchmark will be publicly available.
</summary>
    <author>
      <name>Wei Ji</name>
    </author>
    <author>
      <name>Xiangyan Liu</name>
    </author>
    <author>
      <name>Yingfei Sun</name>
    </author>
    <author>
      <name>Jiajun Deng</name>
    </author>
    <author>
      <name>You Qin</name>
    </author>
    <author>
      <name>Ammar Nuwanna</name>
    </author>
    <author>
      <name>Mengyao Qiu</name>
    </author>
    <author>
      <name>Lina Wei</name>
    </author>
    <author>
      <name>Roger Zimmermann</name>
    </author>
    <link href="http://arxiv.org/abs/2407.05610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.05610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2110.07922v2</id>
    <updated>2021-10-28T07: 23: 14Z</updated>
    <published>2021-10-15T08: 07: 31Z</published>
    <title>Anomaly Detection in Multi-Agent Trajectories for Automated Driving</title>
    <summary>  Human drivers can recognise fast abnormal driving situations to avoid
accidents. Similar to humans, automated vehicles are supposed to perform
anomaly detection. In this work, we propose the spatio-temporal graph
auto-encoder for learning normal driving behaviours. Our innovation is the
ability to jointly learn multiple trajectories of a dynamic number of agents.
To perform anomaly detection, we first estimate a density function of the
learned trajectory feature representation and then detect anomalies in
low-density regions. Due to the lack of multi-agent trajectory datasets for
anomaly detection in automated driving, we introduce our dataset using a
driving simulator for normal and abnormal manoeuvres. Our evaluations show that
our approach learns the relation between different agents and delivers
promising results compared to the related works. The code, simulation and the
dataset are publicly available on https: //github.com/againerju/maad_highway.
</summary>
    <author>
      <name>Julian Wiederer</name>
    </author>
    <author>
      <name>Arij Bouazizi</name>
    </author>
    <author>
      <name>Marco Troina</name>
    </author>
    <author>
      <name>Ulrich Kressel</name>
    </author>
    <author>
      <name>Vasileios Belagiannis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages incl. supplementary material,
                            8 figures,
                            4 tables (accepted
  by CoRL 2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.07922v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.07922v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2407.06849v1</id>
    <updated>2024-07-09T13: 32: 33Z</updated>
    <published>2024-07-09T13: 32: 33Z</published>
    <title>TeVAE: A Variational Autoencoder Approach for Discrete Online Anomaly
  Detection in Variable-state Multivariate Time-series Data</title>
    <summary>  As attention to recorded data grows in the realm of automotive testing and
manual evaluation reaches its limits, there is a growing need for automatic
online anomaly detection. This real-world data is complex in many ways and
requires the modelling of testee behaviour. To address this, we propose a
temporal variational autoencoder (TeVAE) that can detect anomalies with minimal
false positives when trained on unlabelled data. Our approach also avoids the
bypass phenomenon and introduces a new method to remap individual windows to a
continuous time series. Furthermore, we propose metrics to evaluate the
detection delay and root-cause capability of our approach and present results
from experiments on a real-world industrial data set. When properly configured,
TeVAE flags anomalies only 6% of the time wrongly and detects 65% of anomalies
present. It also has the potential to perform well with a smaller training and
validation subset but requires a more sophisticated threshold estimation
method.
</summary>
    <author>
      <name>Lucas Correia</name>
    </author>
    <author>
      <name>Jan-Christoph Goos</name>
    </author>
    <author>
      <name>Philipp Klein</name>
    </author>
    <author>
      <name>Thomas Bäck</name>
    </author>
    <author>
      <name>Anna V. Kononova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Studies in Computational Intelligence Journal. arXiv
  admin note: substantial text overlap with arXiv: 2309.02253</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.06849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.06849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2210.02186v3</id>
    <updated>2023-04-12T02: 34: 03Z</updated>
    <published>2022-10-05T12: 19: 51Z</published>
    <title>TimesNet: Temporal 2D-Variation Modeling for General Time Series
  Analysis</title>
    <summary>  Time series analysis is of immense importance in extensive applications, such
as weather forecasting, anomaly detection, and action recognition. This paper
focuses on temporal variation modeling, which is the common key problem of
extensive analysis tasks. Previous methods attempt to accomplish this directly
from the 1D time series, which is extremely challenging due to the intricate
temporal patterns. Based on the observation of multi-periodicity in time
series, we ravel out the complex temporal variations into the multiple
intraperiod- and interperiod-variations. To tackle the limitations of 1D time
series in representation capability, we extend the analysis of temporal
variations into the 2D space by transforming the 1D time series into a set of
2D tensors based on multiple periods. This transformation can embed the
intraperiod- and interperiod-variations into the columns and rows of the 2D
tensors respectively, making the 2D-variations to be easily modeled by 2D
kernels. Technically, we propose the TimesNet with TimesBlock as a task-general
backbone for time series analysis. TimesBlock can discover the
multi-periodicity adaptively and extract the complex temporal variations from
transformed 2D tensors by a parameter-efficient inception block. Our proposed
TimesNet achieves consistent state-of-the-art in five mainstream time series
analysis tasks, including short- and long-term forecasting, imputation,
classification, and anomaly detection. Code is available at this repository:
https: //github.com/thuml/TimesNet.
</summary>
    <author>
      <name>Haixu Wu</name>
    </author>
    <author>
      <name>Tengge Hu</name>
    </author>
    <author>
      <name>Yong Liu</name>
    </author>
    <author>
      <name>Hang Zhou</name>
    </author>
    <author>
      <name>Jianmin Wang</name>
    </author>
    <author>
      <name>Mingsheng Long</name>
    </author>
    <link href="http://arxiv.org/abs/2210.02186v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.02186v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2206.02559v1</id>
    <updated>2022-06-02T08: 05: 02Z</updated>
    <published>2022-06-02T08: 05: 02Z</published>
    <title>Conversation Group Detection With Spatio-Temporal Context</title>
    <summary>  In this work, we propose an approach for detecting conversation groups in
social scenarios like cocktail parties and networking events, from overhead
camera recordings. We posit the detection of conversation groups as a learning
problem that could benefit from leveraging the spatial context of the
surroundings, and the inherent temporal context in interpersonal dynamics which
is reflected in the temporal dynamics in human behavior signals, an aspect that
has not been addressed in recent prior works. This motivates our approach which
consists of a dynamic LSTM-based deep learning model that predicts continuous
pairwise affinity values indicating how likely two people are in the same
conversation group. These affinity values are also continuous in time, since
relationships and group membership do not occur instantaneously, even though
the ground truths of group membership are binary. Using the predicted affinity
values, we apply a graph clustering method based on Dominant Set extraction to
identify the conversation groups. We benchmark the proposed method against
established methods on multiple social interaction datasets. Our results showed
that the proposed method improves group detection performance in data that has
more temporal granularity in conversation group labels. Additionally, we
provide an analysis in the predicted affinity values in relation to the
conversation group detection. Finally, we demonstrate the usability of the
predicted affinity values in a forecasting framework to predict group
membership for a given forecast horizon.
</summary>
    <author>
      <name>Stephanie Tan</name>
    </author>
    <author>
      <name>David M. J. Tax</name>
    </author>
    <author>
      <name>Hayley Hung</name>
    </author>
    <link href="http://arxiv.org/abs/2206.02559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.02559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1712.01090v2</id>
    <updated>2017-12-07T15: 21: 15Z</updated>
    <published>2017-12-04T14: 31: 42Z</published>
    <title>Robust 3D Action Recognition through Sampling Local Appearances and
  Global Distributions</title>
    <summary>  3D action recognition has broad applications in human-computer interaction
and intelligent surveillance. However, recognizing similar actions remains
challenging since previous literature fails to capture motion and shape cues
effectively from noisy depth data. In this paper, we propose a novel two-layer
Bag-of-Visual-Words (BoVW) model, which suppresses the noise disturbances and
jointly encodes both motion and shape cues. First, background clutter is
removed by a background modeling method that is designed for depth data. Then,
motion and shape cues are jointly used to generate robust and distinctive
spatial-temporal interest points (STIPs): motion-based STIPs and shape-based
STIPs. In the first layer of our model, a multi-scale 3D local steering kernel
(M3DLSK) descriptor is proposed to describe local appearances of cuboids around
motion-based STIPs. In the second layer, a spatial-temporal vector (STV)
descriptor is proposed to describe the spatial-temporal distributions of
shape-based STIPs. Using the Bag-of-Visual-Words (BoVW) model, motion and shape
cues are combined to form a fused action representation. Our model performs
favorably compared with common STIP detection and description methods. Thorough
experiments verify that our model is effective in distinguishing similar
actions and robust to background clutter, partial occlusions and pepper noise.
</summary>
    <author>
      <name>Mengyuan Liu</name>
    </author>
    <author>
      <name>Hong Liu</name>
    </author>
    <author>
      <name>Chen Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1712.01090v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.01090v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.01891v1</id>
    <updated>2024-04-02T12: 29: 04Z</updated>
    <published>2024-04-02T12: 29: 04Z</published>
    <title>ASTRA: An Action Spotting TRAnsformer for Soccer Videos</title>
    <summary>  In this paper, we introduce ASTRA, a Transformer-based model designed for the
task of Action Spotting in soccer matches. ASTRA addresses several challenges
inherent in the task and dataset, including the requirement for precise action
localization, the presence of a long-tail data distribution, non-visibility in
certain actions, and inherent label noise. To do so, ASTRA incorporates (a) a
Transformer encoder-decoder architecture to achieve the desired output temporal
resolution and to produce precise predictions, (b) a balanced mixup strategy to
handle the long-tail distribution of the data, (c) an uncertainty-aware
displacement head to capture the label variability, and (d) input audio signal
to enhance detection of non-visible actions. Results demonstrate the
effectiveness of ASTRA, achieving a tight Average-mAP of 66.82 on the test set.
Moreover, in the SoccerNet 2023 Action Spotting challenge, we secure the 3rd
position with an Average-mAP of 70.21 on the challenge set.
</summary>
    <author>
      <name>Artur Xarles</name>
    </author>
    <author>
      <name>Sergio Escalera</name>
    </author>
    <author>
      <name>Thomas B. Moeslund</name>
    </author>
    <author>
      <name>Albert Clapés</name>
    </author>
    <link href="http://arxiv.org/abs/2404.01891v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.01891v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2103.03409v1</id>
    <updated>2021-03-05T00: 48: 23Z</updated>
    <published>2021-03-05T00: 48: 23Z</published>
    <title>A General Method to Find Highly Coordinating Communities in Social Media
  through Inferred Interaction Links</title>
    <summary>  Political misinformation, astroturfing and organised trolling are online
malicious behaviours with significant real-world effects. Many previous
approaches examining these phenomena have focused on broad campaigns rather
than the small groups responsible for instigating or sustaining them. To reveal
latent (i.e., hidden) networks of cooperating accounts, we propose a novel
temporal window approach that relies on account interactions and metadata
alone. It detects groups of accounts engaging in various behaviours that, in
concert, come to execute different goal-based strategies, a number of which we
describe. The approach relies upon a pipeline that extracts relevant elements
from social media posts, infers connections between accounts based on criteria
matching the coordination strategies to build an undirected weighted network of
accounts, which is then mined for communities exhibiting high levels of
evidence of coordination using a novel community extraction method. We address
the temporal aspect of the data by using a windowing mechanism, which may be
suitable for near real-time application. We further highlight consistent
coordination with a sliding frame across multiple windows and application of a
decay factor. Our approach is compared with other recent similar processing
approaches and community detection methods and is validated against two
relevant datasets with ground truth data, using content, temporal, and network
analyses, as well as with the design, training and application of three
one-class classifiers built using the ground truth; its utility is furthermore
demonstrated in two case studies of contentious online discussions.
</summary>
    <author>
      <name>Derek Weber</name>
    </author>
    <author>
      <name>Frank Neumann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s13278-021-00815-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s13278-021-00815-2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">58 pages,
                            25 figures, submitted to the International Journal of
  Social Network Analysis and Mining (SNAM) as an expansion to an ASONAM'20
  paper (arXiv: 2010.08180)</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.03409v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.03409v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2305.03799v2</id>
    <updated>2023-10-23T23: 55: 21Z</updated>
    <published>2023-05-05T19: 01: 40Z</published>
    <title>Detecting disruption of HER2 membrane protein organization in cell
  membranes with nanoscale precision</title>
    <summary>  The spatio-temporal organization of proteins within the cell membrane can
affect numerous biological functions, including cell signaling, communication,
and transportation. Deviations from normal spatial arrangements have been
observed in various diseases, and better understanding this process is a key
stepping-stone to advancing development of clinical interventions. However,
given the nanometer length scales involved, detecting these subtle changes has
primarily relied on complex super resolution and single molecule imaging
methods. In this work, we demonstrate an alternative fluorescent imaging
strategy for detecting protein organization based on a material that exhibits a
unique photophysical behavior known as aggregation induced emission (AIE).
Organic AIE molecules have an increase in emission signal when they are in
close proximity and the molecular motion is restricted. This property
simultaneously addresses the high background noise and low detection signal
that limit conventional widefield fluorescent imaging. To demonstrate the
potential of this approach, the fluorescent molecule sensor is conjugated to a
human epidermal growth factor receptor 2 (HER2) specific antibody and used to
investigate the spatio-temporal behavior of HER2 clustering in the membrane of
HER2-overexpressing breast cancer cells. Notably, the disruption of HER2
clusters in response to an FDA-approved monoclonal antibody therapeutic
(Trastuzumab) is successfully detected using a simple widefield fluorescent
microscope. While the sensor demonstrated here is optimized for sensing HER2
clustering, it is an easily adaptable platform. Moreover, given the
compatibility with widefield imaging, the system has the potential to be used
with high-throughput imaging techniques, accelerating investigations into
membrane protein spatio-temporal organization.
</summary>
    <author>
      <name>Yasaman Moradi</name>
    </author>
    <author>
      <name>Jerry SH Lee</name>
    </author>
    <author>
      <name>Andrea M. Armani</name>
    </author>
    <link href="http://arxiv.org/abs/2305.03799v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.03799v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1003.0519v1</id>
    <updated>2010-03-02T09: 13: 21Z</updated>
    <published>2010-03-02T09: 13: 21Z</published>
    <title>Comparative study of different scattering geometries for the proposed
  Indian X-ray polarization measurement experiment using Geant4</title>
    <summary>  Polarization measurements in X-rays can provide unique opportunity to study
the behavior of matter and radiation under extreme magnetic fields and extreme
gravitational fields. Unfortunately, over past two decades, when X-ray
astronomy witnessed multiple order of magnitude improvement in temporal,
spatial and spectral sensitivities, there is no (or very little) progress in
the field of polarization measurements of astrophysical X-rays. Recently, a
proposal has been submitted to ISRO for a dedicated small satellite based
experiment to carry out X-ray polarization measurement, which aims to provide
the first X-ray polarization measurements since 1976. This experiment will be
based on the well known principle of polarization measurement by Thomson
scattering and employs the baseline design of a central low Z scatterer
surrounded by X-ray detectors to measure the angular intensity distribution of
the scattered X-rays. The sensitivity of such experiment is determined by the
collecting area, scattering and detection efficiency, X-ray detector
background, and the modulation factor. Therefore, it is necessary to carefully
select the scattering geometry which can provide the highest modulation factor
and thus highest sensitivity within the specified experimental constraints. The
effective way to determine optimum scattering geometry is by studying various
possible scattering geometries by means of Monte Carlo simulations. Here we
present results of our detailed comparative study based on Geant4 simulations
of five different scattering geometries which can be considered within the
weight and size constraints of the proposed small satellite based X-ray
polarization measurement experiment.
</summary>
    <author>
      <name>S. V. Vadawale</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Physical Research Laboratory, Ahmedabad</arxiv:affiliation>
    </author>
    <author>
      <name>B. Paul</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Raman Research Institute, Bangalore</arxiv:affiliation>
    </author>
    <author>
      <name>J. Pendharkar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Physical Research Laboratory, Ahmedabad</arxiv:affiliation>
    </author>
    <author>
      <name>Sachindra Naik</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Physical Research Laboratory, Ahmedabad</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.nima.2010.02.116</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.nima.2010.02.116" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages,
                            6 figures, accepted for publication in "Nuclear Inst. and
  Methods in Physics Research, A"</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.0519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.0519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2210.02033v1</id>
    <updated>2022-10-05T05: 46: 40Z</updated>
    <published>2022-10-05T05: 46: 40Z</published>
    <title>Learning Video-independent Eye Contact Segmentation from In-the-Wild
  Videos</title>
    <summary>  Human eye contact is a form of non-verbal communication and can have a great
influence on social behavior. Since the location and size of the eye contact
targets vary across different videos, learning a generic video-independent eye
contact detector is still a challenging task. In this work, we address the task
of one-way eye contact detection for videos in the wild. Our goal is to build a
unified model that can identify when a person is looking at his gaze targets in
an arbitrary input video. Considering that this requires time-series relative
eye movement information, we propose to formulate the task as a temporal
segmentation. Due to the scarcity of labeled training data, we further propose
a gaze target discovery method to generate pseudo-labels for unlabeled videos,
which allows us to train a generic eye contact segmentation model in an
unsupervised way using in-the-wild videos. To evaluate our proposed approach,
we manually annotated a test dataset consisting of 52 videos of human
conversations. Experimental results show that our eye contact segmentation
model outperforms the previous video-dependent eye contact detector and can
achieve 71.88% framewise accuracy on our annotated test set. Our code and
evaluation dataset are available at
https: //github.com/ut-vision/Video-Independent-ECS.
</summary>
    <author>
      <name>Tianyi Wu</name>
    </author>
    <author>
      <name>Yusuke Sugano</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACCV2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.02033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.02033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1703.02529v3</id>
    <updated>2017-08-08T21: 34: 30Z</updated>
    <published>2017-03-07T18: 54: 28Z</published>
    <title>NoScope: Optimizing Neural Network Queries over Video at Scale</title>
    <summary>  Recent advances in computer vision-in the form of deep neural networks-have
made it possible to query increasing volumes of video data with high accuracy.
However, neural network inference is computationally expensive at scale:
applying a state-of-the-art object detector in real time (i.e.,
                            30+ frames per
second) to a single video requires a $4000 GPU. In response, we present
NoScope, a system for querying videos that can reduce the cost of neural
network video analysis by up to three orders of magnitude via
inference-optimized model search. Given a target video, object to detect, and
reference neural network, NoScope automatically searches for and trains a
sequence, or cascade, of models that preserves the accuracy of the reference
network but is specialized to the target video and are therefore far less
computationally expensive. NoScope cascades two types of models: specialized
models that forego the full generality of the reference model but faithfully
mimic its behavior for the target video and object; and difference detectors
that highlight temporal differences across frames. We show that the optimal
cascade architecture differs across videos and objects, so NoScope uses an
efficient cost-based optimizer to search across models and cascades. With this
approach, NoScope achieves two to three order of magnitude speed-ups
(265-15,
                            500x real-time) on binary classification tasks over fixed-angle webcam
and surveillance video while maintaining accuracy within 1-5% of
state-of-the-art neural networks.
</summary>
    <author>
      <name>Daniel Kang</name>
    </author>
    <author>
      <name>John Emmons</name>
    </author>
    <author>
      <name>Firas Abuzaid</name>
    </author>
    <author>
      <name>Peter Bailis</name>
    </author>
    <author>
      <name>Matei Zaharia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PVLDB 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.02529v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.02529v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2304.12586v2</id>
    <updated>2023-09-28T05: 23: 00Z</updated>
    <published>2023-04-25T05: 43: 54Z</published>
    <title>Unsupervised Discovery of Extreme Weather Events Using Universal
  Representations of Emergent Organization</title>
    <summary>  Spontaneous self-organization is ubiquitous in systems far from thermodynamic
equilibrium. While organized structures that emerge dominate transport
properties, universal representations that identify and describe these key
objects remain elusive. Here, we introduce a theoretically-grounded framework
for describing emergent organization that, via data-driven algorithms, is
constructive in practice. Its building blocks are spacetime lightcones that
embody how information propagates across a system through local interactions.
We show that predictive equivalence classes of lightcones -- local causal
states -- capture organized behaviors and coherent structures in complex
spatiotemporal systems. Employing an unsupervised physics-informed machine
learning algorithm and a high-performance computing implementation, we
demonstrate automatically discovering coherent structures in two real world
domain science problems. We show that local causal states identify vortices and
track their power-law decay behavior in two-dimensional fluid turbulence. We
then show how to detect and track familiar extreme weather events -- hurricanes
and atmospheric rivers -- and discover other novel coherent structures
associated with precipitation extremes in high-resolution climate data at the
grid-cell level.
</summary>
    <author>
      <name>Adam Rupe</name>
    </author>
    <author>
      <name>Karthik Kashinath</name>
    </author>
    <author>
      <name>Nalini Kumar</name>
    </author>
    <author>
      <name>James P. Crutchfield</name>
    </author>
    <link href="http://arxiv.org/abs/2304.12586v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.12586v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.PS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2309.14334v1</id>
    <updated>2023-09-25T17: 58: 23Z</updated>
    <published>2023-09-25T17: 58: 23Z</published>
    <title>Tasks Makyth Models: Machine Learning Assisted Surrogates for Tipping
  Points</title>
    <summary>  We present a machine learning (ML)-assisted framework bridging manifold
learning, neural networks, Gaussian processes, and Equation-Free multiscale
modeling, for (a) detecting tipping points in the emergent behavior of complex
systems, and (b) characterizing probabilities of rare events (here,
catastrophic shifts) near them. Our illustrative example is an event-driven,
stochastic agent-based model (ABM) describing the mimetic behavior of traders
in a simple financial market. Given high-dimensional spatiotemporal data --
generated by the stochastic ABM -- we construct reduced-order models for the
emergent dynamics at different scales: (a) mesoscopic Integro-Partial
Differential Equations (IPDEs); and (b) mean-field-type Stochastic Differential
Equations (SDEs) embedded in a low-dimensional latent space, targeted to the
neighborhood of the tipping point. We contrast the uses of the different models
and the effort involved in learning them.
</summary>
    <author>
      <name>Gianluca Fabiani</name>
    </author>
    <author>
      <name>Nikolaos Evangelou</name>
    </author>
    <author>
      <name>Tianqi Cui</name>
    </author>
    <author>
      <name>Juan M. Bello-Rivas</name>
    </author>
    <author>
      <name>Cristina P. Martin-Linares</name>
    </author>
    <author>
      <name>Constantinos Siettos</name>
    </author>
    <author>
      <name>Ioannis G. Kevrekidis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages,
                            8 figures,
                            6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.14334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.14334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.TR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1706.04122v1</id>
    <updated>2017-06-13T15: 30: 16Z</updated>
    <published>2017-06-13T15: 30: 16Z</published>
    <title>Joint Max Margin and Semantic Features for Continuous Event Detection in
  Complex Scenes</title>
    <summary>  In this paper the problem of complex event detection in the continuous domain
(i.e. events with unknown starting and ending locations) is addressed. Existing
event detection methods are limited to features that are extracted from the
local spatial or spatio-temporal patches from the videos. However, this makes
the model vulnerable to the events with similar concepts e.g. "Open drawer" and
"Open cupboard". In this work, in order to address the aforementioned
limitations we present a novel model based on the combination of semantic and
temporal features extracted from video frames. We train a max-margin classifier
on top of the extracted features in an adaptive framework that is able to
detect the events with unknown starting and ending locations. Our model is
based on the Bidirectional Region Neural Network and large margin Structural
Output SVM. The generality of our model allows it to be simply applied to
different labeled and unlabeled datasets. We finally test our algorithm on
three challenging datasets,
                            "UCF 101-Action Recognition", "MPII Cooking
Activities" and "Hollywood", and we report state-of-the-art performance.
</summary>
    <author>
      <name>Iman Abbasnejad</name>
    </author>
    <author>
      <name>Sridha Sridharan</name>
    </author>
    <author>
      <name>Simon Denman</name>
    </author>
    <author>
      <name>Clinton Fookes</name>
    </author>
    <author>
      <name>Simon Lucey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submit to journal of Computer Vision and Image Understanding</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.04122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.14517v1</id>
    <updated>2025-03-14T02: 52: 41Z</updated>
    <published>2025-03-14T02: 52: 41Z</published>
    <title>Cafe-Talk: Generating 3D Talking Face Animation with Multimodal Coarse-
  and Fine-grained Control</title>
    <summary>  Speech-driven 3D talking face method should offer both accurate lip
synchronization and controllable expressions. Previous methods solely adopt
discrete emotion labels to globally control expressions throughout sequences
while limiting flexible fine-grained facial control within the spatiotemporal
domain. We propose a diffusion-transformer-based 3D talking face generation
model, Cafe-Talk, which simultaneously incorporates coarse- and fine-grained
multimodal control conditions. Nevertheless, the entanglement of multiple
conditions challenges achieving satisfying performance. To disentangle speech
audio and fine-grained conditions, we employ a two-stage training pipeline.
Specifically, Cafe-Talk is initially trained using only speech audio and
coarse-grained conditions. Then, a proposed fine-grained control adapter
gradually adds fine-grained instructions represented by action units (AUs),
preventing unfavorable speech-lip synchronization. To disentangle coarse- and
fine-grained conditions, we design a swap-label training mechanism, which
enables the dominance of the fine-grained conditions. We also devise a
mask-based CFG technique to regulate the occurrence and intensity of
fine-grained control. In addition, a text-based detector is introduced with
text-AU alignment to enable natural language user input and further support
multimodal control. Extensive experimental results prove that Cafe-Talk
achieves state-of-the-art lip synchronization and expressiveness performance
and receives wide acceptance in fine-grained control in user studies. Project
page: https: //harryxd2018.github.io/cafe-talk/
</summary>
    <author>
      <name>Hejia Chen</name>
    </author>
    <author>
      <name>Haoxian Zhang</name>
    </author>
    <author>
      <name>Shoulong Zhang</name>
    </author>
    <author>
      <name>Xiaoqiang Liu</name>
    </author>
    <author>
      <name>Sisi Zhuang</name>
    </author>
    <author>
      <name>Yuan Zhang</name>
    </author>
    <author>
      <name>Pengfei Wan</name>
    </author>
    <author>
      <name>Di Zhang</name>
    </author>
    <author>
      <name>Shuai Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICLR'25</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.14517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.14517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1502.00501v1</id>
    <updated>2015-02-02T15: 03: 25Z</updated>
    <published>2015-02-02T15: 03: 25Z</published>
    <title>An Expressive Deep Model for Human Action Parsing from A Single Image</title>
    <summary>  This paper aims at one newly raising task in vision and multimedia research:
recognizing human actions from still images. Its main challenges lie in the
large variations in human poses and appearances, as well as the lack of
temporal motion information. Addressing these problems, we propose to develop
an expressive deep model to naturally integrate human layout and surrounding
contexts for higher level action understanding from still images. In
particular, a Deep Belief Net is trained to fuse information from different
noisy sources such as body part detection and object detection. To bridge the
semantic gap, we used manually labeled data to greatly improve the
effectiveness and efficiency of the pre-training and fine-tuning stages of the
DBN training. The resulting framework is shown to be robust to sometimes
unreliable inputs (e.g., imprecise detections of human parts and objects), and
outperforms the state-of-the-art approaches.
</summary>
    <author>
      <name>Zhujin Liang</name>
    </author>
    <author>
      <name>Xiaolong Wang</name>
    </author>
    <author>
      <name>Rui Huang</name>
    </author>
    <author>
      <name>Liang Lin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICME.2014.6890158</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICME.2014.6890158" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,
                            8 figures, ICME 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.00501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2205.00364v1</id>
    <updated>2022-04-30T23: 14: 08Z</updated>
    <published>2022-04-30T23: 14: 08Z</published>
    <title>RADNet: A Deep Neural Network Model for Robust Perception in Moving
  Autonomous Systems</title>
    <summary>  Interactive autonomous applications require robustness of the perception
engine to artifacts in unconstrained videos. In this paper, we examine the
effect of camera motion on the task of action detection. We develop a novel
ranking method to rank videos based on the degree of global camera motion. For
the high ranking camera videos we show that the accuracy of action detection is
decreased. We propose an action detection pipeline that is robust to the camera
motion effect and verify it empirically. Specifically, we do actor feature
alignment across frames and couple global scene features with local
actor-specific features. We do feature alignment using a novel formulation of
the Spatio-temporal Sampling Network (STSN) but with multi-scale offset
prediction and refinement using a pyramid structure. We also propose a novel
input dependent weighted averaging strategy for fusing local and global
features. We show the applicability of our network on our dataset of moving
camera videos with high camera motion (MOVE dataset) with a 4.1% increase in
frame mAP and 17% increase in video mAP.
</summary>
    <author>
      <name>Burhan A. Mudassar</name>
    </author>
    <author>
      <name>Sho Ko</name>
    </author>
    <author>
      <name>Maojingjing Li</name>
    </author>
    <author>
      <name>Priyabrata Saha</name>
    </author>
    <author>
      <name>Saibal Mukhopadhyay</name>
    </author>
    <link href="http://arxiv.org/abs/2205.00364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.00364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1904.10824v3</id>
    <updated>2019-07-17T12: 21: 26Z</updated>
    <published>2019-04-24T14: 00: 05Z</published>
    <title>Learning Bodily and Temporal Attention in Protective Movement Behavior
  Detection</title>
    <summary>  For people with chronic pain, the assessment of protective behavior during
physical functioning is essential to understand their subjective pain-related
experiences (e.g., fear and anxiety toward pain and injury) and how they deal
with such experiences (avoidance or reliance on specific body joints), with the
ultimate goal of guiding intervention. Advances in deep learning (DL) can
enable the development of such intervention. Using the EmoPain MoCap dataset,
we investigate how attention-based DL architectures can be used to improve the
detection of protective behavior by capturing the most informative temporal and
body configurational cues characterizing specific movements and the strategies
used to perform them. We propose an end-to-end deep learning architecture named
BodyAttentionNet (BANet). BANet is designed to learn temporal and bodily parts
that are more informative to the detection of protective behavior. The approach
addresses the variety of ways people execute a movement (including healthy
people) independently of the type of movement analyzed. Through extensive
comparison experiments with other state-of-the-art machine learning techniques
used with motion capture data, we show statistically significant improvements
achieved by using these attention mechanisms. In addition, the BANet
architecture requires a much lower number of parameters than the state of the
art for comparable if not higher performances.
</summary>
    <author>
      <name>Chongyang Wang</name>
    </author>
    <author>
      <name>Min Peng</name>
    </author>
    <author>
      <name>Temitayo A. Olugbade</name>
    </author>
    <author>
      <name>Nicholas D. Lane</name>
    </author>
    <author>
      <name>Amanda C. De C. Williams</name>
    </author>
    <author>
      <name>Nadia Bianchi-Berthouze</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages,
                            3 figures,
                            2 tables, code available, accepted in ACII 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.10824v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.10824v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2403.17779v1</id>
    <updated>2024-03-26T15: 12: 46Z</updated>
    <published>2024-03-26T15: 12: 46Z</published>
    <title>Optical Flow Based Detection and Tracking of Moving Objects for
  Autonomous Vehicles</title>
    <summary>  Accurate velocity estimation of surrounding moving objects and their
trajectories are critical elements of perception systems in
Automated/Autonomous Vehicles (AVs) with a direct impact on their safety. These
are non-trivial problems due to the diverse types and sizes of such objects and
their dynamic and random behaviour. Recent point cloud based solutions often
use Iterative Closest Point (ICP) techniques, which are known to have certain
limitations. For example, their computational costs are high due to their
iterative nature, and their estimation error often deteriorates as the relative
velocities of the target objects increase (&gt;2 m/sec). Motivated by such
shortcomings, this paper first proposes a novel Detection and Tracking of
Moving Objects (DATMO) for AVs based on an optical flow technique, which is
proven to be computationally efficient and highly accurate for such problems.
\textcolor{black
                            }{This is achieved by representing the driving scenario as a
vector field and applying vector calculus theories to ensure spatiotemporal
continuity.
                            } We also report the results of a comprehensive performance
evaluation of the proposed DATMO technique, carried out in this study using
synthetic and real-world data. The results of this study demonstrate the
superiority of the proposed technique, compared to the DATMO techniques in the
literature, in terms of estimation accuracy and processing time in a wide range
of relative velocities of moving objects. Finally, we evaluate and discuss the
sensitivity of the estimation error of the proposed DATMO technique to various
system and environmental parameters, as well as the relative velocities of the
moving objects.
</summary>
    <author>
      <name>MReza Alipour Sormoli</name>
    </author>
    <author>
      <name>Mehrdad Dianati</name>
    </author>
    <author>
      <name>Sajjad Mozaffari</name>
    </author>
    <author>
      <name>Roger woodman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TITS.2024.3382495</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TITS.2024.3382495" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This manuscript has been accepted as a regular paper in Transactions
  on Intelligent Transportation Systems (DOI: 10.1109/TITS.2024.3382495)</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.17779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.17779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2501.11786v1</id>
    <updated>2025-01-20T23: 19: 15Z</updated>
    <published>2025-01-20T23: 19: 15Z</published>
    <title>Synthetic Data Can Mislead Evaluations: Membership Inference as Machine
  Text Detection</title>
    <summary>  Recent work shows membership inference attacks (MIAs) on large language
models (LLMs) produce inconclusive results, partly due to difficulties in
creating non-member datasets without temporal shifts. While researchers have
turned to synthetic data as an alternative, we show this approach can be
fundamentally misleading. Our experiments indicate that MIAs function as
machine-generated text detectors, incorrectly identifying synthetic data as
training samples regardless of the data source. This behavior persists across
different model architectures and sizes, from open-source models to commercial
ones such as GPT-3.5. Even synthetic text generated by different, potentially
larger models is classified as training data by the target model. Our findings
highlight a serious concern: using synthetic data in membership evaluations may
lead to false conclusions about model memorization and data leakage. We caution
that this issue could affect other evaluations using model signals such as loss
where synthetic or machine-generated translated data substitutes for real-world
samples.
</summary>
    <author>
      <name>Ali Naseh</name>
    </author>
    <author>
      <name>Niloofar Mireshghallah</name>
    </author>
    <link href="http://arxiv.org/abs/2501.11786v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.11786v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2106.08692v1</id>
    <updated>2021-06-16T10: 51: 58Z</updated>
    <published>2021-06-16T10: 51: 58Z</published>
    <title>Detecting message modification attacks on the CAN bus with Temporal
  Convolutional Networks</title>
    <summary>  Multiple attacks have shown that in-vehicle networks have vulnerabilities
which can be exploited. Securing the Controller Area Network (CAN) for modern
vehicles has become a necessary task for car manufacturers. Some attacks inject
potentially large amount of fake messages into the CAN network; however, such
attacks are relatively easy to detect. In more sophisticated attacks, the
original messages are modified, making the detection a more complex problem. In
this paper, we present a novel machine learning based intrusion detection
method for CAN networks. We focus on detecting message modification attacks,
which do not change the timing patterns of communications. Our proposed
temporal convolutional network-based solution can learn the normal behavior of
CAN signals and differentiate them from malicious ones. The method is evaluated
on multiple CAN-bus message IDs from two public datasets including different
types of attacks. Performance results show that our lightweight approach
compares favorably to the state-of-the-art unsupervised learning approach,
achieving similar or better accuracy for a wide range of scenarios with a
significantly lower false positive rate.
</summary>
    <author>
      <name>Irina Chiscop</name>
    </author>
    <author>
      <name>András Gazdag</name>
    </author>
    <author>
      <name>Joost Bosman</name>
    </author>
    <author>
      <name>Gergely Biczók</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5220/0010445504880496</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5220/0010445504880496" rel="related"/>
    <link href="http://arxiv.org/abs/2106.08692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.08692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2005.12524v1</id>
    <updated>2020-05-26T05: 54: 28Z</updated>
    <published>2020-05-26T05: 54: 28Z</published>
    <title>A New Unified Method for Detecting Text from Marathon Runners and Sports
  Players in Video</title>
    <summary>  Detecting text located on the torsos of marathon runners and sports players
in video is a challenging issue due to poor quality and adverse effects caused
by flexible/colorful clothing, and different structures of human bodies or
actions. This paper presents a new unified method for tackling the above
challenges. The proposed method fuses gradient magnitude and direction
coherence of text pixels in a new way for detecting candidate regions.
Candidate regions are used for determining the number of temporal frame
clusters obtained by K-means clustering on frame differences. This process in
turn detects key frames. The proposed method explores Bayesian probability for
skin portions using color values at both pixel and component levels of temporal
frames, which provides fused images with skin components. Based on skin
information, the proposed method then detects faces and torsos by finding
structural and spatial coherences between them. We further propose adaptive
pixels linking a deep learning model for text detection from torso regions. The
proposed method is tested on our own dataset collected from marathon/sports
video and three standard datasets, namely, RBNR, MMM and R-ID of marathon
images, to evaluate the performance. In addition, the proposed method is also
tested on the standard natural scene datasets, namely, CTW1500 and MS-COCO text
datasets, to show the objectiveness of the proposed method. A comparative study
with the state-of-the-art methods on bib number/text detection of different
datasets shows that the proposed method outperforms the existing methods.
</summary>
    <author>
      <name>Sauradip Nag</name>
    </author>
    <author>
      <name>Palaiahnakote Shivakumara</name>
    </author>
    <author>
      <name>Umapada Pal</name>
    </author>
    <author>
      <name>Tong Lu</name>
    </author>
    <author>
      <name>Michael Blumenstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in Pattern Recognition, Elsevier</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.12524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.12524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2206.07519v1</id>
    <updated>2022-06-08T19: 39: 51Z</updated>
    <published>2022-06-08T19: 39: 51Z</published>
    <title>Smart Meter Data Anomaly Detection using Variational Recurrent
  Autoencoders with Attention</title>
    <summary>  In the digitization of energy systems, sensors and smart meters are
increasingly being used to monitor production, operation and demand. Detection
of anomalies based on smart meter data is crucial to identify potential risks
and unusual events at an early stage, which can serve as a reference for timely
initiation of appropriate actions and improving management. However, smart
meter data from energy systems often lack labels and contain noise and various
patterns without distinctively cyclical. Meanwhile, the vague definition of
anomalies in different energy scenarios and highly complex temporal
correlations pose a great challenge for anomaly detection. Many traditional
unsupervised anomaly detection algorithms such as cluster-based or
distance-based models are not robust to noise and not fully exploit the
temporal dependency in a time series as well as other dependencies amongst
multiple variables (sensors). This paper proposes an unsupervised anomaly
detection method based on a Variational Recurrent Autoencoder with attention
mechanism. with "dirty" data from smart meters, our method pre-detects missing
values and global anomalies to shrink their contribution while training. This
paper makes a quantitative comparison with the VAE-based baseline approach and
four other unsupervised learning methods, demonstrating its effectiveness and
superiority. This paper further validates the proposed method by a real case
study of detecting the anomalies of water supply temperature from an industrial
heating plant.
</summary>
    <author>
      <name>Wenjing Dai</name>
    </author>
    <author>
      <name>Xiufeng Liu</name>
    </author>
    <author>
      <name>Alfred Heller</name>
    </author>
    <author>
      <name>Per Sieverts Nielsen</name>
    </author>
    <link href="http://arxiv.org/abs/2206.07519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.07519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2304.07225v1</id>
    <updated>2023-04-14T16: 12: 30Z</updated>
    <published>2023-04-14T16: 12: 30Z</published>
    <title>Distributed detection of ARMA signals</title>
    <summary>  This paper considers a distributed detection setup where agents in a network
want to detect a time-varying signal embedded in temporally correlated noise.
The signal of interest is the impulse response of an ARMA (auto-regressive
moving average) filter, and the noise is the output of yet another ARMA filter
which is fed white Gaussian noise. For this extended problem setup, which can
prompt novel behaviour, we propose a comprehensive solution. First, we extend
the well-known running consensus detector (RCD) to this correlated setup; then,
we design an efficient implementation of the RCD by exploiting the underlying
ARMA structures; and, finally, we derive the theoretical asymptotic performance
of the RCD in this ARMA setup. It turns out that the error probability at each
agent exhibits one of two regimes: either (a) the error probability decays
exponentially fast to zero or (b) it converges to a strictly positive error
floor. While regime (a) spans staple results in large deviation theory, regime
(b) is new in distributed detection and is elicited by the ARMA setup. We fully
characterize these two scenarios: we give necessary and sufficient conditions,
phrased in terms of the zero and poles of the underlying ARMA models, for the
emergence of each regime, and provide closed-form expressions for both the
decay rates of regime (a) and the positive error floors of regime (b). Our
analysis also shows that the ARMA setup leads to two novel features: (1) the
threshold level used in RCD can influence the asymptotics of the error
probabilities and (2) some agents might be weakly informative, in the sense
that their observations do not improve the asymptotic performance of RCD and,
as such, can be safely muted to save sensing resources. Numerical simulations
illustrate and confirm the theoretical findings.
</summary>
    <author>
      <name>João Domingos</name>
    </author>
    <author>
      <name>João Xavier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages,
                            3 figures. Originally submitted in -IEEE Transactions on
  Signal Processing- on 13-Apr-2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.07225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.07225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2002.03137v1</id>
    <updated>2020-02-08T10: 48: 43Z</updated>
    <published>2020-02-08T10: 48: 43Z</published>
    <title>Symbiotic Attention with Privileged Information for Egocentric Action
  Recognition</title>
    <summary>  Egocentric video recognition is a natural testbed for diverse interaction
reasoning. Due to the large action vocabulary in egocentric video datasets,
recent studies usually utilize a two-branch structure for action recognition,
ie, one branch for verb classification and the other branch for noun
classification. However, correlation studies between the verb and the noun
branches have been largely ignored. Besides, the two branches fail to exploit
local features due to the absence of a position-aware attention mechanism. In
this paper, we propose a novel Symbiotic Attention framework leveraging
Privileged information (SAP) for egocentric video recognition. Finer
position-aware object detection features can facilitate the understanding of
actor's interaction with the object. We introduce these features in action
recognition and regard them as privileged information. Our framework enables
mutual communication among the verb branch, the noun branch, and the privileged
information. This communication process not only injects local details into
global features but also exploits implicit guidance about the spatio-temporal
position of an on-going action. We introduce novel symbiotic attention (SA) to
enable effective communication. It first normalizes the detection guided
features on one branch to underline the action-relevant information from the
other branch. SA adaptively enhances the interactions among the three sources.
To further catalyze this communication, spatial relations are uncovered for the
selection of most action-relevant information. It identifies the most valuable
and discriminative feature for classification. We validate the effectiveness of
our SAP quantitatively and qualitatively. Notably, it achieves the
state-of-the-art on two large-scale egocentric video datasets.
</summary>
    <author>
      <name>Xiaohan Wang</name>
    </author>
    <author>
      <name>Yu Wu</name>
    </author>
    <author>
      <name>Linchao Zhu</name>
    </author>
    <author>
      <name>Yi Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI-2020(Oral)</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.03137v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.03137v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.09951v1</id>
    <updated>2024-04-15T17: 24: 57Z</updated>
    <published>2024-04-15T17: 24: 57Z</published>
    <title>Unifying Global and Local Scene Entities Modelling for Precise Action
  Spotting</title>
    <summary>  Sports videos pose complex challenges, including cluttered backgrounds,
camera angle changes, small action-representing objects, and imbalanced action
class distribution. Existing methods for detecting actions in sports videos
heavily rely on global features, utilizing a backbone network as a black box
that encompasses the entire spatial frame. However, these approaches tend to
overlook the nuances of the scene and struggle with detecting actions that
occupy a small portion of the frame. In particular, they face difficulties when
dealing with action classes involving small objects, such as balls or
yellow/red cards in soccer, which only occupy a fraction of the screen space.
To address these challenges, we introduce a novel approach that analyzes and
models scene entities using an adaptive attention mechanism. Particularly, our
model disentangles the scene content into the global environment feature and
local relevant scene entities feature. To efficiently extract environmental
features while considering temporal information with less computational cost,
we propose the use of a 2D backbone network with a time-shift mechanism. To
accurately capture relevant scene entities, we employ a Vision-Language model
in conjunction with the adaptive attention mechanism. Our model has
demonstrated outstanding performance, securing the 1st place in the
SoccerNet-v2 Action Spotting, FineDiving, and FineGym challenge with a
substantial performance improvement of 1.6,
                            2.0, and 1.3 points in avg-mAP
compared to the runner-up methods. Furthermore, our approach offers
interpretability capabilities in contrast to other deep learning models, which
are often designed as black boxes. Our code and models are released at:
https: //github.com/Fsoft-AIC/unifying-global-local-feature.
</summary>
    <author>
      <name>Kim Hoang Tran</name>
    </author>
    <author>
      <name>Phuc Vuong Do</name>
    </author>
    <author>
      <name>Ngoc Quoc Ly</name>
    </author>
    <author>
      <name>Ngan Le</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IJCNN 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.09951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.09951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1704.07595v1</id>
    <updated>2017-04-25T09: 09: 00Z</updated>
    <published>2017-04-25T09: 09: 00Z</published>
    <title>Skeleton-based Action Recognition with Convolutional Neural Networks</title>
    <summary>  Current state-of-the-art approaches to skeleton-based action recognition are
mostly based on recurrent neural networks (RNN). In this paper, we propose a
novel convolutional neural networks (CNN) based framework for both action
classification and detection. Raw skeleton coordinates as well as skeleton
motion are fed directly into CNN for label prediction. A novel skeleton
transformer module is designed to rearrange and select important skeleton
joints automatically. With a simple 7-layer network, we obtain 89.3% accuracy
on validation set of the NTU RGB+D dataset. For action detection in untrimmed
videos, we develop a window proposal network to extract temporal segment
proposals, which are further classified within the same network. On the recent
PKU-MMD dataset, we achieve 93.7% mAP, surpassing the baseline by a large
margin.
</summary>
    <author>
      <name>Chao Li</name>
    </author>
    <author>
      <name>Qiaoyong Zhong</name>
    </author>
    <author>
      <name>Di Xie</name>
    </author>
    <author>
      <name>Shiliang Pu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2017.2678539</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2017.2678539" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICMEW 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.07595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.07595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2302.13564v1</id>
    <updated>2023-02-27T07: 48: 10Z</updated>
    <published>2023-02-27T07: 48: 10Z</published>
    <title>Visuo-Tactile-Based Slip Detection Using A Multi-Scale Temporal
  Convolution Network</title>
    <summary>  Humans can accurately determine whether the object in hand has slipped or not
by visual and tactile perception. However, it is still a challenge for robots
to detect in-hand object slip through visuo-tactile fusion. To address this
issue, a novel visuo-tactile fusion deep neural network is proposed to detect
slip, which is a time-dependent continuous action. By using the multi-scale
temporal convolution network (MS-TCN) to extract the temporal features of
visual and tactile data, the slip can be detected effectively. In this paper, a
7-dregree-of-freedom (7-DoF) robot manipulator equipped with a camera and a
tactile sensor is used for data collection on 50 daily objects with different
shapes, materials, sizes, and weights. Therefore, a dataset is built, where the
grasping data of 40 objects and 10 objects are used for network training and
testing, respectively. The detection accuracy is 96.96% based on the proposed
model. Also, the proposed model is compared with a visuo-tactile fusion deep
neural network (DNN) based on long short-term memory network (LSTM) on the
collected dataset and a public dataset using the GelSight tactile sensor. The
results demonstrate that the proposed model performs better on both dataset.
The proposed model can help robots grasp daily objects reliably. In addition,
it can be used in grasping force control, grasping policy generation and
dexterous manipulation.
</summary>
    <author>
      <name>Junli Gao</name>
    </author>
    <author>
      <name>Zhaoji Huang</name>
    </author>
    <author>
      <name>Zhaonian Tang</name>
    </author>
    <author>
      <name>Haitao Song</name>
    </author>
    <author>
      <name>Wenyu Liang</name>
    </author>
    <link href="http://arxiv.org/abs/2302.13564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.13564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2402.09272v2</id>
    <updated>2024-10-04T13: 15: 22Z</updated>
    <published>2024-02-14T15: 59: 24Z</published>
    <title>Insights and caveats from mining local and global temporal motifs in
  cryptocurrency transaction networks</title>
    <summary>  Distributed ledger technologies have opened up a wealth of fine-grained
transaction data from cryptocurrencies like Bitcoin and Ethereum. This allows
research into problems like anomaly detection, anti-money laundering, pattern
mining and activity clustering (where data from traditional currencies is
rarely available). The formalism of temporal networks offers a natural way of
representing this data and offers access to a wealth of metrics and models.
However, the large scale of the data presents a challenge using standard graph
analysis techniques. We use temporal motifs to analyse two Bitcoin datasets and
one NFT dataset, using sequences of three transactions and up to three users.
We show that the commonly used technique of simply counting temporal motifs
over all users and all time can give misleading conclusions. Here we also study
the motifs contributed by each user and discover that the motif distribution is
heavy-tailed and that the key players have diverse motif signatures. We study
the motifs that occur in different time periods and find events and anomalous
activity that cannot be seen just by a count on the whole dataset. Studying
motif completion time reveals dynamics driven by human behaviour as well as
algorithmic behaviour.
</summary>
    <author>
      <name>Naomi A. Arnold</name>
    </author>
    <author>
      <name>Peijie Zhong</name>
    </author>
    <author>
      <name>Cheick Tidiane Ba</name>
    </author>
    <author>
      <name>Ben Steer</name>
    </author>
    <author>
      <name>Raul Mondragon</name>
    </author>
    <author>
      <name>Felix Cuadrado</name>
    </author>
    <author>
      <name>Renaud Lambiotte</name>
    </author>
    <author>
      <name>Richard G. Clegg</name>
    </author>
    <link href="http://arxiv.org/abs/2402.09272v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.09272v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2004.11475v2</id>
    <updated>2020-05-19T17: 45: 25Z</updated>
    <published>2020-04-23T22: 20: 10Z</published>
    <title>Gabriella: An Online System for Real-Time Activity Detection in
  Untrimmed Security Videos</title>
    <summary>  Activity detection in security videos is a difficult problem due to multiple
factors such as large field of view, presence of multiple activities, varying
scales and viewpoints, and its untrimmed nature. The existing research in
activity detection is mainly focused on datasets, such as UCF-101, JHMDB,
THUMOS, and AVA, which partially address these issues. The requirement of
processing the security videos in real-time makes this even more challenging.
In this work we propose Gabriella, a real-time online system to perform
activity detection on untrimmed security videos. The proposed method consists
of three stages: tubelet extraction, activity classification, and online
tubelet merging. For tubelet extraction, we propose a localization network
which takes a video clip as input and spatio-temporally detects potential
foreground regions at multiple scales to generate action tubelets. We propose a
novel Patch-Dice loss to handle large variations in actor size. Our online
processing of videos at a clip level drastically reduces the computation time
in detecting activities. The detected tubelets are assigned activity class
scores by the classification network and merged together using our proposed
Tubelet-Merge Action-Split (TMAS) algorithm to form the final action
detections. The TMAS algorithm efficiently connects the tubelets in an online
fashion to generate action detections which are robust against varying length
activities. We perform our experiments on the VIRAT and MEVA (Multiview
Extended Video with Activities) datasets and demonstrate the effectiveness of
the proposed approach in terms of speed (~100 fps) and performance with
state-of-the-art results. The code and models will be made publicly available.
</summary>
    <author>
      <name>Mamshad Nayeem Rizve</name>
    </author>
    <author>
      <name>Ugur Demir</name>
    </author>
    <author>
      <name>Praveen Tirupattur</name>
    </author>
    <author>
      <name>Aayush Jung Rana</name>
    </author>
    <author>
      <name>Kevin Duarte</name>
    </author>
    <author>
      <name>Ishan Dave</name>
    </author>
    <author>
      <name>Yogesh Singh Rawat</name>
    </author>
    <author>
      <name>Mubarak Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.11475v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.11475v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2410.17136v2</id>
    <updated>2024-11-10T13: 45: 57Z</updated>
    <published>2024-10-22T16: 08: 09Z</published>
    <title>AlphaChimp: Tracking and Behavior Recognition of Chimpanzees</title>
    <summary>  Understanding non-human primate behavior is crucial for improving animal
welfare, modeling social behavior, and gaining insights into both distinctly
human and shared behaviors. Despite recent advances in computer vision,
automated analysis of primate behavior remains challenging due to the
complexity of their social interactions and the lack of specialized algorithms.
Existing methods often struggle with the nuanced behaviors and frequent
occlusions characteristic of primate social dynamics. This study aims to
develop an effective method for automated detection, tracking, and recognition
of chimpanzee behaviors in video footage. Here we show that our proposed
method, AlphaChimp, an end-to-end approach that simultaneously detects
chimpanzee positions and estimates behavior categories from videos,
significantly outperforms existing methods in behavior recognition. AlphaChimp
achieves approximately 10% higher tracking accuracy and a 20% improvement in
behavior recognition compared to state-of-the-art methods, particularly
excelling in the recognition of social behaviors. This superior performance
stems from AlphaChimp's innovative architecture, which integrates temporal
feature fusion with a Transformer-based self-attention mechanism, enabling more
effective capture and interpretation of complex social interactions among
chimpanzees. Our approach bridges the gap between computer vision and
primatology, enhancing technical capabilities and deepening our understanding
of primate communication and sociality. We release our code and models and hope
this will facilitate future research in animal social dynamics. This work
contributes to ethology, cognitive science, and artificial intelligence,
offering new perspectives on social intelligence.
</summary>
    <author>
      <name>Xiaoxuan Ma</name>
    </author>
    <author>
      <name>Yutang Lin</name>
    </author>
    <author>
      <name>Yuan Xu</name>
    </author>
    <author>
      <name>Stephan P. Kaufhold</name>
    </author>
    <author>
      <name>Jack Terwilliger</name>
    </author>
    <author>
      <name>Andres Meza</name>
    </author>
    <author>
      <name>Yixin Zhu</name>
    </author>
    <author>
      <name>Federico Rossano</name>
    </author>
    <author>
      <name>Yizhou Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An extension of ChimpACT [arXiv: 2310.16447
                            ], proposes AlphaChimp for
  tracking and behavior recognition of chimpanzees. arXiv admin note:
  substantial text overlap with arXiv: 2310.16447</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.17136v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.17136v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2203.06355v2</id>
    <updated>2023-05-11T04: 19: 51Z</updated>
    <published>2022-03-12T06: 19: 22Z</published>
    <title>EventFormer: AU Event Transformer for Facial Action Unit Event Detection</title>
    <summary>  Facial action units (AUs) play an indispensable role in human emotion
analysis. We observe that although AU-based high-level emotion analysis is
urgently needed by real-world applications, frame-level AU results provided by
previous works cannot be directly used for such analysis. Moreover, as AUs are
dynamic processes, the utilization of global temporal information is important
but has been gravely ignored in the literature. To this end, we propose
EventFormer for AU event detection, which is the first work directly detecting
AU events from a video sequence by viewing AU event detection as a multiple
class-specific sets prediction problem. Extensive experiments conducted on a
commonly used AU benchmark dataset, BP4D, show the superiority of EventFormer
under suitable metrics.
</summary>
    <author>
      <name>Yingjie Chen</name>
    </author>
    <author>
      <name>Jiarui Zhang</name>
    </author>
    <author>
      <name>Tao Wang</name>
    </author>
    <author>
      <name>Yun Liang</name>
    </author>
    <link href="http://arxiv.org/abs/2203.06355v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.06355v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2301.02307v2</id>
    <updated>2023-07-18T17: 29: 16Z</updated>
    <published>2023-01-05T21: 43: 19Z</published>
    <title>What You Say Is What You Show: Visual Narration Detection in
  Instructional Videos</title>
    <summary>  Narrated ''how-to'' videos have emerged as a promising data source for a wide
range of learning problems, from learning visual representations to training
robot policies. However, this data is extremely noisy, as the narrations do not
always describe the actions demonstrated in the video. To address this problem
we introduce the novel task of visual narration detection, which entails
determining whether a narration is visually depicted by the actions in the
video. We propose What You Say is What You Show (WYS^2), a method that
leverages multi-modal cues and pseudo-labeling to learn to detect visual
narrations with only weakly labeled data. Our model successfully detects visual
narrations in in-the-wild videos, outperforming strong baselines, and we
demonstrate its impact for state-of-the-art summarization and temporal
alignment of instructional videos.
</summary>
    <author>
      <name>Kumar Ashutosh</name>
    </author>
    <author>
      <name>Rohit Girdhar</name>
    </author>
    <author>
      <name>Lorenzo Torresani</name>
    </author>
    <author>
      <name>Kristen Grauman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.02307v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.02307v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2212.06206v2</id>
    <updated>2022-12-17T06: 29: 37Z</updated>
    <published>2022-12-12T19: 29: 07Z</published>
    <title>Contextual Explainable Video Representation: Human Perception-based
  Understanding</title>
    <summary>  Video understanding is a growing field and a subject of intense research,
which includes many interesting tasks to understanding both spatial and
temporal information, e.g., action detection, action recognition, video
captioning, video retrieval. One of the most challenging problems in video
understanding is dealing with feature extraction, i.e. extract contextual
visual representation from given untrimmed video due to the long and
complicated temporal structure of unconstrained videos. Different from existing
approaches, which apply a pre-trained backbone network as a black-box to
extract visual representation, our approach aims to extract the most contextual
information with an explainable mechanism. As we observed, humans typically
perceive a video through the interactions between three main factors, i.e., the
actors, the relevant objects, and the surrounding environment. Therefore, it is
very crucial to design a contextual explainable video representation extraction
that can capture each of such factors and model the relationships between them.
In this paper, we discuss approaches, that incorporate the human perception
process into modeling actors, objects, and the environment. We choose video
paragraph captioning and temporal action detection to illustrate the
effectiveness of human perception based-contextual representation in video
understanding. Source code is publicly available at
https: //github.com/UARK-AICV/Video_Representation.
</summary>
    <author>
      <name>Khoa Vo</name>
    </author>
    <author>
      <name>Kashu Yamazaki</name>
    </author>
    <author>
      <name>Phong X. Nguyen</name>
    </author>
    <author>
      <name>Phat Nguyen</name>
    </author>
    <author>
      <name>Khoa Luu</name>
    </author>
    <author>
      <name>Ngan Le</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in Asilomar Conference 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.06206v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.06206v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2502.10713v1</id>
    <updated>2025-02-15T08: 02: 38Z</updated>
    <published>2025-02-15T08: 02: 38Z</published>
    <title>Improving action segmentation via explicit similarity measurement</title>
    <summary>  Existing supervised action segmentation methods depend on the quality of
frame-wise classification using attention mechanisms or temporal convolutions
to capture temporal dependencies. Even boundary detection-based methods
primarily depend on the accuracy of an initial frame-wise classification, which
can overlook precise identification of segments and boundaries in case of
low-quality prediction. To address this problem, this paper proposes ASESM
(Action Segmentation via Explicit Similarity Measurement) to enhance the
segmentation accuracy by incorporating explicit similarity evaluation across
frames and predictions. Our supervised learning architecture uses frame-level
multi-resolution features as input to multiple Transformer encoders. The
resulting multiple frame-wise predictions are used for similarity voting to
obtain high quality initial prediction. We apply a newly proposed boundary
correction algorithm that operates based on feature similarity between
consecutive frames to adjust the boundary locations iteratively through the
learning process. The corrected prediction is then further refined through
multiple stages of temporal convolutions. As post-processing, we optionally
apply boundary correction again followed by a segment smoothing method that
removes outlier classes within segments using similarity measurement between
consecutive predictions. Additionally, we propose a fully unsupervised boundary
detection-correction algorithm that identifies segment boundaries based solely
on feature similarity without any training. Experiments on 50Salads, GTEA, and
Breakfast datasets show the effectiveness of both the supervised and
unsupervised algorithms. Code and models are made available on Github.
</summary>
    <author>
      <name>Kamel Aouaidjia</name>
    </author>
    <author>
      <name>Wenhao Zhang</name>
    </author>
    <author>
      <name>Aofan Li</name>
    </author>
    <author>
      <name>Chongsheng Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages,
                            5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.10713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.10713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2412.18287v1</id>
    <updated>2024-12-24T08: 48: 48Z</updated>
    <published>2024-12-24T08: 48: 48Z</published>
    <title>Semi-supervised Credit Card Fraud Detection via Attribute-Driven Graph
  Representation</title>
    <summary>  Credit card fraud incurs a considerable cost for both cardholders and issuing
banks. Contemporary methods apply machine learning-based classifiers to detect
fraudulent behavior from labeled transaction records. But labeled data are
usually a small proportion of billions of real transactions due to expensive
labeling costs, which implies that they do not well exploit many natural
features from unlabeled data. Therefore, we propose a semi-supervised graph
neural network for fraud detection. Specifically, we leverage transaction
records to construct a temporal transaction graph, which is composed of
temporal transactions (nodes) and interactions (edges) among them. Then we pass
messages among the nodes through a Gated Temporal Attention Network (GTAN) to
learn the transaction representation. We further model the fraud patterns
through risk propagation among transactions. The extensive experiments are
conducted on a real-world transaction dataset and two publicly available fraud
detection datasets. The result shows that our proposed method, namely GTAN,
outperforms other state-of-the-art baselines on three fraud detection datasets.
Semi-supervised experiments demonstrate the excellent fraud detection
performance of our model with only a tiny proportion of labeled data.
</summary>
    <author>
      <name>Sheng Xiang</name>
    </author>
    <author>
      <name>Mingzhi Zhu</name>
    </author>
    <author>
      <name>Dawei Cheng</name>
    </author>
    <author>
      <name>Enxia Li</name>
    </author>
    <author>
      <name>Ruihui Zhao</name>
    </author>
    <author>
      <name>Yi Ouyang</name>
    </author>
    <author>
      <name>Ling Chen</name>
    </author>
    <author>
      <name>Yefeng Zheng</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1609/aaai.v37i12.26702</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1609/aaai.v37i12.26702" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages,
                            5 figures, AAAI 2023, code:
  https: //github.com/AI4Risk/antifraud</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the AAAI Conference on Artificial Intelligence.
  Vol. 37. No. 12. 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2412.18287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.18287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1805.09554v2</id>
    <updated>2018-08-17T17: 39: 31Z</updated>
    <published>2018-05-24T08: 56: 19Z</published>
    <title>Hierarchical burst model for complex bursty dynamics</title>
    <summary>  Temporal inhomogeneities observed in various natural and social phenomena
have often been characterized in terms of scaling behaviors in the
autocorrelation function with a decaying exponent $\gamma$, the interevent time
distribution with a power-law exponent $\alpha$, and the burst size
distributions. Here the interevent time is defined as a time interval between
two consecutive events in the event sequence, and the burst size denotes the
number of events in a bursty train detected for a given time window. In order
to understand such temporal scaling behaviors implying a hierarchical temporal
structure, we devise a hierarchical burst model by assuming that each observed
event might be a consequence of the multi-level causal or decision-making
process. By studying our model analytically and numerically, we confirm the
scaling relation $\alpha+\gamma=2$, established for the uncorrelated interevent
times, despite of the existence of correlations between interevent times. Such
correlations between interevent times are supported by the stretched
exponential burst size distributions, for which we provide an analytic
argument. In addition, by imposing conditions for the ordering of events, we
observe an additional feature of log-periodic behavior in the autocorrelation
function. Our modeling approach for the hierarchical temporal structure can
help us better understand the underlying mechanisms behind complex bursty
dynamics showing temporal scaling behaviors.
</summary>
    <author>
      <name>Byoung-Hwa Lee</name>
    </author>
    <author>
      <name>Woo-Sung Jung</name>
    </author>
    <author>
      <name>Hang-Hyun Jo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.98.022316</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.98.022316" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,
                            5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. E 98,
                            022316 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.09554v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09554v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2108.04001v1</id>
    <updated>2021-08-09T12: 49: 48Z</updated>
    <published>2021-08-09T12: 49: 48Z</published>
    <title>Development of Human Motion Prediction Strategy using Inception Residual
  Block</title>
    <summary>  Human Motion Prediction is a crucial task in computer vision and robotics. It
has versatile application potentials such as in the area of human-robot
interactions, human action tracking for airport security systems, autonomous
car navigation, computer gaming to name a few. However, predicting human motion
based on past actions is an extremely challenging task due to the difficulties
in detecting spatial and temporal features correctly. To detect temporal
features in human poses, we propose an Inception Residual Block(IRB), due to
its inherent capability of processing multiple kernels to capture salient
features. Here, we propose to use multiple 1-D Convolution Neural Network (CNN)
with different kernel sizes and input sequence lengths and concatenate them to
get proper embedding. As kernels strides over different receptive fields, they
detect smaller and bigger salient features at multiple temporal scales. Our
main contribution is to propose a residual connection between input and the
output of the inception block to have a continuity between the previously
observed pose and the next predicted pose. With this proposed architecture, it
learns prior knowledge much better about human poses and we achieve much higher
prediction accuracy as detailed in the paper. Subsequently, we further propose
to feed the output of the inception residual block as an input to the Graph
Convolution Neural Network (GCN) due to its better spatial feature learning
capability. We perform a parametric analysis for better designing of our model
and subsequently, we evaluate our approach on the Human 3.6M dataset and
compare our short-term as well as long-term predictions with the state of the
art papers, where our model outperforms most of the pose results, the detailed
reasons of which have been elaborated in the paper.
</summary>
    <author>
      <name>Shekhar Gupta</name>
    </author>
    <author>
      <name>Gaurav Kumar Yadav</name>
    </author>
    <author>
      <name>G. C. Nandi</name>
    </author>
    <link href="http://arxiv.org/abs/2108.04001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.04001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2011.12050v1</id>
    <updated>2020-11-24T11: 58: 06Z</updated>
    <published>2020-11-24T11: 58: 06Z</published>
    <title>Role of inhibitory neurons in temporal correlations of critical and
  supercritical spontaneous activity</title>
    <summary>  Experimental and numerical results suggest that the brain can be viewed as a
system acting close to a critical point, as confirmed by scale-free
distributions of relevant quantities in a variety of different systems and
models. Less attention has received the investigation of the temporal
correlation functions in brain activity in different, healthy and pathological,
conditions. Here we perform this analysis by means of a model with short and
long-term plasticity which implements the novel feature of different recovery
rates for excitatory and inhibitory neurons, found experimentally. We evidence
the important role played by inhibitory neurons in the supercritical state: We
detect an unexpected oscillatory behaviour of the correlation decay, whose
frequency depends on the fraction of inhibitory neurons and their connectivity
degree. This behaviour can be rationalized by the observation that bursts in
activity become more frequent and with a smaller amplitude as inhibition
becomes more relevant.
</summary>
    <author>
      <name>Dario Raimo</name>
    </author>
    <author>
      <name>Alessandro Sarracino</name>
    </author>
    <author>
      <name>Lucilla de Arcangelis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2020.125555</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2020.125555" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages,
                            6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physica A,
                            125555 (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2011.12050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.12050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2209.00812v1</id>
    <updated>2022-09-02T04: 30: 47Z</updated>
    <published>2022-09-02T04: 30: 47Z</published>
    <title>Explainable AI for Android Malware Detection: Towards Understanding Why
  the Models Perform So Well?</title>
    <summary>  Machine learning (ML)-based Android malware detection has been one of the
most popular research topics in the mobile security community. An increasing
number of research studies have demonstrated that machine learning is an
effective and promising approach for malware detection, and some works have
even claimed that their proposed models could achieve 99\% detection accuracy,
leaving little room for further improvement. However, numerous prior studies
have suggested that unrealistic experimental designs bring substantial biases,
resulting in over-optimistic performance in malware detection. Unlike previous
research that examined the detection performance of ML classifiers to locate
the causes, this study employs Explainable AI (XAI) approaches to explore what
ML-based models learned during the training process, inspecting and
interpreting why ML-based malware classifiers perform so well under unrealistic
experimental settings. We discover that temporal sample inconsistency in the
training dataset brings over-optimistic classification performance (up to 99\%
F1 score and accuracy). Importantly, our results indicate that ML models
classify malware based on temporal differences between malware and benign,
rather than the actual malicious behaviors. Our evaluation also confirms the
fact that unrealistic experimental designs lead to not only unrealistic
detection performance but also poor reliability, posing a significant obstacle
to real-world applications. These findings suggest that XAI approaches should
be used to help practitioners/researchers better understand how do AI/ML models
(i.e., malware detection) work -- not just focusing on accuracy improvement.
</summary>
    <author>
      <name>Yue Liu</name>
    </author>
    <author>
      <name>Chakkrit Tantithamthavorn</name>
    </author>
    <author>
      <name>Li Li</name>
    </author>
    <author>
      <name>Yepang Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by the 33rd IEEE International Symposium on Software
  Reliability Engineering (ISSRE 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.00812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.00812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2405.08344v1</id>
    <updated>2024-05-14T06: 32: 40Z</updated>
    <published>2024-05-14T06: 32: 40Z</published>
    <title>No Time to Waste: Squeeze Time into Channel for Mobile Video
  Understanding</title>
    <summary>  Current architectures for video understanding mainly build upon 3D
convolutional blocks or 2D convolutions with additional operations for temporal
modeling. However, these methods all regard the temporal axis as a separate
dimension of the video sequence, which requires large computation and memory
budgets and thus limits their usage on mobile devices. In this paper, we
propose to squeeze the time axis of a video sequence into the channel dimension
and present a lightweight video recognition network, term as
\textit{SqueezeTime
                            }, for mobile video understanding. To enhance the temporal
modeling capability of the proposed network, we design a Channel-Time Learning
(CTL) Block to capture temporal dynamics of the sequence. This module has two
complementary branches, in which one branch is for temporal importance learning
and another branch with temporal position restoring capability is to enhance
inter-temporal object modeling ability. The proposed SqueezeTime is much
lightweight and fast with high accuracies for mobile video understanding.
Extensive experiments on various video recognition and action detection
benchmarks, i.e., Kinetics400, Kinetics600, HMDB51, AVA2.1 and THUMOS14,
demonstrate the superiority of our model. For example, our SqueezeTime achieves
$+1.2\%$ accuracy and $+80\%$ GPU throughput gain on Kinetics400 than prior
methods. Codes are publicly available at
https: //github.com/xinghaochen/SqueezeTime and
https: //github.com/mindspore-lab/models/tree/master/research/huawei-noah/SqueezeTime.
</summary>
    <author>
      <name>Yingjie Zhai</name>
    </author>
    <author>
      <name>Wenshuo Li</name>
    </author>
    <author>
      <name>Yehui Tang</name>
    </author>
    <author>
      <name>Xinghao Chen</name>
    </author>
    <author>
      <name>Yunhe Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2405.08344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.08344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1404.7464v2</id>
    <updated>2014-08-04T10: 04: 01Z</updated>
    <published>2014-04-29T19: 10: 06Z</published>
    <title>Temporal behavior of laser induced elastic plate resonances</title>
    <summary>  This paper investigates the dependence on Poisson's ratio of local plate
resonances in low attenuating materials. In our experiments, these resonances
are generated by a pulse laser source and detected with a heterodyne
interferometer measuring surface displacement normal to the plate. The laser
impact induces a set of resonances that are dominated by Zero Group Velocity
(ZGV) Lamb modes. For some Poisson's ratio, thickness-shear resonances are also
detected. These experiments confirm that the temporal decay of ZGV modes
follows a $t^{
                                -0.5
                            }$ law and show that the temporal decay of the thickness
resonances is much faster. Similar decays are obtained by numerical simulations
achieved with a finite difference code. A simple model is proposed to describe
the thickness resonances. It predicts that a thickness mode decays as
$t^{
                                -1.5
                            }$ for large times and that the resonance amplitude is proportional to
$D^{
                                -1.5
                            }$ where $D$ is the curvature of the dispersion curve $\omega(k)$ at
$k=0$. This curvature depends on the order of the mode and on the Poisson's
ratio, and it explains why some thickness resonances are well detected while
others are not.
</summary>
    <author>
      <name>Jérôme Laurent</name>
    </author>
    <author>
      <name>Daniel Royer</name>
    </author>
    <author>
      <name>Claire Prada</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.wavemoti.2014.04.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.wavemoti.2014.04.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,
                            7 figures. Wave Motion 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.7464v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.7464v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.class-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.class-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1810.04954v1</id>
    <updated>2018-10-11T11: 15: 17Z</updated>
    <published>2018-10-11T11: 15: 17Z</published>
    <title>Globally Continuous and Non-Markovian Activity Analysis from Videos</title>
    <summary>  Automatically recognizing activities in video is a classic problem in vision
and helps to understand behaviors, describe scenes and detect anomalies. We
propose an unsupervised method for such purposes. Given video data, we discover
recurring activity patterns that appear, peak, wane and disappear over time. By
using non-parametric Bayesian methods, we learn coupled spatial and temporal
patterns with minimum prior knowledge. To model the temporal changes of
patterns, previous works compute Markovian progressions or locally continuous
motifs whereas we model time in a globally continuous and non-Markovian way.
Visually, the patterns depict flows of major activities. Temporally, each
pattern has its own unique appearance-disappearance cycles. To compute compact
pattern representations, we also propose a hybrid sampling method. By combining
these patterns with detailed environment information, we interpret the
semantics of activities and report anomalies. Also, our method fits data better
and detects anomalies that were difficult to detect previously.
</summary>
    <author>
      <name>He Wang</name>
    </author>
    <author>
      <name>Carol O'Sullivan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-46454-1_32</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-46454-1_32" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint of our ECCV 2016 spotlight paper</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Wang H., O'Sullivan C. (2016) Globally Continuous and
  Non-Markovian Crowd Activity Analysis from Videos. In ECCV 2016. Lecture
  Notes in Computer Science, vol 9909. Springer</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1810.04954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.04954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2111.00626v1</id>
    <updated>2021-10-31T23: 50: 59Z</updated>
    <published>2021-10-31T23: 50: 59Z</published>
    <title>Intrusion Detection using Spatial-Temporal features based on Riemannian
  Manifold</title>
    <summary>  Network traffic data is a combination of different data bytes packets under
different network protocols. These traffic packets have complex time-varying
non-linear relationships. Existing state-of-the-art methods rise up to this
challenge by fusing features into multiple subsets based on correlations and
using hybrid classification techniques that extract spatial and temporal
characteristics. This often requires high computational cost and manual support
that limit them for real-time processing of network traffic. To address this,
we propose a new novel feature extraction method based on covariance matrices
that extract spatial-temporal characteristics of network traffic data for
detecting malicious network traffic behavior. The covariance matrices in our
proposed method not just naturally encode the mutual relationships between
different network traffic values but also have well-defined geometry that falls
in the Riemannian manifold. Riemannian manifold is embedded with distance
metrics that facilitate extracting discriminative features for detecting
malicious network traffic. We evaluated our model on NSL-KDD and UNSW-NB15
datasets and showed our proposed method significantly outperforms the
conventional method and other existing studies on the dataset.
</summary>
    <author>
      <name>Amardeep Singh</name>
    </author>
    <author>
      <name>Julian Jang-Jaccard</name>
    </author>
    <link href="http://arxiv.org/abs/2111.00626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.00626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2012.09344v2</id>
    <updated>2021-03-21T23: 23: 14Z</updated>
    <published>2020-12-17T01: 05: 50Z</published>
    <title>Machine Learning for Detecting Data Exfiltration: A Review</title>
    <summary>  Context: Research at the intersection of cybersecurity, Machine Learning
(ML), and Software Engineering (SE) has recently taken significant steps in
proposing countermeasures for detecting sophisticated data exfiltration
attacks. It is important to systematically review and synthesize the ML-based
data exfiltration countermeasures for building a body of knowledge on this
important topic. Objective: This paper aims at systematically reviewing
ML-based data exfiltration countermeasures to identify and classify ML
approaches, feature engineering techniques, evaluation datasets, and
performance metrics used for these countermeasures. This review also aims at
identifying gaps in research on ML-based data exfiltration countermeasures.
Method: We used a Systematic Literature Review (SLR) method to select and
review {
                                92
                            } papers. Results: The review has enabled us to (a) classify the ML
approaches used in the countermeasures into data-driven, and behaviour-driven
approaches, (b) categorize features into six types: behavioural, content-based,
statistical, syntactical, spatial and temporal, (c) classify the evaluation
datasets into simulated, synthesized, and real datasets and (d) identify 11
performance measures used by these studies. Conclusion: We conclude that: (i)
the integration of data-driven and behaviour-driven approaches should be
explored; (ii) There is a need of developing high quality and large size
evaluation datasets; (iii) Incremental ML model training should be incorporated
in countermeasures; (iv) resilience to adversarial learning should be
considered and explored during the development of countermeasures to avoid
poisoning attacks; and (v) the use of automated feature engineering should be
encouraged for efficiently detecting data exfiltration attacks.
</summary>
    <author>
      <name>Bushra Sabir</name>
    </author>
    <author>
      <name>Faheem Ullah</name>
    </author>
    <author>
      <name>M. Ali Babar</name>
    </author>
    <author>
      <name>Raj Gaire</name>
    </author>
    <link href="http://arxiv.org/abs/2012.09344v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.09344v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2405.11515v1</id>
    <updated>2024-05-19T10: 56: 25Z</updated>
    <published>2024-05-19T10: 56: 25Z</published>
    <title>Towards solving the origin of circular polarisation in FRB 20180301A</title>
    <summary>  Fast Radio Bursts (FRBs) are short-timescale transients of extragalactic
origin. The number of detected FRBs has grown dramatically since their
serendipitous discovery from archival data. Some FRBs have also been seen to
repeat. The polarimetric properties of repeating FRBs show diverse behaviour
and, at times, extreme polarimetric morphology, suggesting a complex
magneto-ionic circumburst environment for this class of FRB. The polarimetric
properties such as circular polarisation behaviour of FRBs are crucial for
understanding their surrounding magnetic-ionic environment. The circular
polarisation previously observed in some of the repeating FRB sources has been
attributed to propagation effects such as generalised Faraday rotation (GFR),
where conversion from linear to circular polarisation occurs due to the
non-circular modes of transmission in relativistic plasma. The discovery burst
from the repeating FRB$~$20180301A showed significant frequency-dependent
circular polarisation behaviour, which was initially speculated to be
instrumental due to a sidelobe detection. Here we revisit the properties given
the subsequent interferometric localisation of the burst, which indicates that
the burst was detected in the primary beam of the Parkes/Murriyang 20-cm
multibeam receiver. We develop a Bayesian Stokes-Q, U, and V fit method to
model the GFR effect, which is independent of the total polarised flux
parameter. Using the GFR model we show that the rotation measure (RM) estimated
is two orders of magnitude smaller and opposite sign ($\sim$28 rad$\,$m$^{
                                -2
                            }$)
than the previously reported value. We interpret the implication of the
circular polarisation on its local magnetic environment and reinterpret its
long-term temporal evolution in RM.
</summary>
    <author>
      <name>Pavan Uttarkar</name>
    </author>
    <author>
      <name>Ryan M. Shannon</name>
    </author>
    <author>
      <name>Marcus E. Lower</name>
    </author>
    <author>
      <name>Pravir Kumar</name>
    </author>
    <author>
      <name>Danny C. Price</name>
    </author>
    <author>
      <name>A. T. Deller</name>
    </author>
    <author>
      <name>K. Gourdji</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages,
                            9 figures, submitted to MNRAS</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.11515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.11515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2011.00449v2</id>
    <updated>2021-02-11T03: 16: 57Z</updated>
    <published>2020-11-01T08: 47: 33Z</published>
    <title>Improving Cyberbully Detection with User Interaction</title>
    <summary>  Cyberbullying, identified as intended and repeated online bullying behavior,
has become increasingly prevalent in the past few decades. Despite the
significant progress made thus far, the focus of most existing work on
cyberbullying detection lies in the independent content analysis of different
comments within a social media session. We argue that such leading notions of
analysis suffer from three key limitations: they overlook the temporal
correlations among different comments; they only consider the content within a
single comment rather than the topic coherence across comments; they remain
generic and exploit limited interactions between social media users. In this
work, we observe that user comments in the same session may be inherently
related, e.g., discussing similar topics, and their interaction may evolve over
time. We also show that modeling such topic coherence and temporal interaction
are critical to capture the repetitive characteristics of bullying behavior,
thus leading to better predicting performance. To achieve the goal, we first
construct a unified temporal graph for each social media session. Drawing on
recent advances in graph neural network, we then propose a principled
graph-based approach for modeling the temporal dynamics and topic coherence
throughout user interactions. We empirically evaluate the effectiveness of our
approach with the tasks of session-level bullying detection and comment-level
case study. Our code is released to public.
</summary>
    <author>
      <name>Suyu Ge</name>
    </author>
    <author>
      <name>Lu Cheng</name>
    </author>
    <author>
      <name>Huan Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3442381.3449828</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3442381.3449828" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The Web Conference 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.00449v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.00449v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/q-bio/0604033v1</id>
    <updated>2006-04-26T18: 59: 25Z</updated>
    <published>2006-04-26T18: 59: 25Z</published>
    <title>Online detection and sorting of extracellularly recorded action
  potentials in human medial temporal lobe recordings, in vivo</title>
    <summary>  Understanding the function of complex cortical circuits requires the
simultaneous recording of action potentials from many neurons in awake and
behaving animals. Practically, this can be achieved by extracellularly
recording from multiple brain sites using single wire electrodes. However, in
densely packed neural structures such as the human hippocampus, a single
electrode can record the activity of multiple neurons. Thus, analytic
techniques that differentiate action potentials of different neurons are
required. Offline spike sorting approaches are currently used to detect and
sort action potentials after finishing the experiment. Because the
opportunities to record from the human brain are relatively rare, it is
desirable to analyze large numbers of simultaneous recordings quickly using
online sorting and detection algorithms. In this way, the experiment can be
optimized for the particular response properties of the recorded neurons. Here
we present and evaluate a method that is capable of detecting and sorting
extracellular single-wire recordings in realtime. We demonstrate the utility of
the method by applying it to an extensive data set we acquired from
chronically-implanted depth electrodes in the hippocampus of human epilepsy
patients. This dataset is particularly challenging because it was recorded in a
noisy clinical environment. This method will allow the development of
closed-loop experiments, which immediately adapt the experimental stimuli
and/or tasks to the neural response observed.
</summary>
    <author>
      <name>Ueli Rutishauser</name>
    </author>
    <author>
      <name>Erin M. Schuman</name>
    </author>
    <author>
      <name>Adam N. Mamelak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jneumeth.2005.12.033</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jneumeth.2005.12.033" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 figures,
                            2 tables. Journal of Neuroscience Methods 2006 (in press).
  Journal of Neuroscience Methods,
                            2006 (in press)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J Neurosci Methods,
                            2006; 154(1-2): 204-224</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/q-bio/0604033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0604033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2205.00400v3</id>
    <updated>2024-05-03T15: 17: 12Z</updated>
    <published>2022-05-01T05: 30: 53Z</published>
    <title>Convex Combination Consistency between Neighbors for Weakly-supervised
  Action Localization</title>
    <summary>  Weakly-supervised temporal action localization (WTAL) intends to detect
action instances with only weak supervision, e.g., video-level labels. The
current~\textit{de facto
                            } pipeline locates action instances by thresholding and
grouping continuous high-score regions on temporal class activation sequences.
In this route, the capacity of the model to recognize the relationships between
adjacent snippets is of vital importance which determines the quality of the
action boundaries. However, it is error-prone since the variations between
adjacent snippets are typically subtle, and unfortunately this is overlooked in
the literature. To tackle the issue, we propose a novel WTAL approach named
Convex Combination Consistency between Neighbors (C$^3$BN). C$^3$BN consists of
two key ingredients: a micro data augmentation strategy that increases the
diversity in-between adjacent snippets by convex combination of adjacent
snippets, and a macro-micro consistency regularization that enforces the model
to be invariant to the transformations~\textit{w.r.t.
                            } video semantics, snippet
predictions, and snippet representations. Consequently, fine-grained patterns
in-between adjacent snippets are enforced to be explored, thereby resulting in
a more robust action boundary localization. Experimental results demonstrate
the effectiveness of C$^3$BN on top of various baselines for WTAL with
video-level and point-level supervisions. Code is at
https: //github.com/Qinying-Liu/C3BN.
</summary>
    <author>
      <name>Qinying Liu</name>
    </author>
    <author>
      <name>Zilei Wang</name>
    </author>
    <author>
      <name>Ruoxi Chen</name>
    </author>
    <author>
      <name>Zhilin Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICME2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.00400v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.00400v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1401.3046v1</id>
    <updated>2014-01-14T01: 50: 06Z</updated>
    <published>2014-01-14T01: 50: 06Z</published>
    <title>Investigating Cellular Automata Based Network Intrusion Detection System
  For Fixed Networks (NIDWCA)</title>
    <summary>  Network Intrusion Detection Systems (NIDS) are computer systems which monitor
a network with the aim of discerning malicious from benign activity on that
network. With the recent growth of the Internet such security limitations are
becoming more and more pressing. Most of the current network intrusion
detection systems relay on labeled training data. An Unsupervised CA based
anomaly detection technique that was trained with unlabelled data is capable of
detecting previously unseen attacks. This new approach, based on the Cellular
Automata classifier (CAC) with Genetic Algorithms (GA), is used to classify
program behavior as normal or intrusive. Parameters and evolution process for
CAC with GA are discussed in detail. This implementation considers both
temporal and spatial information of network connections in encoding the network
connection information into rules in NIDS. Preliminary experiments with KDD Cup
data set show that the CAC classifier with Genetic Algorithms can effectively
detect intrusive attacks and achieve a low false positive rate. Training a
NIDWCA (Network Intrusion Detection with Cellular Automata) classifier takes
significantly shorter time than any other conventional techniques.
</summary>
    <author>
      <name>Pokkuluri Kiran Sree</name>
    </author>
    <author>
      <name>Inampudi Ramesh Babu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICACTE.2008.159</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICACTE.2008.159" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2008 International Conference on Advanced Computer Theory and
  Engineering</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.3046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.3046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2207.02279v1</id>
    <updated>2022-07-05T19: 44: 34Z</updated>
    <published>2022-07-05T19: 44: 34Z</published>
    <title>Leveraging Trajectory Prediction for Pedestrian Video Anomaly Detection</title>
    <summary>  Video anomaly detection is a core problem in vision. Correctly detecting and
identifying anomalous behaviors in pedestrians from video data will enable
safety-critical applications such as surveillance, activity monitoring, and
human-robot interaction. In this paper, we propose to leverage trajectory
localization and prediction for unsupervised pedestrian anomaly event
detection. Different than previous reconstruction-based approaches, our
proposed framework rely on the prediction errors of normal and abnormal
pedestrian trajectories to detect anomalies spatially and temporally. We
present experimental results on real-world benchmark datasets on varying
timescales and show that our proposed trajectory-predictor-based anomaly
detection pipeline is effective and efficient at identifying anomalous
activities of pedestrians in videos. Code will be made available at
https: //github.com/akanuasiegbu/Leveraging-Trajectory-Prediction-for-Pedestrian-Video-Anomaly-Detection.
</summary>
    <author>
      <name>Asiegbu Miracle Kanu-Asiegbu</name>
    </author>
    <author>
      <name>Ram Vasudevan</name>
    </author>
    <author>
      <name>Xiaoxiao Du</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SSCI50451.2021.9660004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SSCI50451.2021.9660004" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to 2021 IEEE Symposium Series on Computational Intelligence
  (SSCI)</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.02279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.02279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2001.05979v1</id>
    <updated>2020-01-16T18: 26: 34Z</updated>
    <published>2020-01-16T18: 26: 34Z</published>
    <title>Contextual Sense Making by Fusing Scene Classification, Detections, and
  Events in Full Motion Video</title>
    <summary>  With the proliferation of imaging sensors, the volume of multi-modal imagery
far exceeds the ability of human analysts to adequately consume and exploit it.
Full motion video (FMV) possesses the extra challenge of containing large
amounts of redundant temporal data. We aim to address the needs of human
analysts to consume and exploit data given aerial FMV. We have investigated and
designed a system capable of detecting events and activities of interest that
deviate from the baseline patterns of observation given FMV feeds. We have
divided the problem into three tasks: (1) Context awareness, (2) object
cataloging, and (3) event detection. The goal of context awareness is to
constraint the problem of visual search and detection in video data. A custom
image classifier categorizes the scene with one or multiple labels to identify
the operating context and environment. This step helps reducing the semantic
search space of downstream tasks in order to increase their accuracy. The
second step is object cataloging, where an ensemble of object detectors locates
and labels any known objects found in the scene (people, vehicles, boats,
planes, buildings, etc.). Finally, context information and detections are sent
to the event detection engine to monitor for certain behaviors. A series of
analytics monitor the scene by tracking object counts, and object interactions.
If these object interactions are not declared to be commonly observed in the
current scene, the system will report, geolocate, and log the event. Events of
interest include identifying a gathering of people as a meeting and/or a crowd,
alerting when there are boats on a beach unloading cargo, increased count of
people entering a building, people getting in and/or out of vehicles of
interest, etc. We have applied our methods on data from different sensors at
different resolutions in a variety of geographical areas.
</summary>
    <author>
      <name>Marc Bosch</name>
    </author>
    <author>
      <name>Joseph Nassar</name>
    </author>
    <author>
      <name>Benjamin Ortiz</name>
    </author>
    <author>
      <name>Brendan Lammers</name>
    </author>
    <author>
      <name>David Lindenbaum</name>
    </author>
    <author>
      <name>John Wahl</name>
    </author>
    <author>
      <name>Robert Mangum</name>
    </author>
    <author>
      <name>Margaret Smith</name>
    </author>
    <link href="http://arxiv.org/abs/2001.05979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1908.04519v1</id>
    <updated>2019-08-13T07: 23: 44Z</updated>
    <published>2019-08-13T07: 23: 44Z</published>
    <title>Three Branches: Detecting Actions With Richer Features</title>
    <summary>  We present our three branch solutions for International Challenge on Activity
Recognition at CVPR2019. This model seeks to fuse richer information of global
video clip, short human attention and long-term human activity into a unified
model. We have participated in two tasks: Task A, the Kinetics challenge and
Task B, spatio-temporal action localization challenge. For Kinetics, we achieve
21.59% error rate. For the AVA challenge, our final model obtains 32.49% mAP on
the test sets, which outperforms all submissions to the AVA challenge at CVPR
2018 for more than 10% mAP. As the future work, we will introduce human
activity knowledge, which is a new dataset including key information of human
activity.
</summary>
    <author>
      <name>Jin Xia</name>
    </author>
    <author>
      <name>Jiajun Tang</name>
    </author>
    <author>
      <name>Cewu Lu</name>
    </author>
    <link href="http://arxiv.org/abs/1908.04519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.04519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2411.00881v1</id>
    <updated>2024-10-31T13: 58: 42Z</updated>
    <published>2024-10-31T13: 58: 42Z</published>
    <title>Technical Report for SoccerNet Challenge 2022 -- Replay Grounding Task</title>
    <summary>  In order to make full use of video information, we transform the replay
grounding problem into a video action location problem. We apply a unified
network Faster-TAD proposed by us for temporal action detection to get the
results of replay grounding. Finally, by observing the data distribution of the
training data, we refine the output of the model to get the final submission.
</summary>
    <author>
      <name>Shimin Chen</name>
    </author>
    <author>
      <name>Wei Li</name>
    </author>
    <author>
      <name>Jiaming Chu</name>
    </author>
    <author>
      <name>Chen Chen</name>
    </author>
    <author>
      <name>Chen Zhang</name>
    </author>
    <author>
      <name>Yandong Guo</name>
    </author>
    <link href="http://arxiv.org/abs/2411.00881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.00881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2306.07703v2</id>
    <updated>2023-08-24T10: 38: 39Z</updated>
    <published>2023-06-13T11: 41: 15Z</published>
    <title>E2E-LOAD: End-to-End Long-form Online Action Detection</title>
    <summary>  Recently, there has been a growing trend toward feature-based approaches for
Online Action Detection (OAD). However, these approaches have limitations due
to their fixed backbone design, which ignores the potential capability of a
trainable backbone. In this paper, we propose the first end-to-end OAD model,
termed E2E-LOAD, designed to address the major challenge of OAD, namely,
long-term understanding and efficient online reasoning. Specifically, our
proposed approach adopts an initial spatial model that is shared by all frames
and maintains a long sequence cache for inference at a low computational cost.
We also advocate an asymmetric spatial-temporal model for long-form and
short-form modeling effectively. Furthermore, we propose a novel and efficient
inference mechanism that accelerates heavy spatial-temporal exploration.
Extensive ablation studies and experiments demonstrate the effectiveness and
efficiency of our proposed method. Notably, we achieve 17.3 (+12.6) FPS for
end-to-end OAD with 72.4%~(+1.2%),
                            90.3%~(+0.7%), and 48.1%~(+26.0%) mAP on
THMOUS14, TVSeries, and HDD, respectively, which is 3x faster than previous
approaches. The source code will be made publicly available.
</summary>
    <author>
      <name>Shuqiang Cao</name>
    </author>
    <author>
      <name>Weixin Luo</name>
    </author>
    <author>
      <name>Bairui Wang</name>
    </author>
    <author>
      <name>Wei Zhang</name>
    </author>
    <author>
      <name>Lin Ma</name>
    </author>
    <link href="http://arxiv.org/abs/2306.07703v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.07703v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2407.18289v2</id>
    <updated>2024-08-05T06: 53: 43Z</updated>
    <published>2024-07-25T15: 18: 28Z</published>
    <title>MARINE: A Computer Vision Model for Detecting Rare Predator-Prey
  Interactions in Animal Videos</title>
    <summary>  Encounters between predator and prey play an essential role in ecosystems,
but their rarity makes them difficult to detect in video recordings. Although
advances in action recognition (AR) and temporal action detection (AD),
especially transformer-based models and vision foundation models, have achieved
high performance on human action datasets, animal videos remain relatively
under-researched. This thesis addresses this gap by proposing the model MARINE,
which utilizes motion-based frame selection designed for fast animal actions
and DINOv2 feature extraction with a trainable classification head for action
recognition. MARINE outperforms VideoMAE in identifying predator attacks in
videos of fish, both on a small and specific coral reef dataset (81.53\%
against 52.64\% accuracy), and on a subset of the more extensive Animal Kingdom
dataset (94.86\% against 83.14\% accuracy). In a multi-label setting on a
representative sample of Animal Kingdom, MARINE achieves 23.79\% mAP,
positioning it mid-field among existing benchmarks. Furthermore, in an AD task
on the coral reef dataset, MARINE achieves 80.78\% AP (against VideoMAE's
34.89\%) although at a lowered t-IoU threshold of 25\%. Therefore, despite room
for improvement, MARINE offers an effective starter framework to apply to AR
and AD tasks on animal recordings and thus contribute to the study of natural
ecosystems.
</summary>
    <author>
      <name>Zsófia Katona</name>
    </author>
    <author>
      <name>Seyed Sahand Mohammadi Ziabari</name>
    </author>
    <author>
      <name>Fatemeh Karimi Nejadasl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an MSc thesis by Zsofia Katona, supervised by the two other
  authors</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.18289v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.18289v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2109.03475v1</id>
    <updated>2021-09-08T07: 50: 47Z</updated>
    <published>2021-09-08T07: 50: 47Z</published>
    <title>A Bottom-up method Towards the Automatic and Objective Monitoring of
  Smoking Behavior In-the-wild using Wrist-mounted Inertial Sensors</title>
    <summary>  The consumption of tobacco has reached global epidemic proportions and is
characterized as the leading cause of death and illness. Among the different
ways of consuming tobacco (e.g., smokeless, cigars), smoking cigarettes is the
most widespread. In this paper, we present a two-step, bottom-up algorithm
towards the automatic and objective monitoring of cigarette-based, smoking
behavior during the day, using the 3D acceleration and orientation velocity
measurements from a commercial smartwatch. In the first step, our algorithm
performs the detection of individual smoking gestures (i.e., puffs) using an
artificial neural network with both convolutional and recurrent layers. In the
second step, we make use of the detected puff density to achieve the temporal
localization of smoking sessions that occur throughout the day. In the
experimental section we provide extended evaluation regarding each step of the
proposed algorithm, using our publicly available, realistic Smoking Event
Detection (SED) and Free-living Smoking Event Detection (SED-FL) datasets
recorded under semi-controlled and free-living conditions, respectively. In
particular, leave-one-subject-out (LOSO) experiments reveal an F1-score of
0.863 for the detection of puffs and an F1-score/Jaccard index equal to
0.878/0.604 towards the temporal localization of smoking sessions during the
day. Finally, to gain further insight, we also compare the puff detection part
of our algorithm with a similar approach found in the recent literature.
</summary>
    <author>
      <name>Athanasios Kirmizis</name>
    </author>
    <author>
      <name>Konstantinos Kyritsis</name>
    </author>
    <author>
      <name>Anastasios Delopoulos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/EMBC46164.2021.9630491</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/EMBC46164.2021.9630491" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Manuscript accepted to be published in the 43rd Annual International
  Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)
  2021. Acceptance date: 16-July-2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.03475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.03475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1711.04150v1</id>
    <updated>2017-11-11T15: 19: 27Z</updated>
    <published>2017-11-11T15: 19: 27Z</published>
    <title>STWalk: Learning Trajectory Representations in Temporal Graphs</title>
    <summary>  Analyzing the temporal behavior of nodes in time-varying graphs is useful for
many applications such as targeted advertising, community evolution and outlier
detection. In this paper, we present a novel approach, STWalk, for learning
trajectory representations of nodes in temporal graphs. The proposed framework
makes use of structural properties of graphs at current and previous time-steps
to learn effective node trajectory representations. STWalk performs random
walks on a graph at a given time step (called space-walk) as well as on graphs
from past time-steps (called time-walk) to capture the spatio-temporal behavior
of nodes. We propose two variants of STWalk to learn trajectory
representations. In one algorithm, we perform space-walk and time-walk as part
of a single step. In the other variant, we perform space-walk and time-walk
separately and combine the learned representations to get the final trajectory
embedding. Extensive experiments on three real-world temporal graph datasets
validate the effectiveness of the learned representations when compared to
three baseline methods. We also show the goodness of the learned trajectory
embeddings for change point detection, as well as demonstrate that arithmetic
operations on these trajectory representations yield interesting and
interpretable results.
</summary>
    <author>
      <name>Supriya Pandhre</name>
    </author>
    <author>
      <name>Himangi Mittal</name>
    </author>
    <author>
      <name>Manish Gupta</name>
    </author>
    <author>
      <name>Vineeth N Balasubramanian</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3152494.3152512</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3152494.3152512" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,
                            5 figures,
                            2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.04150v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04150v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2104.01198v1</id>
    <updated>2021-04-02T18: 59: 09Z</updated>
    <published>2021-04-02T18: 59: 09Z</published>
    <title>Beyond Short Clips: End-to-End Video-Level Learning with Collaborative
  Memories</title>
    <summary>  The standard way of training video models entails sampling at each iteration
a single clip from a video and optimizing the clip prediction with respect to
the video-level label. We argue that a single clip may not have enough temporal
coverage to exhibit the label to recognize, since video datasets are often
weakly labeled with categorical information but without dense temporal
annotations. Furthermore, optimizing the model over brief clips impedes its
ability to learn long-term temporal dependencies. To overcome these
limitations, we introduce a collaborative memory mechanism that encodes
information across multiple sampled clips of a video at each training
iteration. This enables the learning of long-range dependencies beyond a single
clip. We explore different design choices for the collaborative memory to ease
the optimization difficulties. Our proposed framework is end-to-end trainable
and significantly improves the accuracy of video classification at a negligible
computational overhead. Through extensive experiments, we demonstrate that our
framework generalizes to different video architectures and tasks, outperforming
the state of the art on both action recognition (e.g., Kinetics-400 &amp; 700,
Charades, Something-Something-V1) and action detection (e.g., AVA v2.1 &amp; v2.2).
</summary>
    <author>
      <name>Xitong Yang</name>
    </author>
    <author>
      <name>Haoqi Fan</name>
    </author>
    <author>
      <name>Lorenzo Torresani</name>
    </author>
    <author>
      <name>Larry Davis</name>
    </author>
    <author>
      <name>Heng Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.01198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.01198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2103.15662v1</id>
    <updated>2021-03-29T14: 37: 35Z</updated>
    <published>2021-03-29T14: 37: 35Z</published>
    <title>Unified Graph Structured Models for Video Understanding</title>
    <summary>  Accurate video understanding involves reasoning about the relationships
between actors, objects and their environment, often over long temporal
intervals. In this paper, we propose a message passing graph neural network
that explicitly models these spatio-temporal relations and can use explicit
representations of objects, when supervision is available, and implicit
representations otherwise. Our formulation generalises previous structured
models for video understanding, and allows us to study how different design
choices in graph structure and representation affect the model's performance.
We demonstrate our method on two different tasks requiring relational reasoning
in videos -- spatio-temporal action detection on AVA and UCF101-24, and video
scene graph classification on the recent Action Genome dataset -- and achieve
state-of-the-art results on all three datasets. Furthermore, we show
quantitatively and qualitatively how our method is able to more effectively
model relationships between relevant entities in the scene.
</summary>
    <author>
      <name>Anurag Arnab</name>
    </author>
    <author>
      <name>Chen Sun</name>
    </author>
    <author>
      <name>Cordelia Schmid</name>
    </author>
    <link href="http://arxiv.org/abs/2103.15662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.15662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2109.14306v1</id>
    <updated>2021-09-29T09: 43: 21Z</updated>
    <published>2021-09-29T09: 43: 21Z</published>
    <title>Three-Stream 3D/1D CNN for Fine-Grained Action Classification and
  Segmentation in Table Tennis</title>
    <summary>  This paper proposes a fusion method of modalities extracted from video
through a three-stream network with spatio-temporal and temporal convolutions
for fine-grained action classification in sport. It is applied to TTStroke-21
dataset which consists of untrimmed videos of table tennis games. The goal is
to detect and classify table tennis strokes in the videos, the first step of a
bigger scheme aiming at giving feedback to the players for improving their
performance. The three modalities are raw RGB data, the computed optical flow
and the estimated pose of the player. The network consists of three branches
with attention blocks. Features are fused at the latest stage of the network
using bilinear layers. Compared to previous approaches, the use of three
modalities allows faster convergence and better performances on both tasks:
classification of strokes with known temporal boundaries and joint segmentation
and classification. The pose is also further investigated in order to offer
richer feedback to the athletes.
</summary>
    <author>
      <name>Pierre-Etienne Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MPI-EVA</arxiv:affiliation>
    </author>
    <author>
      <name>Jenny Benois-Pineau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UB</arxiv:affiliation>
    </author>
    <author>
      <name>Renaud Péteri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIA</arxiv:affiliation>
    </author>
    <author>
      <name>Julien Morlier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UB</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3475722.3482793</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3475722.3482793" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MMSports '21, October 20,
                            2021, Virtual Event,
                            , Oct 2021, Chengdu,
  China</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.14306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.14306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1410.5099v1</id>
    <updated>2014-10-19T17: 50: 43Z</updated>
    <published>2014-10-19T17: 50: 43Z</published>
    <title>Identifying features in spike trains using binless similarity measures</title>
    <summary>  Neurons in the central nervous system communicate with each other with the
help of series of Action Potentials, or spike trains. Various studies have
shown that neurons encode information in different features of spike trains,
such as the fine temporal structure, mean firing rate, synchrony etc. An
important step in understanding the encoding of information by neurons, is to
obtain a reliable measure of correlation between different spike trains. In
this paper, two new binless similarity measures for spike trains are proposed.
The performance of the new measures are compared with some existing measures in
their ability to detect important features of spike trains, such as their
firing rate, sensitivity to bursts and common periods of silence and detecting
synchronous activity.
</summary>
    <author>
      <name>Shubhanshu Shekhar</name>
    </author>
    <author>
      <name>Kaushik Majumdar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages,
                            4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.5099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.5099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1812.05788v2</id>
    <updated>2019-08-25T11: 56: 47Z</updated>
    <published>2018-12-14T05: 23: 49Z</published>
    <title>AU R-CNN: Encoding Expert Prior Knowledge into R-CNN for Action Unit
  Detection</title>
    <summary>  Detecting action units (AUs) on human faces is challenging because various
AUs make subtle facial appearance change over various regions at different
scales. Current works have attempted to recognize AUs by emphasizing important
regions. However, the incorporation of expert prior knowledge into region
definition remains under-exploited, and current AU detection approaches do not
use regional convolutional neural networks (R-CNN) with expert prior knowledge
to directly focus on AU-related regions adaptively. By incorporating expert
prior knowledge, we propose a novel R-CNN based model named AU R-CNN. The
proposed solution offers two main contributions: (1) AU R-CNN directly observes
different facial regions, where various AUs are located. Specifically, we
define an AU partition rule which encodes the expert prior knowledge into the
region definition and RoI-level label definition. This design produces
considerably better detection performance than existing approaches. (2) We
integrate various dynamic models (including convolutional long short-term
memory, two stream network, conditional random field, and temporal action
localization network) into AU R-CNN and then investigate and analyze the reason
behind the performance of dynamic models. Experiment results demonstrate that
\textit{only
                            } static RGB image information and no optical flow-based AU R-CNN
surpasses the one fused with dynamic models. AU R-CNN is also superior to
traditional CNNs that use the same backbone on varying image resolutions.
State-of-the-art recognition performance of AU detection is achieved. The
complete network is end-to-end trainable. Experiments on BP4D and DISFA
datasets show the effectiveness of our approach. The implementation code is
available online.
</summary>
    <author>
      <name>Chen Ma</name>
    </author>
    <author>
      <name>Li Chen</name>
    </author>
    <author>
      <name>Junhai Yong</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neucom.2019.03.082</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neucom.2019.03.082" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages,
                            10 figures, published on Neurocomputing</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neurocomputing 355 (2019) 35-47</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.05788v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.05788v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="2010" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2108.13486v1</id>
    <updated>2021-08-30T19: 16: 00Z</updated>
    <published>2021-08-30T19: 16: 00Z</published>
    <title>Automated Tracking of Primate Behavior</title>
    <summary>  Understanding primate behavior is a mission-critical goal of both biology and
biomedicine. Despite the importance of behavior, our ability to rigorously
quantify it has heretofore been limited to low-information measures like
preference, looking time, and reaction time, or to non-scaleable measures like
ethograms. However, recent technological advances have led to a major
revolution in behavioral measurement. Specifically, digital video cameras and
automated pose tracking software can provide detailed measures of full body
position (i.e., pose) of multiple primates over time (i.e., behavior) with high
spatial and temporal resolution. Pose-tracking technology in turn can be used
to detect behavioral states, such as eating, sleeping, and mating. The
availability of such data has in turn spurred developments in data analysis
techniques. Together, these changes are poised to lead to major advances in
scientific fields that rely on behavioral as a dependent variable. In this
review, we situate the tracking revolution in the history of the study of
behavior, argue for investment in and development of analytical and research
techniques that can profit from the advent of the era of big behavior, and
propose that zoos will have a central role to play in this era.
</summary>
    <author>
      <name>Benjamin Hayden</name>
    </author>
    <author>
      <name>Hyun Soo Park</name>
    </author>
    <author>
      <name>Jan Zimmermann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Invited manuscript to AJP</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.13486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.13486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2309.03724v1</id>
    <updated>2023-09-07T14: 06: 15Z</updated>
    <published>2023-09-07T14: 06: 15Z</published>
    <title>HSTF-Model: an HTTP-based Trojan Detection Model via the Hierarchical
  Spatio-Temporal Features of Traffics</title>
    <summary>  HTTP-based Trojan is extremely threatening, and it is difficult to be
effectively detected because of its concealment and confusion. Previous
detection methods usually are with poor generalization ability due to outdated
datasets and reliance on manual feature extraction, which makes these methods
always perform well under their private dataset, but poorly or even fail to
work in real network environment. In this paper, we propose an HTTP-based
Trojan detection model via the Hierarchical Spatio-Temporal Features of
traffics (HSTF-Model) based on the formalized description of traffic
spatio-temporal behavior from both packet level and flow level. In this model,
we employ Convolutional Neural Network (CNN) to extract spatial information and
Long Short-Term Memory (LSTM) to extract temporal information. In addition, we
present a dataset consisting of Benign and Trojan HTTP Traffic (BTHT-2018).
Experimental results show that our model can guarantee high accuracy (the F1 of
98.62%-99.81% and the FPR of 0.34%-0.02% in BTHT-2018). More importantly, our
model has a huge advantage over other related methods in generalization
ability. HSTF-Model trained with BTHT-2018 can reach the F1 of 93.51% on the
public dataset ISCX-2012, which is 20+% better than the best of related machine
learning methods.
</summary>
    <author>
      <name>Jiang Xie</name>
    </author>
    <author>
      <name>Shuhao Lia</name>
    </author>
    <author>
      <name>Xiaochun Yun</name>
    </author>
    <author>
      <name>Yongzheng Zhang</name>
    </author>
    <author>
      <name>Peng Chang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cose.2020.101923</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cose.2020.101923" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages,
                            11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.03724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.03724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1612.05489v1</id>
    <updated>2016-12-16T14: 40: 43Z</updated>
    <published>2016-12-16T14: 40: 43Z</published>
    <title>On-bird Sound Recordings: Automatic Acoustic Recognition of Activities
  and Contexts</title>
    <summary>  We introduce a novel approach to studying animal behaviour and the context in
which it occurs, through the use of microphone backpacks carried on the backs
of individual free-flying birds. These sensors are increasingly used by animal
behaviour researchers to study individual vocalisations of freely behaving
animals, even in the field. However such devices may record more than an
animals vocal behaviour, and have the potential to be used for investigating
specific activities (movement) and context (background) within which
vocalisations occur. To facilitate this approach, we investigate the automatic
annotation of such recordings through two different sound scene analysis
paradigms: a scene-classification method using feature learning, and an
event-detection method using probabilistic latent component analysis (PLCA). We
analyse recordings made with Eurasian jackdaws (Corvus monedula) in both
captive and field settings. Results are comparable with the state of the art in
sound scene analysis; we find that the current recognition quality level
enables scalable automatic annotation of audio logger data, given partial
annotation, but also find that individual differences between animals and/or
their backpacks limit the generalisation from one individual to another. we
consider the interrelation of 'scenes' and 'events' in this particular task,
and issues of temporal resolution.
</summary>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Emmanouil Benetos</name>
    </author>
    <author>
      <name>Lisa F. Gill</name>
    </author>
    <link href="http://arxiv.org/abs/1612.05489v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05489v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1904.02356v1</id>
    <updated>2019-04-04T05: 30: 34Z</updated>
    <published>2019-04-04T05: 30: 34Z</published>
    <title>Constant Angular Velocity Regulation for Visually Guided Terrain
  Following</title>
    <summary>  Insects use visual cues to control their flight behaviours. By estimating the
angular velocity of the visual stimuli and regulating it to a constant value,
honeybees can perform a terrain following task which keeps the certain height
above the undulated ground. For mimicking this behaviour in a bio-plausible
computation structure, this paper presents a new angular velocity decoding
model based on the honeybee's behavioural experiments. The model consists of
three parts, the texture estimation layer for spatial information extraction,
the motion detection layer for temporal information extraction and the decoding
layer combining information from pervious layers to estimate the angular
velocity. Compared to previous methods on this field, the proposed model
produces responses largely independent of the spatial frequency and contrast in
grating experiments. The angular velocity based control scheme is proposed to
implement the model into a bee simulated by the game engine Unity. The perfect
terrain following above patterned ground and successfully flying over irregular
textured terrain show its potential for micro unmanned aerial vehicles' terrain
following.
</summary>
    <author>
      <name>Huatian Wang</name>
    </author>
    <author>
      <name>Qinbing Fu</name>
    </author>
    <author>
      <name>Hongxin Wang</name>
    </author>
    <author>
      <name>Jigen Peng</name>
    </author>
    <author>
      <name>Shigang Yue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages,
                            7 figures, conference, Springer format</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.02356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.02356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.06526v1</id>
    <updated>2025-03-09T09: 11: 26Z</updated>
    <published>2025-03-09T09: 11: 26Z</published>
    <title>TimeLoc: A Unified End-to-End Framework for Precise Timestamp
  Localization in Long Videos</title>
    <summary>  Temporal localization in untrimmed videos, which aims to identify specific
timestamps, is crucial for video understanding but remains challenging. This
task encompasses several subtasks, including temporal action localization,
temporal video grounding, moment retrieval, and generic event boundary
detection. Existing methods in each subfield are typically designed for
specific tasks and lack generalizability across domains. In this paper, we
propose TimeLoc, a unified end-to-end framework for timestamp localization that
can handle multiple tasks. First, our approach employs a simple yet effective
one-stage localization model that supports text queries as input and multiple
actions as output. Second, we jointly train the video encoder and localization
model in an end-to-end manner. To efficiently process long videos, we introduce
temporal chunking, enabling the handling of videos with over 30k frames. Third,
we find that fine-tuning pre-trained text encoders with a multi-stage training
strategy further enhances text-conditioned localization. TimeLoc achieves
state-of-the-art results across multiple benchmarks: +1.3% and +1.9% mAP over
previous best methods on THUMOS14 and EPIC-Kitchens-100, +1.1% on
Kinetics-GEBD, +2.94% mAP on QVHighlights, and significant improvements in
temporal video grounding (+11.5% on TACoS and +6.7% on Charades-STA under
R1@0.5). Our code and checkpoints will be released at
https: //github.com/sming256/TimeLoc.
</summary>
    <author>
      <name>Chen-Lin Zhang</name>
    </author>
    <author>
      <name>Lin Sui</name>
    </author>
    <author>
      <name>Shuming Liu</name>
    </author>
    <author>
      <name>Fangzhou Mu</name>
    </author>
    <author>
      <name>Zhangcheng Wang</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code &amp; models will be released at
  https: //github.com/sming256/TimeLoc. The first 4 authors contributes equally</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.06526v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.06526v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2009.09253v1</id>
    <updated>2020-09-19T15: 16: 28Z</updated>
    <published>2020-09-19T15: 16: 28Z</published>
    <title>Understanding the Spatio-temporal Topic Dynamics of Covid-19 using
  Nonnegative Tensor Factorization: A Case Study</title>
    <summary>  Social media platforms facilitate mankind a data-driven world by enabling
billions of people to share their thoughts and activities ubiquitously. This
huge collection of data, if analysed properly, can provide useful insights into
people's behavior. More than ever, now is a crucial time under the Covid-19
pandemic to understand people's online behaviors detailing what topics are
being discussed, and where (space) and when (time) they are discussed. Given
the high complexity and poor quality of the huge social media data, an
effective spatio-temporal topic detection method is needed. This paper proposes
a tensor-based representation of social media data and Non-negative Tensor
Factorization (NTF) to identify the topics discussed in social media data along
with the spatio-temporal topic dynamics. A case study on Covid-19 related
tweets from the Australia Twittersphere is presented to identify and visualize
spatio-temporal topic dynamics on Covid-19
</summary>
    <author>
      <name>Thirunavukarasu Balasubramaniam</name>
    </author>
    <author>
      <name>Richi Nayak</name>
    </author>
    <author>
      <name>Md Abul Bashar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in 18th Australasian Data Mining Conference (AusDM)</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.09253v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09253v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2209.13116v1</id>
    <updated>2022-09-27T02: 19: 31Z</updated>
    <published>2022-09-27T02: 19: 31Z</published>
    <title>Spatio-Temporal Relation Learning for Video Anomaly Detection</title>
    <summary>  Anomaly identification is highly dependent on the relationship between the
object and the scene, as different/same object actions in same/different scenes
may lead to various degrees of normality and anomaly. Therefore, object-scene
relation actually plays a crucial role in anomaly detection but is inadequately
explored in previous works. In this paper, we propose a Spatial-Temporal
Relation Learning (STRL) framework to tackle the video anomaly detection task.
First, considering dynamic characteristics of the objects as well as scene
areas, we construct a Spatio-Temporal Auto-Encoder (STAE) to jointly exploit
spatial and temporal evolution patterns for representation learning. For better
pattern extraction, two decoding branches are designed in the STAE module, i.e.
an appearance branch capturing spatial cues by directly predicting the next
frame, and a motion branch focusing on modeling the dynamics via optical flow
prediction. Then, to well concretize the object-scene relation, a Relation
Learning (RL) module is devised to analyze and summarize the normal relations
by introducing the Knowledge Graph Embedding methodology. Specifically in this
process, the plausibility of object-scene relation is measured by jointly
modeling object/scene features and optimizable object-scene relation maps.
Extensive experiments are conducted on three public datasets, and the superior
performance over the state-of-the-art methods demonstrates the effectiveness of
our method.
</summary>
    <author>
      <name>Hui Lv</name>
    </author>
    <author>
      <name>Zhen Cui</name>
    </author>
    <author>
      <name>Biao Wang</name>
    </author>
    <author>
      <name>Jian Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,
                            5 figures,Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.13116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.13116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2405.08359v2</id>
    <updated>2024-12-17T17: 31: 46Z</updated>
    <published>2024-05-14T06: 55: 16Z</published>
    <title>GPS-IDS: An Anomaly-based GPS Spoofing Attack Detection Framework for
  Autonomous Vehicles</title>
    <summary>  Autonomous Vehicles (AVs) heavily rely on sensors and communication networks
like Global Positioning System (GPS) to navigate autonomously. Prior research
has indicated that networks like GPS are vulnerable to cyber-attacks such as
spoofing and jamming, thus posing serious risks like navigation errors and
system failures. These threats are expected to intensify with the widespread
deployment of AVs, making it crucial to detect and mitigate such attacks. This
paper proposes GPS Intrusion Detection System, or GPS-IDS, an Anomaly-based
intrusion detection framework to detect GPS spoofing attacks on AVs. The
framework uses a novel physics-based vehicle behavior model where a GPS
navigation model is integrated into the conventional dynamic bicycle model for
accurate AV behavior representation. Temporal features derived from this
behavior model are analyzed using machine learning to detect normal and
abnormal navigation behaviors. The performance of the GPS-IDS framework is
evaluated on the AV-GPS-Dataset -- a GPS security dataset for AVs comprising
real-world data collected using an AV testbed, and simulated data representing
urban traffic environments. To the best of our knowledge, this dataset is the
first of its kind and has been publicly released for the global research
community to address such security challenges.
</summary>
    <author>
      <name>Murad Mehrab Abrar</name>
    </author>
    <author>
      <name>Amal Youssef</name>
    </author>
    <author>
      <name>Raian Islam</name>
    </author>
    <author>
      <name>Shalaka Satam</name>
    </author>
    <author>
      <name>Banafsheh Saber Latibari</name>
    </author>
    <author>
      <name>Salim Hariri</name>
    </author>
    <author>
      <name>Sicong Shao</name>
    </author>
    <author>
      <name>Soheil Salehi</name>
    </author>
    <author>
      <name>Pratik Satam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Article under review at IEEE Transactions on Dependable and Secure
  Computing. For associated AV-GPS-Dataset, see
  https: //github.com/mehrab-abrar/AV-GPS-Dataset</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.08359v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.08359v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1812.07667v1</id>
    <updated>2018-12-18T22: 20: 37Z</updated>
    <published>2018-12-18T22: 20: 37Z</published>
    <title>GD-GAN: Generative Adversarial Networks for Trajectory Prediction and
  Group Detection in Crowds</title>
    <summary>  This paper presents a novel deep learning framework for human trajectory
prediction and detecting social group membership in crowds. We introduce a
generative adversarial pipeline which preserves the spatio-temporal structure
of the pedestrian's neighbourhood, enabling us to extract relevant attributes
describing their social identity. We formulate the group detection task as an
unsupervised learning problem, obviating the need for supervised learning of
group memberships via hand labeled databases, allowing us to directly employ
the proposed framework in different surveillance settings. We evaluate the
proposed trajectory prediction and group detection frameworks on multiple
public benchmarks, and for both tasks the proposed method demonstrates its
capability to better anticipate human sociological behaviour compared to the
existing state-of-the-art methods.
</summary>
    <author>
      <name>Tharindu Fernando</name>
    </author>
    <author>
      <name>Simon Denman</name>
    </author>
    <author>
      <name>Sridha Sridharan</name>
    </author>
    <author>
      <name>Clinton Fookes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in ACCV 2108</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.07667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.07667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1906.02524v1</id>
    <updated>2019-06-06T11: 25: 57Z</updated>
    <published>2019-06-06T11: 25: 57Z</published>
    <title>Degree-based Outlier Detection within IP Traffic Modelled as a Link
  Stream</title>
    <summary>  This paper aims at precisely detecting and identifying anomalous events in IP
traffic. To this end, we adopt the link stream formalism which properly
captures temporal and structural features of the data. Within this framework,
we focus on finding anomalous behaviours with respect to the degree of IP
addresses over time. Due to diversity in IP profiles, this feature is typically
distributed heterogeneously, preventing us to directly find anomalies. To deal
with this challenge, we design a method to detect outliers as well as precisely
identify their cause in a sequence of similar heterogeneous distributions. We
apply it to several MAWI captures of IP traffic and we show that it succeeds in
detecting relevant patterns in terms of anomalous network activity.
</summary>
    <author>
      <name>Audrey Wilmet</name>
    </author>
    <author>
      <name>Tiphaine Viard</name>
    </author>
    <author>
      <name>Matthieu Latapy</name>
    </author>
    <author>
      <name>Robin Lamarche-Perrin</name>
    </author>
    <link href="http://arxiv.org/abs/1906.02524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.02524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1907.04098v1</id>
    <updated>2019-07-09T11: 46: 55Z</updated>
    <published>2019-07-09T11: 46: 55Z</published>
    <title>Using Temporal and Topological Features for Intrusion Detection in
  Operational Networks</title>
    <summary>  Until two decades ago, industrial networks were deemed secure due to physical
separation from public networks. An abundance of successful attacks proved that
assumption wrong. Intrusion detection solutions for industrial application need
to meet certain requirements that differ from home- and office-environments,
such as working without feedback to the process and compatibility with legacy
systems. Industrial systems are commonly used for several decades, updates are
often difficult and expensive. Furthermore, most industrial protocols do not
have inherent authentication or encryption mechanisms, allowing for easy
lateral movement of an intruder once the perimeter is breached. In this work,
an algorithm for motif discovery in time series, Matrix Profiles, is used to
detect outliers in the timing behaviour of an industrial process. This process
was monitored in an experimental environment, containing ground truth labels
after attacks were performed. Furthermore, the graph representations of a
different industrial data set that has been emulated are used to detect
malicious activities. These activities can be derived from anomalous
communication patterns, represented as edges in the graph. Finally, an
integration concept for both methods is proposed.
</summary>
    <author>
      <name>Simon D. Duque Anton</name>
    </author>
    <author>
      <name>Daniel Fraunholz</name>
    </author>
    <author>
      <name>Hans Dieter Schotten</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3339252.3341476</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3339252.3341476" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint of a work accepted but not published yet at the ARES 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.04098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1805.02031v1</id>
    <updated>2018-05-05T09: 52: 36Z</updated>
    <published>2018-05-05T09: 52: 36Z</published>
    <title>Weakly-supervised Visual Instrument-playing Action Detection in Videos</title>
    <summary>  Instrument playing is among the most common scenes in music-related videos,
which represent nowadays one of the largest sources of online videos. In order
to understand the instrument-playing scenes in the videos, it is important to
know what instruments are played, when they are played, and where the playing
actions occur in the scene. While audio-based recognition of instruments has
been widely studied, the visual aspect of the music instrument playing remains
largely unaddressed in the literature. One of the main obstacles is the
difficulty in collecting annotated data of the action locations for
training-based methods. To address this issue, we propose a weakly-supervised
framework to find when and where the instruments are played in the videos. We
propose to use two auxiliary models, a sound model and an object model, to
provide supervisions for training the instrument-playing action model. The
sound model provides temporal supervisions, while the object model provides
spatial supervisions. They together can simultaneously provide temporal and
spatial supervisions. The resulted model only needs to analyze the visual part
of a music video to deduce which, when and where instruments are played. We
found that the proposed method significantly improves the localization
accuracy. We evaluate the result of the proposed method temporally and
spatially on a small dataset (totally 5,
                            400 frames) that we manually annotated.
</summary>
    <author>
      <name>Jen-Yu Liu</name>
    </author>
    <author>
      <name>Yi-Hsuan Yang</name>
    </author>
    <author>
      <name>Shyh-Kang Jeng</name>
    </author>
    <link href="http://arxiv.org/abs/1805.02031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1904.04231v1</id>
    <updated>2019-04-08T17: 57: 27Z</updated>
    <published>2019-04-08T17: 57: 27Z</published>
    <title>Relational Action Forecasting</title>
    <summary>  This paper focuses on multi-person action forecasting in videos. More
precisely, given a history of H previous frames, the goal is to detect actors
and to predict their future actions for the next T frames. Our approach jointly
models temporal and spatial interactions among different actors by constructing
a recurrent graph, using actor proposals obtained with Faster R-CNN as nodes.
Our method learns to select a subset of discriminative relations without
requiring explicit supervision, thus enabling us to tackle challenging visual
data. We refer to our model as Discriminative Relational Recurrent Network
(DRRN). Evaluation of action prediction on AVA demonstrates the effectiveness
of our proposed method compared to simpler baselines. Furthermore, we
significantly improve performance on the task of early action classification on
J-HMDB, from the previous SOTA of 48% to 60%.
</summary>
    <author>
      <name>Chen Sun</name>
    </author>
    <author>
      <name>Abhinav Shrivastava</name>
    </author>
    <author>
      <name>Carl Vondrick</name>
    </author>
    <author>
      <name>Rahul Sukthankar</name>
    </author>
    <author>
      <name>Kevin Murphy</name>
    </author>
    <author>
      <name>Cordelia Schmid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2019 (oral)</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.04231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.04231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2407.16945v3</id>
    <updated>2025-03-13T03: 12: 34Z</updated>
    <published>2024-07-24T02: 24: 21Z</published>
    <title>Affective Behaviour Analysis via Progressive Learning</title>
    <summary>  Affective Behavior Analysis aims to develop emotionally intelligent
technology that can recognize and respond to human emotions. To advance this
field, the 7th Affective Behavior Analysis in-the-wild (ABAW) competition holds
the Multi-Task Learning Challenge based on the s-Aff-Wild2 database. The
participants are required to develop a framework that achieves Valence-Arousal
Estimation, Expression Recognition, and AU detection simultaneously. To achieve
this goal, we propose a progressive multi-task learning framework that fully
leverages the distinct focuses of each task on facial emotion features.
Specifically, our method design can be summarized into three main aspects: 1)
Separate Training and Joint Training: We first train each task model separately
and then perform joint training based on the pre-trained models, fully
utilizing the feature focus aspects of each task to improve the overall
framework performance. 2) Feature Fusion and Temporal Modeling:
                        } We investigate
effective strategies for fusing features extracted from each task-specific
model and incorporate temporal feature modeling during the joint training
phase, which further refines the performance of each task. 3) Joint Training
Strategy Optimization: To identify the optimal joint training approach, we
conduct a comprehensive strategy search, experimenting with various task
combinations and training methodologies to further elevate the overall
performance of each task. According to the official results, our team achieves
first place in the MTL challenge with a total score of 1.5286 (i.e., AU F-score
0.5580, Expression F-score 0.4286, CCC VA score 0.5420). Our code is publicly
available at https: //github.com/YenanLiu/ABAW7th.
</summary>
    <author>
      <name>Chen Liu</name>
    </author>
    <author>
      <name>Wei Zhang</name>
    </author>
    <author>
      <name>Feng Qiu</name>
    </author>
    <author>
      <name>Lincheng Li</name>
    </author>
    <author>
      <name>Xin Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ECCVWorkshop as the report of the first place in 7th ABAW
  Track1 Competition</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.16945v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.16945v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1412.3594v3</id>
    <updated>2015-06-20T13: 43: 22Z</updated>
    <published>2014-12-11T10: 27: 57Z</published>
    <title>Large system analysis of a GLRT for detection with large sensor arrays
  in temporally white noise</title>
    <summary>  This paper addresses the behaviour of a classical multi-antenna GLRT test
that allows to detect the presence of a known signal corrupted by a multi-path
propagation channel and by an additive white Gaussian noise with unknown
spatial covariance matrix. The paper is focused on the case where the number of
sensors M is large, and of the same order of magnitude as the sample size N, a
context which is modeled by the large system asymptotic regime M goes to
infinity, N goes to infinity in such a way that M/N goes to c for c in (0,
infinity). The purpose of this paper is to study the behaviour of a GLRT test
statistics in this regime, and to show that the corresponding theoretical
analysis allows to accurately predict the performance of the test when M and N
are of the same order of magnitude.
</summary>
    <author>
      <name>Sonja Hiltunen</name>
    </author>
    <author>
      <name>Philippe Loubaton</name>
    </author>
    <author>
      <name>Pascal Chevalier</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2015.2452220</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2015.2452220" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages,
                        9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.3594v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3594v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60G35" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1809.04197v2</id>
    <updated>2019-03-22T14: 12: 48Z</updated>
    <published>2018-09-11T23: 36: 31Z</published>
    <title>Change-Point Detection on Hierarchical Circadian Models</title>
    <summary>  This paper addresses the problem of change-point detection on sequences of
high-dimensional and heterogeneous observations, which also possess a periodic
temporal structure. Due to the dimensionality problem, when the time between
change-points is on the order of the dimension of the model parameters, drifts
in the underlying distribution can be misidentified as changes. To overcome
this limitation, we assume that the observations lie in a lower-dimensional
manifold that admits a latent variable representation. In particular, we
propose a hierarchical model that is computationally feasible, widely
applicable to heterogeneous data and robust to missing instances. Additionally,
the observations' periodic dependencies are captured by non-stationary periodic
covariance functions. The proposed technique is particularly fitted to (and
motivated by) the problem of detecting changes in human behavior using
smartphones and its application to relapse detection in psychiatric patients.
Finally, we validate the technique on synthetic examples and we demonstrate its
utility in the detection of behavioral changes using real data acquired by
smartphones.
</summary>
    <author>
      <name>Pablo Moreno-Muñoz</name>
    </author>
    <author>
      <name>David Ramírez</name>
    </author>
    <author>
      <name>Antonio Artés-Rodríguez</name>
    </author>
    <link href="http://arxiv.org/abs/1809.04197v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.04197v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2501.14555v1</id>
    <updated>2025-01-24T14: 57: 27Z</updated>
    <published>2025-01-24T14: 57: 27Z</published>
    <title>Exploring Answer Set Programming for Provenance Graph-Based Cyber Threat
  Detection: A Novel Approach</title>
    <summary>  Provenance graphs are useful and powerful tools for representing system-level
activities in cybersecurity; however, existing approaches often struggle with
complex queries and flexible reasoning. This paper presents a novel approach
using Answer Set Programming (ASP) to model and analyze provenance graphs. We
introduce an ASP-based representation that captures intricate relationships
between system entities, including temporal and causal dependencies. Our model
enables sophisticated analysis capabilities such as attack path tracing, data
exfiltration detection, and anomaly identification. The declarative nature of
ASP allows for concise expression of complex security patterns and policies,
facilitating both real-time threat detection and forensic analysis. We
demonstrate our approach's effectiveness through case studies showcasing its
threat detection capabilities. Experimental results illustrate the model's
ability to handle large-scale provenance graphs while providing expressive
querying. The model's extensibility allows for incorporation of new system
behaviors and security rules, adapting to evolving cyber threats. This work
contributes a powerful, flexible, and explainable framework for reasoning about
system behaviors and security incidents, advancing the development of effective
threat detection and forensic investigation tools.
</summary>
    <author>
      <name>Fang Li</name>
    </author>
    <author>
      <name>Fei Zuo</name>
    </author>
    <author>
      <name>Gopal Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 27th International Symposium on Practical Aspects of Declarative
  Languages (PADL 2025)</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.14555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.14555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1510.03909v1</id>
    <updated>2015-10-13T21: 57: 47Z</updated>
    <published>2015-10-13T21: 57: 47Z</published>
    <title>Variable-state Latent Conditional Random Fields for Facial Expression
  Recognition and Action Unit Detection</title>
    <summary>  Automated recognition of facial expressions of emotions, and detection of
facial action units (AUs), from videos depends critically on modeling of their
dynamics. These dynamics are characterized by changes in temporal phases
(onset-apex-offset) and intensity of emotion expressions and AUs, the
appearance of which may vary considerably among target subjects, making the
recognition/detection task very challenging. The state-of-the-art Latent
Conditional Random Fields (L-CRF) framework allows one to efficiently encode
these dynamics through the latent states accounting for the temporal
consistency in emotion expression and ordinal relationships between its
intensity levels, these latent states are typically assumed to be either
unordered (nominal) or fully ordered (ordinal). Yet, such an approach is often
too restrictive. For instance, in the case of AU detection, the goal is to
discriminate between the segments of an image sequence in which this AU is
active or inactive. While the sequence segments containing activation of the
target AU may better be described using ordinal latent states, the inactive
segments better be described using unordered (nominal) latent states, as no
assumption can be made about their underlying structure (since they can contain
either neutral faces or activations of non-target AUs). To address this, we
propose the variable-state L-CRF (VSL-CRF) model that automatically selects the
optimal latent states for the target image sequence. To reduce the model
overfitting either the nominal or ordinal latent states, we propose a novel
graph-Laplacian regularization of the latent states. Our experiments on three
public expression databases show that the proposed model achieves better
generalization performance compared to traditional L-CRFs and other related
state-of-the-art models.
</summary>
    <author>
      <name>Robert Walecki</name>
    </author>
    <author>
      <name>Ognjen Rudovic</name>
    </author>
    <author>
      <name>Vladimir Pavlovic</name>
    </author>
    <author>
      <name>Maja Pantic</name>
    </author>
    <link href="http://arxiv.org/abs/1510.03909v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.03909v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2001.01168v2</id>
    <updated>2023-05-17T03: 18: 31Z</updated>
    <published>2020-01-05T05: 14: 03Z</published>
    <title>Facial Action Unit Detection via Adaptive Attention and Relation</title>
    <summary>  Facial action unit (AU) detection is challenging due to the difficulty in
capturing correlated information from subtle and dynamic AUs. Existing methods
often resort to the localization of correlated regions of AUs, in which
predefining local AU attentions by correlated facial landmarks often discards
essential parts, or learning global attention maps often contains irrelevant
areas. Furthermore, existing relational reasoning methods often employ common
patterns for all AUs while ignoring the specific way of each AU. To tackle
these limitations, we propose a novel adaptive attention and relation (AAR)
framework for facial AU detection. Specifically, we propose an adaptive
attention regression network to regress the global attention map of each AU
under the constraint of attention predefinition and the guidance of AU
detection, which is beneficial for capturing both specified dependencies by
landmarks in strongly correlated regions and facial globally distributed
dependencies in weakly correlated regions. Moreover, considering the diversity
and dynamics of AUs, we propose an adaptive spatio-temporal graph convolutional
network to simultaneously reason the independent pattern of each AU, the
inter-dependencies among AUs, as well as the temporal dependencies. Extensive
experiments show that our approach (i) achieves competitive performance on
challenging benchmarks including BP4D, DISFA, and GFT in constrained scenarios
and Aff-Wild2 in unconstrained scenarios, and (ii) can precisely learn the
regional correlation distribution of each AU.
</summary>
    <author>
      <name>Zhiwen Shao</name>
    </author>
    <author>
      <name>Yong Zhou</name>
    </author>
    <author>
      <name>Jianfei Cai</name>
    </author>
    <author>
      <name>Hancheng Zhu</name>
    </author>
    <author>
      <name>Rui Yao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2023.3277794</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2023.3277794" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted by IEEE Transactions on Image Processing
  (TIP)</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.01168v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01168v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2403.03400v1</id>
    <updated>2024-03-06T01: 49: 28Z</updated>
    <published>2024-03-06T01: 49: 28Z</published>
    <title>Contrastive Learning of Person-independent Representations for Facial
  Action Unit Detection</title>
    <summary>  Facial action unit (AU) detection, aiming to classify AU present in the
facial image, has long suffered from insufficient AU annotations. In this
paper, we aim to mitigate this data scarcity issue by learning AU
representations from a large number of unlabelled facial videos in a
contrastive learning paradigm. We formulate the self-supervised AU
representation learning signals in two-fold: (1) AU representation should be
frame-wisely discriminative within a short video clip; (2) Facial frames
sampled from different identities but show analogous facial AUs should have
consistent AU representations. As to achieve these goals, we propose to
contrastively learn the AU representation within a video clip and devise a
cross-identity reconstruction mechanism to learn the person-independent
representations. Specially, we adopt a margin-based temporal contrastive
learning paradigm to perceive the temporal AU coherence and evolution
characteristics within a clip that consists of consecutive input facial frames.
Moreover, the cross-identity reconstruction mechanism facilitates pushing the
faces from different identities but show analogous AUs close in the latent
embedding space. Experimental results on three public AU datasets demonstrate
that the learned AU representation is discriminative for AU detection. Our
method outperforms other contrastive learning methods and significantly closes
the performance gap between the self-supervised and supervised AU detection
approaches.
</summary>
    <author>
      <name>Yong Li</name>
    </author>
    <author>
      <name>Shiguang Shan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2023.3279978</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2023.3279978" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Transaction on Image Processing 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2403.03400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.03400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1908.06552v1</id>
    <updated>2019-08-19T01: 33: 14Z</updated>
    <published>2019-08-19T01: 33: 14Z</published>
    <title>Weakly-supervised Action Localization with Background Modeling</title>
    <summary>  We describe a latent approach that learns to detect actions in long sequences
given training videos with only whole-video class labels. Our approach makes
use of two innovations to attention-modeling in weakly-supervised learning.
First, and most notably, our framework uses an attention model to extract both
foreground and background frames whose appearance is explicitly modeled. Most
prior works ignore the background, but we show that modeling it allows our
system to learn a richer notion of actions and their temporal extents. Second,
we combine bottom-up, class-agnostic attention modules with top-down,
class-specific activation maps, using the latter as form of self-supervision
for the former. Doing so allows our model to learn a more accurate model of
attention without explicit temporal supervision. These modifications lead to
10% AP@IoU=0.5 improvement over existing systems on THUMOS14. Our proposed
weaklysupervised system outperforms recent state-of-the-arts by at least 4.3%
AP@IoU=0.5. Finally, we demonstrate that weakly-supervised learning can be used
to aggressively scale-up learning to in-the-wild, uncurated Instagram videos.
The addition of these videos significantly improves localization performance of
our weakly-supervised model
</summary>
    <author>
      <name>Phuc Xuan Nguyen</name>
    </author>
    <author>
      <name>Deva Ramanan</name>
    </author>
    <author>
      <name>Charless C. Fowlkes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at ICCV 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.06552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.06552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2312.13377v4</id>
    <updated>2025-02-22T17: 38: 46Z</updated>
    <published>2023-12-20T19: 08: 49Z</published>
    <title>SADA: Semantic adversarial unsupervised domain adaptation for Temporal
  Action Localization</title>
    <summary>  Temporal Action Localization (TAL) is a complex task that poses relevant
challenges, particularly when attempting to generalize on new -- unseen --
domains in real-world applications. These scenarios, despite realistic, are
often neglected in the literature, exposing these solutions to important
performance degradation. In this work, we tackle this issue by introducing, for
the first time, an approach for Unsupervised Domain Adaptation (UDA) in sparse
TAL, which we refer to as Semantic Adversarial unsupervised Domain Adaptation
(SADA). Our contributions are threefold: (1) we pioneer the development of a
domain adaptation model that operates on realistic sparse action detection
benchmarks; (2) we tackle the limitations of global-distribution alignment
techniques by introducing a novel adversarial loss that is sensitive to local
class distributions, ensuring finer-grained adaptation; and (3) we present a
novel set of benchmarks based on EpicKitchens100 and CharadesEgo, that evaluate
multiple domain shifts in a comprehensive manner. Our experiments indicate that
SADA improves the adaptation across domains when compared to fully supervised
state-of-the-art and alternative UDA methods, attaining a performance boost of
up to 6.14% mAP.
</summary>
    <author>
      <name>David Pujol-Perich</name>
    </author>
    <author>
      <name>Albert Clapés</name>
    </author>
    <author>
      <name>Sergio Escalera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to WACV 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.13377v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.13377v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2004.10165v1</id>
    <updated>2020-04-21T17: 19: 06Z</updated>
    <published>2020-04-21T17: 19: 06Z</published>
    <title>4D Spatio-Temporal Deep Learning with 4D fMRI Data for Autism Spectrum
  Disorder Classification</title>
    <summary>  Autism spectrum disorder (ASD) is associated with behavioral and
communication problems. Often, functional magnetic resonance imaging (fMRI) is
used to detect and characterize brain changes related to the disorder.
Recently, machine learning methods have been employed to reveal new patterns by
trying to classify ASD from spatio-temporal fMRI images. Typically, these
methods have either focused on temporal or spatial information processing.
Instead, we propose a 4D spatio-temporal deep learning approach for ASD
classification where we jointly learn from spatial and temporal data. We employ
4D convolutional neural networks and convolutional-recurrent models which
outperform a previous approach with an F1-score of 0.71 compared to an F1-score
of 0.65.
</summary>
    <author>
      <name>Marcel Bengs</name>
    </author>
    <author>
      <name>Nils Gessert</name>
    </author>
    <author>
      <name>Alexander Schlaefer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at MIDL 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.10165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.10165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2003.06843v1</id>
    <updated>2020-03-15T14: 48: 50Z</updated>
    <published>2020-03-15T14: 48: 50Z</published>
    <title>Bayesian Inference of Spatio-Temporal Changes of Arctic Sea Ice</title>
    <summary>  Arctic sea ice extent has drawn increasing interest and alarm from
geoscientists, owing to its rapid decline. In this article, we propose a
Bayesian spatio-temporal hierarchical statistical model for binary Arctic sea
ice data over two decades, where a latent dynamic spatio-temporal Gaussian
process is used to model the data-dependence through a logit link function. Our
ultimate goal is to perform inference on the dynamic spatial behavior of Arctic
sea ice over a period of two decades. Physically motivated covariates are
assessed using autologistic diagnostics. Our Bayesian spatio-temporal model
shows how parameter uncertainty in such a complex hierarchical model can
influence spatio-temporal prediction. The posterior distributions of new
summary statistics are proposed to detect the changing patterns of Arctic sea
ice over two decades since 1997.
</summary>
    <author>
      <name>Bohai Zhang</name>
    </author>
    <author>
      <name>Noel Cressie</name>
    </author>
    <link href="http://arxiv.org/abs/2003.06843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.06843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62F15, 60G15, 62J12, 62P12" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2408.13629v1</id>
    <updated>2024-08-24T17: 12: 36Z</updated>
    <published>2024-08-24T17: 12: 36Z</published>
    <title>Temporally-consistent 3D Reconstruction of Birds</title>
    <summary>  This paper deals with 3D reconstruction of seabirds which recently came into
focus of environmental scientists as valuable bio-indicators for environmental
change. Such 3D information is beneficial for analyzing the bird's behavior and
physiological shape, for example by tracking motion, shape, and appearance
changes. From a computer vision perspective birds are especially challenging
due to their rapid and oftentimes non-rigid motions. We propose an approach to
reconstruct the 3D pose and shape from monocular videos of a specific breed of
seabird - the common murre. Our approach comprises a full pipeline of
detection, tracking, segmentation, and temporally consistent 3D reconstruction.
Additionally, we propose a temporal loss that extends current single-image 3D
bird pose estimators to the temporal domain. Moreover, we provide a real-world
dataset of 10000 frames of video observations on average capture nine birds
simultaneously, comprising a large variety of motions and interactions,
including a smaller test set with bird-specific keypoint labels. Using our
temporal optimization, we achieve state-of-the-art performance for the
challenging sequences in our dataset.
</summary>
    <author>
      <name>Johannes Hägerlind</name>
    </author>
    <author>
      <name>Jonas Hentati-Sundberg</name>
    </author>
    <author>
      <name>Bastian Wandt</name>
    </author>
    <link href="http://arxiv.org/abs/2408.13629v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.13629v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2501.17048v1</id>
    <updated>2025-01-28T16: 14: 10Z</updated>
    <published>2025-01-28T16: 14: 10Z</published>
    <title>Cortical Temporal Mismatch Compensation in Bimodal Cochlear Implant
  Users: Selective Attention Decoding and Pupillometry Study</title>
    <summary>  Bimodal stimulation, combining cochlear implant (CI) and acoustic input from
the opposite ear, typically enhances speech perception but varies due to
factors like temporal mismatch. Previously, we used cortical auditory evoked
potentials (CAEPs) to estimate this mismatch based on N1 latency differences.
This study expands on that by assessing the impact of temporal mismatch
compensation on speech perception. We tested bimodal CI users in three
conditions: clinical, compensated temporal mismatch, and a 50 ms mismatch.
Measures included speech understanding, pupillometry, CAEPs, selective
attention decoding, and parietal alpha power. Despite stable speech
understanding across conditions, neural measures showed stronger effects. CAEP
N1P2 amplitudes were highest in the compensated condition. Phase-locking value
(PLV) and selective attention decoding improved but lacked significance.
Parietal alpha power increased under 50 ms mismatch, suggesting cognitive
resource allocation. Pupillometry correlated with speech understanding but
showed limited sensitivity. Findings highlight that neural metrics are more
sensitive than behavioral tests for detecting interaural mismatch. While CAEP
N1P2 amplitudes significantly improved with compensation, other neural measures
showed limited effects, suggesting the need for combined temporal and spectral
compensation strategies.
</summary>
    <author>
      <name>Hanna Dolhopiatenko</name>
    </author>
    <author>
      <name>Waldo Nogueira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages,
                        15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.17048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.17048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2110.04119v1</id>
    <updated>2021-10-07T14: 50: 43Z</updated>
    <published>2021-10-07T14: 50: 43Z</published>
    <title>A Multi-viewpoint Outdoor Dataset for Human Action Recognition</title>
    <summary>  Advancements in deep neural networks have contributed to near perfect results
for many computer vision problems such as object recognition, face recognition
and pose estimation. However, human action recognition is still far from
human-level performance. Owing to the articulated nature of the human body, it
is challenging to detect an action from multiple viewpoints, particularly from
an aerial viewpoint. This is further compounded by a scarcity of datasets that
cover multiple viewpoints of actions. To fill this gap and enable research in
wider application areas, we present a multi-viewpoint outdoor action
recognition dataset collected from YouTube and our own drone. The dataset
consists of 20 dynamic human action classes,
                        2324 video clips and 503086
frames. All videos are cropped and resized to 720x720 without distorting the
original aspect ratio of the human subjects in videos. This dataset should be
useful to many research areas including action recognition, surveillance and
situational awareness. We evaluated the dataset with a two-stream CNN
architecture coupled with a recently proposed temporal pooling scheme called
kernelized rank pooling that produces nonlinear feature subspace
representations. The overall baseline action recognition accuracy is 74.0%.
</summary>
    <author>
      <name>Asanka G. Perera</name>
    </author>
    <author>
      <name>Yee Wei Law</name>
    </author>
    <author>
      <name>Titilayo T. Ogunwa</name>
    </author>
    <author>
      <name>Javaan Chahl</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/THMS.2020.2971958</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/THMS.2020.2971958" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,
                        4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Human-Machine Systems, Volume: 50, Issue: 5,
  Oct. 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2110.04119v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.04119v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2411.00862v1</id>
    <updated>2024-10-30T17: 27: 44Z</updated>
    <published>2024-10-30T17: 27: 44Z</published>
    <title>A Simple and Effective Temporal Grounding Pipeline for Basketball
  Broadcast Footage</title>
    <summary>  We present a reliable temporal grounding pipeline for video-to-analytic
alignment of basketball broadcast footage. Given a series of frames as input,
our method quickly and accurately extracts time-remaining and quarter values
from basketball broadcast scenes. Our work intends to expedite the development
of large, multi-modal video datasets to train data-hungry video models in the
sports action recognition domain. Our method aligns a pre-labeled corpus of
play-by-play annotations containing dense event annotations to video frames,
enabling quick retrieval of labeled video segments. Unlike previous methods, we
forgo the need to localize game clocks by fine-tuning an out-of-the-box object
detector to find semantic text regions directly. Our end-to-end approach
improves the generality of our work. Additionally, interpolation and
parallelization techniques prepare our pipeline for deployment in a large
computing cluster. All code is made publicly available.
</summary>
    <author>
      <name>Levi Harris</name>
    </author>
    <link href="http://arxiv.org/abs/2411.00862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.00862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1504.06800v1</id>
    <updated>2015-04-26T10: 21: 43Z</updated>
    <published>2015-04-26T10: 21: 43Z</published>
    <title>A local theory of Quantum Mechanics</title>
    <summary>  It is shown that Quantum Mechanics is ambiguous when predicting relative
frequencies for an entangled system if the measurements of both subsystems are
performed in spatially separated events. This ambiguity gives way to unphysical
consequences: the projection rule could be applied in one or the other
temporal(?) order of measurements (being non local in any case), but symmetry
of the roles of both subsystems would be broken.
  An alternative theory is presented in which this ambiguity does not exist.
Observable relative frequencies differ from those of orthodox Quantum
Mechanics, and a {\it gendaken
                        } experiment is proposed to falsify one or the
other theory. In the alternative theory, each subsystem has an individual state
in its own Hilbert space, and the total system state is direct product (rank
one) of both, so there is no entanglement. Correlation between subsystems
appears through a hidden label that prescribes the output of arbitrary
hypothetical measurements. Measurement is treated as a usual reversible
interaction, and this postulate allows to determine relative frequencies when
the value of a magnitude is known without in any way perturbing the system, by
measurement of the correlated companion.
  It is predicted the existence of an accompanying system, the de Broglie wave,
introduced in order to preserve the action reaction principle in indirect
measurements, when there is no interaction of detector and particle. Some
action on the detector, different from the one cause by a particle, should be
observable.
</summary>
    <author>
      <name>Carlos Lopez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10701-015-9976-4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10701-015-9976-4" rel="related"/>
    <link href="http://arxiv.org/abs/1504.06800v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.06800v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2403.02824v1</id>
    <updated>2024-03-05T09: 51: 14Z</updated>
    <published>2024-03-05T09: 51: 14Z</published>
    <title>Fluorescent nano- and microparticles for sensing cellular
  microenvironment: past, present and future applications</title>
    <summary>  The tumor microenvironment (TME) features distinct hallmarks, including
acidosis, hypoxia, reactive oxygen species (ROS) generation, and altered ion
fluxes, which are crucial targets for early cancer biomarker detection, tumor
diagnosis, and therapeutic strategies. A variety of imaging and sensing
techniques have been developed and employed in both research and clinical
settings to visualize and monitor cellular and TME dynamics. Among these,
ratiometric fluorescence-based sensors have emerged as powerful analytical
tools, providing precise and sensitive insights into the TME and enabling
real-time detection and tracking of dynamic changes. In this comprehensive
review, we discuss the latest advancements in ratiometric fluorescent probes
designed for optical mapping of pH, oxygen, ROS, ions, and biomarkers within
the TME. We elucidate their structural designs and sensing mechanisms, as well
as their applications in in vitro and in vivo detection. Furthermore, we
explore integrated sensing platforms that reveal the spatiotemporal behavior of
complex tumor cultures, highlighting the potential of high-resolution imaging
techniques combined with computational methods. This review aims to provide a
solid foundation for understanding the current state of the art and the future
potential of fluorescent nano- and microparticles in the field of cellular
microenvironment sensing.
</summary>
    <author>
      <name>Giuliana Grasso</name>
    </author>
    <author>
      <name>Francesco Colella</name>
    </author>
    <author>
      <name>Stefania Forciniti</name>
    </author>
    <author>
      <name>Valentina Onesto</name>
    </author>
    <author>
      <name>Helena Iuele</name>
    </author>
    <author>
      <name>Anna Chiara Siciliano</name>
    </author>
    <author>
      <name>Federica Carnevali</name>
    </author>
    <author>
      <name>Anil Chandra</name>
    </author>
    <author>
      <name>Giuseppe Gigli</name>
    </author>
    <author>
      <name>Loretta L. del Mercato</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1039/D3NA00218G</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1039/D3NA00218G" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nanoscale Advances 2023,
                        2023,
                        5,
                        4311-4336</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2403.02824v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.02824v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2504.02313v1</id>
    <updated>2025-04-03T06: 42: 26Z</updated>
    <published>2025-04-03T06: 42: 26Z</published>
    <title>Distributed Temporal Graph Learning with Provenance for APT Detection in
  Supply Chains</title>
    <summary>  Cyber supply chain, encompassing digital asserts, software, hardware, has
become an essential component of modern Information and Communications
Technology (ICT) provisioning. However, the growing inter-dependencies have
introduced numerous attack vectors, making supply chains a prime target for
exploitation. In particular, advanced persistent threats (APTs) frequently
leverage supply chain vulnerabilities (SCVs) as entry points, benefiting from
their inherent stealth. Current defense strategies primarly focus on prevention
through blockchain for integrity assurance or detection using plain-text source
code analysis in open-source software (OSS). However, these approaches overlook
scenarios where source code is unavailable and fail to address detection and
defense during runtime. To bridge this gap, we propose a novel approach that
integrates multi-source data, constructs a comprehensive dynamic provenance
graph, and detects APT behavior in real time using temporal graph learning.
Given the lack of tailored datasets in both industry and academia, we also aim
to simulate a custom dataset by replaying real-world supply chain exploits with
multi-source monitoring.
</summary>
    <author>
      <name>Zhuoran Tan</name>
    </author>
    <author>
      <name>Christos Anagnostopoulos</name>
    </author>
    <author>
      <name>Jeremy Singer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted at 45th IEEE International Conference on
  Distributed Computing Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.02313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.02313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2003.08275v1</id>
    <updated>2020-03-18T15: 30: 03Z</updated>
    <published>2020-03-18T15: 30: 03Z</published>
    <title>PIC: Permutation Invariant Convolution for Recognizing Long-range
  Activities</title>
    <summary>  Neural operations as convolutions, self-attention, and vector aggregation are
the go-to choices for recognizing short-range actions. However, they have three
limitations in modeling long-range activities. This paper presents PIC,
Permutation Invariant Convolution, a novel neural layer to model the temporal
structure of long-range activities. It has three desirable properties. i.
Unlike standard convolution, PIC is invariant to the temporal permutations of
features within its receptive field, qualifying it to model the weak temporal
structures. ii. Different from vector aggregation, PIC respects local
connectivity, enabling it to learn long-range temporal abstractions using
cascaded layers. iii. In contrast to self-attention, PIC uses shared weights,
making it more capable of detecting the most discriminant visual evidence
across long and noisy videos. We study the three properties of PIC and
demonstrate its effectiveness in recognizing the long-range activities of
Charades, Breakfast, and MultiThumos.
</summary>
    <author>
      <name>Noureldien Hussein</name>
    </author>
    <author>
      <name>Efstratios Gavves</name>
    </author>
    <author>
      <name>Arnold W. M. Smeulders</name>
    </author>
    <link href="http://arxiv.org/abs/2003.08275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.08275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2205.05609v1</id>
    <updated>2022-05-11T16: 27: 47Z</updated>
    <published>2022-05-11T16: 27: 47Z</published>
    <title>Video-ReTime: Learning Temporally Varying Speediness for Time Remapping</title>
    <summary>  We propose a method for generating a temporally remapped video that matches
the desired target duration while maximally preserving natural video dynamics.
Our approach trains a neural network through self-supervision to recognize and
accurately localize temporally varying changes in the video playback speed. To
re-time videos, we 1. use the model to infer the slowness of individual video
frames, and 2. optimize the temporal frame sub-sampling to be consistent with
the model's slowness predictions. We demonstrate that this model can detect
playback speed variations more accurately while also being orders of magnitude
more efficient than prior approaches. Furthermore, we propose an optimization
for video re-timing that enables precise control over the target duration and
performs more robustly on longer videos than prior methods. We evaluate the
model quantitatively on artificially speed-up videos, through transfer to
action recognition, and qualitatively through user studies.
</summary>
    <author>
      <name>Simon Jenni</name>
    </author>
    <author>
      <name>Markus Woodson</name>
    </author>
    <author>
      <name>Fabian Caba Heilbron</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the AI for Content Creation (AICC) workshop at CVPR 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.05609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.05609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2405.16925v1</id>
    <updated>2024-05-27T08: 18: 41Z</updated>
    <published>2024-05-27T08: 18: 41Z</published>
    <title>OED: Towards One-stage End-to-End Dynamic Scene Graph Generation</title>
    <summary>  Dynamic Scene Graph Generation (DSGG) focuses on identifying visual
relationships within the spatial-temporal domain of videos. Conventional
approaches often employ multi-stage pipelines, which typically consist of
object detection, temporal association, and multi-relation classification.
However, these methods exhibit inherent limitations due to the separation of
multiple stages, and independent optimization of these sub-problems may yield
sub-optimal solutions. To remedy these limitations, we propose a one-stage
end-to-end framework, termed OED, which streamlines the DSGG pipeline. This
framework reformulates the task as a set prediction problem and leverages
pair-wise features to represent each subject-object pair within the scene
graph. Moreover, another challenge of DSGG is capturing temporal dependencies,
we introduce a Progressively Refined Module (PRM) for aggregating temporal
context without the constraints of additional trackers or handcrafted
trajectories, enabling end-to-end optimization of the network. Extensive
experiments conducted on the Action Genome benchmark demonstrate the
effectiveness of our design. The code and models are available at
\url{https: //github.com/guanw-pku/OED}.
</summary>
    <author>
      <name>Guan Wang</name>
    </author>
    <author>
      <name>Zhimin Li</name>
    </author>
    <author>
      <name>Qingchao Chen</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVPR'24</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.16925v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.16925v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2010.09398v2</id>
    <updated>2021-05-14T15: 39: 17Z</updated>
    <published>2020-10-19T11: 36: 24Z</published>
    <title>Online network monitoring</title>
    <summary>  The application of network analysis has found great success in a wide variety
of disciplines; however, the popularity of these approaches has revealed the
difficulty in handling networks whose complexity scales rapidly. One of the
main interests in network analysis is the online detection of anomalous
behaviour. To overcome the curse of dimensionality, we introduce a network
surveillance method bringing together network modelling and statistical process
control. Our approach is to apply multivariate control charts based on
exponential smoothing and cumulative sums in order to monitor networks
determined by temporal exponential random graph models (TERGM). This allows us
to account for temporal dependence, while simultaneously reducing the number of
parameters to be monitored. The performance of the proposed charts is evaluated
by calculating the average run length for both simulated and real data. To
prove the appropriateness of the TERGM to describe network data, some measures
of goodness of fit are inspected. We demonstrate the effectiveness of the
proposed approach by an empirical application, monitoring daily flights in the
United States to detect anomalous patterns.
</summary>
    <author>
      <name>Anna Malinovskaya</name>
    </author>
    <author>
      <name>Philipp Otto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10260-021-00589-z</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10260-021-00589-z" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Statistical Methods &amp; Applications 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.09398v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09398v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1810.07796v1</id>
    <updated>2018-09-12T11: 06: 16Z</updated>
    <published>2018-09-12T11: 06: 16Z</published>
    <title>Using Intuitionistic Fuzzy Set for Anomaly Detection of Network Traffic
  from Flow Interaction</title>
    <summary>  We present a method to detect anomalies in a time series of flow interaction
patterns. There are many existing methods for anomaly detection in network
traffic, such as number of packets. However, there is non established method
detecting anomalies in a time series of flow interaction patterns that can be
represented as complex network. Firstly, based on proposed multivariate flow
similarity method on temporal locality, a complex network model (MFS-TL) is
constructed to describe the interactive behaviors of traffic flows. Having
analyzed the relationships between MFS-TL characteristics, temporal locality
window and multivariate flow similarity critical threshold, an approach for
parameter determination is established. Having observed the evolution of MFS-TL
characteristics, three non-deterministic correlations are defined for network
states (i.e. normal or abnormal). Furthermore, intuitionistic fuzzy set (IFS)
is introduced to quantify three non-deterministic correlations, and then a
anomaly detection method is put forward for single characteristic sequence. To
build an objective IFS, we design a Gaussian distribution-based membership
function with a variable hesitation degree. To determine the mapping of IFS's
clustering intervals to network states, a distinction index is developed. Then,
an IFS ensemble method (IFSE-AD) is proposed to eliminate the impacts of the
inconsistent about MFS-TL characteristic to network state and improve detection
performance. Finally, we carried out extensive experiments on several network
traffic datasets for anomaly detection, and the results demonstrate the
superiority of IFSE-AD to state-of-the-art approaches, validating the
effectiveness of our method.
</summary>
    <author>
      <name>Jinfa Wang</name>
    </author>
    <author>
      <name>Hai Zhao</name>
    </author>
    <author>
      <name>Jiuqiang Xu</name>
    </author>
    <author>
      <name>Hequn Li</name>
    </author>
    <author>
      <name>Shuai Chao</name>
    </author>
    <author>
      <name>Chuangyang Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages,
                            4 figures,
                            5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.07796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.07796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2409.04609v1</id>
    <updated>2024-09-06T20: 47: 21Z</updated>
    <published>2024-09-06T20: 47: 21Z</published>
    <title>Detection of False Data Injection Attacks (FDIA) on Power Dynamical
  Systems With a State Prediction Method</title>
    <summary>  With the deeper penetration of inverter-based resources in power systems,
                            false data injection attacks (FDIA) are a growing cyber-security concern. They
have the potential to disrupt the system's stability like frequency stability,
thereby leading to catastrophic failures. Therefore, an FDIA detection method
would be valuable to protect power systems. FDIAs typically induce a
discrepancy between the desired and the effective behavior of the power system
dynamics. A suitable detection method can leverage power dynamics predictions
to identify whether such a discrepancy was induced by an FDIA. This work
investigates the efficacy of temporal and spatio-temporal state prediction
models, such as Long Short-Term Memory (LSTM) and a combination of Graph Neural
Networks (GNN) with LSTM, for predicting frequency dynamics in the absence of
an FDIA but with noisy measurements, and thereby identify FDIA events. For
demonstration purposes, the IEEE 39 New England Kron-reduced model simulated
with a swing equation is considered. It is shown that the proposed state
prediction models can be used as a building block for developing an effective
FDIA detection method that can maintain high detection accuracy across various
attack and deployment settings. It is also shown how the FDIA detection should
be deployed to limit its exposure to detection inaccuracies and mitigate its
computational burden.
</summary>
    <author>
      <name>Abhijeet Sahu</name>
    </author>
    <author>
      <name>Truc Nguyen</name>
    </author>
    <author>
      <name>Kejun Chen</name>
    </author>
    <author>
      <name>Xiangyu Zhang</name>
    </author>
    <author>
      <name>Malik Hassanaly</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.04609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.04609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2303.16867v1</id>
    <updated>2023-03-29T17: 24: 21Z</updated>
    <published>2023-03-29T17: 24: 21Z</published>
    <title>A Video-based End-to-end Pipeline for Non-nutritive Sucking Action
  Recognition and Segmentation in Young Infants</title>
    <summary>  We present an end-to-end computer vision pipeline to detect non-nutritive
sucking (NNS) -- an infant sucking pattern with no nutrition delivered -- as a
potential biomarker for developmental delays, using off-the-shelf baby monitor
video footage. One barrier to clinical (or algorithmic) assessment of NNS stems
from its sparsity, requiring experts to wade through hours of footage to find
minutes of relevant activity. Our NNS activity segmentation algorithm solves
this problem by identifying periods of NNS with high certainty -- up to 94.0\%
average precision and 84.9\% average recall across 30 heterogeneous 60 s clips,
drawn from our manually annotated NNS clinical in-crib dataset of 183 hours of
overnight baby monitor footage from 19 infants. Our method is based on an
underlying NNS action recognition algorithm, which uses spatiotemporal deep
learning networks and infant-specific pose estimation, achieving 94.9\%
accuracy in binary classification of 960 2.5 s balanced NNS vs. non-NNS clips.
Tested on our second, independent, and public NNS in-the-wild dataset, NNS
recognition classification reaches 92.3\% accuracy, and NNS segmentation
achieves 90.8\% precision and 84.2\% recall.
</summary>
    <author>
      <name>Shaotong Zhu</name>
    </author>
    <author>
      <name>Michael Wan</name>
    </author>
    <author>
      <name>Elaheh Hatamimajoumerd</name>
    </author>
    <author>
      <name>Kashish Jain</name>
    </author>
    <author>
      <name>Samuel Zlota</name>
    </author>
    <author>
      <name>Cholpady Vikram Kamath</name>
    </author>
    <author>
      <name>Cassandra B. Rowan</name>
    </author>
    <author>
      <name>Emma C. Grace</name>
    </author>
    <author>
      <name>Matthew S. Goodwin</name>
    </author>
    <author>
      <name>Marie J. Hayes</name>
    </author>
    <author>
      <name>Rebecca A. Schwartz-Mette</name>
    </author>
    <author>
      <name>Emily Zimmerman</name>
    </author>
    <author>
      <name>Sarah Ostadabbas</name>
    </author>
    <link href="http://arxiv.org/abs/2303.16867v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.16867v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2408.03589v1</id>
    <updated>2024-08-07T07: 05: 32Z</updated>
    <published>2024-08-07T07: 05: 32Z</published>
    <title>Deep-learning-based electrode action potential mapping (DEAP Mapping)
  from annotation-free unipolar electrogram</title>
    <summary>  Catheter ablation has limited therapeutic efficacy against non-paroxysmal
atrial fibrillation (AF), and electrophysiological studies using mapping
catheters have been applied to evaluate the AF substrate. However, many of
these approaches rely on detecting excitation timing from electrograms (ECGs),
potentially compromising their effectiveness in complex AF scenarios. Herein,
we introduce Deep-learning-based Electrode Action Potential Mapping (DEAP
Mapping), a deep learning model designed to reconstruct membrane potential
images from annotation-free unipolar ECG signals. We conducted ex vivo
experiments using porcine hearts (N = 6) to evaluate the accuracy of DEAP
Mapping by simultaneously performing fluorescence measurement of membrane
potentials and measurements of epicardial unipolar ECGs. Membrane potentials
estimated via DEAP Mapping were compared with those measured via optical
mapping. We assessed the clinical applicability of DEAP Mapping by comparing
the DEAP Mapping's estimations from clinically measured catheter electrode
signals with those from established electrode-mapping techniques. DEAP Mapping
accurately estimated conduction delays and blocks in ex vivo experiments. Phase
variance analysis, an AF substrate evaluation method, revealed that the
substrate identified from optical mapping closely resembled that identified
from DEAP Mapping estimations (structural similarity index of &gt;0.8). In
clinical evaluations, DEAP Mapping estimation observed several conduction
delays and blocks that were not observed with existing methods, indicating that
DEAP Mapping can estimate excitation patterns with higher spatiotemporal
resolution. DEAP Mapping has a potential to derive detailed changes in membrane
potential from intra-operative catheter electrode signals, offering enhanced
visualisation of the AF substrate from the estimated membrane potentials.
</summary>
    <author>
      <name>Hiroshi Seno</name>
    </author>
    <author>
      <name>Toshiya Kojima</name>
    </author>
    <author>
      <name>Masatoshi Yamazaki</name>
    </author>
    <author>
      <name>Ichiro Sakuma</name>
    </author>
    <author>
      <name>Katsuhito Fujiu</name>
    </author>
    <author>
      <name>Naoki Tomii</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages,
                            7 figures,
                            6 supplemental movies</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.03589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.03589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92C55, 68T07" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; J.3; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2011.14097v5</id>
    <updated>2021-03-05T00: 24: 56Z</updated>
    <published>2020-11-28T09: 36: 18Z</published>
    <title>Time Series Change Point Detection with Self-Supervised Contrastive
  Predictive Coding</title>
    <summary>  Change Point Detection (CPD) methods identify the times associated with
changes in the trends and properties of time series data in order to describe
the underlying behaviour of the system. For instance, detecting the changes and
anomalies associated with web service usage, application usage or human
behaviour can provide valuable insights for downstream modelling tasks. We
propose a novel approach for self-supervised Time Series Change Point detection
method based onContrastivePredictive coding (TS-CP^2). TS-CP^2 is the first
approach to employ a contrastive learning strategy for CPD by learning an
embedded representation that separates pairs of embeddings of time adjacent
intervals from pairs of interval embeddings separated across time. Through
extensive experiments on three diverse, widely used time series datasets, we
demonstrate that our method outperforms five state-of-the-art CPD methods,
which include unsupervised and semi-supervisedapproaches. TS-CP^2 is shown to
improve the performance of methods that use either handcrafted statistical or
temporal features by 79.4% and deep learning-based methods by 17.0% with
respect to the F1-score averaged across the three datasets.
</summary>
    <author>
      <name>Shohreh Deldari</name>
    </author>
    <author>
      <name>Daniel V. Smith</name>
    </author>
    <author>
      <name>Hao Xue</name>
    </author>
    <author>
      <name>Flora D. Salim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3442381.3449903</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3442381.3449903" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at The WEB Conference 2021 (WWW'21)</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.14097v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.14097v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2305.05538v1</id>
    <updated>2023-05-07T16: 05: 30Z</updated>
    <published>2023-05-07T16: 05: 30Z</published>
    <title>Efficient pattern-based anomaly detection in a network of multivariate
  devices</title>
    <summary>  Many organisations manage service quality and monitor a large set devices and
servers where each entity is associated with telemetry or physical sensor data
series. Recently, various methods have been proposed to detect behavioural
anomalies, however existing approaches focus on multivariate time series and
ignore communication between entities. Moreover, we aim to support end-users in
not only in locating entities and sensors causing an anomaly at a certain
period, but also explain this decision. We propose a scalable approach to
detect anomalies using a two-step approach. First, we recover relations between
entities in the network, since relations are often dynamic in nature and caused
by an unknown underlying process. Next, we report anomalies based on an
embedding of sequential patterns. Pattern mining is efficient and supports
interpretation, i.e. patterns represent frequent occurring behaviour in time
series. We extend pattern mining to filter sequential patterns based on
frequency, temporal constraints and minimum description length. We collect and
release two public datasets for international broadcasting and X from an
Internet company. \textit{BAD
                            } achieves an overall F1-Score of 0.78 on 9
benchmark datasets, significantly outperforming the best baseline by 3\%.
Additionally, \textit{BAD
                            } is also an order-of-magnitude faster than
state-of-the-art anomaly detection methods.
</summary>
    <author>
      <name>Len Feremans</name>
    </author>
    <author>
      <name>Boris Cule</name>
    </author>
    <author>
      <name>Bart Goethals</name>
    </author>
    <link href="http://arxiv.org/abs/2305.05538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.05538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2405.11141v2</id>
    <updated>2024-07-13T02: 07: 29Z</updated>
    <published>2024-05-18T02: 10: 41Z</published>
    <title>Enhancing Automata Learning with Statistical Machine Learning: A Network
  Security Case Study</title>
    <summary>  Intrusion detection systems are crucial for network security. Verification of
these systems is complicated by various factors, including the heterogeneity of
network platforms and the continuously changing landscape of cyber threats. In
this paper, we use automata learning to derive state machines from
network-traffic data with the objective of supporting behavioural verification
of intrusion detection systems. The most innovative aspect of our work is
addressing the inability to directly apply existing automata learning
techniques to network-traffic data due to the numeric nature of such data.
Specifically, we use interpretable machine learning (ML) to partition numeric
ranges into intervals that strongly correlate with a system's decisions
regarding intrusion detection. These intervals are subsequently used to
abstract numeric ranges before automata learning. We apply our ML-enhanced
automata learning approach to a commercial network intrusion detection system
developed by our industry partner, RabbitRun Technologies. Our approach results
in an average 67.5% reduction in the number of states and transitions of the
learned state machines, while achieving an average 28% improvement in accuracy
compared to using expertise-based numeric data abstraction. Furthermore, the
resulting state machines help practitioners in verifying system-level security
requirements and exploring previously unknown system behaviours through model
checking and temporal query checking. We make our implementation and
experimental data available online.
</summary>
    <author>
      <name>Negin Ayoughi</name>
    </author>
    <author>
      <name>Shiva Nejati</name>
    </author>
    <author>
      <name>Mehrdad Sabetzadeh</name>
    </author>
    <author>
      <name>Patricio Saavedra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted at the 27th ACM / IEEE International
  Conference on Model Driven Engineering Languages and Systems (MODELS 2024)</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.11141v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.11141v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1309.7340v3</id>
    <updated>2013-11-18T21: 09: 39Z</updated>
    <published>2013-09-27T19: 47: 11Z</published>
    <title>Early Stage Influenza Detection from Twitter</title>
    <summary>  Influenza is an acute respiratory illness that occurs virtually every year
and results in substantial disease, death and expense. Detection of Influenza
in its earliest stage would facilitate timely action that could reduce the
spread of the illness. Existing systems such as CDC and EISS which try to
collect diagnosis data, are almost entirely manual, resulting in about two-week
delays for clinical data acquisition. Twitter, a popular microblogging service,
provides us with a perfect source for early-stage flu detection due to its
real- time nature. For example, when a flu breaks out, people that get the flu
may post related tweets which enables the detection of the flu breakout
promptly. In this paper, we investigate the real-time flu detection problem on
Twitter data by proposing Flu Markov Network (Flu-MN): a spatio-temporal
unsupervised Bayesian algorithm based on a 4 phase Markov Network, trying to
identify the flu breakout at the earliest stage. We test our model on real
Twitter datasets from the United States along with baselines in multiple
applications, such as real-time flu breakout detection, future epidemic phase
prediction, or Influenza-like illness (ILI) physician visits. Experimental
results show the robustness and effectiveness of our approach. We build up a
real time flu reporting system based on the proposed approach, and we are
hopeful that it would help government or health organizations in identifying
flu outbreaks and facilitating timely actions to decrease unnecessary
mortality.
</summary>
    <author>
      <name>Jiwei Li</name>
    </author>
    <author>
      <name>Claire Cardie</name>
    </author>
    <link href="http://arxiv.org/abs/1309.7340v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.7340v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2001.03463v1</id>
    <updated>2020-01-10T14: 26: 03Z</updated>
    <published>2020-01-10T14: 26: 03Z</published>
    <title>Compressive sensing based privacy for fall detection</title>
    <summary>  Fall detection holds immense importance in the field of healthcare, where
timely detection allows for instant medical assistance. In this context, we
propose a 3D ConvNet architecture which consists of 3D Inception modules for
fall detection. The proposed architecture is a custom version of Inflated 3D
(I3D) architecture, that takes compressed measurements of video sequence as
spatio-temporal input, obtained from compressive sensing framework, rather than
video sequence as input, as in the case of I3D convolutional neural network.
This is adopted since privacy raises a huge concern for patients being
monitored through these RGB cameras. The proposed framework for fall detection
is flexible enough with respect to a wide variety of measurement matrices. Ten
action classes randomly selected from Kinetics-400 with no fall examples, are
employed to train our 3D ConvNet post compressive sensing with different types
of sensing matrices on the original video clips. Our results show that 3D
ConvNet performance remains unchanged with different sensing matrices. Also,
the performance obtained with Kinetics pre-trained 3D ConvNet on compressively
sensed fall videos from benchmark datasets is better than the state-of-the-art
techniques.
</summary>
    <author>
      <name>Ronak Gupta</name>
    </author>
    <author>
      <name>Prashant Anand</name>
    </author>
    <author>
      <name>Santanu Chaudhury</name>
    </author>
    <author>
      <name>Brejesh Lall</name>
    </author>
    <author>
      <name>Sanjay Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted in NCVPRIPG 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.03463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.03463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2502.18328v1</id>
    <updated>2025-02-25T16: 22: 42Z</updated>
    <published>2025-02-25T16: 22: 42Z</published>
    <title>From Vision to Sound: Advancing Audio Anomaly Detection with
  Vision-Based Algorithms</title>
    <summary>  Recent advances in Visual Anomaly Detection (VAD) have introduced
sophisticated algorithms leveraging embeddings generated by pre-trained feature
extractors. Inspired by these developments, we investigate the adaptation of
such algorithms to the audio domain to address the problem of Audio Anomaly
Detection (AAD). Unlike most existing AAD methods, which primarily classify
anomalous samples, our approach introduces fine-grained temporal-frequency
localization of anomalies within the spectrogram, significantly improving
explainability. This capability enables a more precise understanding of where
and when anomalies occur, making the results more actionable for end users. We
evaluate our approach on industrial and environmental benchmarks, demonstrating
the effectiveness of VAD techniques in detecting anomalies in audio signals.
Moreover, they improve explainability by enabling localized anomaly
identification, making audio anomaly detection systems more interpretable and
practical.
</summary>
    <author>
      <name>Manuel Barusco</name>
    </author>
    <author>
      <name>Francesco Borsatti</name>
    </author>
    <author>
      <name>Davide Dalle Pezze</name>
    </author>
    <author>
      <name>Francesco Paissan</name>
    </author>
    <author>
      <name>Elisabetta Farella</name>
    </author>
    <author>
      <name>Gian Antonio Susto</name>
    </author>
    <link href="http://arxiv.org/abs/2502.18328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.18328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2105.12734v1</id>
    <updated>2021-05-26T17: 20: 18Z</updated>
    <published>2021-05-26T17: 20: 18Z</published>
    <title>Analyzing time series activity of Twitter political spambots</title>
    <summary>  The presence and complexity of political Twitter bots has increased in recent
years, making it a very difficult task to recognize these accounts from real,
human users. We intended to provide an answer to the following question: are
temporal patterns of activity qualitatively different in fake and human
accounts? We collected a large sample of tweets during the post-electoral
conflict in the US in 2020 and performed supervised and non-supervised
statistical learning technique sto quantify the predictive power of time-series
features for human-bot recognition. Our results show that there are no
substantial differences, suggesting that political bots are nowadays very
capable of mimicking human behaviour. This finding reveals the need for novel,
more sophisticated bot-detection techniques.
</summary>
    <author>
      <name>Oscar Fontanelli</name>
    </author>
    <author>
      <name>Aldo Venegas</name>
    </author>
    <author>
      <name>Ricardo Mansilla</name>
    </author>
    <link href="http://arxiv.org/abs/2105.12734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.12734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2112.01447v2</id>
    <updated>2023-03-03T15: 50: 47Z</updated>
    <published>2021-12-02T17: 43: 30Z</published>
    <title>Hydroclimatic time series features at multiple time scales</title>
    <summary>  A comprehensive understanding of the behaviours of the various geophysical
processes and an effective evaluation of time series (else referred to as
"stochastic") simulation models require, among others, detailed investigations
across temporal scales. In this work, we propose a novel and detailed
methodological framework for advancing and enriching such investigations in a
hydroclimatic context. This specific framework is primarily based on a new
feature compilation for multi-scale hydroclimatic analyses, and can facilitate
largely interpretable feature investigations and comparisons in terms of
temporal dependence, temporal variation,
                            "forecastability", lumpiness,
stability, nonlinearity (and linearity), trends, spikiness, curvature and
seasonality. Multifaceted characterizations are herein obtained by computing
the values of the proposed feature compilation across nine temporal resolutions
(i.e., the 1-day,
                            2-day,
                            3-day,
                            7-day,
                            0.5-month,
                            1-month,
                            2-month,
                            3-month and
6-month ones) and three hydroclimatic time series types (i.e., temperature,
precipitation and streamflow) for 34-year-long time series records originating
from 511 geographical locations across the contiguous United States. Based on
the acquired information and knowledge, similarities and differences between
the examined time series types with respect to the evolution patterns
characterizing their feature values with increasing (or decreasing) temporal
resolution are identified. Moreover, the computed features are used as inputs
to unsupervised random forests for detecting any meaningful clusters between
the examined hydroclimatic time series. This clustering plays an illustrative
role within this research, as it facilitates the identification of spatial
patterns (with them consisting an important scientific target in hydroclimatic
research) and their cross-scale comparison...
</summary>
    <author>
      <name>Georgia Papacharalampous</name>
    </author>
    <author>
      <name>Hristos Tyralis</name>
    </author>
    <author>
      <name>Yannis Markonis</name>
    </author>
    <author>
      <name>Martin Hanel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jhydrol.2023.129160</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jhydrol.2023.129160" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Hydrology 618 (2023) 129160</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2112.01447v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.01447v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1301.6965v1</id>
    <updated>2013-01-29T16: 11: 21Z</updated>
    <published>2013-01-29T16: 11: 21Z</published>
    <title>Observation of the prior earthquake effect on the flux of environmental
  neutrons, gamma-radiation, and on the local electric field in Tien Shan
  mountain</title>
    <summary>  A search for the possible precursors of an earthquake and its effect on the
data series of diverse geophysical parameters has been made in the mountain of
Northern Tien Shan. The complex installation included the NM64 type neutron
supermonitor, detectors of the environmental low-energy neutrons, the
scintillation gamma-detector, the sensor of the local electric field, a
seismograph, and a weather-station. The specialized data filtration methodic
was elaborated with an enhanced sensitivity to the transient signals of seismic
origin. On the eve of, and after a 5.4 magnitude earthquake the fine features
in temporal behavior of the intensity of low-energy neutron and gamma-radiation
background, so as irregularities of the local electric field were observed
which seem to be in a close correlation with each other. These results may be
an evidence of the possibility of experimental identification of earthquake's
precursors in the time up to 2-3 days before the beginning of a period of
intensive tectonic activity.
</summary>
    <author>
      <name>N. M. Salikhov</name>
    </author>
    <author>
      <name>A. L. Shepetov</name>
    </author>
    <author>
      <name>A. P. Chubenko</name>
    </author>
    <author>
      <name>O. N. Kryakunova</name>
    </author>
    <author>
      <name>G. D. Pak</name>
    </author>
    <link href="http://arxiv.org/abs/1301.6965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.6965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.geo-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2303.07181v1</id>
    <updated>2023-03-13T15: 22: 51Z</updated>
    <published>2023-03-13T15: 22: 51Z</published>
    <title>Probabilistic Uncertainty-Aware Risk Spot Detector for Naturalistic
  Driving</title>
    <summary>  Risk assessment is a central element for the development and validation of
Autonomous Vehicles (AV). It comprises a combination of occurrence probability
and severity of future critical events. Time Headway (TH) as well as
Time-To-Contact (TTC) are commonly used risk metrics and have qualitative
relations to occurrence probability. However, they lack theoretical derivations
and additionally they are designed to only cover special types of traffic
scenarios (e.g. following between single car pairs). In this paper, we present
a probabilistic situation risk model based on survival analysis considerations
and extend it to naturally incorporate sensory, temporal and behavioral
uncertainties as they arise in real-world scenarios. The resulting Risk Spot
Detector (RSD) is applied and tested on naturalistic driving data of a
multi-lane boulevard with several intersections, enabling the visualization of
road criticality maps. Compared to TH and TTC, our approach is more selective
and specific in predicting risk. RSD concentrates on driving sections of high
vehicle density where large accelerations and decelerations or approaches with
high velocity occur.
</summary>
    <author>
      <name>Tim Puphal</name>
    </author>
    <author>
      <name>Malte Probst</name>
    </author>
    <author>
      <name>Julian Eggert</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIV.2019.2919465</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIV.2019.2919465" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Transactions on Intelligent Vehicles 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2303.07181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.07181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1711.02750v1</id>
    <updated>2017-11-07T22: 22: 23Z</updated>
    <published>2017-11-07T22: 22: 23Z</published>
    <title>A Search for Temporal Changes on Pluto and Charon</title>
    <summary>  A search for temporal changes on Pluto and Charon was motivated by (1) the
discovery of young surfaces in the Pluto system that imply ongoing or recent
geologic activity, (2) the detection of active plumes on Triton during the
Voyager 2 flyby, and (3) the abundant and detailed information that observing
geologic processes in action provides about the processes. A thorough search
for temporal changes using New Horizons images was completed. Images that
covered the same region were blinked and manually inspected for any differences
in appearance. The search included full-disk images such that all illuminated
regions of both bodies were investigated and higher resolution images such that
parts of the encounter hemispheres were investigated at finer spatial scales.
Changes of appearance between different images were observed but in all cases
were attributed to variability of the imaging parameters (especially geometry)
or artifacts. No differences of appearance that are strongly indicative of a
temporal change were found on the surface or in the atmosphere of either Pluto
or Charon. Limits on temporal changes as a function of spatial scale and
temporal interval during the New Horizons encounter are determined. The longest
time interval constraint is one Pluto/Charon rotation period (~6.4 Earth days).
Contrast reversal and high-phase bright features that change in appearance with
solar phase angle are identified. The change of appearance of these features is
most likely due to the change in phase angle rather than a temporal change. Had
active plumes analogous to the plumes discovered on Triton been present on the
encounter hemispheres of either Pluto or Charon, they would have been detected.
The absence of active plumes may be due to temporal variability (i.e., plumes
do occur but none were active on the encounter hemispheres during the epoch of
the New Horizons encounter ...
</summary>
    <author>
      <name>J. D. Hofgartner</name>
    </author>
    <author>
      <name>B. J. Buratti</name>
    </author>
    <author>
      <name>S. L. Devins</name>
    </author>
    <author>
      <name>R. A. Beyer</name>
    </author>
    <author>
      <name>P. Schenk</name>
    </author>
    <author>
      <name>S. A. Stern</name>
    </author>
    <author>
      <name>H. A. Weaver</name>
    </author>
    <author>
      <name>C. B. Olkin</name>
    </author>
    <author>
      <name>A. Cheng</name>
    </author>
    <author>
      <name>K. Ennico</name>
    </author>
    <author>
      <name>T. R. Lauer</name>
    </author>
    <author>
      <name>W. B. McKinnon</name>
    </author>
    <author>
      <name>J. Spencer</name>
    </author>
    <author>
      <name>L. A. Young</name>
    </author>
    <author>
      <name>the New Horizons Science Team</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.icarus.2017.10.044</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.icarus.2017.10.044" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Icarus</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.02750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.02750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.EP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1612.02224v1</id>
    <updated>2016-12-07T12: 41: 54Z</updated>
    <published>2016-12-07T12: 41: 54Z</published>
    <title>Systematic variations of macrospicule properties observed by SDO/AIA
  over half a decade</title>
    <summary>  Macrospicules (MS) are localised small-scale jet-like phenomena in the solar
atmosphere, which have the potential to transport considerable amount of
momentum and energy from the lower solar atmospheric regions to the Transition
Region and the low corona. A detailed statistical analysis of their temporal
behaviour and spatial properties is carried out in this work. By means of
state-of-the-art spatial and temporal resolution observations, yielded by the
Atmospheric Imaging Assembly (AIA) of Solar Dynamics Observatory (SDO), we
constructed a database covering a 5.5-year long period, containing 301
macrospicules that occurred between June 2010 and December 2015 detected at
30.4 nm wavelength. Here, we report the long-term variation of the height,
length, average speed and width of MS in Coronal Holes and Quiet Sun areas both
in the northern and southern hemisphere of the Sun. This new database helps to
refine our knowledge about the physical properties of MS. Cross-correlation of
these properties show a relatively strong correlation, but not always a
dominant one. However, a more detailed analysis indicates a wave-like signature
in the behaviour of MS properties in time. The period of these long-term
oscillatory behaviours are just under two years. Also, in terms of solar
north/south hemispheres, a strong asymmetry was found in the spatial
distribution of MS properties, which may be accounted for the solar dynamo.
This latter feature may then indicate a strong and rather intrinsic link
between global internal and local atmospheric phenomena in the Sun.
</summary>
    <author>
      <name>T. S. Kiss</name>
    </author>
    <author>
      <name>N. Gyenge</name>
    </author>
    <author>
      <name>R. Erdelyi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3847/1538-4357/aa5272</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3847/1538-4357/aa5272" rel="related"/>
    <link href="http://arxiv.org/abs/1612.02224v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.02224v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2409.07667v1</id>
    <updated>2024-09-11T23: 59: 59Z</updated>
    <published>2024-09-11T23: 59: 59Z</published>
    <title>Unsupervised anomaly detection in spatio-temporal stream network sensor
  data</title>
    <summary>  The use of in-situ digital sensors for water quality monitoring is becoming
increasingly common worldwide. While these sensors provide near real-time data
for science, the data are prone to technical anomalies that can undermine the
trustworthiness of the data and the accuracy of statistical inferences,
particularly in spatial and temporal analyses. Here we propose a framework for
detecting anomalies in sensor data recorded in stream networks, which takes
advantage of spatial and temporal autocorrelation to improve detection rates.
The proposed framework involves the implementation of effective data imputation
to handle missing data, alignment of time-series to address temporal
disparities, and the identification of water quality events. We explore the
effectiveness of a suite of state-of-the-art statistical methods including
posterior predictive distributions, finite mixtures, and Hidden Markov Models
(HMM). We showcase the practical implementation of automated anomaly detection
in near-real time by employing a Bayesian recursive approach. This
demonstration is conducted through a comprehensive simulation study and a
practical application to a substantive case study situated in the Herbert
River, located in Queensland, Australia, which flows into the Great Barrier
Reef. We found that methods such as posterior predictive distributions and HMM
produce the best performance in detecting multiple types of anomalies.
Utilizing data from multiple sensors deployed relatively near one another
enhances the ability to distinguish between water quality events and technical
anomalies, thereby significantly improving the accuracy of anomaly detection.
Thus, uncertainty and biases in water quality reporting, interpretation, and
modelling are reduced, and the effectiveness of subsequent management actions
improved.
</summary>
    <author>
      <name>Edgar Santos-Fernandez</name>
    </author>
    <author>
      <name>Jay M. Ver Hoef</name>
    </author>
    <author>
      <name>Erin E. Peterson</name>
    </author>
    <author>
      <name>James McGree</name>
    </author>
    <author>
      <name>Cesar A. Villa</name>
    </author>
    <author>
      <name>Catherine Leigh</name>
    </author>
    <author>
      <name>Ryan Turner</name>
    </author>
    <author>
      <name>Cameron Roberts</name>
    </author>
    <author>
      <name>Kerrie Mengersen</name>
    </author>
    <link href="http://arxiv.org/abs/2409.07667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.07667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2201.12474v1</id>
    <updated>2022-01-29T01: 58: 16Z</updated>
    <published>2022-01-29T01: 58: 16Z</published>
    <title>Diffraction properties of lights with transverse orbital angular
  momentum</title>
    <summary>  Spatiotemporal optical vortex (STOV) is a unique optical vortex with phase
singularity in the space-time domain and the photons in a STOV can carry
transverse orbital angular momentum (OAM). The STOV shows many fantastic
properties which are worth exploring. Here, we theoretically and experimentally
study the diffraction property of STOV, which is a fundamental wave phenomenon.
The diffraction behaviors of STOVs are obviously affected by the transverse
OAM. The diffraction patterns of STOV pulses diffracted by a grating show
multi-lobe structure with each gap corresponding to 1 topological charge. The
diffraction properties of lights with transverse OAM are demonstrated clearly
and help us understanding the physical properties of STOV, which will be of
special applications, such as the realization of fast detection of STOVs with
different topological charges, which may pay the way for STOV based optical
communication.
</summary>
    <author>
      <name>Shunlin Huang</name>
    </author>
    <author>
      <name>Peng Wang</name>
    </author>
    <author>
      <name>Xiong Shen</name>
    </author>
    <author>
      <name>Jun Liu</name>
    </author>
    <author>
      <name>Ruxin Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages,
                            8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.12474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.12474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1708.00759v1</id>
    <updated>2017-08-02T14: 30: 11Z</updated>
    <published>2017-08-02T14: 30: 11Z</published>
    <title>Population Density-based Hospital Recommendation with Mobile LBS Big
  Data</title>
    <summary>  The difficulty of getting medical treatment is one of major livelihood issues
in China. Since patients lack prior knowledge about the spatial distribution
and the capacity of hospitals, some hospitals have abnormally high or sporadic
population densities. This paper presents a new model for estimating the
spatiotemporal population density in each hospital based on location-based
service (LBS) big data, which would be beneficial to guiding and dispersing
outpatients. To improve the estimation accuracy, several approaches are
proposed to denoise the LBS data and classify people by detecting their various
behaviors. In addition, a long short-term memory (LSTM) based deep learning is
presented to predict the trend of population density. By using Baidu
large-scale LBS logs database, we apply the proposed model to 113 hospitals in
Beijing, P. R. China, and constructed an online hospital recommendation system
which can provide users with a hospital rank list basing the real-time
population density information and the hospitals' basic information such as
hospitals' levels and their distances. We also mine several interesting
patterns from these LBS logs by using our proposed system.
</summary>
    <author>
      <name>Hanqing Chao</name>
    </author>
    <author>
      <name>Yuan Cao</name>
    </author>
    <author>
      <name>Junping Zhang</name>
    </author>
    <author>
      <name>Fen Xia</name>
    </author>
    <author>
      <name>Ye Zhou</name>
    </author>
    <author>
      <name>Hongming Shan</name>
    </author>
    <link href="http://arxiv.org/abs/1708.00759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1910.00443v3</id>
    <updated>2020-07-29T09: 01: 32Z</updated>
    <published>2019-10-01T14: 29: 23Z</published>
    <title>Towards Automatic Embryo Staging in 3D+T Microscopy Images using
  Convolutional Neural Networks and PointNets</title>
    <summary>  Automatic analyses and comparisons of different stages of embryonic
development largely depend on a highly accurate spatiotemporal alignment of the
investigated data sets. In this contribution, we assess multiple approaches for
automatic staging of developing embryos that were imaged with time-resolved 3D
light-sheet microscopy. The methods comprise image-based convolutional neural
networks as well as an approach based on the PointNet architecture that
directly operates on 3D point clouds of detected cell nuclei centroids. The
experiments with four wild-type zebrafish embryos render both approaches
suitable for automatic staging with average deviations of 21 - 34 minutes.
Moreover, a proof-of-concept evaluation based on simulated 3D+t point cloud
data sets shows that average deviations of less than 7 minutes are possible.
</summary>
    <author>
      <name>Manuel Traub</name>
    </author>
    <author>
      <name>Johannes Stegmaier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,
                            3 figures,
                            1 table, accepted paper at the Simulation and
  Synthesis in Medical Imaging (SASHIMI) Workshop held at MICCAI 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.00443v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.00443v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.CB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2010.10626v1</id>
    <updated>2020-10-20T21: 06: 50Z</updated>
    <published>2020-10-20T21: 06: 50Z</published>
    <title>Data-driven Identification of 2D Partial Differential Equations using
  extracted physical features</title>
    <summary>  Many scientific phenomena are modeled by Partial Differential Equations
(PDEs). The development of data gathering tools along with the advances in
machine learning (ML) techniques have raised opportunities for data-driven
identification of governing equations from experimentally observed data. We
propose an ML method to discover the terms involved in the equation from
two-dimensional spatiotemporal data. Robust and useful physical features are
extracted from data samples to represent the specific behaviors imposed by each
mathematical term in the equation. Compared to the previous models, this idea
provides us with the ability to discover 2D equations with time derivatives of
different orders, and also to identify new underlying physics on which the
model has not been trained. Moreover, the model can work with small sets of
low-resolution data while avoiding numerical differentiations. The results
indicate robustness of the features extracted based on prior knowledge in
comparison to automatically detected features by a Three-dimensional
Convolutional Neural Network (3D CNN) given the same amounts of data. Although
particular PDEs are studied in this work, the idea of the proposed approach
could be extended for reliable identification of various PDEs.
</summary>
    <author>
      <name>Kazem Meidani</name>
    </author>
    <author>
      <name>Amir Barati Farimani</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cma.2021.113831</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cma.2021.113831" rel="related"/>
    <link href="http://arxiv.org/abs/2010.10626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.10626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2303.16024v1</id>
    <updated>2023-03-28T14: 52: 03Z</updated>
    <published>2023-03-28T14: 52: 03Z</published>
    <title>Egocentric Auditory Attention Localization in Conversations</title>
    <summary>  In a noisy conversation environment such as a dinner party, people often
exhibit selective auditory attention, or the ability to focus on a particular
speaker while tuning out others. Recognizing who somebody is listening to in a
conversation is essential for developing technologies that can understand
social behavior and devices that can augment human hearing by amplifying
particular sound sources. The computer vision and audio research communities
have made great strides towards recognizing sound sources and speakers in
scenes. In this work, we take a step further by focusing on the problem of
localizing auditory attention targets in egocentric video, or detecting who in
a camera wearer's field of view they are listening to. To tackle the new and
challenging Selective Auditory Attention Localization problem, we propose an
end-to-end deep learning approach that uses egocentric video and multichannel
audio to predict the heatmap of the camera wearer's auditory attention. Our
approach leverages spatiotemporal audiovisual features and holistic reasoning
about the scene to make predictions, and outperforms a set of baselines on a
challenging multi-speaker conversation dataset. Project page:
https: //fkryan.github.io/saal
</summary>
    <author>
      <name>Fiona Ryan</name>
    </author>
    <author>
      <name>Hao Jiang</name>
    </author>
    <author>
      <name>Abhinav Shukla</name>
    </author>
    <author>
      <name>James M. Rehg</name>
    </author>
    <author>
      <name>Vamsi Krishna Ithapu</name>
    </author>
    <link href="http://arxiv.org/abs/2303.16024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.16024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2312.14401v1</id>
    <updated>2023-12-22T03: 03: 19Z</updated>
    <published>2023-12-22T03: 03: 19Z</published>
    <title>Towards an Exploratory Visual Analytics System for Griefer
  Identification in MOBA Games</title>
    <summary>  Multiplayer Online Battle Arenas (MOBAs) have gained a significant player
base worldwide, generating over two billion US dollars in annual game revenue.
However, the presence of griefers, who deliberately irritate and harass other
players within the game, can have a detrimental impact on players' experience,
compromising game fairness and potentially leading to the emergence of gray
industries. Unfortunately, the absence of a standardized criterion, and the
lack of high-quality labeled and annotated data has made it challenging to
detect the presence of griefers. Given the complexity of the multivariant
spatiotemporal data for MOBA games, game developers heavily rely on manual
review of entire game video recordings to label and annotate griefers, which is
a time-consuming process. To alleviate this issue, we have collaborated with a
team of game specialists to develop an interactive visual analysis interface,
called GrieferLens. It overviews players' behavior analysis and synthesizes
their key match events. By presenting multiple views of information,
GrieferLens can help the game design team efficiently recognize and label
griefers in MOBA games and build up a foundation for creating a more enjoyable
and fair gameplay environment.
</summary>
    <author>
      <name>Zixin Chen</name>
    </author>
    <author>
      <name>Shiyi Liu</name>
    </author>
    <author>
      <name>Zhihua Jin</name>
    </author>
    <author>
      <name>Gaoping Huang</name>
    </author>
    <author>
      <name>Yang Chao</name>
    </author>
    <author>
      <name>Zhenchuan Yang</name>
    </author>
    <author>
      <name>Quan Li</name>
    </author>
    <author>
      <name>Huamin Qu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE VIS 2023 (Poster)</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.14401v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.14401v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2501.02038v1</id>
    <updated>2025-01-03T14: 12: 40Z</updated>
    <published>2025-01-03T14: 12: 40Z</published>
    <title>Architecture for Trajectory-Based Fishing Ship Classification with AIS
  Data</title>
    <summary>  This paper proposes a data preparation process for managing real-world
kinematic data and detecting fishing vessels. The solution is a binary
classification that classifies ship trajectories into either fishing or
non-fishing ships. The data used are characterized by the typical problems
found in classic data mining applications using real-world data, such as noise
and inconsistencies. The two classes are also clearly unbalanced in the data, a
problem which is addressed using algorithms that resample the instances. For
classification, a series of features are extracted from spatiotemporal data
that represent the trajectories of the ships, available from sequences of
Automatic Identification System (AIS) reports. These features are proposed for
the modelling of ship behavior but, because they do not contain context-related
information, the classification can be applied in other scenarios.
Experimentation shows that the proposed data preparation process is useful for
the presented classification problem. In addition, positive results are
obtained using minimal information.
</summary>
    <author>
      <name>David Sánchez Pedroche</name>
    </author>
    <author>
      <name>Daniel Amigo</name>
    </author>
    <author>
      <name>Jesús García</name>
    </author>
    <author>
      <name>Jose M. Molina</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/s20133782</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/s20133782" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Sensors 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.02038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.02038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2106.11149v1</id>
    <updated>2021-06-21T14: 39: 35Z</updated>
    <published>2021-06-21T14: 39: 35Z</published>
    <title>OadTR: Online Action Detection with Transformers</title>
    <summary>  Most recent approaches for online action detection tend to apply Recurrent
Neural Network (RNN) to capture long-range temporal structure. However, RNN
suffers from non-parallelism and gradient vanishing, hence it is hard to be
optimized. In this paper, we propose a new encoder-decoder framework based on
Transformers, named OadTR, to tackle these problems. The encoder attached with
a task token aims to capture the relationships and global interactions between
historical observations. The decoder extracts auxiliary information by
aggregating anticipated future clip representations. Therefore, OadTR can
recognize current actions by encoding historical information and predicting
future context simultaneously. We extensively evaluate the proposed OadTR on
three challenging datasets: HDD, TVSeries, and THUMOS14. The experimental
results show that OadTR achieves higher training and inference speeds than
current RNN based approaches, and significantly outperforms the
state-of-the-art methods in terms of both mAP and mcAP. Code is available at
https: //github.com/wangxiang1230/OadTR.
</summary>
    <author>
      <name>Xiang Wang</name>
    </author>
    <author>
      <name>Shiwei Zhang</name>
    </author>
    <author>
      <name>Zhiwu Qing</name>
    </author>
    <author>
      <name>Yuanjie Shao</name>
    </author>
    <author>
      <name>Zhengrong Zuo</name>
    </author>
    <author>
      <name>Changxin Gao</name>
    </author>
    <author>
      <name>Nong Sang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code is available at https: //github.com/wangxiang1230/OadTR</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.11149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.11149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1805.02860v1</id>
    <updated>2018-05-08T07: 09: 12Z</updated>
    <published>2018-05-08T07: 09: 12Z</published>
    <title>Visual Attribute-augmented Three-dimensional Convolutional Neural
  Network for Enhanced Human Action Recognition</title>
    <summary>  Visual attributes in individual video frames, such as the presence of
characteristic objects and scenes, offer substantial information for action
recognition in videos. With individual 2D video frame as input, visual
attributes extraction could be achieved effectively and efficiently with more
sophisticated convolutional neural network than current 3D CNNs with
spatio-temporal filters, thanks to fewer parameters in 2D CNNs. In this paper,
the integration of visual attributes (including detection, encoding and
classification) into multi-stream 3D CNN is proposed for action recognition in
trimmed videos, with the proposed visual Attribute-augmented 3D CNN (A3D)
framework. The visual attribute pipeline includes an object detection network,
an attributes encoding network and a classification network. Our proposed A3D
framework achieves state-of-the-art performance on both the HMDB51 and the
UCF101 datasets.
</summary>
    <author>
      <name>Yunfeng Wang</name>
    </author>
    <author>
      <name>Wengang Zhou</name>
    </author>
    <author>
      <name>Qilin Zhang</name>
    </author>
    <author>
      <name>Houqiang Li</name>
    </author>
    <link href="http://arxiv.org/abs/1805.02860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2104.00646v1</id>
    <updated>2021-04-01T17: 44: 34Z</updated>
    <published>2021-04-01T17: 44: 34Z</published>
    <title>Motion Guided Attention Fusion to Recognize Interactions from Videos</title>
    <summary>  We present a dual-pathway approach for recognizing fine-grained interactions
from videos. We build on the success of prior dual-stream approaches, but make
a distinction between the static and dynamic representations of objects and
their interactions explicit by introducing separate motion and object detection
pathways. Then, using our new Motion-Guided Attention Fusion module, we fuse
the bottom-up features in the motion pathway with features captured from object
detections to learn the temporal aspects of an action. We show that our
approach can generalize across appearance effectively and recognize actions
where an actor interacts with previously unseen objects. We validate our
approach using the compositional action recognition task from the
Something-Something-v2 dataset where we outperform existing state-of-the-art
methods. We also show that our method can generalize well to real world tasks
by showing state-of-the-art performance on recognizing humans assembling
various IKEA furniture on the IKEA-ASM dataset.
</summary>
    <author>
      <name>Tae Soo Kim</name>
    </author>
    <author>
      <name>Jonathan Jones</name>
    </author>
    <author>
      <name>Gregory D. Hager</name>
    </author>
    <link href="http://arxiv.org/abs/2104.00646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.00646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2306.11546v2</id>
    <updated>2023-10-23T04: 26: 44Z</updated>
    <published>2023-06-20T13: 59: 20Z</published>
    <title>Bullying10K: A Large-Scale Neuromorphic Dataset towards
  Privacy-Preserving Bullying Recognition</title>
    <summary>  The prevalence of violence in daily life poses significant threats to
individuals' physical and mental well-being. Using surveillance cameras in
public spaces has proven effective in proactively deterring and preventing such
incidents. However, concerns regarding privacy invasion have emerged due to
their widespread deployment. To address the problem, we leverage Dynamic Vision
Sensors (DVS) cameras to detect violent incidents and preserve privacy since it
captures pixel brightness variations instead of static imagery. We introduce
the Bullying10K dataset, encompassing various actions, complex movements, and
occlusions from real-life scenarios. It provides three benchmarks for
evaluating different tasks: action recognition, temporal action localization,
and pose estimation. With 10,
                            000 event segments, totaling 12 billion events and
255 GB of data, Bullying10K contributes significantly by balancing violence
detection and personal privacy persevering. And it also poses a challenge to
the neuromorphic dataset. It will serve as a valuable resource for training and
developing privacy-protecting video systems. The Bullying10K opens new
possibilities for innovative approaches in these domains.
</summary>
    <author>
      <name>Yiting Dong</name>
    </author>
    <author>
      <name>Yang Li</name>
    </author>
    <author>
      <name>Dongcheng Zhao</name>
    </author>
    <author>
      <name>Guobin Shen</name>
    </author>
    <author>
      <name>Yi Zeng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the 37th Conference on Neural Information Processing
  Systems (NeurIPS 2023) Track on Datasets and Benchmarks</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.11546v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.11546v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2304.02934v1</id>
    <updated>2023-04-06T08: 48: 01Z</updated>
    <published>2023-04-06T08: 48: 01Z</published>
    <title>Boundary-Denoising for Video Activity Localization</title>
    <summary>  Video activity localization aims at understanding the semantic content in
long untrimmed videos and retrieving actions of interest. The retrieved action
with its start and end locations can be used for highlight generation, temporal
action detection, etc. Unfortunately, learning the exact boundary location of
activities is highly challenging because temporal activities are continuous in
time, and there are often no clear-cut transitions between actions. Moreover,
the definition of the start and end of events is subjective, which may confuse
the model. To alleviate the boundary ambiguity, we propose to study the video
activity localization problem from a denoising perspective. Specifically, we
propose an encoder-decoder model named DenoiseLoc. During training, a set of
action spans is randomly generated from the ground truth with a controlled
noise scale. Then we attempt to reverse this process by boundary denoising,
allowing the localizer to predict activities with precise boundaries and
resulting in faster convergence speed. Experiments show that DenoiseLoc
advances %in several video activity understanding tasks. For example, we
observe a gain of +12.36% average mAP on QV-Highlights dataset and +1.64%
mAP@0.5 on THUMOS'14 dataset over the baseline. Moreover, DenoiseLoc achieves
state-of-the-art performance on TACoS and MAD datasets, but with much fewer
predictions compared to other current methods.
</summary>
    <author>
      <name>Mengmeng Xu</name>
    </author>
    <author>
      <name>Mattia Soldan</name>
    </author>
    <author>
      <name>Jialin Gao</name>
    </author>
    <author>
      <name>Shuming Liu</name>
    </author>
    <author>
      <name>Juan-Manuel Pérez-Rúa</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <link href="http://arxiv.org/abs/2304.02934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.02934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.05559v2</id>
    <updated>2024-04-09T07: 43: 29Z</updated>
    <published>2024-04-08T14: 30: 42Z</published>
    <title>TIM: A Time Interval Machine for Audio-Visual Action Recognition</title>
    <summary>  Diverse actions give rise to rich audio-visual signals in long videos. Recent
works showcase that the two modalities of audio and video exhibit different
temporal extents of events and distinct labels. We address the interplay
between the two modalities in long videos by explicitly modelling the temporal
extents of audio and visual events. We propose the Time Interval Machine (TIM)
where a modality-specific time interval poses as a query to a transformer
encoder that ingests a long video input. The encoder then attends to the
specified interval, as well as the surrounding context in both modalities, in
order to recognise the ongoing action.
  We test TIM on three long audio-visual video datasets: EPIC-KITCHENS,
Perception Test, and AVE, reporting state-of-the-art (SOTA) for recognition. On
EPIC-KITCHENS, we beat previous SOTA that utilises LLMs and significantly
larger pre-training by 2.9% top-1 action recognition accuracy. Additionally, we
show that TIM can be adapted for action detection, using dense multi-scale
interval queries, outperforming SOTA on EPIC-KITCHENS-100 for most metrics, and
showing strong performance on the Perception Test. Our ablations show the
critical role of integrating the two modalities and modelling their time
intervals in achieving this performance. Code and models at:
https: //github.com/JacobChalk/TIM
</summary>
    <author>
      <name>Jacob Chalk</name>
    </author>
    <author>
      <name>Jaesung Huh</name>
    </author>
    <author>
      <name>Evangelos Kazakos</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
    <author>
      <name>Dima Damen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CVPR 2024. Project Webpage:
  https: //jacobchalk.github.io/TIM-Project</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.05559v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.05559v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1706.04729v1</id>
    <updated>2017-06-15T03: 42: 02Z</updated>
    <published>2017-06-15T03: 42: 02Z</published>
    <title>Sequential detection of low-rank changes using extreme eigenvalues</title>
    <summary>  We study the problem of detecting an abrupt change to the signal covariance
matrix. In particular, the covariance changes from a "white" identity matrix to
an unknown spiked or low-rank matrix. Two sequential change-point detection
procedures are presented, based on the largest and the smallest eigenvalues of
the sample covariance matrix. To control false-alarm-rate, we present an
accurate theoretical approximation to the average-run-length (ARL) and expected
detection delay (EDD) of the detection, leveraging the extreme eigenvalue
distributions from random matrix theory and by capturing a non-negligible
temporal correlation in the sequence of scan statistics due to the sliding
window approach. Real data examples demonstrate the good performance of our
method for detecting behavior change of a swarm.
</summary>
    <author>
      <name>Liyan Xie</name>
    </author>
    <author>
      <name>Yao Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.04729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1710.02458v1</id>
    <updated>2017-10-06T15: 43: 44Z</updated>
    <published>2017-10-06T15: 43: 44Z</published>
    <title>Machine Learning for Drug Overdose Surveillance</title>
    <summary>  We describe two recently proposed machine learning approaches for discovering
emerging trends in fatal accidental drug overdoses. The Gaussian Process Subset
Scan enables early detection of emerging patterns in spatio-temporal data,
accounting for both the non-iid nature of the data and the fact that detecting
subtle patterns requires integration of information across multiple spatial
areas and multiple time steps. We apply this approach to 17 years of
county-aggregated data for monthly opioid overdose deaths in the New York City
metropolitan area, showing clear advantages in the utility of discovered
patterns as compared to typical anomaly detection approaches.
  To detect and characterize emerging overdose patterns that differentially
affect a subpopulation of the data, including geographic, demographic, and
behavioral patterns (e.g., which combinations of drugs are involved), we apply
the Multidimensional Tensor Scan to 8 years of case-level overdose data from
Allegheny County, PA. We discover previously unidentified overdose patterns
which reveal unusual demographic clusters, show impacts of drug legislation,
and demonstrate potential for early detection and targeted intervention. These
approaches to early detection of overdose patterns can inform prevention and
response efforts, as well as understanding the effects of policy changes.
</summary>
    <author>
      <name>Daniel B. Neill</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Carnegie Mellon University</arxiv:affiliation>
    </author>
    <author>
      <name>William Herlands</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Carnegie Mellon University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the Data For Good Exchange 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.02458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2110.12148v2</id>
    <updated>2023-02-13T22: 02: 59Z</updated>
    <published>2021-10-23T05: 52: 03Z</published>
    <title>Event Detection on Dynamic Graphs</title>
    <summary>  Event detection is a critical task for timely decision-making in graph
analytics applications. Despite the recent progress towards deep learning on
graphs, event detection on dynamic graphs presents particular challenges to
existing architectures. Real-life events are often associated with sudden
deviations of the normal behavior of the graph. However, existing approaches
for dynamic node embedding are unable to capture the graph-level dynamics
related to events. In this paper, we propose DyGED, a simple yet novel deep
learning model for event detection on dynamic graphs. DyGED learns correlations
between the graph macro dynamics -- i.e. a sequence of graph-level
representations -- and labeled events. Moreover, our approach combines
structural and temporal self-attention mechanisms to account for
application-specific node and time importances effectively. Our experimental
evaluation, using a representative set of datasets, demonstrates that DyGED
outperforms competing solutions in terms of event detection accuracy by up to
8.5% while being more scalable than the top alternatives. We also present case
studies illustrating key features of our model.
</summary>
    <author>
      <name>Mert Kosan</name>
    </author>
    <author>
      <name>Arlei Silva</name>
    </author>
    <author>
      <name>Sourav Medya</name>
    </author>
    <author>
      <name>Brian Uzzi</name>
    </author>
    <author>
      <name>Ambuj Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Longer version of "Graph Macro Dynamics with Self-Attention for Event
  Detection" accepted to DLG-AAAI 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.12148v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.12148v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2112.07661v1</id>
    <updated>2021-12-14T18: 57: 44Z</updated>
    <published>2021-12-14T18: 57: 44Z</published>
    <title>Approaches Toward Physical and General Video Anomaly Detection</title>
    <summary>  In recent years, many works have addressed the problem of finding
never-seen-before anomalies in videos. Yet, most work has been focused on
detecting anomalous frames in surveillance videos taken from security cameras.
Meanwhile, the task of anomaly detection (AD) in videos exhibiting anomalous
mechanical behavior, has been mostly overlooked. Anomaly detection in such
videos is both of academic and practical interest, as they may enable automatic
detection of malfunctions in many manufacturing, maintenance, and real-life
settings. To assess the potential of the different approaches to detect such
anomalies, we evaluate two simple baseline approaches: (i) Temporal-pooled
image AD techniques. (ii) Density estimation of videos represented with
features pretrained for video-classification.
  Development of such methods calls for new benchmarks to allow evaluation of
different possible approaches. We introduce the Physical Anomalous Trajectory
or Motion (PHANTOM) dataset, which contains six different video classes. Each
class consists of normal and anomalous videos. The classes differ in the
presented phenomena, the normal class variability, and the kind of anomalies in
the videos. We also suggest an even harder benchmark where anomalous activities
should be spotted on highly variable scenes.
</summary>
    <author>
      <name>Laura Kart</name>
    </author>
    <author>
      <name>Niv Cohen</name>
    </author>
    <link href="http://arxiv.org/abs/2112.07661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.07661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.12134v1</id>
    <updated>2024-04-18T12: 35: 24Z</updated>
    <published>2024-04-18T12: 35: 24Z</published>
    <title>Warped Time Series Anomaly Detection</title>
    <summary>  This paper addresses the problem of detecting time series outliers, focusing
on systems with repetitive behavior, such as industrial robots operating on
production lines.Notable challenges arise from the fact that a task performed
multiple times may exhibit different duration in each repetition and that the
time series reported by the sensors are irregularly sampled because of data
gaps. The anomaly detection approach presented in this paper consists of three
stages.The first stage identifies the repetitive cycles in the lengthy time
series and segments them into individual time series corresponding to one task
cycle, while accounting for possible temporal distortions.The second stage
computes a prototype for the cycles using a GPU-based barycenter algorithm,
specifically tailored for very large time series.The third stage uses the
prototype to detect abnormal cycles by computing an anomaly score for each
cycle.The overall approach, named WarpEd Time Series ANomaly Detection
(WETSAND), makes use of the Dynamic Time Warping algorithm and its variants
because they are suited to the distorted nature of the time series.The
experiments show that \wetsand scales to large signals, computes human-friendly
prototypes, works with very little data, and outperforms some general purpose
anomaly detection approaches such as autoencoders.
</summary>
    <author>
      <name>Charlotte Lacoquelle</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS-DISCO, UT</arxiv:affiliation>
    </author>
    <author>
      <name>Xavier Pucel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS-DISCO, UT</arxiv:affiliation>
    </author>
    <author>
      <name>Louise Travé-Massuyès</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAAS-DISCO, UT</arxiv:affiliation>
    </author>
    <author>
      <name>Axel Reymonet</name>
    </author>
    <author>
      <name>Benoît Enaux</name>
    </author>
    <link href="http://arxiv.org/abs/2404.12134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.12134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2306.00576v1</id>
    <updated>2023-06-01T11: 45: 33Z</updated>
    <published>2023-06-01T11: 45: 33Z</published>
    <title>MammalNet: A Large-scale Video Benchmark for Mammal Recognition and
  Behavior Understanding</title>
    <summary>  Monitoring animal behavior can facilitate conservation efforts by providing
key insights into wildlife health, population status, and ecosystem function.
Automatic recognition of animals and their behaviors is critical for
capitalizing on the large unlabeled datasets generated by modern video devices
and for accelerating monitoring efforts at scale. However, the development of
automated recognition systems is currently hindered by a lack of appropriately
labeled datasets. Existing video datasets 1) do not classify animals according
to established biological taxonomies; 2) are too small to facilitate
large-scale behavioral studies and are often limited to a single species; and
3) do not feature temporally localized annotations and therefore do not
facilitate localization of targeted behaviors within longer video sequences.
Thus, we propose MammalNet, a new large-scale animal behavior dataset with
taxonomy-guided annotations of mammals and their common behaviors. MammalNet
contains over 18K videos totaling 539 hours, which is ~10 times larger than the
largest existing animal behavior dataset. It covers 17 orders,
                            69 families, and
173 mammal categories for animal categorization and captures 12 high-level
animal behaviors that received focus in previous animal behavior studies. We
establish three benchmarks on MammalNet: standard animal and behavior
recognition, compositional low-shot animal and behavior recognition, and
behavior detection. Our dataset and code have been made available at:
https: //mammal-net.github.io.
</summary>
    <author>
      <name>Jun Chen</name>
    </author>
    <author>
      <name>Ming Hu</name>
    </author>
    <author>
      <name>Darren J. Coker</name>
    </author>
    <author>
      <name>Michael L. Berumen</name>
    </author>
    <author>
      <name>Blair Costelloe</name>
    </author>
    <author>
      <name>Sara Beery</name>
    </author>
    <author>
      <name>Anna Rohrbach</name>
    </author>
    <author>
      <name>Mohamed Elhoseiny</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2023 proceeding</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.00576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.00576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1601.07000v1</id>
    <updated>2016-01-26T12: 56: 24Z</updated>
    <published>2016-01-26T12: 56: 24Z</published>
    <title>INTEGRAL study of temporal properties of bright flares in Supergiant
  Fast X-ray Transients</title>
    <summary>  We have characterized the typical temporal behaviour of the bright X-ray
flares detected from the three Supergiant Fast X-ray Transients showing the
most extreme transient behaviour (XTEJ1739-302, IGRJ17544-2619,
SAXJ1818.6-1703). We focus here on the cumulative distributions of the
waiting-time (time interval between two consecutive X-ray flares), and the
duration of the hard X-ray activity (duration of the brightest phase of an SFXT
outburst), as observed by INTEGRAL/IBIS in the energy band 17-50 keV. Adopting
the cumulative distribution of waiting-times, it is possible to identify the
typical timescale that clearly separates different outbursts, each composed by
several single flares at ks timescale. This allowed us to measure the duration
of the brightest phase of the outbursts from these three targets, finding that
they show heavy-tailed cumulative distributions. We observe a correlation
between the total energy emitted during SFXT outbursts and the time interval
covered by the outbursts (defined as the elapsed time between the first and the
last flare belonging to the same outburst as observed by INTEGRAL). We show
that temporal properties of flares and outbursts of the sources, which share
common properties regardless different orbital parameters, can be interpreted
in the model of magnetized stellar winds with fractal structure from the
OB-supergiant stars.
</summary>
    <author>
      <name>L. Sidoli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INAF/IASF-Milano, Italy</arxiv:affiliation>
    </author>
    <author>
      <name>A. Paizis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INAF/IASF-Milano, Italy</arxiv:affiliation>
    </author>
    <author>
      <name>K. Postnov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Moscow Lomonosov State University, Russia</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/mnras/stw237</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/mnras/stw237" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,
                            8 figures,
                            1 table. Accepted for publication in MNRAS
  (Accepted 2016 January 26. Received 2016 January 25 ; in original form 2015
  December 15)</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.07000v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.07000v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1605.09628v3</id>
    <updated>2016-12-12T15: 52: 20Z</updated>
    <published>2016-05-31T13: 38: 58Z</published>
    <title>Spatio-temporal correlations in models of collective motion ruled by
  different dynamical laws</title>
    <summary>  Information transfer is an essential factor in determining the robustness of
collective behaviour in biological systems with distributed control. The most
direct way to study the information transfer mechanisms is to experimentally
detect the propagation across the system of a signal triggered by some
perturbation. However, for field experiments this method is inefficient, as the
possibilities of the observer to perturb the group are limited and empirical
observations must rely on rare natural perturbations. An alternative way is to
use spatio-temporal correlations to assess the information transfer mechanism
directly from the spontaneous fluctuations of the system, without the need to
have an actual propagating signal on record. We test the approach on ground
truth data provided by numerical simulations in three dimensions of two models
of collective behaviour characterized by very different dynamical equations and
information transfer mechanisms: the classic Vicsek model, describing an
overdamped noninertial dynamics and the inertial spin model, characterized by
an un- derdamped inertial dynamics. By using dynamical finite size scaling, we
show that spatio-temporal correlations are able to distinguish unambiguously
the diffusive information transfer mechanism of the Vicsek model from the
linear mechanism of the inertial spin model.
</summary>
    <author>
      <name>Andrea Cavagna</name>
    </author>
    <author>
      <name>Daniele Conti</name>
    </author>
    <author>
      <name>Irene Giardina</name>
    </author>
    <author>
      <name>Tomas S. Grigera</name>
    </author>
    <author>
      <name>Stefania Melillo</name>
    </author>
    <author>
      <name>Massimiliano Viale</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1478-3975/13/6/065001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1478-3975/13/6/065001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">key references about dynamical scaling added; a few typos corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.09628v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09628v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2010.05654v1</id>
    <updated>2020-10-12T12: 50: 30Z</updated>
    <published>2020-10-12T12: 50: 30Z</published>
    <title>The MECCANO Dataset: Understanding Human-Object Interactions from
  Egocentric Videos in an Industrial-like Domain</title>
    <summary>  Wearable cameras allow to collect images and videos of humans interacting
with the world. While human-object interactions have been thoroughly
investigated in third person vision, the problem has been understudied in
egocentric settings and in industrial scenarios. To fill this gap, we introduce
MECCANO, the first dataset of egocentric videos to study human-object
interactions in industrial-like settings. MECCANO has been acquired by 20
participants who were asked to build a motorbike model, for which they had to
interact with tiny objects and tools. The dataset has been explicitly labeled
for the task of recognizing human-object interactions from an egocentric
perspective. Specifically, each interaction has been labeled both temporally
(with action segments) and spatially (with active object bounding boxes). With
the proposed dataset, we investigate four different tasks including 1) action
recognition,
                            2) active object detection,
                            3) active object recognition and 4)
egocentric human-object interaction detection, which is a revisited version of
the standard human-object interaction detection task. Baseline results show
that the MECCANO dataset is a challenging benchmark to study egocentric
human-object interactions in industrial-like scenarios. We publicy release the
dataset at https: //iplab.dmi.unict.it/MECCANO.
</summary>
    <author>
      <name>Francesco Ragusa</name>
    </author>
    <author>
      <name>Antonino Furnari</name>
    </author>
    <author>
      <name>Salvatore Livatino</name>
    </author>
    <author>
      <name>Giovanni Maria Farinella</name>
    </author>
    <link href="http://arxiv.org/abs/2010.05654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.05654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2112.11384v1</id>
    <updated>2021-12-16T10: 17: 59Z</updated>
    <published>2021-12-16T10: 17: 59Z</published>
    <title>Sports Video: Fine-Grained Action Detection and Classification of Table
  Tennis Strokes from Videos for MediaEval 2021</title>
    <summary>  Sports video analysis is a prevalent research topic due to the variety of
application areas, ranging from multimedia intelligent devices with
user-tailored digests up to analysis of athletes' performance. The Sports Video
task is part of the MediaEval 2021 benchmark. This task tackles fine-grained
action detection and classification from videos. The focus is on recordings of
table tennis games. Running since 2019, the task has offered a classification
challenge from untrimmed video recorded in natural conditions with known
temporal boundaries for each stroke. This year, the dataset is extended and
offers, in addition, a detection challenge from untrimmed videos without
annotations. This work aims at creating tools for sports coaches and players in
order to analyze sports performance. Movement analysis and player profiling may
be built upon such technology to enrich the training experience of athletes and
improve their performance.
</summary>
    <author>
      <name>Pierre-Etienne Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI, MPI-EVA, UB</arxiv:affiliation>
    </author>
    <author>
      <name>Jordan Calandre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIA</arxiv:affiliation>
    </author>
    <author>
      <name>Boris Mansencal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Jenny Benois-Pineau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Renaud Péteri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIA</arxiv:affiliation>
    </author>
    <author>
      <name>Laurent Mascarilla</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIA</arxiv:affiliation>
    </author>
    <author>
      <name>Julien Morlier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMS</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MediaEval 2021, Dec 2021, Online, Germany</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.11384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.11384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2005.04437v5</id>
    <updated>2020-08-14T15: 56: 51Z</updated>
    <published>2020-05-09T13: 05: 06Z</published>
    <title>Understanding Dynamic Scenes using Graph Convolution Networks</title>
    <summary>  We present a novel Multi-Relational Graph Convolutional Network (MRGCN) based
framework to model on-road vehicle behaviors from a sequence of temporally
ordered frames as grabbed by a moving monocular camera. The input to MRGCN is a
multi-relational graph where the graph's nodes represent the active and passive
agents/objects in the scene, and the bidirectional edges that connect every
pair of nodes are encodings of their Spatio-temporal relations. We show that
this proposed explicit encoding and usage of an intermediate spatio-temporal
interaction graph to be well suited for our tasks over learning end-end
directly on a set of temporally ordered spatial relations. We also propose an
attention mechanism for MRGCNs that conditioned on the scene dynamically scores
the importance of information from different interaction types. The proposed
framework achieves significant performance gain over prior methods on
vehicle-behavior classification tasks on four datasets. We also show a seamless
transfer of learning to multiple datasets without resorting to fine-tuning.
Such behavior prediction methods find immediate relevance in a variety of
navigation tasks such as behavior planning, state estimation, and applications
relating to the detection of traffic violations over videos.
</summary>
    <author>
      <name>Sravan Mylavarapu</name>
    </author>
    <author>
      <name>Mahtab Sandhu</name>
    </author>
    <author>
      <name>Priyesh Vijayan</name>
    </author>
    <author>
      <name>K Madhava Krishna</name>
    </author>
    <author>
      <name>Balaraman Ravindran</name>
    </author>
    <author>
      <name>Anoop Namboodiri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at IROS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.04437v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.04437v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2409.18844v3</id>
    <updated>2024-10-29T13: 47: 35Z</updated>
    <published>2024-09-27T15: 41: 31Z</published>
    <title>Classification and Spatiotemporal Correlation of Dominant Fluctuations
  in Complex Dynamical Systems</title>
    <summary>  The behavior of many complex systems, from nanostructured materials to animal
colonies, is governed by local transitions that, while involving a restricted
number of interacting units, may generate collective cascade phenomena.
Tracking such local events and understanding how they emerge and propagate
throughout these systems represent often a challenge. Common strategies monitor
specific parameters, tailored ad hoc to describe certain systems, over time.
However, such approaches typically require prior knowledge of the underpinning
physics and are poorly transferable to different systems. Here we present LEAP,
a general, transferable, agnostic analysis approach that can reveal precious
information on the physics of a variety of complex dynamical systems simply
starting from the trajectory of their constitutive units. Built on a bivariate
combination of two abstract descriptors, LENS and {\tau
                            }SOAP, the LEAP analysis
allows (i) detecting the emergence of local fluctuations in simulation or
experimentally-acquired trajectories of any type of multicomponent system, (ii)
classifying fluctuations into categories, and (iii) correlating them in space
and time. We demonstrate how LEAP, just building on the abstract concepts of
local fluctuations and their spatiotemporal correlation, efficiently reveals
precious insights on the emergence and propagation of local and collective
phenomena in a variety of complex dynamical systems ranging from the atomic- to
the microscopic-scale. Given its abstract character, we expect that LEAP will
offer an important tool to understand and predict the behavior of systems whose
physics is unknown a priori, as well as to revisit a variety of known complex
physical phenomena under a new perspective.
</summary>
    <author>
      <name>Cristina Caruso</name>
    </author>
    <author>
      <name>Martina Crippa</name>
    </author>
    <author>
      <name>Annalisa Cardellini</name>
    </author>
    <author>
      <name>Matteo Cioni</name>
    </author>
    <author>
      <name>Mattia Perrone</name>
    </author>
    <author>
      <name>Massimo Delle Piane</name>
    </author>
    <author>
      <name>Giovanni M. Pavan</name>
    </author>
    <link href="http://arxiv.org/abs/2409.18844v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.18844v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2208.03648v1</id>
    <updated>2022-08-07T05: 49: 55Z</updated>
    <published>2022-08-07T05: 49: 55Z</published>
    <title>Weakly Supervised Online Action Detection for Infant General Movements</title>
    <summary>  To make the earlier medical intervention of infants' cerebral palsy (CP),
early diagnosis of brain damage is critical. Although general movements
assessment(GMA) has shown promising results in early CP detection, it is
laborious. Most existing works take videos as input to make fidgety
movements(FMs) classification for the GMA automation. Those methods require a
complete observation of videos and can not localize video frames containing
normal FMs. Therefore we propose a novel approach named WO-GMA to perform FMs
localization in the weakly supervised online setting. Infant body keypoints are
first extracted as the inputs to WO-GMA. Then WO-GMA performs local
spatio-temporal extraction followed by two network branches to generate pseudo
clip labels and model online actions. With the clip-level pseudo labels, the
action modeling branch learns to detect FMs in an online fashion. Experimental
results on a dataset with 757 videos of different infants show that WO-GMA can
get state-of-the-art video-level classification and cliplevel detection
results. Moreover, only the first 20% duration of the video is needed to get
classification results as good as fully observed, implying a significantly
shortened FMs diagnosis time. Code is available at:
https: //github.com/scofiedluo/WO-GMA.
</summary>
    <author>
      <name>Tongyi Luo</name>
    </author>
    <author>
      <name>Jia Xiao</name>
    </author>
    <author>
      <name>Chuncao Zhang</name>
    </author>
    <author>
      <name>Siheng Chen</name>
    </author>
    <author>
      <name>Yuan Tian</name>
    </author>
    <author>
      <name>Guangjun Yu</name>
    </author>
    <author>
      <name>Kang Dang</name>
    </author>
    <author>
      <name>Xiaowei Ding</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MICCAI 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.03648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.03648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T06" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; I.4; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2212.00501v1</id>
    <updated>2022-12-01T13: 52: 32Z</updated>
    <published>2022-12-01T13: 52: 32Z</published>
    <title>Crowd-level Abnormal Behavior Detection via Multi-scale Motion
  Consistency Learning</title>
    <summary>  Detecting abnormal crowd motion emerging from complex interactions of
individuals is paramount to ensure the safety of crowds. Crowd-level abnormal
behaviors (CABs), e.g., counter flow and crowd turbulence, are proven to be the
crucial causes of many crowd disasters. In the recent decade, video anomaly
detection (VAD) techniques have achieved remarkable success in detecting
individual-level abnormal behaviors (e.g., sudden running, fighting and
stealing), but research on VAD for CABs is rather limited. Unlike
individual-level anomaly, CABs usually do not exhibit salient difference from
the normal behaviors when observed locally, and the scale of CABs could vary
from one scenario to another. In this paper, we present a systematic study to
tackle the important problem of VAD for CABs with a novel crowd motion learning
framework, multi-scale motion consistency network (MSMC-Net). MSMC-Net first
captures the spatial and temporal crowd motion consistency information in a
graph representation. Then, it simultaneously trains multiple feature graphs
constructed at different scales to capture rich crowd patterns. An attention
network is used to adaptively fuse the multi-scale features for better CAB
detection. For the empirical study, we consider three large-scale crowd event
datasets, UMN, Hajj and Love Parade. Experimental results show that MSMC-Net
could substantially improve the state-of-the-art performance on all the
datasets.
</summary>
    <author>
      <name>Linbo Luo</name>
    </author>
    <author>
      <name>Yuanjing Li</name>
    </author>
    <author>
      <name>Haiyan Yin</name>
    </author>
    <author>
      <name>Shangwei Xie</name>
    </author>
    <author>
      <name>Ruimin Hu</name>
    </author>
    <author>
      <name>Wentong Cai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version with appendix for the AAAI-23 publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.00501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.00501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2309.03739v1</id>
    <updated>2023-09-07T14: 28: 31Z</updated>
    <published>2023-09-07T14: 28: 31Z</published>
    <title>Detecting unknown HTTP-based malicious communication behavior via
  generated adversarial flows and hierarchical traffic features</title>
    <summary>  Malicious communication behavior is the network communication behavior
generated by malware (bot-net, spyware, etc.) after victim devices are
infected. Experienced adversaries often hide malicious information in HTTP
traffic to evade detection. However, related detection methods have inadequate
generalization ability because they are usually based on artificial feature
engineering and outmoded datasets. In this paper, we propose an HTTP-based
Malicious Communication traffic Detection Model (HMCD-Model) based on generated
adversarial flows and hierarchical traffic features. HMCD-Model consists of two
parts. The first is a generation algorithm based on WGAN-GP to generate
HTTP-based malicious communication traffic for data enhancement. The second is
a hybrid neural network based on CNN and LSTM to extract hierarchical
spatial-temporal features of traffic. In addition, we collect and publish a
dataset, HMCT-2020, which consists of large-scale malicious and benign traffic
during three years (2018-2020). Taking the data in HMCT-2020(18) as the
training set and the data in other datasets as the test set, the experimental
results show that the HMCD-Model can effectively detect unknown HTTP-based
malicious communication traffic. It can reach F1 = 98.66% in the dataset
HMCT-2020(19-20), F1 = 90.69% in the public dataset CIC-IDS-2017, and F1 =
83.66% in the real traffic, which is 20+% higher than other representative
methods on average. This validates that HMCD-Model has the ability to discover
unknown HTTP-based malicious communication behavior.
</summary>
    <author>
      <name>Xiaochun Yun</name>
    </author>
    <author>
      <name>Jiang Xie</name>
    </author>
    <author>
      <name>Shuhao Li</name>
    </author>
    <author>
      <name>Yongzheng Zhang</name>
    </author>
    <author>
      <name>Peishuai Sun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cose.2022.102834</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cose.2022.102834" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages,
                            9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.03739v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.03739v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2211.10144v1</id>
    <updated>2022-11-18T10: 37: 51Z</updated>
    <published>2022-11-18T10: 37: 51Z</published>
    <title>Computational Short Cuts in Infinite Domain Constraint Satisfaction</title>
    <summary>  A backdoor in a finite-domain CSP instance is a set of variables where each
possible instantiation moves the instance into a polynomial-time solvable
class. Backdoors have found many applications in artificial intelligence and
elsewhere, and the algorithmic problem of finding such backdoors has
consequently been intensively studied. Sioutis and Janhunen (Proc. 42nd German
Conference on AI (KI-2019)) have proposed a generalised backdoor concept
suitable for infinite-domain CSP instances over binary constraints. We
generalise their concept into a large class of CSPs that allow for higher-arity
constraints. We show that this kind of infinite-domain backdoors have many of
the positive computational properties that finite-domain backdoors have: the
associated computational problems are fixed-parameter tractable whenever the
underlying constraint language is finite. On the other hand, we show that
infinite languages make the problems considerably harder: the general backdoor
detection problem is W[
                                2
                            ]-hard and fixed-parameter tractability is ruled out
under standard complexity-theoretic assumptions. We demonstrate that backdoors
may have suboptimal behaviour on binary constraints -- this is detrimental from
an AI perspective where binary constraints are predominant in, for instance,
spatiotemporal applications. In response to this, we introduce sidedoors as an
alternative to backdoors. The fundamental computational problems for sidedoors
remain fixed-parameter tractable for finite constraint language (possibly also
containing non-binary relations). Moreover, the sidedoor approach has appealing
computational properties that sometimes leads to faster algorithms than the
backdoor approach.
</summary>
    <author>
      <name>Peter Jonsson</name>
    </author>
    <author>
      <name>Victor Lagerkvist</name>
    </author>
    <author>
      <name>Sebastian Ordyniak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1613/jair.1.13787</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1613/jair.1.13787" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Artificial Intelligence Research. 75 (2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2211.10144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.10144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2002.01132v1</id>
    <updated>2020-02-04T05: 32: 21Z</updated>
    <published>2020-02-04T05: 32: 21Z</published>
    <title>3D ResNet with Ranking Loss Function for Abnormal Activity Detection in
  Videos</title>
    <summary>  Abnormal activity detection is one of the most challenging tasks in the field
of computer vision. This study is motivated by the recent state-of-art work of
abnormal activity detection, which utilizes both abnormal and normal videos in
learning abnormalities with the help of multiple instance learning by providing
the data with video-level information. In the absence of temporal-annotations,
such a model is prone to give a false alarm while detecting the abnormalities.
For this reason, in this paper, we focus on the task of minimizing the false
alarm rate while performing an abnormal activity detection task. The mitigation
of these false alarms and recent advancement of 3D deep neural network in video
action recognition task collectively give us motivation to exploit the 3D
ResNet in our proposed method, which helps to extract spatial-temporal features
from the videos. Afterwards, using these features and deep multiple instance
learning along with the proposed ranking loss, our model learns to predict the
abnormality score at the video segment level. Therefore, our proposed method 3D
deep Multiple Instance Learning with ResNet (MILR) along with the new proposed
ranking loss function achieves the best performance on the UCF-Crime benchmark
dataset, as compared to other state-of-art methods. The effectiveness of our
proposed method is demonstrated on the UCF-Crime dataset.
</summary>
    <author>
      <name>Shikha Dubey</name>
    </author>
    <author>
      <name>Abhijeet Boragule</name>
    </author>
    <author>
      <name>Moongu Jeon</name>
    </author>
    <link href="http://arxiv.org/abs/2002.01132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.01132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.00322v1</id>
    <updated>2024-03-30T11: 21: 11Z</updated>
    <published>2024-03-30T11: 21: 11Z</published>
    <title>Instrument-tissue Interaction Detection Framework for Surgical Video
  Understanding</title>
    <summary>  Instrument-tissue interaction detection task, which helps understand surgical
activities, is vital for constructing computer-assisted surgery systems but
with many challenges. Firstly, most models represent instrument-tissue
interaction in a coarse-grained way which only focuses on classification and
lacks the ability to automatically detect instruments and tissues. Secondly,
existing works do not fully consider relations between intra- and inter-frame
of instruments and tissues. In the paper, we propose to represent
instrument-tissue interaction as &lt;instrument class, instrument bounding box,
tissue class, tissue bounding box, action class&gt; quintuple and present an
Instrument-Tissue Interaction Detection Network (ITIDNet) to detect the
quintuple for surgery videos understanding. Specifically, we propose a Snippet
Consecutive Feature (SCF) Layer to enhance features by modeling relationships
of proposals in the current frame using global context information in the video
snippet. We also propose a Spatial Corresponding Attention (SCA) Layer to
incorporate features of proposals between adjacent frames through spatial
encoding. To reason relationships between instruments and tissues, a Temporal
Graph (TG) Layer is proposed with intra-frame connections to exploit
relationships between instruments and tissues in the same frame and inter-frame
connections to model the temporal information for the same instance. For
evaluation, we build a cataract surgery video (PhacoQ) dataset and a
cholecystectomy surgery video (CholecQ) dataset. Experimental results
demonstrate the promising performance of our model, which outperforms other
state-of-the-art models on both datasets.
</summary>
    <author>
      <name>Wenjun Lin</name>
    </author>
    <author>
      <name>Yan Hu</name>
    </author>
    <author>
      <name>Huazhu Fu</name>
    </author>
    <author>
      <name>Mingming Yang</name>
    </author>
    <author>
      <name>Chin-Boon Chng</name>
    </author>
    <author>
      <name>Ryo Kawasaki</name>
    </author>
    <author>
      <name>Cheekong Chui</name>
    </author>
    <author>
      <name>Jiang Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2404.00322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.00322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2008.11308v2</id>
    <updated>2021-05-18T08: 13: 48Z</updated>
    <published>2020-08-25T23: 35: 01Z</published>
    <title>Identifying Coordinated Accounts on Social Media through Hidden
  Influence and Group Behaviours</title>
    <summary>  Disinformation campaigns on social media, involving coordinated activities
from malicious accounts towards manipulating public opinion, have become
increasingly prevalent. Existing approaches to detect coordinated accounts
either make very strict assumptions about coordinated behaviours, or require
part of the malicious accounts in the coordinated group to be revealed in order
to detect the rest. To address these drawbacks, we propose a generative model,
AMDN-HAGE (Attentive Mixture Density Network with Hidden Account Group
Estimation) which jointly models account activities and hidden group behaviours
based on Temporal Point Processes (TPP) and Gaussian Mixture Model (GMM), to
capture inherent characteristics of coordination which is, accounts that
coordinate must strongly influence each other's activities, and collectively
appear anomalous from normal accounts. To address the challenges of optimizing
the proposed model, we provide a bilevel optimization algorithm with
theoretical guarantee on convergence. We verified the effectiveness of the
proposed method and training algorithm on real-world social network data
collected from Twitter related to coordinated campaigns from Russia's Internet
Research Agency targeting the 2016 U.S. Presidential Elections, and to identify
coordinated campaigns related to the COVID-19 pandemic. Leveraging the learned
model, we find that the average influence between coordinated account pairs is
the highest.On COVID-19, we found coordinated group spreading anti-vaccination,
anti-masks conspiracies that suggest the pandemic is a hoax and political scam.
</summary>
    <author>
      <name>Karishma Sharma</name>
    </author>
    <author>
      <name>Yizhou Zhang</name>
    </author>
    <author>
      <name>Emilio Ferrara</name>
    </author>
    <author>
      <name>Yan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">KDD'2021 (Accepted)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data
  Mining 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2008.11308v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11308v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2212.09518v1</id>
    <updated>2022-12-19T14: 57: 52Z</updated>
    <published>2022-12-19T14: 57: 52Z</published>
    <title>FedTADBench: Federated Time-Series Anomaly Detection Benchmark</title>
    <summary>  Time series anomaly detection strives to uncover potential abnormal behaviors
and patterns from temporal data, and has fundamental significance in diverse
application scenarios. Constructing an effective detection model usually
requires adequate training data stored in a centralized manner, however, this
requirement sometimes could not be satisfied in realistic scenarios. As a
prevailing approach to address the above problem, federated learning has
demonstrated its power to cooperate with the distributed data available while
protecting the privacy of data providers. However, it is still unclear that how
existing time series anomaly detection algorithms perform with decentralized
data storage and privacy protection through federated learning. To study this,
we conduct a federated time series anomaly detection benchmark, named
FedTADBench, which involves five representative time series anomaly detection
algorithms and four popular federated learning methods. We would like to answer
the following questions: (1)How is the performance of time series anomaly
detection algorithms when meeting federated learning? (2) Which federated
learning method is the most appropriate one for time series anomaly detection?
(3) How do federated time series anomaly detection approaches perform on
different partitions of data in clients? Numbers of results as well as
corresponding analysis are provided from extensive experiments with various
settings. The source code of our benchmark is publicly available at
https: //github.com/fanxingliu2020/FedTADBench.
</summary>
    <author>
      <name>Fanxing Liu</name>
    </author>
    <author>
      <name>Cheng Zeng</name>
    </author>
    <author>
      <name>Le Zhang</name>
    </author>
    <author>
      <name>Yingjie Zhou</name>
    </author>
    <author>
      <name>Qing Mu</name>
    </author>
    <author>
      <name>Yanru Zhang</name>
    </author>
    <author>
      <name>Ling Zhang</name>
    </author>
    <author>
      <name>Ce Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,
                            6 figures, published by IEEE HPCC 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.09518v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.09518v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2001.07158v4</id>
    <updated>2020-07-27T14: 40: 49Z</updated>
    <published>2020-01-20T16: 13: 27Z</published>
    <title>Finding path motifs in large temporal graphs using algebraic
  fingerprints</title>
    <summary>  We study a family of pattern-detection problems in vertex-colored temporal
graphs. In particular, given a vertex-colored temporal graph and a multiset of
colors as a query, we search for temporal paths in the graph that contain the
colors specified in the query. These types of problems have several
applications, for example in recommending tours for tourists or detecting
abnormal behavior in a network of financial transactions. For the family of
pattern-detection problems we consider, we establish complexity results and
design an algebraic-algorithmic framework based on constrained multilinear
sieving. We demonstrate that our solution scales to massive graphs with up to a
billion edges for a multiset query with five colors and up to hundred million
edges for a multiset query with ten colors, despite the problems being NP-hard.
Our implementation, which is publicly available, exhibits practical edge-linear
scalability and is highly optimized. For instance, in a real-world graph
dataset with more than six million edges and a multiset query with ten colors,
we can extract an optimum solution in less than eight minutes on a Haswell
desktop with four cores.
</summary>
    <author>
      <name>Suhas Thejaswi</name>
    </author>
    <author>
      <name>Aristides Gionis</name>
    </author>
    <author>
      <name>Juho Lauri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">version prior to peer review</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.07158v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.07158v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.1; F.2.2; G.2.1; G.2.2; I.5.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2408.15185v2</id>
    <updated>2025-03-17T14: 05: 49Z</updated>
    <published>2024-08-27T16: 40: 14Z</published>
    <title>Human-Centric Video Anomaly Detection Through Spatio-Temporal Pose
  Tokenization and Transformer</title>
    <summary>  Video Anomaly Detection (VAD) presents a significant challenge in computer
vision, particularly due to the unpredictable and infrequent nature of
anomalous events, coupled with the diverse and dynamic environments in which
they occur. Human-centric VAD, a specialized area within this domain, faces
additional complexities, including variations in human behavior, potential
biases in data, and substantial privacy concerns related to human subjects.
These issues complicate the development of models that are both robust and
generalizable. To address these challenges, recent advancements have focused on
pose-based VAD, which leverages human pose as a high-level feature to mitigate
privacy concerns, reduce appearance biases, and minimize background
interference. In this paper, we introduce SPARTA, a novel transformer-based
architecture designed specifically for human-centric pose-based VAD. SPARTA
introduces an innovative Spatio-Temporal Pose and Relative Pose (ST-PRP)
tokenization method that produces an enriched representation of human motion
over time. This approach ensures that the transformer's attention mechanism
captures both spatial and temporal patterns simultaneously, rather than
focusing on only one aspect. The addition of the relative pose further
emphasizes subtle deviations from normal human movements. The architecture's
core, a novel Unified Encoder Twin Decoders (UETD) transformer, significantly
improves the detection of anomalous behaviors in video data. Extensive
evaluations across multiple benchmark datasets demonstrate that SPARTA
consistently outperforms existing methods, establishing a new state-of-the-art
in pose-based VAD.
</summary>
    <author>
      <name>Ghazal Alinezhad Noghre</name>
    </author>
    <author>
      <name>Armin Danesh Pazho</name>
    </author>
    <author>
      <name>Hamed Tabkhi</name>
    </author>
    <link href="http://arxiv.org/abs/2408.15185v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.15185v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1906.05277v1</id>
    <updated>2019-06-07T08: 05: 34Z</updated>
    <published>2019-06-07T08: 05: 34Z</published>
    <title>A Combination of Temporal Sequence Learning and Data Description for
  Anomaly-based NIDS</title>
    <summary>  Through continuous observation and modeling of normal behavior in networks,
Anomaly-based Network Intrusion Detection System (A-NIDS) offers a way to find
possible threats via deviation from the normal model. The analysis of network
traffic based on the time series model has the advantage of exploiting the
relationship between packages within network traffic and observing trends of
behaviors over a period of time. It will generate new sequences with good
features that support anomaly detection in network traffic and provide the
ability to detect new attacks. Besides, an anomaly detection technique, which
focuses on the normal data and aims to build a description of it, will be an
effective technique for anomaly detection in imbalanced data. In this paper, we
propose a combination model of Long Short Term Memory (LSTM) architecture for
processing time series and a data description Support Vector Data Description
(SVDD) for anomaly detection in A-NIDS to obtain the advantages of them. This
model helps parameters in LSTM and SVDD are jointly trained with the joint
optimization method. Our experimental results with KDD99 dataset show that the
proposed combined model obtains high performance in intrusion detection,
especially DoS and Probe attacks with 98.0% and 99.8%, respectively.
</summary>
    <author>
      <name>Nguyen Thanh Van</name>
    </author>
    <author>
      <name>Tran Ngoc Thinh</name>
    </author>
    <author>
      <name>Le Thanh Sach</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijnsa.2019.11307</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijnsa.2019.11307" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages,
                            2 figures,
                            4 tables, International Journal of Network
  Security &amp; Its Applications (IJNSA)</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.05277v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05277v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1804.04527v2</id>
    <updated>2018-04-22T13: 05: 03Z</updated>
    <published>2018-04-12T14: 19: 50Z</published>
    <title>SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos</title>
    <summary>  In this paper, we introduce SoccerNet, a benchmark for action spotting in
soccer videos. The dataset is composed of 500 complete soccer games from six
main European leagues, covering three seasons from 2014 to 2017 and a total
duration of 764 hours. A total of 6,
                            637 temporal annotations are automatically
parsed from online match reports at a one minute resolution for three main
classes of events (Goal, Yellow/Red Card, and Substitution). As such, the
dataset is easily scalable. These annotations are manually refined to a one
second resolution by anchoring them at a single timestamp following
well-defined soccer rules. With an average of one event every 6.9 minutes, this
dataset focuses on the problem of localizing very sparse events within long
videos. We define the task of spotting as finding the anchors of soccer events
in a video. Making use of recent developments in the realm of generic action
recognition and detection in video, we provide strong baselines for detecting
soccer events. We show that our best model for classifying temporal segments of
length one minute reaches a mean Average Precision (mAP) of 67.8%. For the
spotting task, our baseline reaches an Average-mAP of 49.7% for tolerances
$\delta$ ranging from 5 to 60 seconds. Our dataset and models are available at
https: //silviogiancola.github.io/SoccerNet.
</summary>
    <author>
      <name>Silvio Giancola</name>
    </author>
    <author>
      <name>Mohieddine Amine</name>
    </author>
    <author>
      <name>Tarek Dghaily</name>
    </author>
    <author>
      <name>Bernard Ghanem</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CVPRW.2018.00223</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CVPRW.2018.00223" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR Workshop on Computer Vision in Sports 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04527v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04527v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.02923v1</id>
    <updated>2024-03-31T01: 20: 01Z</updated>
    <published>2024-03-31T01: 20: 01Z</published>
    <title>An Unsupervised Adversarial Autoencoder for Cyber Attack Detection in
  Power Distribution Grids</title>
    <summary>  Detection of cyber attacks in smart power distribution grids with unbalanced
configurations poses challenges due to the inherent nonlinear nature of these
uncertain and stochastic systems. It originates from the intermittent
characteristics of the distributed energy resources (DERs) generation and load
variations. Moreover, the unknown behavior of cyber attacks, especially false
data injection attacks (FDIAs) in the distribution grids with complex temporal
correlations and the limited amount of labeled data increases the vulnerability
of the grids and imposes a high risk in the secure and reliable operation of
the grids. To address these challenges, this paper proposes an unsupervised
adversarial autoencoder (AAE) model to detect FDIAs in unbalanced power
distribution grids integrated with DERs, i.e., PV systems and wind generation.
The proposed method utilizes long short-term memory (LSTM) in the structure of
the autoencoder to capture the temporal dependencies in the time-series
measurements and leverages the power of generative adversarial networks (GANs)
for better reconstruction of the input data. The advantage of the proposed
data-driven model is that it can detect anomalous points for the system
operation without reliance on abstract models or mathematical representations.
To evaluate the efficacy of the approach, it is tested on IEEE 13-bus and
123-bus systems with historical meteorological data (wind speed, ambient
temperature, and solar irradiance) as well as historical real-world load data
under three types of data falsification functions. The comparison of the
detection results of the proposed model with other unsupervised learning
methods verifies its superior performance in detecting cyber attacks in
unbalanced power distribution grids.
</summary>
    <author>
      <name>Mehdi Jabbari Zideh</name>
    </author>
    <author>
      <name>Mohammad Reza Khalghani</name>
    </author>
    <author>
      <name>Sarika Khushalani Solanki</name>
    </author>
    <link href="http://arxiv.org/abs/2404.02923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.02923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2108.03825v1</id>
    <updated>2021-08-09T06: 11: 14Z</updated>
    <published>2021-08-09T06: 11: 14Z</published>
    <title>Weakly-Supervised Spatio-Temporal Anomaly Detection in Surveillance
  Video</title>
    <summary>  In this paper, we introduce a novel task, referred to as Weakly-Supervised
Spatio-Temporal Anomaly Detection (WSSTAD) in surveillance video. Specifically,
given an untrimmed video, WSSTAD aims to localize a spatio-temporal tube (i.e.,
a sequence of bounding boxes at consecutive times) that encloses the abnormal
event, with only coarse video-level annotations as supervision during training.
To address this challenging task, we propose a dual-branch network which takes
as input the proposals with multi-granularities in both spatial-temporal
domains. Each branch employs a relationship reasoning module to capture the
correlation between tubes/videolets, which can provide rich contextual
information and complex entity relationships for the concept learning of
abnormal behaviors. Mutually-guided Progressive Refinement framework is set up
to employ dual-path mutual guidance in a recurrent manner, iteratively sharing
auxiliary supervision information across branches. It impels the learned
concepts of each branch to serve as a guide for its counterpart, which
progressively refines the corresponding branch and the whole framework.
Furthermore, we contribute two datasets, i.e., ST-UCF-Crime and STRA,
consisting of videos containing spatio-temporal abnormal annotations to serve
as the benchmarks for WSSTAD. We conduct extensive qualitative and quantitative
evaluations to demonstrate the effectiveness of the proposed approach and
analyze the key factors that contribute more to handle this task.
</summary>
    <author>
      <name>Jie Wu</name>
    </author>
    <author>
      <name>Wei Zhang</name>
    </author>
    <author>
      <name>Guanbin Li</name>
    </author>
    <author>
      <name>Wenhao Wu</name>
    </author>
    <author>
      <name>Xiao Tan</name>
    </author>
    <author>
      <name>Yingying Li</name>
    </author>
    <author>
      <name>Errui Ding</name>
    </author>
    <author>
      <name>Liang Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by IJCAI2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.03825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.03825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/astro-ph/0001081v1</id>
    <updated>2000-01-06T14: 55: 26Z</updated>
    <published>2000-01-06T14: 55: 26Z</published>
    <title>A limit-cycle model for the binary supersoft X-ray source RX
  J0513.9-6951</title>
    <summary>  We present new results of our X-ray monitoring of the transient binary
supersoft X-ray source RX J0513.9-6951 in the LMC and of our re-analysis of
optical light curves obtained during the MACHO project. We have covered a
complete X-ray outburst cycle with the ROSAT HRI detector. From the amplitude
and timescale of the soft X-ray variability, tight limits are derived for the
temporal behaviour of the white-dwarf radius and the effective temperature of
its envelope. A limit-cycle model is proposed to explain the observed optical
and X-ray variability, the characteristic timescales of the durations of the
X-ray on and off states, and those of the transitions between both states. Our
observations confirm that the radius changes of the white-dwarf envelope occur
on the Kelvin-Helmholtz timescale. The duration of the X-ray on and off states
is compatible with the viscous timescales of the inner and outer accretion
disk, respectively.
</summary>
    <author>
      <name>K. Reinsch</name>
    </author>
    <author>
      <name>A. van Teeseling</name>
    </author>
    <author>
      <name>A. R. King</name>
    </author>
    <author>
      <name>K. Beuermann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages,
                            4 figures, accepted for publication in Astronomy and
  Astrophysics Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/astro-ph/0001081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/0001081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1209.4549v1</id>
    <updated>2012-09-20T14: 41: 13Z</updated>
    <published>2012-09-20T14: 41: 13Z</published>
    <title>The interaction between air ions and aerosol particles in the atmosphere</title>
    <summary>  Charged particles are continually generated in atmospheric air, and the
interaction between natural ionisation and atmospheric particles is
complicated. It is of some climatic importance to establish if ions are
implicated in particle formation. Atmospheric ion concentrations have been
investigated here at high temporal resolution, using Gerdien ion analysers at a
site where synchronous meteorological measurements were also made. The
background ionisation rate was also monitored with a Geiger counter, enabling
ion production from natural radioactivity to be distinguished from other
effects. Measurements at 1Hz offer some promise in establishing the atmospheric
electrical influences in ionic nucleation bursts, although combinations of
other meteorological factors are also known to be significant. High time
resolution meteorological and ion measurements are therefore clearly necessary
in advancing basic understanding in the behaviour of atmospheric aerosol.
</summary>
    <author>
      <name>KL Aplin</name>
    </author>
    <author>
      <name>RG Harrison</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 10th Int. Conf. on Electrostatics, Cambridge,
                            28-31 March 1999. Please cite refereed proceedings listed below</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Inst. Phys. Conf. Ser. 163,
                            411-414 (1999)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.4549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2110.03965v1</id>
    <updated>2021-10-08T08: 31: 17Z</updated>
    <published>2021-10-08T08: 31: 17Z</published>
    <title>Joint Scattering for Automatic Chick Call Recognition</title>
    <summary>  Animal vocalisations contain important information about health, emotional
state, and behaviour, thus can be potentially used for animal welfare
monitoring. Motivated by the spectro-temporal patterns of chick calls in the
time$-$frequency domain, in this paper we propose an automatic system for chick
call recognition using the joint time$-$frequency scattering transform (JTFS).
Taking full-length recordings as input, the system first extracts chick call
candidates by an onset detector and silence removal. After computing their JTFS
features, a support vector machine classifier groups each candidate into
different chick call types. Evaluating on a dataset comprising 3013 chick calls
collected in laboratory conditions, the proposed recognition system using the
JTFS features improves the frame- and event-based macro F-measures by 9.5% and
11.7%, respectively, than that of a mel-frequency cepstral coefficients
baseline.
</summary>
    <author>
      <name>Changhong Wang</name>
    </author>
    <author>
      <name>Emmanouil Benetos</name>
    </author>
    <author>
      <name>Shuge Wang</name>
    </author>
    <author>
      <name>Elisabetta Versace</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, submitted to ICASSP 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.03965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.03965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1703.02635v1</id>
    <updated>2017-02-23T10: 17: 34Z</updated>
    <published>2017-02-23T10: 17: 34Z</published>
    <title>A Computational Model of a Single-Photon Avalanche Diode Sensor for
  Transient Imaging</title>
    <summary>  Single-Photon Avalanche Diodes (SPAD) are affordable photodetectors, capable
to collect extremely fast low-energy events, due to their single-photon
sensibility. This makes them very suitable for time-of-flight-based range
imaging systems, allowing to reduce costs and power requirements, without
sacrifizing much temporal resolution. In this work we describe a computational
model to simulate the behaviour of SPAD sensors, aiming to provide a realistic
camera model for time-resolved light transport simulation, with applications on
prototyping new reconstructions techniques based on SPAD time-of-flight data.
Our model accounts for the major effects of the sensor on the incoming signal.
We compare our model against real-world measurements, and apply it to a variety
of scenarios, including complex multiply-scattered light transport.
</summary>
    <author>
      <name>Quercus Hernandez</name>
    </author>
    <author>
      <name>Diego Gutierrez</name>
    </author>
    <author>
      <name>Adrian Jarabo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages,
                            11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.02635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.02635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1906.05571v1</id>
    <updated>2019-06-13T09: 41: 00Z</updated>
    <published>2019-06-13T09: 41: 00Z</published>
    <title>Learning Spatio-Temporal Representation with Local and Global Diffusion</title>
    <summary>  Convolutional Neural Networks (CNN) have been regarded as a powerful class of
models for visual recognition problems. Nevertheless, the convolutional filters
in these networks are local operations while ignoring the large-range
dependency. Such drawback becomes even worse particularly for video
recognition, since video is an information-intensive media with complex
temporal variations. In this paper, we present a novel framework to boost the
spatio-temporal representation learning by Local and Global Diffusion (LGD).
Specifically, we construct a novel neural network architecture that learns the
local and global representations in parallel. The architecture is composed of
LGD blocks, where each block updates local and global features by modeling the
diffusions between these two representations. Diffusions effectively interact
two aspects of information, i.e., localized and holistic, for more powerful way
of representation learning. Furthermore, a kernelized classifier is introduced
to combine the representations from two aspects for video recognition. Our LGD
networks achieve clear improvements on the large-scale Kinetics-400 and
Kinetics-600 video classification datasets against the best competitors by 3.5%
and 0.7%. We further examine the generalization of both the global and local
representations produced by our pre-trained LGD networks on four different
benchmarks for video action recognition and spatio-temporal action detection
tasks. Superior performances over several state-of-the-art techniques on these
benchmarks are reported. Code is available at:
https: //github.com/ZhaofanQiu/local-and-global-diffusion-networks.
</summary>
    <author>
      <name>Zhaofan Qiu</name>
    </author>
    <author>
      <name>Ting Yao</name>
    </author>
    <author>
      <name>Chong-Wah Ngo</name>
    </author>
    <author>
      <name>Xinmei Tian</name>
    </author>
    <author>
      <name>Tao Mei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.05571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.05571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2302.08063v2</id>
    <updated>2023-03-17T20: 46: 33Z</updated>
    <published>2023-02-16T04: 00: 03Z</published>
    <title>MINOTAUR: Multi-task Video Grounding From Multimodal Queries</title>
    <summary>  Video understanding tasks take many forms, from action detection to visual
query localization and spatio-temporal grounding of sentences. These tasks
differ in the type of inputs (only video, or video-query pair where query is an
image region or sentence) and outputs (temporal segments or spatio-temporal
tubes). However, at their core they require the same fundamental understanding
of the video, i.e., the actors and objects in it, their actions and
interactions. So far these tasks have been tackled in isolation with
individual, highly specialized architectures, which do not exploit the
interplay between tasks. In contrast, in this paper, we present a single,
unified model for tackling query-based video understanding in long-form videos.
In particular, our model can address all three tasks of the Ego4D Episodic
Memory benchmark which entail queries of three different forms: given an
egocentric video and a visual, textual or activity query, the goal is to
determine when and where the answer can be seen within the video. Our model
design is inspired by recent query-based approaches to spatio-temporal
grounding, and contains modality-specific query encoders and task-specific
sliding window inference that allow multi-task training with diverse input
modalities and different structured outputs. We exhaustively analyze
relationships among the tasks and illustrate that cross-task learning leads to
improved performance on each individual task, as well as the ability to
generalize to unseen tasks, such as zero-shot spatial localization of language
queries.
</summary>
    <author>
      <name>Raghav Goyal</name>
    </author>
    <author>
      <name>Effrosyni Mavroudi</name>
    </author>
    <author>
      <name>Xitong Yang</name>
    </author>
    <author>
      <name>Sainbayar Sukhbaatar</name>
    </author>
    <author>
      <name>Leonid Sigal</name>
    </author>
    <author>
      <name>Matt Feiszli</name>
    </author>
    <author>
      <name>Lorenzo Torresani</name>
    </author>
    <author>
      <name>Du Tran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages,
                            8 figures and 13 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.08063v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.08063v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1902.04506v1</id>
    <updated>2019-02-12T17: 15: 17Z</updated>
    <published>2019-02-12T17: 15: 17Z</published>
    <title>RTbust: Exploiting Temporal Patterns for Botnet Detection on Twitter</title>
    <summary>  Within OSNs, many of our supposedly online friends may instead be fake
accounts called social bots, part of large groups that purposely re-share
targeted content. Here, we study retweeting behaviors on Twitter, with the
ultimate goal of detecting retweeting social bots. We collect a dataset of 10M
retweets. We design a novel visualization that we leverage to highlight benign
and malicious patterns of retweeting activity. In this way, we uncover a
'normal' retweeting pattern that is peculiar of human-operated accounts, and 3
suspicious patterns related to bot activities. Then, we propose a bot detection
technique that stems from the previous exploration of retweeting behaviors. Our
technique, called Retweet-Buster (RTbust), leverages unsupervised feature
extraction and clustering. An LSTM autoencoder converts the retweet time series
into compact and informative latent feature vectors, which are then clustered
with a hierarchical density-based algorithm. Accounts belonging to large
clusters characterized by malicious retweeting patterns are labeled as bots.
RTbust obtains excellent detection results, with F1 = 0.87, whereas competitors
achieve F1 &lt; 0.76. Finally, we apply RTbust to a large dataset of retweets,
uncovering 2 previously unknown active botnets with hundreds of accounts.
</summary>
    <author>
      <name>Michele Mazza</name>
    </author>
    <author>
      <name>Stefano Cresci</name>
    </author>
    <author>
      <name>Marco Avvenuti</name>
    </author>
    <author>
      <name>Walter Quattrociocchi</name>
    </author>
    <author>
      <name>Maurizio Tesconi</name>
    </author>
    <link href="http://arxiv.org/abs/1902.04506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.04506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2012.09141v1</id>
    <updated>2020-12-16T18: 26: 17Z</updated>
    <published>2020-12-16T18: 26: 17Z</published>
    <title>Change Detection: A functional analysis perspective</title>
    <summary>  We develop a new approach for detecting changes in the behavior of stochastic
processes and random fields based on tensor product representations such as the
Karhunen-Lo\`{e
                            }ve expansion. From the associated eigenspaces of the covariance
operator a series of nested function spaces are constructed, allowing detection
of signals lying in orthogonal subspaces. In particular this can succeed even
if the stochastic behavior of the signal changes either in a global or local
sense. A mathematical approach is developed to locate and measure sizes of
extraneous components based on construction of multilevel nested subspaces. We
show examples in $\mathbb{R
                            }$ and on a spherical domain $\mathbb{S
                            }^{
                                2
                            }$.
However, the method is flexible, allowing the detection of orthogonal signals
on general topologies, including spatio-temporal domains.
</summary>
    <author>
      <name>Julio Enrique Castrillon-Candas</name>
    </author>
    <author>
      <name>Mark Kon</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jmva.2021.104885</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jmva.2021.104885" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: Hilbert spaces, Karhunen-Lo\`{e
                            }ve Expansions, Stochastic
  Processes, Random Fields, Multilevel spaces, Optimization</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.09141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.09141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2101.02232v1</id>
    <updated>2021-01-06T19: 10: 23Z</updated>
    <published>2021-01-06T19: 10: 23Z</published>
    <title>Single Shot Multitask Pedestrian Detection and Behavior Prediction</title>
    <summary>  Detecting and predicting the behavior of pedestrians is extremely crucial for
self-driving vehicles to plan and interact with them safely. Although there
have been several research works in this area, it is important to have fast and
memory efficient models such that it can operate in embedded hardware in these
autonomous machines. In this work, we propose a novel architecture using
spatial-temporal multi-tasking to do camera based pedestrian detection and
intention prediction. Our approach significantly reduces the latency by being
able to detect and predict all pedestrians' intention in a single shot manner
while also being able to attain better accuracy by sharing features with
relevant object level information and interactions.
</summary>
    <author>
      <name>Prateek Agrawal</name>
    </author>
    <author>
      <name>Pratik Prabhanjan Brahma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,
                            3 figures, Neurips 2020 ML4AD workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.02232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.02232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2211.03895v1</id>
    <updated>2022-11-07T22: 59: 58Z</updated>
    <published>2022-11-07T22: 59: 58Z</published>
    <title>Facial Tic Detection in Untrimmed Videos of Tourette Syndrome Patients</title>
    <summary>  Tourette Syndrome (TS) is a behavior disorder that onsets in childhood and is
characterized by the expression of involuntary movements and sounds commonly
referred to as tics. Behavioral therapy is the first-line treatment for
patients with TS, and it helps patients raise awareness about tic occurrence as
well as develop tic inhibition strategies. However, the limited availability of
therapists and the difficulties for in-home follow up work limits its
effectiveness. An automatic tic detection system that is easy to deploy could
alleviate the difficulties of home-therapy by providing feedback to the
patients while exercising tic awareness. In this work, we propose a novel
architecture (T-Net) for automatic tic detection and classification from
untrimmed videos. T-Net combines temporal detection and segmentation and
operates on features that are interpretable to a clinician. We compare T-Net to
several state-of-the-art systems working on deep features extracted from the
raw videos and T-Net achieves comparable performance in terms of average
precision while relying on interpretable features needed in clinical practice.
</summary>
    <author>
      <name>Yutao Tang</name>
    </author>
    <author>
      <name>Benjamín Béjar</name>
    </author>
    <author>
      <name>Joey K. -Y. Essoe</name>
    </author>
    <author>
      <name>Joseph F. McGuire</name>
    </author>
    <author>
      <name>René Vidal</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICPR56361.2022.9956140</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICPR56361.2022.9956140" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICPR2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2211.03895v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.03895v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2504.02757v1</id>
    <updated>2025-04-03T16: 49: 58Z</updated>
    <published>2025-04-03T16: 49: 58Z</published>
    <title>Echoes of the hidden: Uncovering coordination beyond network structure</title>
    <summary>  The study of connectivity and coordination has drawn increasing attention in
recent decades due to their central role in driving markets, shaping societal
dynamics, and influencing biological systems. Traditionally, observable
connections, such as phone calls, financial transactions, or social media
connections, have been used to infer coordination and connectivity. However,
incomplete, encrypted, or fragmented data, alongside the ubiquity of
communication platforms and deliberate obfuscation, often leave many real-world
connections hidden. In this study, we demonstrate that coordinating individuals
exhibit shared bursty activity patterns, enabling their detection even when
observable links between them are sparse or entirely absent. We further propose
a generative model based on the network of networks formalism to account for
the mechanisms driving this collaborative burstiness, attributing it to shock
propagation across networks rather than isolated individual behavior. Model
simulations demonstrate that when observable connection density is below 70\%,
burstiness significantly improves coordination detection compared to
state-of-the-art temporal and structural methods. This work provides a new
perspective on community and coordination dynamics, advancing both theoretical
understanding and practical detection. By laying the foundation for identifying
hidden connections beyond observable network structures, it enables detection
across different platforms, alongside enhancing system behavior understanding,
informed decision-making, and risk mitigation.
</summary>
    <author>
      <name>Shahar Somin</name>
    </author>
    <author>
      <name>Tom Cohen</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <author>
      <name>Alex Pentland</name>
    </author>
    <link href="http://arxiv.org/abs/2504.02757v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.02757v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2504.14686v1</id>
    <updated>2025-04-20T17: 36: 52Z</updated>
    <published>2025-04-20T17: 36: 52Z</published>
    <title>Uncovering Issues in the Radio Access Network by Looking at the
  Neighbors</title>
    <summary>  Mobile network operators (MNOs) manage Radio Access Networks (RANs) with
massive amounts of cells over multiple radio generations (2G-5G). To handle
such complexity, operations teams rely on monitoring systems, including anomaly
detection tools that identify unexpected behaviors. In this paper, we present
c-ANEMON, a Contextual ANomaly dEtection MONitor for the RAN based on Graph
Neural Networks (GNNs). Our solution captures spatio-temporal variations by
analyzing the behavior of individual cells in relation to their local
neighborhoods, enabling the detection of anomalies that are independent of
external mobility factors. This, in turn, allows focusing on anomalies
associated with network issues (e.g., misconfigurations, equipment failures).
We evaluate c-ANEMON using real-world data from a large European metropolitan
area (7,
                            890 cells; 3 months). First, we show that the GNN model within our
solution generalizes effectively to cells from previously unseen areas,
suggesting the possibility of using a single model across extensive deployment
regions. Then, we analyze the anomalies detected by c-ANEMON through manual
inspection and define several categories of long-lasting anomalies (6+ hours).
Notably,
                            45.95% of these anomalies fall into a category that is more likely to
require intervention by operations teams.
</summary>
    <author>
      <name>José Suárez-Varela</name>
    </author>
    <author>
      <name>Andra Lutu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Machine Learning for
  Communication and Networking,
                            2025</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2504.14686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.14686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1612.06703v1</id>
    <updated>2016-12-20T15: 20: 28Z</updated>
    <published>2016-12-20T15: 20: 28Z</published>
    <title>Dynamic Action Recognition: A convolutional neural network model for
  temporally organized joint location data</title>
    <summary>  Motivation: Recognizing human actions in a video is a challenging task which
has applications in various fields. Previous works in this area have either
used images from a 2D or 3D camera. Few have used the idea that human actions
can be easily identified by the movement of the joints in the 3D space and
instead used a Recurrent Neural Network (RNN) for modeling. Convolutional
neural networks (CNN) have the ability to recognise even the complex patterns
in data which makes it suitable for detecting human actions. Thus, we modeled a
CNN which can predict the human activity using the joint data. Furthermore,
using the joint data representation has the benefit of lower dimensionality
than image or video representations. This makes our model simpler and faster
than the RNN models. In this study, we have developed a six layer convolutional
network, which reduces each input feature vector of the form 15x1961x4 to an
one dimensional binary vector which gives us the predicted activity. Results:
Our model is able to recognise an activity correctly upto 87% accuracy. Joint
data is taken from the Cornell Activity Datasets which have day to day
activities like talking, relaxing, eating, cooking etc.
</summary>
    <author>
      <name>Adhavan Jayabalan</name>
    </author>
    <author>
      <name>Harish Karunakaran</name>
    </author>
    <author>
      <name>Shravan Murlidharan</name>
    </author>
    <author>
      <name>Tesia Shizume</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.06703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2203.14712v2</id>
    <updated>2022-05-01T14: 49: 02Z</updated>
    <published>2022-03-28T12: 59: 50Z</published>
    <title>Assembly101: A Large-Scale Multi-View Video Dataset for Understanding
  Procedural Activities</title>
    <summary>  Assembly101 is a new procedural activity dataset featuring 4321 videos of
people assembling and disassembling 101 "take-apart" toy vehicles. Participants
work without fixed instructions, and the sequences feature rich and natural
variations in action ordering, mistakes, and corrections. Assembly101 is the
first multi-view action dataset, with simultaneous static (8) and egocentric
(4) recordings. Sequences are annotated with more than 100K coarse and 1M
fine-grained action segments, and 18M 3D hand poses. We benchmark on three
action understanding tasks: recognition, anticipation and temporal
segmentation. Additionally, we propose a novel task of detecting mistakes. The
unique recording format and rich set of annotations allow us to investigate
generalization to new toys, cross-view transfer, long-tailed distributions, and
pose vs. appearance. We envision that Assembly101 will serve as a new challenge
to investigate various activity understanding problems.
</summary>
    <author>
      <name>Fadime Sener</name>
    </author>
    <author>
      <name>Dibyadip Chatterjee</name>
    </author>
    <author>
      <name>Daniel Shelepov</name>
    </author>
    <author>
      <name>Kun He</name>
    </author>
    <author>
      <name>Dipika Singhania</name>
    </author>
    <author>
      <name>Robert Wang</name>
    </author>
    <author>
      <name>Angela Yao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2022, https: //assembly-101.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.14712v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.14712v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2210.12686v2</id>
    <updated>2022-11-18T05: 28: 40Z</updated>
    <published>2022-10-23T10: 19: 37Z</published>
    <title>Holistic Interaction Transformer Network for Action Detection</title>
    <summary>  Actions are about how we interact with the environment, including other
people, objects, and ourselves. In this paper, we propose a novel multi-modal
Holistic Interaction Transformer Network (HIT) that leverages the largely
ignored, but critical hand and pose information essential to most human
actions. The proposed "HIT" network is a comprehensive bi-modal framework that
comprises an RGB stream and a pose stream. Each of them separately models
person, object, and hand interactions. Within each sub-network, an
Intra-Modality Aggregation module (IMA) is introduced that selectively merges
individual interaction units. The resulting features from each modality are
then glued using an Attentive Fusion Mechanism (AFM). Finally, we extract cues
from the temporal context to better classify the occurring actions using cached
memory. Our method significantly outperforms previous approaches on the J-HMDB,
UCF101-24, and MultiSports datasets. We also achieve competitive results on
AVA. The code will be available at https: //github.com/joslefaure/HIT.
</summary>
    <author>
      <name>Gueter Josmy Faure</name>
    </author>
    <author>
      <name>Min-Hung Chen</name>
    </author>
    <author>
      <name>Shang-Hong Lai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for WACV 2023. Code: https: //github.com/joslefaure/HIT</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.12686v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.12686v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2112.07703v1</id>
    <updated>2021-12-14T19: 10: 34Z</updated>
    <published>2021-12-14T19: 10: 34Z</published>
    <title>EP-PINNs: Cardiac Electrophysiology Characterisation using
  Physics-Informed Neural Networks</title>
    <summary>  Accurately inferring underlying electrophysiological (EP) tissue properties
from action potential recordings is expected to be clinically useful in the
diagnosis and treatment of arrhythmias such as atrial fibrillation, but it is
notoriously difficult to perform. We present EP-PINNs (Physics-Informed Neural
Networks), a novel tool for accurate action potential simulation and EP
parameter estimation, from sparse amounts of EP data. We demonstrate, using 1D
and 2D in silico data, how EP-PINNs are able to reconstruct the spatio-temporal
evolution of action potentials, whilst predicting parameters related to action
potential duration (APD), excitability and diffusion coefficients. EP-PINNs are
additionally able to identify heterogeneities in EP properties, making them
potentially useful for the detection of fibrosis and other localised pathology
linked to arrhythmias. Finally, we show EP-PINNs effectiveness on biological in
vitro preparations, by characterising the effect of anti-arrhythmic drugs on
APD using optical mapping data. EP-PINNs are a promising clinical tool for the
characterisation and potential treatment guidance of arrhythmias.
</summary>
    <author>
      <name>Clara Herrero Martin</name>
    </author>
    <author>
      <name>Alon Oved</name>
    </author>
    <author>
      <name>Rasheda A Chowdhury</name>
    </author>
    <author>
      <name>Elisabeth Ullmann</name>
    </author>
    <author>
      <name>Nicholas S Peters</name>
    </author>
    <author>
      <name>Anil A Bharath</name>
    </author>
    <author>
      <name>Marta Varela</name>
    </author>
    <link href="http://arxiv.org/abs/2112.07703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.07703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2106.13420v1</id>
    <updated>2021-06-25T04: 19: 14Z</updated>
    <published>2021-06-25T04: 19: 14Z</published>
    <title>Identifying malicious accounts in Blockchains using Domain Names and
  associated temporal properties</title>
    <summary>  The rise in the adoption of blockchain technology has led to increased
illegal activities by cyber-criminals costing billions of dollars. Many machine
learning algorithms are applied to detect such illegal behavior. These
algorithms are often trained on the transaction behavior and, in some cases,
trained on the vulnerabilities that exist in the system. In our approach, we
study the feasibility of using metadata such as Domain Name (DN) associated
with the account in the blockchain and identify whether an account should be
tagged malicious or not. Here, we leverage the temporal aspects attached to the
DNs. Our results identify 144930 DNs that show malicious behavior, and out of
these,
                            54114 DNs show persistent malicious behavior over time. Nonetheless,
none of these identified malicious DNs were reported in new officially tagged
malicious blockchain DNs.
</summary>
    <author>
      <name>Rohit Kumar Sachan</name>
    </author>
    <author>
      <name>Rachit Agarwal</name>
    </author>
    <author>
      <name>Sandeep Kumar Shukla</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to a journal</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.13420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.13420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2410.10818v2</id>
    <updated>2024-10-15T17: 55: 46Z</updated>
    <published>2024-10-14T17: 59: 58Z</published>
    <title>TemporalBench: Benchmarking Fine-grained Temporal Understanding for
  Multimodal Video Models</title>
    <summary>  Understanding fine-grained temporal dynamics is crucial for multimodal video
comprehension and generation. Due to the lack of fine-grained temporal
annotations, existing video benchmarks mostly resemble static image benchmarks
and are incompetent at evaluating models for temporal understanding. In this
paper, we introduce TemporalBench, a new benchmark dedicated to evaluating
fine-grained temporal understanding in videos. TemporalBench consists of ~10K
video question-answer pairs, derived from ~2K high-quality human annotations
detailing the temporal dynamics in video clips. As a result, our benchmark
provides a unique testbed for evaluating various temporal understanding and
reasoning abilities such as action frequency, motion magnitude, event order,
etc. Moreover, it enables evaluations on various tasks like both video question
answering and captioning, both short and long video understanding, as well as
different models such as multimodal video embedding models and text generation
models. Results show that state-of-the-art models like GPT-4o achieve only
38.5% question answering accuracy on TemporalBench, demonstrating a significant
gap (~30%) between humans and AI in temporal understanding. Furthermore, we
notice a critical pitfall for multi-choice QA where LLMs can detect the subtle
changes in negative captions and find a centralized description as a cue for
its prediction, where we propose Multiple Binary Accuracy (MBA) to correct such
bias. We hope that TemporalBench can foster research on improving models'
temporal reasoning capabilities. Both dataset and evaluation code will be made
available.
</summary>
    <author>
      <name>Mu Cai</name>
    </author>
    <author>
      <name>Reuben Tan</name>
    </author>
    <author>
      <name>Jianrui Zhang</name>
    </author>
    <author>
      <name>Bocheng Zou</name>
    </author>
    <author>
      <name>Kai Zhang</name>
    </author>
    <author>
      <name>Feng Yao</name>
    </author>
    <author>
      <name>Fangrui Zhu</name>
    </author>
    <author>
      <name>Jing Gu</name>
    </author>
    <author>
      <name>Yiwu Zhong</name>
    </author>
    <author>
      <name>Yuzhang Shang</name>
    </author>
    <author>
      <name>Yao Dou</name>
    </author>
    <author>
      <name>Jaden Park</name>
    </author>
    <author>
      <name>Jianfeng Gao</name>
    </author>
    <author>
      <name>Yong Jae Lee</name>
    </author>
    <author>
      <name>Jianwei Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https: //temporalbench.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.10818v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.10818v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1910.06056v4</id>
    <updated>2021-08-10T14: 15: 33Z</updated>
    <published>2019-10-14T11: 53: 03Z</published>
    <title>Facial Behavior Analysis using 4D Curvature Statistics for Presentation
  Attack Detection</title>
    <summary>  The human face has a high potential for biometric identification due to its
many individual traits. At the same time, such identification is vulnerable to
biometric copies. These presentation attacks pose a great challenge in
unsupervised authentication settings. As a countermeasure, we propose a method
that automatically analyzes the plausibility of facial behavior based on a
sequence of 3D face scans. A compact feature representation measures facial
behavior using the temporal curvature change. Finally, we train our method only
on genuine faces in an anomaly detection scenario. Our method can detect
presentation attacks using elastic 3D masks, bent photographs with eye holes,
and monitor replay-attacks. For evaluation, we recorded a challenging database
containing such cases using a high-quality 3D sensor. It features 109 4D face
scans including eleven different types of presentation attacks. We achieve
error rates of 11% and 6% for APCER and BPCER, respectively.
</summary>
    <author>
      <name>Martin Thümmel</name>
    </author>
    <author>
      <name>Sven Sickert</name>
    </author>
    <author>
      <name>Joachim Denzler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,
                            6 figures, IEEE International Workshop on Biometrics and
  Forensics 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.06056v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06056v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2312.14406v1</id>
    <updated>2023-12-22T03: 15: 17Z</updated>
    <published>2023-12-22T03: 15: 17Z</published>
    <title>Generative Pretraining at Scale: Transformer-Based Encoding of
  Transactional Behavior for Fraud Detection</title>
    <summary>  In this work, we introduce an innovative autoregressive model leveraging
Generative Pretrained Transformer (GPT) architectures, tailored for fraud
detection in payment systems. Our approach innovatively confronts token
explosion and reconstructs behavioral sequences, providing a nuanced
understanding of transactional behavior through temporal and contextual
analysis. Utilizing unsupervised pretraining, our model excels in feature
representation without the need for labeled data. Additionally, we integrate a
differential convolutional approach to enhance anomaly detection, bolstering
the security and efficacy of one of the largest online payment merchants in
China. The scalability and adaptability of our model promise broad
applicability in various transactional contexts.
</summary>
    <author>
      <name>Ze Yu Zhao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tencent, WeChat Pay</arxiv:affiliation>
    </author>
    <author>
      <name>Zheng Zhu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tencent, WeChat Pay</arxiv:affiliation>
    </author>
    <author>
      <name>Guilin Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tencent, WeChat Pay</arxiv:affiliation>
    </author>
    <author>
      <name>Wenhan Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tencent, WeChat Pay</arxiv:affiliation>
    </author>
    <author>
      <name>Bo Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tencent, WeChat Pay</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2312.14406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.14406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/astro-ph/0302022v1</id>
    <updated>2003-02-03T11: 29: 59Z</updated>
    <published>2003-02-03T11: 29: 59Z</published>
    <title>Multiwavelength observations of the GRB000615 field</title>
    <summary>  We report on prompt and afterglow observations of GRB 000615 detected by
BeppoSAX. The study of the high-energy prompt event is presented along with the
search for its X-ray and optical afterglow. Spectral fits seem to suggest a
temporal evolution of the GRB prompt emission. We possibly find evidence for
intrinsic N_H (at 90% confidence level) and for a transient spectral emission
feature around 8 keV (at 98% confidence level). The X-ray to gamma-ray fluence
ratio of 1.73 +/- 0.22 is one of the largest among BeppoSAX GRBs. A weak X-ray
source is also detected in the MECS, between 1.6 and 4.5 keV, and its position
is compatible with the WFC error box. The behaviour of this source may be
compatible with that of an afterglow. Low significance signal is detected in
the 0.2-1.5 keV at a position consistent with the WFC and MECS error boxes. The
S/N ratio is insufficient to speculate on the nature of this source. There is
no evidence of an optical transient down to R ~ 22.
</summary>
    <author>
      <name>E. Maiorano</name>
    </author>
    <author>
      <name>N. Masetti</name>
    </author>
    <author>
      <name>E. Palazzi</name>
    </author>
    <author>
      <name>L. Nicastro</name>
    </author>
    <author>
      <name>E. Pian</name>
    </author>
    <author>
      <name>L. Amati</name>
    </author>
    <author>
      <name>F. Frontera</name>
    </author>
    <author>
      <name>J. in 't Zand</name>
    </author>
    <author>
      <name>K. Z. Stanek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages,
                            1 figure,
                            1 table, poster presented at the Rome 2002 GRB
  workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/astro-ph/0302022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/0302022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1909.02168v2</id>
    <updated>2019-10-18T21: 48: 03Z</updated>
    <published>2019-09-05T00: 34: 33Z</published>
    <title>Future Frame Prediction Using Convolutional VRNN for Anomaly Detection</title>
    <summary>  Anomaly detection in videos aims at reporting anything that does not conform
the normal behaviour or distribution. However, due to the sparsity of abnormal
video clips in real life, collecting annotated data for supervised learning is
exceptionally cumbersome. Inspired by the practicability of generative models
for semi-supervised learning, we propose a novel sequential generative model
based on variational autoencoder (VAE) for future frame prediction with
convolutional LSTM (ConvLSTM). To the best of our knowledge, this is the first
work that considers temporal information in future frame prediction based
anomaly detection framework from the model perspective. Our experiments
demonstrate that our approach is superior to the state-of-the-art methods on
three benchmark datasets.
</summary>
    <author>
      <name>Yiwei Lu</name>
    </author>
    <author>
      <name>Mahesh Kumar Krishna Reddy</name>
    </author>
    <author>
      <name>Seyed shahabeddin Nabavi</name>
    </author>
    <author>
      <name>Yang Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to AVSS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.02168v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.02168v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1801.06055v1</id>
    <updated>2018-01-08T22: 15: 56Z</updated>
    <published>2018-01-08T22: 15: 56Z</published>
    <title>Detecting Low Rapport During Natural Interactions in Small Groups from
  Non-Verbal Behaviour</title>
    <summary>  Rapport, the close and harmonious relationship in which interaction partners
are "in sync" with each other, was shown to result in smoother social
interactions, improved collaboration, and improved interpersonal outcomes. In
this work, we are first to investigate automatic prediction of low rapport
during natural interactions within small groups. This task is challenging given
that rapport only manifests in subtle non-verbal signals that are, in addition,
subject to influences of group dynamics as well as inter-personal
idiosyncrasies. We record videos of unscripted discussions of three to four
people using a multi-view camera system and microphones. We analyse a rich set
of non-verbal signals for rapport detection, namely facial expressions, hand
motion, gaze, speaker turns, and speech prosody. Using facial features, we can
detect low rapport with an average precision of 0.7 (chance level at 0.25),
while incorporating prior knowledge of participants' personalities can even
achieve early prediction without a drop in performance. We further provide a
detailed analysis of different feature sets and the amount of information
contained in different temporal segments of the interactions.
</summary>
    <author>
      <name>Philipp Müller</name>
    </author>
    <author>
      <name>Michael Xuelin Huang</name>
    </author>
    <author>
      <name>Andreas Bulling</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3172944.3172969</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3172944.3172969" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages,
                            6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of the ACM International Conference on Intelligent User
  Interfaces (2018) 153-164</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1801.06055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2002.03843v3</id>
    <updated>2020-03-06T12: 36: 27Z</updated>
    <published>2020-02-07T09: 53: 46Z</published>
    <title>Anomaly Detection using Deep Autoencoders for in-situ Wastewater Systems
  Monitoring Data</title>
    <summary>  Due to the growing amount of data from in-situ sensors in wastewater systems,
it becomes necessary to automatically identify abnormal behaviours and ensure
high data quality. This paper proposes an anomaly detection method based on a
deep autoencoder for in-situ wastewater systems monitoring data. The
autoencoder architecture is based on 1D Convolutional Neural Network (CNN)
layers where the convolutions are performed over the inputs across the temporal
axis of the data. Anomaly detection is then performed based on the
reconstruction error of the decoding stage. The approach is validated on
multivariate time series from in-sewer process monitoring data. We discuss the
results and the challenge of labelling anomalies in complex time series. We
suggest that our proposed approach can support the domain experts in the
identification of anomalies.
</summary>
    <author>
      <name>Stefania Russo</name>
    </author>
    <author>
      <name>Andy Disch</name>
    </author>
    <author>
      <name>Frank Blumensaat</name>
    </author>
    <author>
      <name>Kris Villez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10th IWA Symposium on Modelling and Integrated Assessment (Watermatex
  2019)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">10th IWA Symposium on Modelling and Integrated Assessment
  (Watermatex 2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2002.03843v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.03843v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2111.08370v2</id>
    <updated>2021-11-17T09: 49: 06Z</updated>
    <published>2021-11-16T11: 16: 11Z</published>
    <title>Fight Detection from Still Images in the Wild</title>
    <summary>  Detecting fights from still images shared on social media is an important
task required to limit the distribution of violent scenes in order to prevent
their negative effects. For this reason, in this study, we address the problem
of fight detection from still images collected from the web and social media.
We explore how well one can detect fights from just a single still image. We
also propose a new dataset, named Social Media Fight Images (SMFI), comprising
real-world images of fight actions. Results of the extensive experiments on the
proposed dataset show that fight actions can be recognized successfully from
still images. That is, even without exploiting the temporal information, it is
possible to detect fights with high accuracy by utilizing appearance only. We
also perform cross-dataset experiments to evaluate the representation capacity
of the collected dataset. These experiments indicate that, as in the other
computer vision problems, there exists a dataset bias for the fight recognition
problem. Although the methods achieve close to 100% accuracy when trained and
tested on the same fight dataset, the cross-dataset accuracies are
significantly lower, i.e., around 70% when more representative datasets are
used for training. SMFI dataset is found to be one of the two most
representative datasets among the utilized five fight datasets.
</summary>
    <author>
      <name>Şeymanur Aktı</name>
    </author>
    <author>
      <name>Ferda Ofli</name>
    </author>
    <author>
      <name>Muhammad Imran</name>
    </author>
    <author>
      <name>Hazım Kemal Ekenel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at Winter Conference of Applications on
  Computer Vision Workshops (WACV-W 2022), Workshop on Real-World Surveillance:
  Applications and Challenges</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.08370v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.08370v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1902.00440v7</id>
    <updated>2021-08-24T14: 41: 02Z</updated>
    <published>2019-02-01T16: 26: 51Z</published>
    <title>Spatial-Temporal-Textual Point Processes for Crime Linkage Detection</title>
    <summary>  Crimes emerge out of complex interactions of human behaviors and situations.
Linkages between crime incidents are highly complex. Detecting crime linkage
given a set of incidents is a highly challenging task since we only have
limited information, including text descriptions, incident times, and
locations. In practice, there are very few labels. We propose a new statistical
modeling framework for {\it spatio-temporal-textual
                            } data and demonstrate its
usage on crime linkage detection. We capture linkages of crime incidents via
multivariate marked spatio-temporal Hawkes processes and treat embedding
vectors of the free-text as {\it marks
                            } of the incident, inspired by the notion
of {\it modus operandi
                            } (M.O.) in crime analysis. Numerical results using real
data demonstrate the good performance of our method as well as reveals
interesting patterns in the crime data: the joint modeling of space, time, and
text information enhances crime linkage detection compared with the
state-of-the-art, and the learned spatial dependence from data can be useful
for police operations.
</summary>
    <author>
      <name>Shixiang Zhu</name>
    </author>
    <author>
      <name>Yao Xie</name>
    </author>
    <link href="http://arxiv.org/abs/1902.00440v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.00440v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/astro-ph/9707015v1</id>
    <updated>1997-07-02T05: 44: 09Z</updated>
    <published>1997-07-02T05: 44: 09Z</published>
    <title>Solar Neutrinos and The Standard Solar Model</title>
    <summary>  The standard solar model (SSM) yield a $^8$B solar neutrino flux which is
consistent within the theoretical and experimental uncertainties with that
observed at Super-Kamiokande. The combined results from the Super- Kamiokande
and the Chlorine solar neutrino experiments do not provide a solid evidence for
neutrino properties beyond the minimal standard electroweak model. The results
from the Gallium experiments and independently the combined results from
Super-Kamiokande and the Chlorine experiment imply that the $^7$Be solar
neutrino flux is strongly suppressed compared with that predicted by the SSM.
This conclusion, however, is valid only if the neutrino absorption cross
sections near threshold in Gallium and Chlorine do not differ significantly
from their theoretical estimates. Such a departure has not been ruled out by
the Chromium source experiments in Gallium. Even if the $^7$Be solar neutrino
flux is suppressed compared with that predicted by the SSM, still it can be due
to astrophysical effects not included in the simplistic SSM. Such effects
include spatial and/or temporal variations in the temperature in the solar core
induced by the convective layer through g-modes or by rotational mixing in the
solar core, and dense plasma effects which may strongly enhance p-capture by
$^7$Be relative to e-capture. The new generation of solar observations, which
already look non stop deep into the sun, like Super-Kamiokande through
neutrinos, and SOHO and GONG through acoustic waves, may be able to point at
the correct solution; astrophysical solutions if they detect unexpected
temporal and/or spatial behaviour, or particle physics solutions if
Super-Kamiokande detects characteristic spectral distortion or temporal
variations (e.g., the day-night effect) of the $^8$B solar neutrino flux . If
</summary>
    <author>
      <name>Arnon Dar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Proceedings of the 1997 ITEP Winter School on
  particle physics</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Surveys High Energ.Phys.12: 157-176,
                            1998</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/astro-ph/9707015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/9707015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2201.06123v1</id>
    <updated>2022-01-16T19: 35: 59Z</updated>
    <published>2022-01-16T19: 35: 59Z</published>
    <title>Modeling the Repetition-based Recovering of Acoustic and Visual Sources
  with Dendritic Neurons</title>
    <summary>  In natural auditory environments, acoustic signals originate from the
temporal superimposition of different sound sources. The problem of inferring
individual sources from ambiguous mixtures of sounds is known as blind source
decomposition. Experiments on humans have demonstrated that the auditory system
can identify sound sources as repeating patterns embedded in the acoustic
input. Source repetition produces temporal regularities that can be detected
and used for segregation. Specifically, listeners can identify sounds occurring
more than once across different mixtures, but not sounds heard only in a single
mixture. However, whether such a behaviour can be computationally modelled has
not yet been explored. Here, we propose a biologically inspired computational
model to perform blind source separation on sequences of mixtures of acoustic
stimuli. Our method relies on a somatodendritic neuron model trained with a
Hebbian-like learning rule which can detect spatio-temporal patterns recurring
in synaptic inputs. We show that the segregation capabilities of our model are
reminiscent of the features of human performance in a variety of experimental
settings involving synthesized sounds with naturalistic properties.
Furthermore, we extend the study to investigate the properties of segregation
on task settings not yet explored with human subjects, namely natural sounds
and images. Overall, our work suggests that somatodendritic neuron models offer
a promising neuro-inspired learning strategy to account for the characteristics
of the brain segregation capabilities as well as to make predictions on yet
untested experimental settings.
</summary>
    <author>
      <name>Giorgia Dellaferrera</name>
    </author>
    <author>
      <name>Toshitake Asabuki</name>
    </author>
    <author>
      <name>Tomoki Fukai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3389/fnins.2022.855753</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3389/fnins.2022.855753" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Frontiers in Neuroscience 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2201.06123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.06123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/physics/0401013v1</id>
    <updated>2004-01-06T14: 04: 53Z</updated>
    <published>2004-01-06T14: 04: 53Z</published>
    <title>Existence of Dynamical Scaling in the Temporal Signal of Time Projection
  Chamber</title>
    <summary>  The temporal signals from a large gas detector may show dynamical scaling due
to many correlated space points created by the charged particles while passing
through the tracking medium. This has been demonstrated through simulation
using realistic parameters of a Time Projection Chamber (TPC) being fabricated
to be used in ALICE collider experiment at CERN. An interesting aspect of this
dynamical behavior is the existence of an universal scaling which does not
depend on the multiplicity of the collision. This aspect can be utilised
further to study physics at the device level and also for the online monitoring
of certain physical observables including electronics noise which are a few
crucial parameters for the optimal TPC performance.
</summary>
    <author>
      <name>A. K. Mohanty</name>
    </author>
    <author>
      <name>D. Favretto</name>
    </author>
    <author>
      <name>F. Carminati</name>
    </author>
    <author>
      <name>K. Safarik</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1748-0221/8/12/C12020</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1748-0221/8/12/C12020" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages,
                            6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/physics/0401013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/physics/0401013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1909.01903v1</id>
    <updated>2019-09-04T15: 53: 31Z</updated>
    <published>2019-09-04T15: 53: 31Z</published>
    <title>Performance of a temporally multiplexed single-photon source with
  imperfect devices</title>
    <summary>  Scalable photonic quantum technologies require highly efficient sources of
single photons on demand. Although much progress has been done in the field
within the last decade, the requirements impose stringent conditions on the
efficiency of such devices. One of the most promising approaches is to
multiplex a single or several heralded photon sources into temporal modes. In
this work we analyze a specific proposal to synchronize photons from a
continuous source with an external reference clock using imperfect optical
switches, which necessarily degrade the ideal behavior of the devised
arrangement. The performance of the source as a sub-poissonian light emitter is
studied taking into account losses in the multiplexing arrangement, detector
efficiency and dark counts. We estimate a fivefold increase in the single
photon probability achieved for 0.5 dB loss switches.
</summary>
    <author>
      <name>Agustina G. Magnoni</name>
    </author>
    <author>
      <name>Ignacio H. López Grande</name>
    </author>
    <author>
      <name>Laura T. Knoll</name>
    </author>
    <author>
      <name>Miguel A. Larotonda</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11128-019-2417-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11128-019-2417-0" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Quantum Information Processing, vol. 18, no. 10, pp. 311 (2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1909.01903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.01903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2309.02253v2</id>
    <updated>2024-09-19T07: 08: 17Z</updated>
    <published>2023-09-05T14: 05: 37Z</published>
    <title>MA-VAE: Multi-head Attention-based Variational Autoencoder Approach for
  Anomaly Detection in Multivariate Time-series Applied to Automotive Endurance
  Powertrain Testing</title>
    <summary>  A clear need for automatic anomaly detection applied to automotive testing
has emerged as more and more attention is paid to the data recorded and manual
evaluation by humans reaches its capacity. Such real-world data is massive,
diverse, multivariate and temporal in nature, therefore requiring modelling of
the testee behaviour. We propose a variational autoencoder with multi-head
attention (MA-VAE), which, when trained on unlabelled data, not only provides
very few false positives but also manages to detect the majority of the
anomalies presented. In addition to that, the approach offers a novel way to
avoid the bypass phenomenon, an undesirable behaviour investigated in
literature. Lastly, the approach also introduces a new method to remap
individual windows to a continuous time series. The results are presented in
the context of a real-world industrial data set and several experiments are
undertaken to further investigate certain aspects of the proposed model. When
configured properly, it is 9% of the time wrong when an anomaly is flagged and
discovers 67% of the anomalies present. Also, MA-VAE has the potential to
perform well with only a fraction of the training and validation subset,
however, to extract it, a more sophisticated threshold estimation method is
required.
</summary>
    <author>
      <name>Lucas Correia</name>
    </author>
    <author>
      <name>Jan-Christoph Goos</name>
    </author>
    <author>
      <name>Philipp Klein</name>
    </author>
    <author>
      <name>Thomas Bäck</name>
    </author>
    <author>
      <name>Anna V. Kononova</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5220/0012163100003595</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5220/0012163100003595" rel="related"/>
    <link href="http://arxiv.org/abs/2309.02253v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.02253v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/astro-ph/0703007v1</id>
    <updated>2007-02-28T22: 32: 16Z</updated>
    <published>2007-02-28T22: 32: 16Z</published>
    <title>Temporal behavior of the SO 1.707 micron ro-vibronic emission band in
  Io's atmosphere</title>
    <summary>  We report observations of the ro-vibronic transition of SO at 1.707 microns
on Io. These data were taken while Io was eclipsed by Jupiter, on four nights
between July 2000 and March 2003. We analyze these results in conjunction with
a previously published night to investigate the temporal behavior of these
emissions. The observations were all conducted using the near-infrared
spectrometer NIRSPEC on the W.M. Keck II telescope. The integrated emitted
intensity for this band varies from 0.8 x 10^27 to 2.4 x 10^27 photons/sec,
with a possible link to variations in Loki's infrared brightness. The
band-shapes imply rotational temperatures of 550-1000K for the emitting gas,
lending further evidence to a volcanic origin for sulfur monoxide. An attempt
to detect the ro-vibronic transition of SO at 0.97 microns was unsuccessful;
simultaneous detection with the 1.707 micron band would permit determination of
the SO column abundance.
</summary>
    <author>
      <name>Conor Laver</name>
    </author>
    <author>
      <name>Imke de Pater</name>
    </author>
    <author>
      <name>Henry Roe</name>
    </author>
    <author>
      <name>Darrell Strobel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.icarus.2007.02.008</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.icarus.2007.02.008" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages 4 figures. Accepted by Icarus 02/27/2007</arxiv:comment>
    <link href="http://arxiv.org/abs/astro-ph/0703007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/0703007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1206.5065v1</id>
    <updated>2012-06-22T06: 24: 30Z</updated>
    <published>2012-06-22T06: 24: 30Z</published>
    <title>A generic framework for video understanding applied to group behavior
  recognition</title>
    <summary>  This paper presents an approach to detect and track groups of people in
video-surveillance applications, and to automatically recognize their behavior.
This method keeps track of individuals moving together by maintaining a spacial
and temporal group coherence. First, people are individually detected and
tracked. Second, their trajectories are analyzed over a temporal window and
clustered using the Mean-Shift algorithm. A coherence value describes how well
a set of people can be described as a group. Furthermore, we propose a formal
event description language. The group events recognition approach is
successfully validated on 4 camera views from 3 datasets: an airport, a subway,
a shopping center corridor and an entrance hall.
</summary>
    <author>
      <name>Sofia Zaidenberg</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <author>
      <name>Bernard Boulay</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <author>
      <name>François Bremond</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/AVSS.2012.1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/AVSS.2012.1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(20/03/2012)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">9th IEEE International Conference on Advanced Video and
  Signal-Based Surveillance (AVSS 2012) (2012) 136 -142</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1206.5065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.5065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2405.11238v1</id>
    <updated>2024-05-18T09: 37: 04Z</updated>
    <published>2024-05-18T09: 37: 04Z</published>
    <title>SimAD: A Simple Dissimilarity-based Approach for Time Series Anomaly
  Detection</title>
    <summary>  Despite the prevalence of reconstruction-based deep learning methods, time
series anomaly detection remains challenging. Existing approaches often
struggle with limited temporal contexts, inadequate representation of normal
patterns, and flawed evaluation metrics, hindering their effectiveness in
identifying aberrant behavior. To address these issues, we introduce
$\textbf{
                                {SimAD
                                }
                            }$, a $\textbf{
                                {Sim
                                }
                            }$ple dissimilarity-based approach for time
series $\textbf{
                                {A
                                }
                            }$nomaly $\textbf{
                                {D
                                }
                            }$etection. SimAD incorporates an
advanced feature extractor adept at processing extended temporal windows,
utilizes the EmbedPatch encoder to integrate normal behavioral patterns
comprehensively, and introduces an innovative ContrastFusion module designed to
accentuate distributional divergences between normal and abnormal data, thereby
enhancing the robustness of anomaly discrimination. Additionally, we propose
two robust evaluation metrics, UAff and NAff, addressing the limitations of
existing metrics and demonstrating their reliability through theoretical and
experimental analyses. Experiments across $\textbf{seven
                            }$ diverse time series
datasets demonstrate SimAD's superior performance compared to state-of-the-art
methods, achieving relative improvements of $\textbf{
                                19.85%
                            }$ on F1,
$\textbf{
                                4.44%
                            }$ on Aff-F1, $\textbf{
                                77.79%
                            }$ on NAff-F1, and $\textbf{
                                9.69%
                            }$
on AUC on six multivariate datasets. Code and pre-trained models are available
at https: //github.com/EmorZz1G/SimAD.
</summary>
    <author>
      <name>Zhijie Zhong</name>
    </author>
    <author>
      <name>Zhiwen Yu</name>
    </author>
    <author>
      <name>Xing Xi</name>
    </author>
    <author>
      <name>Yue Xu</name>
    </author>
    <author>
      <name>Jiahui Chen</name>
    </author>
    <author>
      <name>Kaixiang Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages,
                            12 figures,
                            7 tables, Under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.11238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.11238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2409.13857v1</id>
    <updated>2024-09-20T19: 11: 39Z</updated>
    <published>2024-09-20T19: 11: 39Z</published>
    <title>Wormhole: Concept-Aware Deep Representation Learning for Co-Evolving
  Sequences</title>
    <summary>  Identifying and understanding dynamic concepts in co-evolving sequences is
crucial for analyzing complex systems such as IoT applications, financial
markets, and online activity logs. These concepts provide valuable insights
into the underlying structures and behaviors of sequential data, enabling
better decision-making and forecasting. This paper introduces Wormhole, a novel
deep representation learning framework that is concept-aware and designed for
co-evolving time sequences. Our model presents a self-representation layer and
a temporal smoothness constraint to ensure robust identification of dynamic
concepts and their transitions. Additionally, concept transitions are detected
by identifying abrupt changes in the latent space, signifying a shift to new
behavior - akin to passing through a wormhole. This novel mechanism accurately
discerns concepts within co-evolving sequences and pinpoints the exact
locations of these wormholes, enhancing the interpretability of the learned
representations. Experiments demonstrate that this method can effectively
segment time series data into meaningful concepts, providing a valuable tool
for analyzing complex temporal patterns and advancing the detection of concept
drifts.
</summary>
    <author>
      <name>Kunpeng Xu</name>
    </author>
    <author>
      <name>Lifei Chen</name>
    </author>
    <author>
      <name>Shengrui Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2409.13857v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.13857v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1909.10695v1</id>
    <updated>2019-09-24T03: 29: 53Z</updated>
    <published>2019-09-24T03: 29: 53Z</published>
    <title>Learning deep representations for video-based intake gesture detection</title>
    <summary>  Automatic detection of individual intake gestures during eating occasions has
the potential to improve dietary monitoring and support dietary
recommendations. Existing studies typically make use of on-body solutions such
as inertial and audio sensors, while video is used as ground truth. Intake
gesture detection directly based on video has rarely been attempted. In this
study, we address this gap and show that deep learning architectures can
successfully be applied to the problem of video-based detection of intake
gestures. For this purpose, we collect and label video data of eating occasions
using 360-degree video of 102 participants. Applying state-of-the-art
approaches from video action recognition, our results show that (1) the best
model achieves an $F_1$ score of 0.858, (2) appearance features contribute more
than motion features, and (3) temporal context in form of multiple video frames
is essential for top model performance.
</summary>
    <author>
      <name>Philipp V. Rouast</name>
    </author>
    <author>
      <name>Marc T. P. Adam</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JBHI.2019.2942845</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JBHI.2019.2942845" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in IEEE Journal of Biomedical and Health Informatics</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.10695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.10695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2301.13576v1</id>
    <updated>2023-01-31T12: 03: 59Z</updated>
    <published>2023-01-31T12: 03: 59Z</published>
    <title>Sport Task: Fine Grained Action Detection and Classification of Table
  Tennis Strokes from Videos for MediaEval 2022</title>
    <summary>  Sports video analysis is a widespread research topic. Its applications are
very diverse, like events detection during a match, video summary, or
fine-grained movement analysis of athletes. As part of the MediaEval 2022
benchmarking initiative, this task aims at detecting and classifying subtle
movements from sport videos. We focus on recordings of table tennis matches.
Conducted since 2019, this task provides a classification challenge from
untrimmed videos recorded under natural conditions with known temporal
boundaries for each stroke. Since 2021, the task also provides a stroke
detection challenge from unannotated, untrimmed videos. This year, the
training, validation, and test sets are enhanced to ensure that all strokes are
represented in each dataset. The dataset is now similar to the one used in [
                                1,
                                2
                            ]. This research is intended to build tools for coaches and athletes who want
to further evaluate their sport performances.
</summary>
    <author>
      <name>Pierre-Etienne Martin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MPI-EVA</arxiv:affiliation>
    </author>
    <author>
      <name>Jordan Calandre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIA</arxiv:affiliation>
    </author>
    <author>
      <name>Boris Mansencal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Jenny Benois-Pineau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Renaud Péteri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIA</arxiv:affiliation>
    </author>
    <author>
      <name>Laurent Mascarilla</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIA</arxiv:affiliation>
    </author>
    <author>
      <name>Julien Morlier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MediaEval 2022 Workshop, Jan 2023, Bergen, Norway. arXiv admin note:
  substantial text overlap with arXiv: 2112.11384</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.13576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.13576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2501.10441v1</id>
    <updated>2025-01-13T22: 28: 04Z</updated>
    <published>2025-01-13T22: 28: 04Z</published>
    <title>A Review of Detection, Evolution, and Data Reconstruction Strategies for
  False Data Injection Attacks in Power Cyber-Physical Systems</title>
    <summary>  The integration of information and physical systems in modern power grids has
heightened vulnerabilities to False Data Injection Attacks (FDIAs), threatening
the secure operation of power cyber-physical systems (CPS). This paper reviews
FDIA detection, evolution, and data reconstruction strategies, highlighting
cross-domain coordination, multi-temporal evolution, and stealth
characteristics. Challenges in existing detection methods, including poor
interpretability and data imbalance, are discussed, alongside advanced
state-aware and action-control data reconstruction techniques. Key issues, such
as modeling FDIA evolution and distinguishing malicious data from regular
faults, are identified. Future directions to enhance system resilience and
detection accuracy are proposed, contributing to the secure operation of power
CPS.
</summary>
    <author>
      <name>Xiaoyong Bo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 pages,
                            4 figures,
                            6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.10441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.10441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1712.08948v1</id>
    <updated>2017-12-24T17: 08: 56Z</updated>
    <published>2017-12-24T17: 08: 56Z</published>
    <title>Change points, memory and epidemic spreading in temporal networks</title>
    <summary>  Dynamic networks exhibit temporal patterns that vary across different time
scales, all of which can potentially affect processes that take place on the
network. However, most data-driven approaches used to model time-varying
networks attempt to capture only a single characteristic time scale in
isolation --- typically associated with the short-time memory of a Markov chain
or with long-time abrupt changes caused by external or systemic events. Here we
propose a unified approach to model both aspects simultaneously, detecting
short and long-time behaviors of temporal networks. We do so by developing an
arbitrary-order mixed Markov model with change points, and using a
nonparametric Bayesian formulation that allows the Markov order and the
position of change points to be determined from data without overfitting. In
addition, we evaluate the quality of the multiscale model in its capacity to
reproduce the spreading of epidemics on the temporal network, and we show that
describing multiple time scales simultaneously has a synergistic effect, where
statistically significant features are uncovered that otherwise would remain
hidden by treating each time scale independently.
</summary>
    <author>
      <name>Tiago P. Peixoto</name>
    </author>
    <author>
      <name>Laetitia Gauvin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages,
                            6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.08948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1510.09035v2</id>
    <updated>2016-03-09T12: 51: 15Z</updated>
    <published>2015-10-30T10: 07: 59Z</published>
    <title>Emergence of spike correlations in periodically forced excitable systems</title>
    <summary>  In sensory neurons the presence of noise can facilitate the detection of weak
information-carrying signals, which are encoded and transmitted via correlated
sequences of spikes. Here we investigate relative temporal order in spike
sequences induced by a subthreshold periodic input, in the presence of white
Gaussian noise. To simulate the spikes, we use the FitzHugh-Nagumo model, and
to investigate the output sequence of inter-spike intervals (ISIs), we use the
symbolic method of ordinal analysis. We find different types of relative
temporal order, in the form of preferred ordinal patterns which depend on both,
the strength of the noise and the period of the input signal. We also
demonstrate a resonance-like behavior, as certain periods and noise levels
enhance temporal ordering in the ISI sequence, maximizing the probability of
the preferred patterns. Our findings could be relevant for understanding the
mechanisms underlying temporal coding, by which single sensory neurons
represent in spike sequences the information about weak periodic stimuli.
</summary>
    <author>
      <name>Jose A. Reinoso</name>
    </author>
    <author>
      <name>M. C. Torrent</name>
    </author>
    <author>
      <name>Cristina Masoller</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevE.94.032218</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevE.94.032218" rel="related"/>
    <link href="http://arxiv.org/abs/1510.09035v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.09035v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1803.04240v1</id>
    <updated>2018-02-16T22: 54: 38Z</updated>
    <published>2018-02-16T22: 54: 38Z</published>
    <title>Discovering demographic data of users from the evolution of their
  spatio-temporal entropy</title>
    <summary>  Inferring information related to users enables to highly improve the quality
of many mobile services. For example, knowing the demographic characteristics
of a user allows a service to display more accurate information. According to
the literature, various works present models to detect them but, to the best of
our knowledge, no one is based on the use of the spatio-temporal entropy and
introduces Generalized Additive models (GAMs) in this context to reach this
goal. In this preliminary work, we present a new approach including these two
key elements. The spatio-temporal entropy enables to capture the regularity of
the mobility behavior of a user, while GAMs help to predict her demographic
data based on several co-variables including the spatio-temporal entropy. The
preliminary results are very encouraging to do future work since we obtain a
prediction accuracy of 87% about the prediction of the working profile of
users.
</summary>
    <author>
      <name>Arielle Moro</name>
    </author>
    <author>
      <name>Benoît Garbinato</name>
    </author>
    <author>
      <name>Valérie Chavez-Demoulin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages (poster)</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.04240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2104.06686v1</id>
    <updated>2021-04-14T08: 16: 08Z</updated>
    <published>2021-04-14T08: 16: 08Z</published>
    <title>Underwater dual-magnification imaging for automated lake plankton
  monitoring</title>
    <summary>  We present an approach for automated in-situ monitoring of phytoplankton and
zooplankton communities based on a dual magnification dark-field imaging
microscope/camera. We describe the Dual Scripps Plankton Camera (DSPC) system
and associated image processing, and assess its capabilities in detecting and
characterizing plankton species of different size and taxonomic categories, and
in measuring their abundances in both laboratory and field applications. In the
laboratory, body size and abundance estimates by the DSPC significantly and
robustly scale with the same measurements derived by traditional microscopy. In
the field, a DSPC installed permanently at 3 m depth in Lake Greifensee
(Switzerland), delivered images of plankton individuals, colonies, and
heterospecific aggregates without disrupting natural arrangements of
interacting organisms, their microenvironment or their behavior at hourly
timescales. The DSPC was able to track the dynamics of taxa in the size range
between ~10 $\mu$m to ~ 1 cm, covering virtually all the components of the
planktonic food web (including parasites and potentially toxic cyanobacteria).
Comparing data from the field-deployed DSPC to traditional sampling and
microscopy revealed a general overall agreement in estimates of plankton
diversity and abundances, despite imaging limitations in detecting small
phytoplankton species and rare and large zooplankton taxa (e.g. carnivorous
zooplankton). The most significant disagreements between traditional methods
and the DSPC resided in the measurements of community properties of
zooplankton, organisms that are heterogeneously distributed spatially and
temporally, and whose demography appeared to be better captured by automated
imaging. Time series collected by the DSPC depicted ecological succession
patterns, algal bloom dynamics and circadian fluctuations with a temporal
frequency and morphological [continues...
                            ]
</summary>
    <author>
      <name>E. Merz</name>
    </author>
    <author>
      <name>T. Kozakiewicz</name>
    </author>
    <author>
      <name>M. Reyes</name>
    </author>
    <author>
      <name>C. Ebi</name>
    </author>
    <author>
      <name>P. Isles</name>
    </author>
    <author>
      <name>M. Baity-Jesi</name>
    </author>
    <author>
      <name>P. Roberts</name>
    </author>
    <author>
      <name>J. S. Jaffe</name>
    </author>
    <author>
      <name>S. Dennis</name>
    </author>
    <author>
      <name>T. Hardeman</name>
    </author>
    <author>
      <name>N. Stevens</name>
    </author>
    <author>
      <name>T. Lorimer</name>
    </author>
    <author>
      <name>F. Pomati</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.watres.2021.117524</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.watres.2021.117524" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">49 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Water Research,
                            203,
                            117524 (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.06686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.06686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2208.09240v1</id>
    <updated>2022-08-19T09: 34: 11Z</updated>
    <published>2022-08-19T09: 34: 11Z</published>
    <title>An Unsupervised Short- and Long-Term Mask Representation for
  Multivariate Time Series Anomaly Detection</title>
    <summary>  Anomaly detection of multivariate time series is meaningful for system
behavior monitoring. This paper proposes an anomaly detection method based on
unsupervised Short- and Long-term Mask Representation learning (SLMR). The main
idea is to extract short-term local dependency patterns and long-term global
trend patterns of the multivariate time series by using multi-scale residual
dilated convolution and Gated Recurrent Unit(GRU) respectively. Furthermore,
our approach can comprehend temporal contexts and feature correlations by
combining spatial-temporal masked self-supervised representation learning and
sequence split. It considers the importance of features is different, and we
introduce the attention mechanism to adjust the contribution of each feature.
Finally, a forecasting-based model and a reconstruction-based model are
integrated to focus on single timestamp prediction and latent representation of
time series. Experiments show that the performance of our method outperforms
other state-of-the-art models on three real-world datasets. Further analysis
shows that our method is good at interpretability.
</summary>
    <author>
      <name>Qiucheng Miao</name>
    </author>
    <author>
      <name>Chuanfu Xu</name>
    </author>
    <author>
      <name>Jun Zhan</name>
    </author>
    <author>
      <name>Dong Zhu</name>
    </author>
    <author>
      <name>Chengkun Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2208.09240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.09240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2412.14768v1</id>
    <updated>2024-12-19T11: 51: 45Z</updated>
    <published>2024-12-19T11: 51: 45Z</published>
    <title>FLAMe: Federated Learning with Attention Mechanism using Spatio-Temporal
  Keypoint Transformers for Pedestrian Fall Detection in Smart Cities</title>
    <summary>  In smart cities, detecting pedestrian falls is a major challenge to ensure
the safety and quality of life of citizens. In this study, we propose a novel
fall detection system using FLAMe (Federated Learning with Attention
Mechanism), a federated learning (FL) based algorithm. FLAMe trains around
important keypoint information and only transmits the trained important weights
to the server, reducing communication costs and preserving data privacy.
Furthermore, the lightweight keypoint transformer model is integrated into the
FL framework to effectively learn spatio-temporal features. We validated the
experiment using 22,
                            672 video samples from the "Fall Accident Risk Behavior
Video-Sensor Pair data" dataset from AI-Hub. As a result of the experiment, the
FLAMe-based system achieved an accuracy of 94.02% with about 190,
                            000
transmission parameters, maintaining performance similar to that of existing
centralized learning while maximizing efficiency by reducing communication
costs by about 40% compared to the existing FL algorithm, FedAvg. Therefore,
the FLAMe algorithm has demonstrated that it provides robust performance in the
distributed environment of smart cities and is a practical and effective
solution for public safety.
</summary>
    <author>
      <name>Byeonghun Kim</name>
    </author>
    <author>
      <name>Byeongjoon Noh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,
                            7 figures, AAAI 2025 FLUID Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.14768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.14768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2504.05271v1</id>
    <updated>2025-04-07T17: 08: 17Z</updated>
    <published>2025-04-07T17: 08: 17Z</published>
    <title>AnomalousNet: A Hybrid Approach with Attention U-Nets and Change Point
  Detection for Accurate Characterization of Anomalous Diffusion in Video Data</title>
    <summary>  Anomalous diffusion occurs in a wide range of systems, including protein
transport within cells, animal movement in complex habitats, pollutant
dispersion in groundwater, and nanoparticle motion in synthetic materials.
Accurately estimating the anomalous diffusion exponent and the diffusion
coefficient from the particle trajectories is essential to distinguish between
sub-diffusive, super-diffusive, or normal diffusion regimes. These estimates
provide a deeper insight into the underlying dynamics of the system,
facilitating the identification of particle behaviors and the detection of
changes in diffusion states. However, analyzing short and noisy video data,
which often yield incomplete and heterogeneous trajectories, poses a
significant challenge for traditional statistical approaches. We introduce a
data-driven method that integrates particle tracking, an attention
  U-Net architecture, and a change-point detection algorithm to address these
issues. This approach not only infers the anomalous diffusion parameters with
high accuracy but also identifies temporal transitions between different
states, even in the presence of noise and limited temporal resolution. Our
methodology demonstrated strong performance in the 2nd Anomalous Diffusion
(AnDi) Challenge benchmark within the top submissions for video tasks.
</summary>
    <author>
      <name>Yusef Ahsini</name>
    </author>
    <author>
      <name>Marc Escoto</name>
    </author>
    <author>
      <name>J. Alberto Conejero</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages,
                            9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.05271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.05271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1904.11101v1</id>
    <updated>2019-04-25T00: 03: 07Z</updated>
    <published>2019-04-25T00: 03: 07Z</published>
    <title>Change Point Estimation in Panel Data with Temporal and Cross-sectional
  Dependence</title>
    <summary>  We study the problem of detecting a common change point in large panel data
based on a mean shift model, wherein the errors exhibit both temporal and
cross-sectional dependence. A least squares based procedure is used to estimate
the location of the change point. Further, we establish the convergence rate
and obtain the asymptotic distribution of the least squares estimator. The form
of the distribution is determined by the behavior of the norm difference of the
means before and after the change point. Since the behavior of this norm
difference is, a priori, unknown to the practitioner, we also develop a novel
data driven adaptive procedure that provides valid confidence intervals for the
common change point, without requiring any such knowledge. Numerical work based
on synthetic data illustrates the performance of the estimator in finite
samples under different settings of temporal and cross-sectional dependence,
sample size and number of panels. Finally, we examine an application to
financial stock data and discuss the identified change points.
</summary>
    <author>
      <name>Monika Bhattacharjee</name>
    </author>
    <author>
      <name>Moulinath Banerjee</name>
    </author>
    <author>
      <name>George Michailidis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">57 pages,
                            1 figure,
                            11 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.11101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.11101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2305.10507v1</id>
    <updated>2023-05-17T18: 24: 43Z</updated>
    <published>2023-05-17T18: 24: 43Z</published>
    <title>ReasonNet: End-to-End Driving with Temporal and Global Reasoning</title>
    <summary>  The large-scale deployment of autonomous vehicles is yet to come, and one of
the major remaining challenges lies in urban dense traffic scenarios. In such
cases, it remains challenging to predict the future evolution of the scene and
future behaviors of objects, and to deal with rare adverse events such as the
sudden appearance of occluded objects. In this paper, we present ReasonNet, a
novel end-to-end driving framework that extensively exploits both temporal and
global information of the driving scene. By reasoning on the temporal behavior
of objects, our method can effectively process the interactions and
relationships among features in different frames. Reasoning about the global
information of the scene can also improve overall perception performance and
benefit the detection of adverse events, especially the anticipation of
potential danger from occluded objects. For comprehensive evaluation on
occlusion events, we also release publicly a driving simulation benchmark
DriveOcclusionSim consisting of diverse occlusion events. We conduct extensive
experiments on multiple CARLA benchmarks, where our model outperforms all prior
methods, ranking first on the sensor track of the public CARLA Leaderboard.
</summary>
    <author>
      <name>Hao Shao</name>
    </author>
    <author>
      <name>Letian Wang</name>
    </author>
    <author>
      <name>Ruobing Chen</name>
    </author>
    <author>
      <name>Steven L. Waslander</name>
    </author>
    <author>
      <name>Hongsheng Li</name>
    </author>
    <author>
      <name>Yu Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.10507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.10507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2304.05292v4</id>
    <updated>2024-01-05T18: 59: 41Z</updated>
    <published>2023-04-11T15: 42: 20Z</published>
    <title>MC-ViViT: Multi-branch Classifier-ViViT to detect Mild Cognitive
  Impairment in older adults using facial videos</title>
    <summary>  Deep machine learning models including Convolutional Neural Networks (CNN)
have been successful in the detection of Mild Cognitive Impairment (MCI) using
medical images, questionnaires, and videos. This paper proposes a novel
Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to
distinguish MCI from those with normal cognition by analyzing facial features.
The data comes from the I-CONECT, a behavioral intervention trial aimed at
improving cognitive function by providing frequent video chats. MC-ViViT
extracts spatiotemporal features of videos in one branch and augments
representations by the MC module. The I-CONECT dataset is challenging as the
dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which
impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy
and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE
loss to address the imbalanced problem. Our experimental results on the
I-CONECT dataset show the great potential of MC-ViViT in predicting MCI with a
high accuracy of 90.63% accuracy on some of the interview videos.
</summary>
    <author>
      <name>Jian Sun</name>
    </author>
    <author>
      <name>Hiroko H. Dodge</name>
    </author>
    <author>
      <name>Mohammad H. Mahoor</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.eswa.2023.121929</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.eswa.2023.121929" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages,
                            7 tables,
                            7 figures,
                            9 equations</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Expert Systems with Applications,
                            238,
                            121929 (2023)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2304.05292v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.05292v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2310.20208v4</id>
    <updated>2024-07-14T09: 02: 22Z</updated>
    <published>2023-10-31T06: 11: 23Z</published>
    <title>ZoomNeXt: A Unified Collaborative Pyramid Network for Camouflaged Object
  Detection</title>
    <summary>  Recent camouflaged object detection (COD) attempts to segment objects
visually blended into their surroundings, which is extremely complex and
difficult in real-world scenarios. Apart from the high intrinsic similarity
between camouflaged objects and their background, objects are usually diverse
in scale, fuzzy in appearance, and even severely occluded. To this end, we
propose an effective unified collaborative pyramid network that mimics human
behavior when observing vague images and videos, \ie zooming in and out.
Specifically, our approach employs the zooming strategy to learn discriminative
mixed-scale semantics by the multi-head scale integration and rich granularity
perception units, which are designed to fully explore imperceptible clues
between candidate objects and background surroundings. The former's intrinsic
multi-head aggregation provides more diverse visual patterns. The latter's
routing mechanism can effectively propagate inter-frame differences in
spatiotemporal scenarios and be adaptively deactivated and output all-zero
results for static representations. They provide a solid foundation for
realizing a unified architecture for static and dynamic COD. Moreover,
considering the uncertainty and ambiguity derived from indistinguishable
textures, we construct a simple yet effective regularization, uncertainty
awareness loss, to encourage predictions with higher confidence in candidate
regions. Our highly task-friendly framework consistently outperforms existing
state-of-the-art methods in image and video COD benchmarks. Our code can be
found at {https: //github.com/lartpang/ZoomNeXt}.
</summary>
    <author>
      <name>Youwei Pang</name>
    </author>
    <author>
      <name>Xiaoqi Zhao</name>
    </author>
    <author>
      <name>Tian-Zhu Xiang</name>
    </author>
    <author>
      <name>Lihe Zhang</name>
    </author>
    <author>
      <name>Huchuan Lu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TPAMI.2024.3417329</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TPAMI.2024.3417329" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extensions to the conference version accepted by TPAMI 2024. Fixed
  value errors. arXiv admin note: substantial text overlap with
  arXiv: 2203.02688</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.20208v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.20208v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2406.19015v3</id>
    <updated>2024-10-30T15: 28: 05Z</updated>
    <published>2024-06-27T09: 00: 05Z</published>
    <title>Gaussian process-based online health monitoring and fault analysis of
  lithium-ion battery systems from field data</title>
    <summary>  Health monitoring, fault analysis, and detection are critical for the safe
and sustainable operation of battery systems. We apply Gaussian process
resistance models on lithium iron phosphate battery field data to effectively
separate the time-dependent and operating point-dependent resistance. The data
set contains 29 battery systems returned to the manufacturer for warranty, each
with eight cells in series, totaling 232 cells and 131 million data rows. We
develop probabilistic fault detection rules using recursive spatiotemporal
Gaussian processes. These processes allow the quick processing of over a
million data points, enabling advanced online monitoring and furthering the
understanding of battery pack failure in the field. The analysis underlines
that often, only a single cell shows abnormal behavior or a knee point,
consistent with weakest-link failure for cells connected in series, amplified
by local resistive heating. The results further the understanding of how
batteries degrade and fail in the field and demonstrate the potential of
efficient online monitoring based on data. We open-source the code and publish
the large data set upon completion of the review of this article.
</summary>
    <author>
      <name>Joachim Schaeffer</name>
    </author>
    <author>
      <name>Eric Lenz</name>
    </author>
    <author>
      <name>Duncan Gulla</name>
    </author>
    <author>
      <name>Martin Z. Bazant</name>
    </author>
    <author>
      <name>Richard D. Braatz</name>
    </author>
    <author>
      <name>Rolf Findeisen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.xcrp.2024.102258</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.xcrp.2024.102258" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This version is outdated. The final version is published as open
  access journal article: https: //doi.org/10.1016/j.xcrp.2024.102258</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Cell Reports Physical Science,
                                102258 (2024)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2406.19015v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.19015v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/0704.2737v1</id>
    <updated>2007-04-20T15: 54: 06Z</updated>
    <published>2007-04-20T15: 54: 06Z</published>
    <title>INTEGRAL high energy detection of the transient IGR J11321-5311</title>
    <summary>  Context: The transient hard X-ray source IGR J11321-5311 was discovered by
INTEGRAL on June 2005, during observations of the Crux spiral arm. To date,
this is the only detection of the source to be reported by any X/gamma-ray
mission. Aims: To characterize the behaviour and hence the nature of the source
through temporal and spectral IBIS analysis. Methods: Detailed spectral and
temporal analysis has been performed using standard INTEGRAL software OSA
v.5.1. Results: To date, IGR J11321-5311 has been detected only once. It was
active for about 3.5 hours, a short and bright flare lasting about 1.5 hours is
evident in the IBIS light curve. It reached a peak flux of about 80 mCrab or
2.2x10E-9 erg cmE-2 sE-1 (20--300 keV),corresponding to a peak luminosity of
1.1x10E37 erg sE-1 (assuming a distance of 6.5 kpc). During the outburst, the
source was detected with a significance of 18 sigma (20--300 keV) and 8 sigma
(100--300 keV). The spectrum of the total outburst activity (17--300 keV) is
best fitted by the sum of a power law (Gamma=0.55+/-0.18) plus a black body
(kT=1.0{+0.2
                                }_{
                                    -0.3
                                } keV), with no evidence for a break up to 300 keV. A
spectral analysis at Science Window level revealed an evident hardening of the
spectrum through the outburst. The IBIS data were searched for pulsations with
no positive result. Conclusions: The X-ray spectral shape and the flaring
behaviour favour the hypothesis that IGR J11321-5311 is an Anomalous X-ray
Pulsar, though a different nature can not be firmly rejected at the present
stage.
</summary>
    <author>
      <name>V. Sguera</name>
    </author>
    <author>
      <name>A. Bazzano</name>
    </author>
    <author>
      <name>A. J. Bird</name>
    </author>
    <author>
      <name>A. B. Hill</name>
    </author>
    <author>
      <name>A. J. Dean</name>
    </author>
    <author>
      <name>L. Bassani</name>
    </author>
    <author>
      <name>A. Malizia</name>
    </author>
    <author>
      <name>P. Ubertini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1051/0004-6361: 20077439</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1051/0004-6361:20077439" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for publication in A&amp;A letter,
                                4 pages,
                                6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0704.2737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.2737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2407.05976v2</id>
    <updated>2024-08-19T06: 20: 41Z</updated>
    <published>2024-07-08T14: 18: 33Z</published>
    <title>Change-Point Detection in Industrial Data Streams based on Online
  Dynamic Mode Decomposition with Control</title>
    <summary>  We propose a novel change-point detection method based on online Dynamic Mode
Decomposition with control (ODMDwC). Leveraging ODMDwC's ability to find and
track linear approximation of a non-linear system while incorporating control
effects, the proposed method dynamically adapts to its changing behavior due to
aging and seasonality. This approach enables the detection of changes in
spatial, temporal, and spectral patterns, providing a robust solution that
preserves correspondence between the score and the extent of change in the
system dynamics. We formulate a truncated version of ODMDwC and utilize
higher-order time-delay embeddings to mitigate noise and extract broad-band
features. Our method addresses the challenges faced in industrial settings
where safety-critical systems generate non-uniform data streams while requiring
timely and accurate change-point detection to protect profit and life. Our
results demonstrate that this method yields intuitive and improved detection
results compared to the Singular-Value-Decomposition-based method. We validate
our approach using synthetic and real-world data, showing its competitiveness
to other approaches on complex systems' benchmark datasets. Provided guidelines
for hyperparameters selection enhance our method's practical applicability.
</summary>
    <author>
      <name>Marek Wadinger</name>
    </author>
    <author>
      <name>Michal Kvasnica</name>
    </author>
    <author>
      <name>Yoshinobu Kawahara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">preprint under review in Elsevier</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.05976v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.05976v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2110.06915v3</id>
    <updated>2022-06-09T20: 48: 45Z</updated>
    <published>2021-10-13T17: 51: 46Z</published>
    <title>Object-Region Video Transformers</title>
    <summary>  Recently, video transformers have shown great success in video understanding,
exceeding CNN performance; yet existing video transformer models do not
explicitly model objects, although objects can be essential for recognizing
actions. In this work, we present Object-Region Video Transformers (ORViT), an
\emph{object-centric
                                } approach that extends video transformer layers with a
block that directly incorporates object representations. The key idea is to
fuse object-centric representations starting from early layers and propagate
them into the transformer-layers, thus affecting the spatio-temporal
representations throughout the network. Our ORViT block consists of two
object-level streams: appearance and dynamics. In the appearance stream, an
"Object-Region Attention" module applies self-attention over the patches and
\emph{object regions
                                }. In this way, visual object regions interact with uniform
patch tokens and enrich them with contextualized object information. We further
model object dynamics via a separate "Object-Dynamics Module", which captures
trajectory interactions, and show how to integrate the two streams. We evaluate
our model on four tasks and five datasets: compositional and few-shot action
recognition on SomethingElse, spatio-temporal action detection on AVA, and
standard action recognition on Something-Something V2, Diving48 and
Epic-Kitchen100. We show strong performance improvement across all tasks and
datasets considered, demonstrating the value of a model that incorporates
object representations into a transformer architecture. For code and pretrained
models, visit the project page at \url{https: //roeiherz.github.io/ORViT/}
</summary>
    <author>
      <name>Roei Herzig</name>
    </author>
    <author>
      <name>Elad Ben-Avraham</name>
    </author>
    <author>
      <name>Karttikeya Mangalam</name>
    </author>
    <author>
      <name>Amir Bar</name>
    </author>
    <author>
      <name>Gal Chechik</name>
    </author>
    <author>
      <name>Anna Rohrbach</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Amir Globerson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.06915v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.06915v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2303.08920v1</id>
    <updated>2023-03-15T20: 33: 50Z</updated>
    <published>2023-03-15T20: 33: 50Z</published>
    <title>EgoViT: Pyramid Video Transformer for Egocentric Action Recognition</title>
    <summary>  Capturing interaction of hands with objects is important to autonomously
detect human actions from egocentric videos. In this work, we present a pyramid
video transformer with a dynamic class token generator for egocentric action
recognition. Different from previous video transformers, which use the same
static embedding as the class token for diverse inputs, we propose a dynamic
class token generator that produces a class token for each input video by
analyzing the hand-object interaction and the related motion information. The
dynamic class token can diffuse such information to the entire model by
communicating with other informative tokens in the subsequent transformer
layers. With the dynamic class token, dissimilarity between videos can be more
prominent, which helps the model distinguish various inputs. In addition,
traditional video transformers explore temporal features globally, which
requires large amounts of computation. However, egocentric videos often have a
large amount of background scene transition, which causes discontinuities
across distant frames. In this case, blindly reducing the temporal sampling
rate will risk losing crucial information. Hence, we also propose a pyramid
architecture to hierarchically process the video from short-term high rate to
long-term low rate. With the proposed architecture, we significantly reduce the
computational cost as well as the memory requirement without sacrificing from
the model performance. We perform comparisons with different baseline video
transformers on the EPIC-KITCHENS-100 and EGTEA Gaze+ datasets. Both
quantitative and qualitative results show that the proposed model can
efficiently improve the performance for egocentric action recognition.
</summary>
    <author>
      <name>Chenbin Pan</name>
    </author>
    <author>
      <name>Zhiqi Zhang</name>
    </author>
    <author>
      <name>Senem Velipasalar</name>
    </author>
    <author>
      <name>Yi Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2303.08920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.08920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2406.02978v2</id>
    <updated>2024-08-26T09: 23: 44Z</updated>
    <published>2024-06-05T06: 21: 54Z</published>
    <title>Self-Supervised Skeleton-Based Action Representation Learning: A
  Benchmark and Beyond</title>
    <summary>  Self-supervised learning (SSL), which aims to learn meaningful prior
representations from unlabeled data, has been proven effective for
skeleton-based action understanding. Different from the image domain, skeleton
data possesses sparser spatial structures and diverse representation forms,
with the absence of background clues and the additional temporal dimension,
presenting new challenges for spatial-temporal motion pretext task design.
Recently, many endeavors have been made for skeleton-based SSL, achieving
remarkable progress. However, a systematic and thorough review is still
lacking. In this paper, we conduct, for the first time, a comprehensive survey
on self-supervised skeleton-based action representation learning. Following the
taxonomy of context-based, generative learning, and contrastive learning
approaches, we make a thorough review and benchmark of existing works and shed
light on the future possible directions. Remarkably, our investigation
demonstrates that most SSL works rely on the single paradigm, learning
representations of a single level, and are evaluated on the action recognition
task solely, which leaves the generalization power of skeleton SSL models
under-explored. To this end, a novel and effective SSL method for skeleton is
further proposed, which integrates versatile representation learning objectives
of different granularity, substantially boosting the generalization capacity
for multiple skeleton downstream tasks. Extensive experiments under three
large-scale datasets demonstrate our method achieves superior generalization
performance on various downstream tasks, including recognition, retrieval,
detection, and few-shot learning.
</summary>
    <author>
      <name>Jiahang Zhang</name>
    </author>
    <author>
      <name>Lilang Lin</name>
    </author>
    <author>
      <name>Shuai Yang</name>
    </author>
    <author>
      <name>Jiaying Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2406.02978v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.02978v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2412.09220v2</id>
    <updated>2024-12-14T05: 42: 51Z</updated>
    <published>2024-12-12T12: 20: 27Z</published>
    <title>USDRL: Unified Skeleton-Based Dense Representation Learning with
  Multi-Grained Feature Decorrelation</title>
    <summary>  Contrastive learning has achieved great success in skeleton-based
representation learning recently. However, the prevailing methods are
predominantly negative-based, necessitating additional momentum encoder and
memory bank to get negative samples, which increases the difficulty of model
training. Furthermore, these methods primarily concentrate on learning a global
representation for recognition and retrieval tasks, while overlooking the rich
and detailed local representations that are crucial for dense prediction tasks.
To alleviate these issues, we introduce a Unified Skeleton-based Dense
Representation Learning framework based on feature decorrelation, called USDRL,
which employs feature decorrelation across temporal, spatial, and instance
domains in a multi-grained manner to reduce redundancy among dimensions of the
representations to maximize information extraction from features. Additionally,
we design a Dense Spatio-Temporal Encoder (DSTE) to capture fine-grained action
representations effectively, thereby enhancing the performance of dense
prediction tasks. Comprehensive experiments, conducted on the benchmarks
NTU-60, NTU-120, PKU-MMD I, and PKU-MMD II, across diverse downstream tasks
including action recognition, action retrieval, and action detection,
conclusively demonstrate that our approach significantly outperforms the
current state-of-the-art (SOTA) approaches. Our code and models are available
at https: //github.com/wengwanjiang/USDRL.
</summary>
    <author>
      <name>Wanjiang Weng</name>
    </author>
    <author>
      <name>Hongsong Wang</name>
    </author>
    <author>
      <name>Junbo Wang</name>
    </author>
    <author>
      <name>Lei He</name>
    </author>
    <author>
      <name>Guosen Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by AAAI 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.09220v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.09220v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.21055v1</id>
    <updated>2025-03-27T00: 03: 55Z</updated>
    <published>2025-03-27T00: 03: 55Z</published>
    <title>What Changed and What Could Have Changed? State-Change Counterfactuals
  for Procedure-Aware Video Representation Learning</title>
    <summary>  Understanding a procedural activity requires modeling both how action steps
transform the scene, and how evolving scene transformations can influence the
sequence of action steps, even those that are accidental or erroneous. Existing
work has studied procedure-aware video representations by proposing novel
approaches such as modeling the temporal order of actions and has not
explicitly learned the state changes (scene transformations). In this work, we
study procedure-aware video representation learning by incorporating
state-change descriptions generated by Large Language Models (LLMs) as
supervision signals for video encoders. Moreover, we generate state-change
counterfactuals that simulate hypothesized failure outcomes, allowing models to
learn by imagining the unseen ``What if'' scenarios. This counterfactual
reasoning facilitates the model's ability to understand the cause and effect of
each step in an activity. To verify the procedure awareness of our model, we
conduct extensive experiments on procedure-aware tasks, including temporal
action segmentation and error detection. Our results demonstrate the
effectiveness of the proposed state-change descriptions and their
counterfactuals and achieve significant improvements on multiple tasks. We will
make our source code and data publicly available soon.
</summary>
    <author>
      <name>Chi-Hsi Kung</name>
    </author>
    <author>
      <name>Frangil Ramirez</name>
    </author>
    <author>
      <name>Juhyung Ha</name>
    </author>
    <author>
      <name>Yi-Ting Chen</name>
    </author>
    <author>
      <name>David Crandall</name>
    </author>
    <author>
      <name>Yi-Hsuan Tsai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages,
                                    4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.21055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.21055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1106.4209v1</id>
    <updated>2011-06-21T14: 06: 37Z</updated>
    <published>2011-06-21T14: 06: 37Z</published>
    <title>IGR J17354-3255 as a candidate intermediate SFXT possibly associated
  with the transient MeV AGL J1734-3310</title>
    <summary>  We present spectral and temporal results from INTEGRAL long-term monitoring
of the unidentified X-ray source IGR J17354-3255. We show that it is a weak
persistent hard X-ray source spending a major fraction of the time in an
out-of-outburst state with average 18-60 keV X-ray flux of about 1.1 mCrab,
occasionally interspersed with fast X-ray flares (duration from a few hours to
a few days) with a dynamic range as high as 200. From archival Swift/XRT
observations, we also show that the dynamic range from non-detection to highest
level of measured X-ray activity is &gt;300. Our IBIS timing analysis strongly
confirms the 8.4 days orbital period previously detected with Swift/BAT, in
addition we show that the shape of the orbital profile is rather smooth and
appears to be dominated by low level X-ray emission rather than by bright
outbursts, the measured degree of outburst recurrence is about 25 per cent. The
spectral and temporal characteristics of IGR J17354-3255 are highly indicative
of a Supergiant High Mass X-ray Binary nature (SGXB). However, our inferred
dynamic ranges both at soft and hard X-rays are significantly greater than
those of classical SGXB systems, but instead are typical of intermediate
Supergiant Fast X-ray Transient (SFXTs). Finally, we note for the first time
that the observed fast flaring X-ray behaviour of IGR J17354-3255 is very
similar to that detected with AGILE from the spatially associated MeV source
AGL J1734-3310, suggesting a possible physical link between the two objects.
</summary>
    <author>
      <name>V. Sguera</name>
    </author>
    <author>
      <name>S. P. Drave</name>
    </author>
    <author>
      <name>A. J. Bird</name>
    </author>
    <author>
      <name>A. Bazzano</name>
    </author>
    <author>
      <name>R. Landi</name>
    </author>
    <author>
      <name>P. Ubertini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/j.1365-2966.2011.19298.x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/j.1365-2966.2011.19298.x" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for publication in MNRAS,
                                    8 pages,
                                    9 figures,
                                    1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.4209v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.4209v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1911.01952v3</id>
    <updated>2021-05-13T14: 20: 55Z</updated>
    <published>2019-11-05T17: 21: 30Z</published>
    <title>Coverage Guided Testing for Recurrent Neural Networks</title>
    <summary>  Recurrent neural networks (RNNs) have been applied to a broad range of
applications, including natural language processing, drug discovery, and video
recognition. Their vulnerability to input perturbation is also known. Aligning
with a view from software defect detection, this paper aims to develop a
coverage guided testing approach to systematically exploit the internal
behaviour of RNNs, with the expectation that such testing can detect defects
with high possibility. Technically, the long short term memory network (LSTM),
a major class of RNNs, is thoroughly studied. A family of three test metrics
are designed to quantify not only the values but also the temporal relations
(including both step-wise and bounded-length) exhibited when LSTM processing
inputs. A genetic algorithm is applied to efficiently generate test cases. The
test metrics and test case generation algorithm are implemented into a tool
TestRNN, which is then evaluated on a set of LSTM benchmarks. Experiments
confirm that TestRNN has advantages over the state-of-art tool DeepStellar and
attack-based defect detection methods, owing to its working with finer temporal
semantics and the consideration of the naturalness of input perturbation.
Furthermore, TestRNN enables meaningful information to be collected and
exhibited for users to understand the testing results, which is an important
step towards interpretable neural network testing.
</summary>
    <author>
      <name>Wei Huang</name>
    </author>
    <author>
      <name>Youcheng Sun</name>
    </author>
    <author>
      <name>Xingyu Zhao</name>
    </author>
    <author>
      <name>James Sharp</name>
    </author>
    <author>
      <name>Wenjie Ruan</name>
    </author>
    <author>
      <name>Jie Meng</name>
    </author>
    <author>
      <name>Xiaowei Huang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TR.2021.3080664</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TR.2021.3080664" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IEEE Transactions on Reliability</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.01952v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01952v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2207.00824v2</id>
    <updated>2022-07-05T09: 41: 02Z</updated>
    <published>2022-07-02T12: 53: 56Z</published>
    <title>Lane-GNN: Integrating GNN for Predicting Drivers' Lane Change Intention</title>
    <summary>  Nowadays, intelligent highway traffic network is playing an important role in
modern transportation infrastructures. A variable speed limit (VSL) system can
be facilitated in the highway traffic network to provide useful and dynamic
speed limit information for drivers to travel with enhanced safety. Such system
is usually designed with a steady advisory speed in mind so that traffic can
move smoothly when drivers follow the speed, rather than speeding up whenever
there is a gap and slowing down at congestion. However, little attention has
been given to the research of vehicles' behaviours when drivers left the road
network governed by a VSL system, which may largely involve unexpected
acceleration, deceleration and frequent lane changes, resulting in chaos for
the subsequent highway road users. In this paper, we focus on the detection of
traffic flow anomaly due to drivers' lane change intention on the highway
traffic networks after a VSL system. More specifically, we apply graph
modelling on the traffic flow data generated by a popular mobility simulator,
SUMO, at road segment levels. We then evaluate the performance of lane changing
detection using the proposed Lane-GNN scheme, an attention temporal graph
convolutional neural network, and compare its performance with a temporal
convolutional neural network (TCNN) as our baseline. Our experimental results
show that the proposed Lane-GNN can detect drivers' lane change intention
within 90 seconds with an accuracy of 99.42% under certain assumptions.
Finally, some interpretation methods are applied to the trained models with a
view to further illustrate our findings.
</summary>
    <author>
      <name>Hongde Wu</name>
    </author>
    <author>
      <name>Mingming Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The manuscript has been accepted by the 25th IEEE International
  Conference on Intelligent Transportation Systems (IEEE ITSC 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.00824v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.00824v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1907.12477v2</id>
    <updated>2019-09-23T17: 26: 35Z</updated>
    <published>2019-07-29T15: 27: 50Z</published>
    <title>Semantic RL with Action Grammars: Data-Efficient Learning of
  Hierarchical Task Abstractions</title>
    <summary>  Hierarchical Reinforcement Learning algorithms have successfully been applied
to temporal credit assignment problems with sparse reward signals. However,
state-of-the-art algorithms require manual specification of sub-task
structures, a sample inefficient exploration phase or lack semantic
interpretability. Humans, on the other hand, efficiently detect hierarchical
sub-structures induced by their surroundings. It has been argued that this
inference process universally applies to language, logical reasoning as well as
motor control. Therefore, we propose a cognitive-inspired Reinforcement
Learning architecture which uses grammar induction to identify sub-goal
policies. By treating an on-policy trajectory as a sentence sampled from the
policy-conditioned language of the environment, we identify hierarchical
constituents with the help of unsupervised grammatical inference. The resulting
set of temporal abstractions is called action grammar (Pastra &amp; Aloimonos,
                                    2012) and unifies symbolic and connectionist approaches to Reinforcement
Learning. It can be used to facilitate efficient imitation, transfer and online
learning.
</summary>
    <author>
      <name>Robert Tjarko Lange</name>
    </author>
    <author>
      <name>Aldo Faisal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages,
                                    8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.12477v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.12477v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2008.11170v1</id>
    <updated>2020-08-25T17: 04: 39Z</updated>
    <published>2020-08-25T17: 04: 39Z</published>
    <title>Boundary Uncertainty in a Single-Stage Temporal Action Localization
  Network</title>
    <summary>  In this paper, we address the problem of temporal action localization with a
single stage neural network. In the proposed architecture we model the boundary
predictions as uni-variate Gaussian distributions in order to model their
uncertainties, which is the first in this area to the best of our knowledge. We
use two uncertainty-aware boundary regression losses: first, the
Kullback-Leibler divergence between the ground truth location of the boundary
and the Gaussian modeling the prediction of the boundary and second, the
expectation of the $\ell_1$ loss under the same Gaussian. We show that with
both uncertainty modeling approaches improve the detection performance by more
than $1.5\%$ in mAP@tIoU=0.5 and that the proposed simple one-stage network
performs closely to more complex one and two stage networks.
</summary>
    <author>
      <name>Ting-Ting Xie</name>
    </author>
    <author>
      <name>Christos Tzelepis</name>
    </author>
    <author>
      <name>Ioannis Patras</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Tech report</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.11170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1905.10580v2</id>
    <updated>2020-01-29T18: 38: 16Z</updated>
    <published>2019-05-25T11: 45: 21Z</published>
    <title>HYPA: Efficient Detection of Path Anomalies in Time Series Data on
  Networks</title>
    <summary>  The unsupervised detection of anomalies in time series data has important
applications in user behavioral modeling, fraud detection, and cybersecurity.
Anomaly detection has, in fact, been extensively studied in categorical
sequences. However, we often have access to time series data that represent
paths through networks. Examples include transaction sequences in financial
networks, click streams of users in networks of cross-referenced documents, or
travel itineraries in transportation networks. To reliably detect anomalies, we
must account for the fact that such data contain a large number of independent
observations of paths constrained by a graph topology. Moreover, the
heterogeneity of real systems rules out frequency-based anomaly detection
techniques, which do not account for highly skewed edge and degree statistics.
To address this problem, we introduce HYPA, a novel framework for the
unsupervised detection of anomalies in large corpora of variable-length
temporal paths in a graph. HYPA provides an efficient analytical method to
detect paths with anomalous frequencies that result from nodes being traversed
in unexpected chronological order.
</summary>
    <author>
      <name>Timothy LaRock</name>
    </author>
    <author>
      <name>Vahan Nanumyan</name>
    </author>
    <author>
      <name>Ingo Scholtes</name>
    </author>
    <author>
      <name>Giona Casiraghi</name>
    </author>
    <author>
      <name>Tina Eliassi-Rad</name>
    </author>
    <author>
      <name>Frank Schweitzer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1137/1.9781611976236.52</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1137/1.9781611976236.52" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages with 8 figures and supplementary material. To appear at SIAM
  Data Mining (SDM 2020)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2020 SIAM International Conference on Data
  Mining (SDM) (2020) 460 - 468</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1905.10580v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.10580v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2301.13199v1</id>
    <updated>2023-01-30T18: 59: 51Z</updated>
    <published>2023-01-30T18: 59: 51Z</published>
    <title>Streaming Anomaly Detection</title>
    <summary>  Anomaly detection is critical for finding suspicious behavior in innumerable
systems. We need to detect anomalies in real-time, i.e. determine if an
incoming entity is anomalous or not, as soon as we receive it, to minimize the
effects of malicious activities and start recovery as soon as possible.
Therefore, online algorithms that can detect anomalies in a streaming manner
are essential.
  We first propose MIDAS which uses a count-min sketch to detect anomalous
edges in dynamic graphs in an online manner, using constant time and memory. We
then propose two variants, MIDAS-R which incorporates temporal and spatial
relations, and MIDAS-F which aims to filter away anomalous edges to prevent
them from negatively affecting the internal data structures.
  We then extend the count-min sketch to a Higher-Order sketch to capture
complex relations in graph data, and to reduce detecting suspicious dense
subgraph problem to finding a dense submatrix in constant time. Using this
sketch, we propose four streaming methods to detect edge and subgraph
anomalies.
  Next, we broaden the graph setting to multi-aspect data. We propose MStream
which detects explainable anomalies in multi-aspect data streams. We further
propose MStream-PCA, MStream-IB, and MStream-AE to incorporate correlation
between features.
  Finally, we consider multi-dimensional data streams with concept drift and
propose MemStream. MemStream leverages the power of a denoising autoencoder to
learn representations and a memory module to learn the dynamically changing
trend in data without the need for labels. We prove a theoretical bound on the
size of memory for effective drift handling. In addition, we allow quick
retraining when the arriving stream becomes sufficiently different from the
training data. Furthermore, MemStream makes use of two architecture design
choices to be robust to memory poisoning.
</summary>
    <author>
      <name>Siddharth Bhatia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Ph.D. Thesis,
                                    215 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.13199v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.13199v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1902.11236v2</id>
    <updated>2020-06-15T18: 43: 45Z</updated>
    <published>2019-02-28T17: 35: 43Z</published>
    <title>Time- and frequency-resolved covariance analysis for detection and
  characterization of seizures from intracraneal EEG recordings</title>
    <summary>  The amount of power in different frequency bands of the electroencephalogram
(EEG) carries information about the behavioral state of a subject. Hence,
neurologists treating epileptic patients monitor the temporal evolution of the
different bands. We propose a covariance-based method to detect and
characterize epileptic seizures operating on the band-filtered EEG signal. The
algorithm is unsupervised, and performs a principal component analysis of
intra-cranial EEG recordings, detecting transient fluctuations of the power in
each frequency band. Its simplicity makes it suitable for online
implementation. Good sampling of the non-ictal periods is required, while no
demands are imposed on the amount of data during ictal activity. We tested the
method with 32 seizures registered in 5 patients. The area below the resulting
receiver-operating characteristic curves was 87\% for the detection of seizures
and 91\% for the detection of recruited electrodes. To identify the
behaviorally relevant correlates of the physiological signal, we identified
transient changes in the variance of each band that were correlated with the
degree of loss of consciousness, the latter assessed by the so-called
Consciousness Seizure Scale, summarizing the performance of the subject in a
number of behavioral tests requested during seizures. We concluded that those
crisis with maximal impairment of consciousness tended to exhibit an increase
of variance approximately 40 seconds after seizure onset, with predominant
power in the theta and alpha bands, and reduced delta and beta activity.
</summary>
    <author>
      <name>Melisa Maidana Capitán</name>
    </author>
    <author>
      <name>Nuria Cámpora</name>
    </author>
    <author>
      <name>Claudio Sebastián</name>
    </author>
    <author>
      <name>Sigvard Silvia Kochen</name>
    </author>
    <author>
      <name>Inés Samengo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages,
                                    4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.11236v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.11236v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/astro-ph/9902096v1</id>
    <updated>1999-02-05T17: 00: 55Z</updated>
    <published>1999-02-05T17: 00: 55Z</published>
    <title>A Giant Outburst from SGR 1900+14 Observed with the BeppoSAX Gamma Ray
  Burst Monitor</title>
    <summary>  We report the detection by the Gamma Ray Burst Monitor onboard BeppoSAX of
the strongest and longest ever detected outburst from SGR 1900+14. Oscillations
are detectable with a period of ~5.16 s for the entire duration of the event
~300 s. The temporal analysis reveals also a remarkable periodic substructure:
after about 35 s from the event onset each 5.16-s pulse shows a pattern of four
subpulses and a dip, each separated by ~1 s. Significant spectral variation is
detected during the event and for each individual oscillation. The first and
most intense part of the outburst is quite hard, and similar to what previously
detected from the `March 5th event'. A hard non-thermal spectral component
persists for ~200 s. SGR 1900+14 was proposed to be a strongly magnetized
neutron star (B&gt;10^14 G) undergoing violent instabilities by internal
magnetic/crustal stresses. However, the onset of an apparent 1-s periodicity
within the 5.16-s pulsations and the observed spectral properties show a
complex behaviour that is not satisfactorily modelled yet.
</summary>
    <author>
      <name>M. Feroci</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IAS/CNR Frascati</arxiv:affiliation>
    </author>
    <author>
      <name>F. Frontera</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ITESRE/CNR Bologna</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Univ. Ferrara</arxiv:affiliation>
    </author>
    <author>
      <name>E. Costa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IAS/CNR Frascati</arxiv:affiliation>
    </author>
    <author>
      <name>L. Amati</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ITESRE/CNR Bologna</arxiv:affiliation>
    </author>
    <author>
      <name>M. Tavani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IFC/CNR Milano and CAL New York</arxiv:affiliation>
    </author>
    <author>
      <name>M. Rapisarda</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IAS/CNR Frascati</arxiv:affiliation>
    </author>
    <author>
      <name>M. Orlandini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ITESRE/CNR Bologna</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1086/311964</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1086/311964" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages,
                                    3 figures, TeX uses aasms4.sty; ApJ Letters in press</arxiv:comment>
    <link href="http://arxiv.org/abs/astro-ph/9902096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/astro-ph/9902096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1609.09300v1</id>
    <updated>2016-09-29T11: 38: 01Z</updated>
    <published>2016-09-29T11: 38: 01Z</published>
    <title>Combining BRITE and ground-based photometry for the Beta Cephei star Nu
  Eridani: impact on photometric pulsation mode identification and detection of
  several g modes</title>
    <summary>  We report a simultaneous ground and space-based photometric study of the Beta
Cephei star Nu Eridani. Half a year of observations have been obtained by four
of the five satellites constituting BRITE-Constellation, supplemented with
ground-based photoelectric photometry. We show that carefully combining the two
data sets virtually eliminates the aliasing problem that often hampers
time-series analyses. We detect 40 periodic signals intrinsic to the star in
the light curves. Despite a lower detection limit we do not recover all the
pressure and mixed modes previously reported in the literature, but we newly
detect six additional gravity modes. This behaviour is a consequence of
temporal changes in the pulsation amplitudes that we also detected for some of
the p modes. We point out that the dependence of theoretically predicted
pulsation amplitude on wavelength is steeper in visual passbands than those
observationally measured, to the extent that the three dominant pulsation modes
of Nu Eridani would be incorrectly identified using data in optical filters
only. We discuss possible reasons for this discrepancy.
</summary>
    <author>
      <name>G. Handler</name>
    </author>
    <author>
      <name>M. Rybicka</name>
    </author>
    <author>
      <name>A. Popowicz</name>
    </author>
    <author>
      <name>A. Pigulski</name>
    </author>
    <author>
      <name>R. Kuschnig</name>
    </author>
    <author>
      <name>E. Zoclonska</name>
    </author>
    <author>
      <name>A. F. J. Moffat</name>
    </author>
    <author>
      <name>W. W. Weiss</name>
    </author>
    <author>
      <name>C. C. Grant</name>
    </author>
    <author>
      <name>H. Pablo</name>
    </author>
    <author>
      <name>G. N. Whittaker</name>
    </author>
    <author>
      <name>S. M. Rucinski</name>
    </author>
    <author>
      <name>T. Ramiaramanantsoa</name>
    </author>
    <author>
      <name>K. Zwintz</name>
    </author>
    <author>
      <name>G. A. Wade</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/mnras/stw2518</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/mnras/stw2518" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages,
                                    5 figures,
                                    4 tables, accepted for publication in MNRAS</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2403.15170v1</id>
    <updated>2024-03-22T12: 46: 58Z</updated>
    <published>2024-03-22T12: 46: 58Z</published>
    <title>Exploring the Task-agnostic Trait of Self-supervised Learning in the
  Context of Detecting Mental Disorders</title>
    <summary>  Self-supervised learning (SSL) has been investigated to generate
task-agnostic representations across various domains. However, such
investigation has not been conducted for detecting multiple mental disorders.
The rationale behind the existence of a task-agnostic representation lies in
the overlapping symptoms among multiple mental disorders. Consequently, the
behavioural data collected for mental health assessment may carry a mixed bag
of attributes related to multiple disorders. Motivated by that, in this study,
we explore a task-agnostic representation derived through SSL in the context of
detecting major depressive disorder (MDD) and post-traumatic stress disorder
(PTSD) using audio and video data collected during interactive sessions. This
study employs SSL models trained by predicting multiple fixed targets or masked
frames. We propose a list of fixed targets to make the generated representation
more efficient for detecting MDD and PTSD. Furthermore, we modify the
hyper-parameters of the SSL encoder predicting fixed targets to generate global
representations that capture varying temporal contexts. Both these innovations
are noted to yield improved detection performances for considered mental
disorders and exhibit task-agnostic traits. In the context of the SSL model
predicting masked frames, the generated global representations are also noted
to exhibit task-agnostic traits.
</summary>
    <author>
      <name>Rohan Kumar Gupta</name>
    </author>
    <author>
      <name>Rohit Sinha</name>
    </author>
    <link href="http://arxiv.org/abs/2403.15170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.15170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2409.03024v2</id>
    <updated>2024-09-06T16: 55: 26Z</updated>
    <published>2024-09-04T18: 31: 24Z</published>
    <title>NUMOSIM: A Synthetic Mobility Dataset with Anomaly Detection Benchmarks</title>
    <summary>  Collecting real-world mobility data is challenging. It is often fraught with
privacy concerns, logistical difficulties, and inherent biases. Moreover,
accurately annotating anomalies in large-scale data is nearly impossible, as it
demands meticulous effort to distinguish subtle and complex patterns. These
challenges significantly impede progress in geospatial anomaly detection
research by restricting access to reliable data and complicating the rigorous
evaluation, comparison, and benchmarking of methodologies. To address these
limitations, we introduce a synthetic mobility dataset, NUMOSIM, that provides
a controlled, ethical, and diverse environment for benchmarking anomaly
detection techniques. NUMOSIM simulates a wide array of realistic mobility
scenarios, encompassing both typical and anomalous behaviours, generated
through advanced deep learning models trained on real mobility data. This
approach allows NUMOSIM to accurately replicate the complexities of real-world
movement patterns while strategically injecting anomalies to challenge and
evaluate detection algorithms based on how effectively they capture the
interplay between demographic, geospatial, and temporal factors. Our goal is to
advance geospatial mobility analysis by offering a realistic benchmark for
improving anomaly detection and mobility modeling techniques. To support this,
we provide open access to the NUMOSIM dataset, along with comprehensive
documentation, evaluation metrics, and benchmark results.
</summary>
    <author>
      <name>Chris Stanford</name>
    </author>
    <author>
      <name>Suman Adari</name>
    </author>
    <author>
      <name>Xishun Liao</name>
    </author>
    <author>
      <name>Yueshuai He</name>
    </author>
    <author>
      <name>Qinhua Jiang</name>
    </author>
    <author>
      <name>Chenchen Kuai</name>
    </author>
    <author>
      <name>Jiaqi Ma</name>
    </author>
    <author>
      <name>Emmanuel Tung</name>
    </author>
    <author>
      <name>Yinlong Qian</name>
    </author>
    <author>
      <name>Lingyi Zhao</name>
    </author>
    <author>
      <name>Zihao Zhou</name>
    </author>
    <author>
      <name>Zeeshan Rasheed</name>
    </author>
    <author>
      <name>Khurram Shafique</name>
    </author>
    <link href="http://arxiv.org/abs/2409.03024v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.03024v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2405.10828v1</id>
    <updated>2024-05-17T14: 48: 37Z</updated>
    <published>2024-05-17T14: 48: 37Z</published>
    <title>Analysis of Impulsive Interference in Digital Audio Broadcasting Systems
  in Electric Vehicles</title>
    <summary>  Recently, new types of interference in electric vehicles (EVs), such as
converters switching and/or battery chargers, have been found to degrade the
performance of wireless digital transmission systems. Measurements show that
such an interference is characterized by impulsive behavior and is widely
varying in time. This paper uses recorded data from our EV testbed to analyze
the impulsive interference in the digital audio broadcasting band. Moreover, we
use our analysis to obtain a corresponding interference model. In particular,
we studied the temporal characteristics of the interference and confirmed that
its amplitude indeed exhibits an impulsive behavior. Our results show that
impulsive events span successive received signal samples and thus indicate a
bursty nature. To this end, we performed a data-driven modification of a
well-established model for bursty impulsive interference, the Markov-Middleton
model, to produce synthetic noise realization. We investigate the optimal
symbol detector design based on the proposed model and show significant
performance gains compared to the conventional detector based on the additive
white Gaussian noise assumption.
</summary>
    <author>
      <name>Chin-Hung Chen</name>
    </author>
    <author>
      <name>Wen-Hung Huang</name>
    </author>
    <author>
      <name>Boris Karanov</name>
    </author>
    <author>
      <name>Alex Young</name>
    </author>
    <author>
      <name>Yan Wu</name>
    </author>
    <author>
      <name>Wim van Houtum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">44th Symposium on Information Theory and Signal Processing in the
  Benelux (SITB 2024), Delft, the Netherlands</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.10828v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.10828v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2502.08299v2</id>
    <updated>2025-02-13T09: 28: 41Z</updated>
    <published>2025-02-12T10: 59: 45Z</published>
    <title>When do they StOP?: A First Step Towards Automatically Identifying Team
  Communication in the Operating Room</title>
    <summary>  Purpose: Surgical performance depends not only on surgeons' technical skills
but also on team communication within and across the different professional
groups present during the operation. Therefore, automatically identifying team
communication in the OR is crucial for patient safety and advances in the
development of computer-assisted surgical workflow analysis and intra-operative
support systems. To take the first step, we propose a new task of detecting
communication briefings involving all OR team members, i.e. the team Time-out
and the StOP?-protocol, by localizing their start and end times in video
recordings of surgical operations. Methods: We generate an OR dataset of real
surgeries, called Team-OR, with more than one hundred hours of surgical videos
captured by the multi-view camera system in the OR. The dataset contains
temporal annotations of 33 Time-out and 22 StOP?-protocol activities in total.
We then propose a novel group activity detection approach, where we encode both
scene context and action features, and use an efficient neural network model to
output the results. Results: The experimental results on the Team-OR dataset
show that our approach outperforms existing state-of-the-art temporal action
detection approaches. It also demonstrates the lack of research on group
activities in the OR, proving the significance of our dataset. Conclusion: We
investigate the Team Time-Out and the StOP?-protocol in the OR, by presenting
the first OR dataset with temporal annotations of group activities protocols,
and introducing a novel group activity detection approach that outperforms
existing approaches. Code is available at
https: //github.com/CAMMA-public/Team-OR.
</summary>
    <author>
      <name>Keqi Chen</name>
    </author>
    <author>
      <name>Lilien Schewski</name>
    </author>
    <author>
      <name>Vinkle Srivastav</name>
    </author>
    <author>
      <name>Joël Lavanchy</name>
    </author>
    <author>
      <name>Didier Mutter</name>
    </author>
    <author>
      <name>Guido Beldi</name>
    </author>
    <author>
      <name>Sandra Keller</name>
    </author>
    <author>
      <name>Nicolas Padoy</name>
    </author>
    <link href="http://arxiv.org/abs/2502.08299v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.08299v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1903.05720v2</id>
    <updated>2019-07-07T07: 52: 59Z</updated>
    <published>2019-03-13T21: 29: 13Z</published>
    <title>Natural Language Interaction with Explainable AI Models</title>
    <summary>  This paper presents an explainable AI (XAI) system that provides explanations
for its predictions. The system consists of two key components -- namely, the
prediction And-Or graph (AOG) model for recognizing and localizing concepts of
interest in input data, and the XAI model for providing explanations to the
user about the AOG's predictions. In this work, we focus on the XAI model
specified to interact with the user in natural language, whereas the AOG's
predictions are considered given and represented by the corresponding parse
graphs (pg's) of the AOG. Our XAI model takes pg's as input and provides
answers to the user's questions using the following types of reasoning: direct
evidence (e.g., detection scores), part-based inference (e.g., detected parts
provide evidence for the concept asked), and other evidences from
spatio-temporal context (e.g., constraints from the spatio-temporal surround).
We identify several correlations between user's questions and the XAI answers
using Youtube Action dataset.
</summary>
    <author>
      <name>Arjun R Akula</name>
    </author>
    <author>
      <name>Sinisa Todorovic</name>
    </author>
    <author>
      <name>Joyce Y Chai</name>
    </author>
    <author>
      <name>Song-Chun Zhu</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2019 Workshop on Explainable AI</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1903.05720v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.05720v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2305.07812v2</id>
    <updated>2023-11-06T04: 36: 11Z</updated>
    <published>2023-05-13T01: 28: 28Z</published>
    <title>Lightweight Delivery Detection on Doorbell Cameras</title>
    <summary>  Despite recent advances in video-based action recognition and robust
spatio-temporal modeling, most of the proposed approaches rely on the abundance
of computational resources to afford running huge and computation-intensive
convolutional or transformer-based neural networks to obtain satisfactory
results. This limits the deployment of such models on edge devices with limited
power and computing resources. In this work we investigate an important smart
home application, video based delivery detection, and present a simple and
lightweight pipeline for this task that can run on resource-constrained
doorbell cameras. Our method relies on motion cues to generate a set of coarse
activity proposals followed by their classification with a mobile-friendly
3DCNN network. To train we design a novel semi-supervised attention module that
helps the network to learn robust spatio-temporal features and adopt an
evidence-based optimization objective that allows for quantifying the
uncertainty of predictions made by the network. Experimental results on our
curated delivery dataset shows the significant effectiveness of our pipeline
and highlights the benefits of our training phase novelties to achieve free and
considerable inference-time performance gains.
</summary>
    <author>
      <name>Pirazh Khorramshahi</name>
    </author>
    <author>
      <name>Zhe Wu</name>
    </author>
    <author>
      <name>Tianchen Wang</name>
    </author>
    <author>
      <name>Luke Deluccia</name>
    </author>
    <author>
      <name>Hongcheng Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the 2024 WACV conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.07812v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.07812v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2402.03417v1</id>
    <updated>2024-02-05T18: 53: 54Z</updated>
    <published>2024-02-05T18: 53: 54Z</published>
    <title>A Computer Vision Based Approach for Stalking Detection Using a
  CNN-LSTM-MLP Hybrid Fusion Model</title>
    <summary>  Criminal and suspicious activity detection has become a popular research
topic in recent years. The rapid growth of computer vision technologies has had
a crucial impact on solving this issue. However, physical stalking detection is
still a less explored area despite the evolution of modern technology.
Nowadays, stalking in public places has become a common occurrence with women
being the most affected. Stalking is a visible action that usually occurs
before any criminal activity begins as the stalker begins to follow, loiter,
and stare at the victim before committing any criminal activity such as
assault, kidnapping, rape, and so on. Therefore, it has become a necessity to
detect stalking as all of these criminal activities can be stopped in the first
place through stalking detection. In this research, we propose a novel deep
learning-based hybrid fusion model to detect potential stalkers from a single
video with a minimal number of frames. We extract multiple relevant features,
such as facial landmarks, head pose estimation, and relative distance, as
numerical values from video frames. This data is fed into a multilayer
perceptron (MLP) to perform a classification task between a stalking and a
non-stalking scenario. Simultaneously, the video frames are fed into a
combination of convolutional and LSTM models to extract the spatio-temporal
features. We use a fusion of these numerical and spatio-temporal features to
build a classifier to detect stalking incidents. Additionally, we introduce a
dataset consisting of stalking and non-stalking videos gathered from various
feature films and television series, which is also used to train the model. The
experimental results show the efficiency and dynamism of our proposed stalker
detection system, achieving 89.58% testing accuracy with a significant
improvement as compared to the state-of-the-art approaches.
</summary>
    <author>
      <name>Murad Hasan</name>
    </author>
    <author>
      <name>Shahriar Iqbal</name>
    </author>
    <author>
      <name>Md. Billal Hossain Faisal</name>
    </author>
    <author>
      <name>Md. Musnad Hossin Neloy</name>
    </author>
    <author>
      <name>Md. Tonmoy Kabir</name>
    </author>
    <author>
      <name>Md. Tanzim Reza</name>
    </author>
    <author>
      <name>Md. Golam Rabiul Alam</name>
    </author>
    <author>
      <name>Md Zia Uddin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review for publication in the PLOS ONE journal,
                                    17 pages,
                                    9
  figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.03417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.03417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1612.06950v2</id>
    <updated>2017-04-14T19: 20: 10Z</updated>
    <published>2016-12-21T02: 29: 53Z</published>
    <title>Temporal Tessellation: A Unified Approach for Video Analysis</title>
    <summary>  We present a general approach to video understanding, inspired by semantic
transfer techniques that have been successfully used for 2D image analysis. Our
method considers a video to be a 1D sequence of clips, each one associated with
its own semantics. The nature of these semantics -- natural language captions
or other labels -- depends on the task at hand. A test video is processed by
forming correspondences between its clips and the clips of reference videos
with known semantics, following which, reference semantics can be transferred
to the test video. We describe two matching methods, both designed to ensure
that (a) reference clips appear similar to test clips and (b), taken together,
the semantics of the selected reference clips is consistent and maintains
temporal coherence. We use our method for video captioning on the LSMDC'16
benchmark, video summarization on the SumMe and TVSum benchmarks, Temporal
Action Detection on the Thumos2014 benchmark, and sound prediction on the
Greatest Hits benchmark. Our method not only surpasses the state of the art, in
four out of five benchmarks, but importantly, it is the only single method we
know of that was successfully applied to such a diverse range of tasks.
</summary>
    <author>
      <name>Dotan Kaufman</name>
    </author>
    <author>
      <name>Gil Levi</name>
    </author>
    <author>
      <name>Tal Hassner</name>
    </author>
    <author>
      <name>Lior Wolf</name>
    </author>
    <link href="http://arxiv.org/abs/1612.06950v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06950v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1708.01246v1</id>
    <updated>2017-08-03T17: 51: 38Z</updated>
    <published>2017-08-03T17: 51: 38Z</published>
    <title>Unsupervised Representation Learning by Sorting Sequences</title>
    <summary>  We present an unsupervised representation learning approach using videos
without semantic labels. We leverage the temporal coherence as a supervisory
signal by formulating representation learning as a sequence sorting task. We
take temporally shuffled frames (i.e., in non-chronological order) as inputs
and train a convolutional neural network to sort the shuffled sequences.
Similar to comparison-based sorting algorithms, we propose to extract features
from all frame pairs and aggregate them to predict the correct order. As
sorting shuffled image sequence requires an understanding of the statistical
temporal structure of images, training with such a proxy task allows us to
learn rich and generalizable visual representation. We validate the
effectiveness of the learned representation using our method as pre-training on
high-level recognition problems. The experimental results show that our method
compares favorably against state-of-the-art methods on action recognition,
image classification and object detection tasks.
</summary>
    <author>
      <name>Hsin-Ying Lee</name>
    </author>
    <author>
      <name>Jia-Bin Huang</name>
    </author>
    <author>
      <name>Maneesh Singh</name>
    </author>
    <author>
      <name>Ming-Hsuan Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV 2017. Project page: http: //vllab1.ucmerced.edu/~hylee/OPN/</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.01246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.01246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2112.09828v2</id>
    <updated>2022-10-19T16: 58: 46Z</updated>
    <published>2021-12-18T03: 02: 11Z</published>
    <title>Exploiting Long-Term Dependencies for Generating Dynamic Scene Graphs</title>
    <summary>  Dynamic scene graph generation from a video is challenging due to the
temporal dynamics of the scene and the inherent temporal fluctuations of
predictions. We hypothesize that capturing long-term temporal dependencies is
the key to effective generation of dynamic scene graphs. We propose to learn
the long-term dependencies in a video by capturing the object-level consistency
and inter-object relationship dynamics over object-level long-term tracklets
using transformers. Experimental results demonstrate that our Dynamic Scene
Graph Detection Transformer (DSG-DETR) outperforms state-of-the-art methods by
a significant margin on the benchmark dataset Action Genome. Our ablation
studies validate the effectiveness of each component of the proposed approach.
The source code is available at https: //github.com/Shengyu-Feng/DSG-DETR.
</summary>
    <author>
      <name>Shengyu Feng</name>
    </author>
    <author>
      <name>Subarna Tripathi</name>
    </author>
    <author>
      <name>Hesham Mostafa</name>
    </author>
    <author>
      <name>Marcel Nassar</name>
    </author>
    <author>
      <name>Somdeb Majumdar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">WACV 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.09828v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.09828v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2308.13494v1</id>
    <updated>2023-08-25T17: 10: 12Z</updated>
    <published>2023-08-25T17: 10: 12Z</published>
    <title>Eventful Transformers: Leveraging Temporal Redundancy in Vision
  Transformers</title>
    <summary>  Vision Transformers achieve impressive accuracy across a range of visual
recognition tasks. Unfortunately, their accuracy frequently comes with high
computational costs. This is a particular issue in video recognition, where
models are often applied repeatedly across frames or temporal chunks. In this
work, we exploit temporal redundancy between subsequent inputs to reduce the
cost of Transformers for video processing. We describe a method for identifying
and re-processing only those tokens that have changed significantly over time.
Our proposed family of models, Eventful Transformers, can be converted from
existing Transformers (often without any re-training) and give adaptive control
over the compute cost at runtime. We evaluate our method on large-scale
datasets for video object detection (ImageNet VID) and action recognition
(EPIC-Kitchens 100). Our approach leads to significant computational savings
(on the order of 2-4x) with only minor reductions in accuracy.
</summary>
    <author>
      <name>Matthew Dutson</name>
    </author>
    <author>
      <name>Yin Li</name>
    </author>
    <author>
      <name>Mohit Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.13494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.13494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2407.15170v1</id>
    <updated>2024-07-21T14: 04: 21Z</updated>
    <published>2024-07-21T14: 04: 21Z</published>
    <title>Semi-Supervised Pipe Video Temporal Defect Interval Localization</title>
    <summary>  In sewer pipe Closed-Circuit Television (CCTV) inspection, accurate temporal
defect localization is essential for effective defect classification,
detection, segmentation and quantification. Industry standards typically do not
require time-interval annotations, even though they are more informative than
time-point annotations for defect localization, resulting in additional
annotation costs when fully supervised methods are used. Additionally,
differences in scene types and camera motion patterns between pipe inspections
and Temporal Action Localization (TAL) hinder the effective transfer of
point-supervised TAL methods. Therefore, this study introduces a
Semi-supervised multi-Prototype-based method incorporating visual Odometry for
enhanced attention guidance (PipeSPO). PipeSPO fully leverages unlabeled data
through unsupervised pretext tasks and utilizes time-point annotated data with
a weakly supervised multi-prototype-based method, relying on visual odometry
features to capture camera pose information. Experiments on real-world datasets
demonstrate that PipeSPO achieves 41.89% average precision across Intersection
over Union (IoU) thresholds of 0.1-0.7, improving by 8.14% over current
state-of-the-art methods.
</summary>
    <author>
      <name>Zhu Huang</name>
    </author>
    <author>
      <name>Gang Pan</name>
    </author>
    <author>
      <name>Chao Kang</name>
    </author>
    <author>
      <name>YaoZhi Lv</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/mice.13403</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/mice.13403" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages,
                                    3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.15170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.15170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1111.5053v2</id>
    <updated>2012-01-10T17: 56: 04Z</updated>
    <published>2011-11-21T23: 04: 36Z</published>
    <title>Multiscale dynamics of solar magnetic structures</title>
    <summary>  Multiscale topological complexity of solar magnetic field is among the
primary factors controlling energy release in the corona, including associated
processes in the photospheric and chromospheric boundaries. We present a new
approach for analyzing multiscale behavior of the photospheric magnetic flux
underlying this dynamics as depicted by a sequence of high-resolution solar
magnetograms. The approach involves two basic processing steps: (1)
identification of timing and location of magnetic flux origin and demise events
(as defined by DeForest et al.,
                                    2007) by tracking spatiotemporal evolution of
unipolar and bipolar photospheric regions, and (2) analysis of collective
behavior of the detected magnetic events using a generalized version of
Grassberger - Procaccia correlation integral algorithm. The scale-free nature
of the developed algorithms makes it possible to characterize the dynamics of
the photospheric network across a wide range of distances and relaxation times.
Three types of photospheric conditions are considered to test the method: a
quiet photosphere, a solar active region (NOAA 10365) in a quiescent
non-flaring state, and the same active region during a period of M-class
flares. The results obtained show (1) the presence of a topologically complex
asymmetrically fragmented magnetic network in the quiet photosphere driven by
meso- and supergranulation, (2) the formation of non-potential magnetic
structures with complex polarity separation lines inside the active region, and
(3) statistical signatures of canceling bipolar magnetic structures coinciding
with flaring activity in the active region. Each of these effects can represent
an unstable magnetic configuration acting as an energy source for coronal
dissipation and heating.
</summary>
    <author>
      <name>Vadim M. Uritsky</name>
    </author>
    <author>
      <name>Joseph M. Davila</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0004-637X/748/1/60</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0004-637X/748/1/60" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages,
                                    7 figures,
                                    1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.5053v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.5053v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1909.08809v1</id>
    <updated>2019-09-19T05: 43: 27Z</updated>
    <published>2019-09-19T05: 43: 27Z</published>
    <title>Topological portraits of multiscale coordination dynamics</title>
    <summary>  Living systems exhibit complex yet organized behavior on multiple
spatiotemporal scales. To investigate the nature of multiscale coordination in
living systems, one needs a meaningful and systematic way to quantify the
complex dynamics, a challenge in both theoretical and empirical realms. The
present work shows how integrating approaches from computational algebraic
topology and dynamical systems may help us meet this challenge. In particular,
we focus on the application of multiscale topological analysis to coordinated
rhythmic processes. First, theoretical arguments are introduced as to why
certain topological features and their scale-dependency are highly relevant to
understanding complex collective dynamics. Second, we propose a method to
capture such dynamically relevant topological information using persistent
homology, which allows us to effectively construct a multiscale topological
portrait of rhythmic coordination. Finally, the method is put to test in
detecting transitions in real data from an experiment of rhythmic coordination
in ensembles of interacting humans. The recurrence plots of topological
portraits highlight collective transitions in coordination patterns that were
elusive to more traditional methods. This sensitivity to collective transitions
would be lost if the behavioral dynamics of individuals were treated as
separate degrees of freedom instead of constituents of the topology that they
collectively forge. Such multiscale topological portraits highlight collective
aspects of coordination patterns that are irreducible to properties of
individual parts. The present work demonstrates how the analysis of multiscale
coordination dynamics can benefit from topological methods, thereby paving the
way for further systematic quantification of complex, high-dimensional dynamics
in living systems.
</summary>
    <author>
      <name>Mengsen Zhang</name>
    </author>
    <author>
      <name>William D. Kalies</name>
    </author>
    <author>
      <name>J. A. Scott Kelso</name>
    </author>
    <author>
      <name>Emmanuelle Tognoli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jneumeth.2020.108672</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jneumeth.2020.108672" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.08809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.08809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2404.12583v1</id>
    <updated>2024-04-19T02: 25: 08Z</updated>
    <published>2024-04-19T02: 25: 08Z</published>
    <title>Analyzing whale calling through Hawkes process modeling</title>
    <summary>  Sound is assumed to be the primary modality of communication among marine
mammal species. Analyzing acoustic recordings helps to understand the function
of the acoustic signals as well as the possible impact of anthropogenic noise
on acoustic behavior. Motivated by a dataset from a network of hydrophones in
Cape Cod Bay, Massachusetts, utilizing automatically detected calls in
recordings, we study the communication process of the endangered North Atlantic
right whale. For right whales an "up-call" is known as a contact call, and
ensuing counter-calling between individuals is presumed to facilitate group
cohesion. We present novel spatiotemporal excitement modeling consisting of a
background process and a counter-call process. The background process intensity
incorporates the influences of diel patterns and ambient noise on occurrence.
The counter-call intensity captures potential excitement, that calling elicits
calling behavior. Call incidence is found to be clustered in space and time; a
call seems to excite more calls nearer to it in time and space. We find
evidence that whales make more calls during twilight hours, respond to other
whales nearby, and are likely to remain quiet in the presence of increased
ambient noise.
</summary>
    <author>
      <name>Bokgyeong Kang</name>
    </author>
    <author>
      <name>Erin M. Schliep</name>
    </author>
    <author>
      <name>Alan E. Gelfand</name>
    </author>
    <author>
      <name>Tina M. Yack</name>
    </author>
    <author>
      <name>Christopher W. Clark</name>
    </author>
    <author>
      <name>Robert S. Schick</name>
    </author>
    <link href="http://arxiv.org/abs/2404.12583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.12583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1712.04538v1</id>
    <updated>2017-12-10T22: 28: 17Z</updated>
    <published>2017-12-10T22: 28: 17Z</published>
    <title>Forecasting Extreme Events in the Complex Dynamics of a Semiconductor
  Laser with Feedback</title>
    <summary>  Complex systems performing spiking dynamics are widespread in Nature. They
cover from earthquakes, to neurons, variable stars, social networks, or stock
markets. Understanding and characterizing their dynamics is relevant in order
to detect transitions, or to predict unwanted extreme events. Here we study the
output intensity of a semiconductor laser with feedback, in a regime where it
develops a complex spiking behavior, under an ordinal patterns analysis. We
unveil that the complex dynamics presents two competing behaviors that can be
distinguished with a thresholding method, and we use temporal correlations to
forecast the extreme events, and transitions between dynamics.
</summary>
    <author>
      <name>Meritxell Colet</name>
    </author>
    <author>
      <name>Andrés Aragoneses</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages,
                                    4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.04538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="37N20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.06938v1</id>
    <updated>2025-03-10T05: 35: 56Z</updated>
    <published>2025-03-10T05: 35: 56Z</published>
    <title>Modeling Human Skeleton Joint Dynamics for Fall Detection</title>
    <summary>  The increasing pace of population aging calls for better care and support
systems. Falling is a frequent and critical problem for elderly people causing
serious long-term health issues. Fall detection from video streams is not an
attractive option for real-life applications due to privacy issues. Existing
methods try to resolve this issue by using very low-resolution cameras or video
encryption. However, privacy cannot be ensured completely with such approaches.
Key points on the body, such as skeleton joints, can convey significant
information about motion dynamics and successive posture changes which are
crucial for fall detection. Skeleton joints have been explored for feature
extraction but with image recognition models that ignore joint dependency
across frames which is important for the classification of actions. Moreover,
existing models are over-parameterized or evaluated on small datasets with very
few activity classes. We propose an efficient graph convolution network model
that exploits spatio-temporal joint dependencies and dynamics of human skeleton
joints for accurate fall detection. Our method leverages dynamic representation
with robust concurrent spatio-temporal characteristics of skeleton joints. We
performed extensive experiments on three large-scale datasets. With a
significantly smaller model size than most existing methods, our proposed
method achieves state-of-the-art results on the large scale NTU datasets.
</summary>
    <author>
      <name>Sania Zahan</name>
    </author>
    <author>
      <name>Ghulam Mubashar Hassan</name>
    </author>
    <author>
      <name>Ajmal Mian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in 2021 Digital Image Computing: Techniques and
  Applications (DICTA)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Digital Image Computing: Techniques and Applications (DICTA), Gold
  Coast, Australia,
                                    2021, pp. 01-07</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2503.06938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.06938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1504.03643v1</id>
    <updated>2015-04-14T18: 01: 32Z</updated>
    <published>2015-04-14T18: 01: 32Z</published>
    <title>Inferring Unusual Crowd Events From Mobile Phone Call Detail Records</title>
    <summary>  The pervasiveness and availability of mobile phone data offer the opportunity
of discovering usable knowledge about crowd behaviors in urban environments.
Cities can leverage such knowledge in order to provide better services (e.g.,
public transport planning, optimized resource allocation) and safer cities.
Call Detail Record (CDR) data represents a practical data source to detect and
monitor unusual events considering the high level of mobile phone penetration,
compared with GPS equipped and open devices. In this paper, we provide a
methodology that is able to detect unusual events from CDR data that typically
has low accuracy in terms of space and time resolution. Moreover, we introduce
a concept of unusual event that involves a large amount of people who expose an
unusual mobility behavior. Our careful consideration of the issues that come
from coarse-grained CDR data ultimately leads to a completely general framework
that can detect unusual crowd events from CDR data effectively and efficiently.
Through extensive experiments on real-world CDR data for a large city in
Africa, we demonstrate that our method can detect unusual events with 16%
higher recall and over 10 times higher precision, compared to state-of-the-art
methods. We implement a visual analytics prototype system to help end users
analyze detected unusual crowd events to best suit different application
scenarios. To the best of our knowledge, this is the first work on the
detection of unusual events from CDR data with considerations of its temporal
and spatial sparseness and distinction between user unusual activities and
daily routines.
</summary>
    <author>
      <name>Yuxiao Dong</name>
    </author>
    <author>
      <name>Fabio Pinelli</name>
    </author>
    <author>
      <name>Yiannis Gkoufas</name>
    </author>
    <author>
      <name>Zubair Nabi</name>
    </author>
    <author>
      <name>Francesco Calabrese</name>
    </author>
    <author>
      <name>Nitesh V. Chawla</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages,
                                    6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.03643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1507.01272v1</id>
    <updated>2015-07-05T20: 53: 26Z</updated>
    <published>2015-07-05T20: 53: 26Z</published>
    <title>VEWS: A Wikipedia Vandal Early Warning System</title>
    <summary>  We study the problem of detecting vandals on Wikipedia before any human or
known vandalism detection system reports flagging potential vandals so that
such users can be presented early to Wikipedia administrators. We leverage
multiple classical ML approaches, but develop 3 novel sets of features. Our
Wikipedia Vandal Behavior (WVB) approach uses a novel set of user editing
patterns as features to classify some users as vandals. Our Wikipedia
Transition Probability Matrix (WTPM) approach uses a set of features derived
from a transition probability matrix and then reduces it via a neural net
auto-encoder to classify some users as vandals. The VEWS approach merges the
previous two approaches. Without using any information (e.g. reverts) provided
by other users, these algorithms each have over 85% classification accuracy.
Moreover, when temporal recency is considered, accuracy goes to almost 90%. We
carry out detailed experiments on a new data set we have created consisting of
about 33K Wikipedia users (including both a black list and a white list of
editors) and containing 770K edits. We describe specific behaviors that
distinguish between vandals and non-vandals. We show that VEWS beats ClueBot NG
and STiki, the best known algorithms today for vandalism detection. Moreover,
VEWS detects far more vandals than ClueBot NG and on average, detects them 2.39
edits before ClueBot NG when both detect the vandal. However, we show that the
combination of VEWS and ClueBot NG can give a fully automated vandal early
warning system with even higher accuracy.
</summary>
    <author>
      <name>Srijan Kumar</name>
    </author>
    <author>
      <name>Francesca Spezzano</name>
    </author>
    <author>
      <name>V. S. Subrahmanian</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2783258.2783367</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2783258.2783367" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceedings of the 21st ACM SIGKDD Conference of
  Knowledge Discovery and Data Mining (KDD 2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.01272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.01272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2203.14925v1</id>
    <updated>2022-03-28T17: 26: 44Z</updated>
    <published>2022-03-28T17: 26: 44Z</published>
    <title>Temporal Cascade Model for Analyzing Spread in Evolving Networks with
  Disease Monitoring Applications</title>
    <summary>  Current approaches for modeling propagation in networks (e.g., spread of
disease) are unable to adequately capture temporal properties of the data such
as order and duration of evolving connections or dynamic likelihoods of
propagation along these connections. Temporal models in evolving networks are
crucial in many applications that need to analyze dynamic spread. For example,
a disease-spreading virus has varying transmissibility based on interactions
between individuals occurring over time with different frequency, proximity,
and venue population density. To capture such behaviors, we first develop the
Temporal Independent Cascade (T-IC) model and propose a novel spread function,
that we prove to be submodular, with a hypergraph-based sampling strategy that
efficiently utilizes dynamic propagation probabilities. We then introduce the
notion of 'reverse spread' using the proposed T-IC processes, and develop
solutions to identify both sentinel/detector nodes and highly susceptible
nodes. The proven guarantees of approximation quality enable scalable analysis
of highly granular temporal networks. Extensive experimental results on a
variety of real-world datasets show that the proposed approach significantly
outperforms the alternatives in modeling both if and how spread occurs, by
considering evolving network topology as well as granular contact/interaction
information. Our approach has numerous applications, including its utility for
the vital challenge of monitoring disease spread. Utilizing the proposed
methods and T-IC, we analyze the impact of various intervention strategies over
real spatio-temporal contact networks. Our approach is shown also to be highly
effective in quantifying the importance of superspreaders, designing targeted
restrictions for controlling spread, and backward contact tracing.
</summary>
    <author>
      <name>Aparajita Haldar</name>
    </author>
    <author>
      <name>Shuang Wang</name>
    </author>
    <author>
      <name>Gunduz Demirci</name>
    </author>
    <author>
      <name>Joe Oakley</name>
    </author>
    <author>
      <name>Hakan Ferhatosmanoglu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ACM Transactions on Spatial Algorithms and Systems
  (TSAS) Journal, September 2022. For code and data, see
  https: //github.com/publiccoderepo/T-IC-model</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.14925v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.14925v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2209.01164v3</id>
    <updated>2022-09-07T02: 16: 35Z</updated>
    <published>2022-09-02T16: 49: 06Z</published>
    <title>In Situ 3D Spatiotemporal Measurement of Soluble Biomarkers in Organoid
  Culture</title>
    <summary>  Advanced cell culture techniques such as 3D bio-printing and hydrogel-based
cell embedding techniques harbor many new and exciting opportunities to study
cells in environments that closely recapitulate in-vivo conditions. Researchers
often study these environments using fluorescence microscopy to visualize the
protein association with objects such as cells within the 3D environment, yet
quantification of concentration profiles in the microenvironment has remained
elusive. Here, we present a method to continuously measure the time-dependent
concentration gradient of various biomarkers within a 3D cell culture assay
using bead-based immunoassays to sequester and concentrate the fluorescence
intensity of these tagged proteins. This assay allows for near real-time in
situ biomarker detection and enables spatiotemporal quantification of biomarker
concentration. Snapshots of concentration profiles can be taken, or time series
analysis can be performed enabling time-varying biomarker production
estimation. Example assays utilize an osteosacroma tumoroid as a case study for
a quantitative single-plexed gel encapsulated assay, and a qualitative
multi-plexed 3D bioprinted assay. In both cases, a time-varying cytokine
concentration gradient is measured. An estimation for the production rate of
the IL-8 cytokine per second per osteosarcoma cell results from fitting an
analytical function for continuous point source diffusion to the measured
concentration gradient and reveals that each cell produces approximately two
IL-8 cytokines per second. Proper calibration and use of this assay is
exhaustively explored for the case of diffusion-limited Langmuir kinetics of a
spherical adsorber.
</summary>
    <author>
      <name>Alexander J McGhee</name>
    </author>
    <author>
      <name>Eric O McGhee</name>
    </author>
    <author>
      <name>Jack E Famiglietti</name>
    </author>
    <author>
      <name>W. Gregory Sawyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages text with 3 figures,
                                    3 pages appendix,
                                    2 pages references,
  manuscript submitted to in vitro models</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.01164v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.01164v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.CB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2409.20503v2</id>
    <updated>2025-03-11T01: 55: 49Z</updated>
    <published>2024-09-30T17: 03: 13Z</published>
    <title>What Information Contributes to Log-based Anomaly Detection? Insights
  from a Configurable Transformer-Based Approach</title>
    <summary>  Log data are generated from logging statements in the source code, providing
insights into the execution processes of software applications and systems.
State-of-the-art log-based anomaly detection approaches typically leverage deep
learning models to capture the semantic or sequential information in the log
data and detect anomalous runtime behaviors. However, the impacts of these
different types of information are not clear. In addition, most existing
approaches ignore the timestamps in log data, which can potentially provide
fine-grained sequential and temporal information. In this work, we propose a
configurable Transformer-based anomaly detection model that can capture the
semantic, sequential, and temporal information in the log data and allows us to
configure the different types of information as the model's features.
Additionally, we train and evaluate the proposed model using log sequences of
different lengths, thus overcoming the constraint of existing methods that rely
on fixed-length or time-windowed log sequences as inputs. With the proposed
model, we conduct a series of experiments with different combinations of input
features to evaluate the roles of different types of information in anomaly
detection. The model can attain competitive and consistently stable performance
compared to the baselines when presented with log sequences of varying lengths.
The results indicate that the event occurrence information plays a key role in
identifying anomalies, while the impact of the sequential and temporal
information is not significant for anomaly detection on the studied public
datasets. On the other hand, the findings also reveal the simplicity of the
studied public datasets and highlight the importance of constructing new
datasets that contain different types of anomalies to better evaluate the
performance of anomaly detection models.
</summary>
    <author>
      <name>Xingfang Wu</name>
    </author>
    <author>
      <name>Heng Li</name>
    </author>
    <author>
      <name>Foutse Khomh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.20503v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.20503v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/q-bio/0602003v1</id>
    <updated>2006-02-03T00: 26: 35Z</updated>
    <published>2006-02-03T00: 26: 35Z</published>
    <title>The Bacterial Chemotactic Response Reflects a Compromise Between
  Transient and Steady State Behavior</title>
    <summary>  Swimming bacteria detect chemical gradients by performing temporal
comparisons of recent measurements of chemical concentration. These comparisons
are described quantitatively by the chemotactic response function, which we
expect to optimize chemotactic behavioral performance. We identify two
independent chemotactic performance criteria: in the short run, a favorable
response function should move bacteria up chemoattractant gradients, while in
the long run, bacteria should aggregate at peaks of chemoattractant
concentration. Surprisingly, these two criteria conflict, so that when one
performance criterion is most favorable, the other is unfavorable. Since both
types of behavior are biologically relevant, we include both behaviors in a
composite optimization that yields a response function that closely resembles
experimental measurements. Our work suggests that the bacterial chemotactic
response function can be derived from simple behavioral considerations, and
sheds light on how the response function contributes to chemotactic
performance.
</summary>
    <author>
      <name>Damon A. Clark</name>
    </author>
    <author>
      <name>Lars C. Grant</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1073/pnas.0407659102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1073/pnas.0407659102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages,
                                    5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. Natl. Acad. Sci. U.S.A.,
                                    102, (26): 9150-9155 (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/q-bio/0602003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/q-bio/0602003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.CB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.CB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2401.03522v2</id>
    <updated>2024-04-15T07: 59: 03Z</updated>
    <published>2024-01-07T15: 47: 19Z</published>
    <title>Text-Driven Traffic Anomaly Detection with Temporal High-Frequency
  Modeling in Driving Videos</title>
    <summary>  Traffic anomaly detection (TAD) in driving videos is critical for ensuring
the safety of autonomous driving and advanced driver assistance systems.
Previous single-stage TAD methods primarily rely on frame prediction, making
them vulnerable to interference from dynamic backgrounds induced by the rapid
movement of the dashboard camera. While two-stage TAD methods appear to be a
natural solution to mitigate such interference by pre-extracting
background-independent features (such as bounding boxes and optical flow) using
perceptual algorithms, they are susceptible to the performance of first-stage
perceptual algorithms and may result in error propagation. In this paper, we
introduce TTHF, a novel single-stage method aligning video clips with text
prompts, offering a new perspective on traffic anomaly detection. Unlike
previous approaches, the supervised signal of our method is derived from
languages rather than orthogonal one-hot vectors, providing a more
comprehensive representation. Further, concerning visual representation, we
propose to model the high frequency of driving videos in the temporal domain.
This modeling captures the dynamic changes of driving scenes, enhances the
perception of driving behavior, and significantly improves the detection of
traffic anomalies. In addition, to better perceive various types of traffic
anomalies, we carefully design an attentive anomaly focusing mechanism that
visually and linguistically guides the model to adaptively focus on the visual
context of interest, thereby facilitating the detection of traffic anomalies.
It is shown that our proposed TTHF achieves promising performance,
outperforming state-of-the-art competitors by +5.4% AUC on the DoTA dataset and
achieving high generalization on the DADA dataset.
</summary>
    <author>
      <name>Rongqin Liang</name>
    </author>
    <author>
      <name>Yuanman Li</name>
    </author>
    <author>
      <name>Jiantao Zhou</name>
    </author>
    <author>
      <name>Xia Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages,
                                    7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.03522v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.03522v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/0707.2599v1</id>
    <updated>2007-07-17T20: 35: 15Z</updated>
    <published>2007-07-17T20: 35: 15Z</published>
    <title>Extreme photo-polarimetric behaviour of the blazar AO 0235+164</title>
    <summary>  We present optical photo-polarimetric observations with high temporal
resolution of the blazar AO 0235+164. Our data, the first to test the
photo-polarimetric behaviour of this object at very short time-scales, show
significant micro-variability in total flux, colour index, linear polarization
degree, and position angle. Strong inter-night variations are also detected for
these parameters. Although no correlation between colour index and total flux
was found, our data seem to support the general bluer-when-brighter trend
already known for this object. The polarization degree, in turn, shows no
correlation with total flux, but a clear trend in the sense that colour index
is redder (the spectrum is softer) when the measured polarization is higher.
</summary>
    <author>
      <name>Sergio A. Cellone</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Facultad de Ciencias Astronomicas y Geofisicas, Universidad Nacional de La Plata, Argentina</arxiv:affiliation>
    </author>
    <author>
      <name>Gustavo E. Romero</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Facultad de Ciencias Astronomicas y Geofisicas, Universidad Nacional de La Plata, Argentina</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inst. Argentino de Radioastronomia</arxiv:affiliation>
    </author>
    <author>
      <name>Jorge A. Combi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inst. Argentino de Radioastronomia</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universidad de Jaen, Spain</arxiv:affiliation>
    </author>
    <author>
      <name>Josep Marti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universidad de Jaen, Spain</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/j.1745-3933.2007.00366.x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/j.1745-3933.2007.00366.x" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by MNRAS (Letters),
                                    5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">MNRAS (Letters) 381 (2007) L60-L64.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0707.2599v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.2599v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1902.03146v1</id>
    <updated>2019-02-08T15: 27: 15Z</updated>
    <published>2019-02-08T15: 27: 15Z</published>
    <title>Spin dynamics of a millisecond pulsar orbiting closely around a massive
  black hole</title>
    <summary>  We investigate the spin dynamics of a millisecond pulsar (MSP) in a tightly
bounded orbit around a massive black hole. These binaries are progenitors of
the extreme-mass-ratio-inspirals (EMRIs) and intermediate-mass-ratio-inspirals
(IMRIs) gravitational wave events. The Mathisson-Papapetrou-Dixon (MPD)
formulation is used to determine the orbital motion and spin modulation and
evolution. We show that the MSP will not be confined in a planar Keplerian
orbit and its spin will exhibit precession and nutation induced by spin-orbit
coupling and spin-curvature interaction. These spin and orbital behaviours will
manifest observationally in the temporal variations in the MSP's pulsed
emission and, with certain geometries, in the self-occultation of the pulsar's
emitting poles. Radio pulsar timing observations will be able to detect such
signatures. These extreme-mass-ratio binaries (EMRBs) and
intermediate-mass-ratio binaries (IMRBs) are also strong gravitational wave
sources. Combining radio pulsar timing and gravitational wave observations will
allow us to determine the dynamics of these systems in high precision and hence
the subtle behaviours of spinning masses in strong gravity.
</summary>
    <author>
      <name>Kaye Jiale Li</name>
    </author>
    <author>
      <name>Kinwah Wu</name>
    </author>
    <author>
      <name>Dinesh Singh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/mnras/stz389</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/mnras/stz389" rel="related"/>
    <link href="http://arxiv.org/abs/1902.03146v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03146v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2010.15586v1</id>
    <updated>2020-10-23T16: 14: 25Z</updated>
    <published>2020-10-23T16: 14: 25Z</published>
    <title>Event-Driven Learning of Systematic Behaviours in Stock Markets</title>
    <summary>  It is reported that financial news, especially financial events expressed in
news, provide information to investors' long/short decisions and influence the
movements of stock markets. Motivated by this, we leverage financial event
streams to train a classification neural network that detects latent
event-stock linkages and stock markets' systematic behaviours in the U.S. stock
market. Our proposed pipeline includes (1) a combined event extraction method
that utilizes Open Information Extraction and neural co-reference resolution,
(2) a BERT/ALBERT enhanced representation of events, and (3) an extended
hierarchical attention network that includes attentions on event, news and
temporal levels. Our pipeline achieves significantly better accuracies and
higher simulated annualized returns than state-of-the-art models when being
applied to predicting Standard\&amp;Poor 500, Dow Jones, Nasdaq indices and 10
individual stocks.
</summary>
    <author>
      <name>Xianchao Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages,
                                    3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.15586v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.15586v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2303.14227v1</id>
    <updated>2023-03-24T18: 47: 44Z</updated>
    <published>2023-03-24T18: 47: 44Z</published>
    <title>Causality Detection for Efficient Multi-Agent Reinforcement Learning</title>
    <summary>  When learning a task as a team, some agents in Multi-Agent Reinforcement
Learning (MARL) may fail to understand their true impact in the performance of
the team. Such agents end up learning sub-optimal policies, demonstrating
undesired lazy behaviours. To investigate this problem, we start by formalising
the use of temporal causality applied to MARL problems. We then show how
causality can be used to penalise such lazy agents and improve their
behaviours. By understanding how their local observations are causally related
to the team reward, each agent in the team can adjust their individual credit
based on whether they helped to cause the reward or not. We show empirically
that using causality estimations in MARL improves not only the holistic
performance of the team, but also the individual capabilities of each agent. We
observe that the improvements are consistent in a set of different
environments.
</summary>
    <author>
      <name>Rafael Pina</name>
    </author>
    <author>
      <name>Varuna De Silva</name>
    </author>
    <author>
      <name>Corentin Artaud</name>
    </author>
    <link href="http://arxiv.org/abs/2303.14227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.14227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2402.05679v1</id>
    <updated>2024-02-08T13: 55: 52Z</updated>
    <published>2024-02-08T13: 55: 52Z</published>
    <title>Heterogeneous drivers of overnight and same-day visits</title>
    <summary>  This paper aims to explore the factors stimulating different tourism
behaviours, with specific reference to same-day visits and overnight stays. To
this aim, we employ mobile network data referred to the area of Lombardy. The
paper highlights that larger availability of tourism accommodations, cultural
and natural endowments are relevant factors explaining overnight stays.
Conversely, temporary entertainment and transportation facilities increase
municipalities attractiveness for same-day visits. The results also highlight a
trade-off in the capability of municipalities of being attractive in connection
to both the tourism behaviours, with higher overnight stays in areas with more
limited same-day visits. Mobile data offer a spatial and temporal granularity
allowing to detect relevant patterns and support the design of tourism
precision policies.
</summary>
    <author>
      <name>Francesco Scotti</name>
    </author>
    <author>
      <name>Andrea Flori</name>
    </author>
    <author>
      <name>Piercesare Secchi</name>
    </author>
    <author>
      <name>Marika Arena</name>
    </author>
    <author>
      <name>Giovanni Azzone</name>
    </author>
    <link href="http://arxiv.org/abs/2402.05679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.05679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2503.13567v1</id>
    <updated>2025-03-17T09: 46: 06Z</updated>
    <published>2025-03-17T09: 46: 06Z</published>
    <title>Graph Discovery and Source Detection in Temporal Graphs</title>
    <summary>  Researchers, policy makers, and engineers need to make sense of data on
spreading processes as diverse as viral infections, water contamination, and
misinformation in social networks. Classical questions include predicting
infection behavior in a given network or deducing the structure of a network
from infection data. We study two central problems in this area. In graph
discovery, we aim to fully reconstruct the structure of a graph from infection
data. In source detection, we observe a limited subset of the infections and
aim to deduce the source of the infection chain. These questions have received
considerable attention and have been analyzed in many settings (e.g., under
different models of spreading processes), yet all previous work shares the
assumption that the network has the same structure at every point in time. For
example, if we consider how a disease spreads, it is unrealistic to assume that
two people can either never or always infect each other, rather such an
infection is possible precisely when they meet. Temporal graphs, in which
connections change over time, have recently been used as a more realistic graph
model to study infections. Despite this recent attention, we are the first to
study graph discovery or source detection in temporal graphs.
  We propose models for temporal graph discovery and source detection that are
consistent with previous work on static graphs and extend it to embrace the
stronger expressiveness of temporal graphs. For this, we employ the standard
susceptible-infected-resistant model of spreading processes, which is
particularly often used to study diseases. We provide algorithms, lower bounds,
and some experimental evaluation.
</summary>
    <author>
      <name>Ben Bals</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is my Master's thesis submitted at the Digital Engineering
  Faculty of the University of Potsdam on September 30, 2024. It has been
  turned into two individual submissions at arXiv: 2412.10881 and
  arXiv: 2412.10877</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.13567v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.13567v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2208.03696v2</id>
    <updated>2023-01-26T09: 36: 44Z</updated>
    <published>2022-08-07T11: 01: 46Z</published>
    <title>Quantum Field Theory based Quantum Information: Measurements and
  Correlations</title>
    <summary>  This is the first in a series of papers aiming to develop a relativistic
quantum information theory in terms of unequal-time correlation functions in
quantum field theory. In this work, we highlight two formalisms which together
can provide a useful theoretical platform suitable for further developments: 1)
Quantum field measurements using the Quantum Temporal Probabilities (QTP)
method; 2) Closed-Time-Path (CTP) formalism for causal time evolutions. QTP
incorporates the detector into the quantum description, while emphasising that
the records of measurement are macroscopic, and they can be expressed in terms
of classical spacetime coordinates. We first present a new, elementary
derivation of the QTP formulas for the probabilities of n measurement events.
We then demonstrate the relation of QTP with the Closed-Time-Path formalism, by
writing an explicit formula that relates the associated generating functionals.
We exploit the path integral representation of the CTP formalism, in order to
express the measured probabilities in terms of path integrals. After this, we
provide some simple applications of the QTP formalism. In particular, we show
how Unruh-DeWitt detector models and Glauber's photodetection theory appear as
limiting cases . Finally, with quantum correlation being the pivotal notion in
relativistic quantum information and measurements, we highlight the role played
by the CTP two-particle irreducible effective action which enables one to tap
into the resources of non-equilibrium quantum field theory for our stated
purpose.
</summary>
    <author>
      <name>Charis Anastopoulos</name>
    </author>
    <author>
      <name>Bei-Lok Hu</name>
    </author>
    <author>
      <name>Konstantina Savvidou</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.aop.2023.169239</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.aop.2023.169239" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages. Arguments expanded and clarified, references and an
  appendix added, to appear in Ann. Phys</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.03696v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.03696v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-th" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2206.04668v1</id>
    <updated>2022-06-09T17: 59: 44Z</updated>
    <published>2022-06-09T17: 59: 44Z</published>
    <title>GateHUB: Gated History Unit with Background Suppression for Online
  Action Detection</title>
    <summary>  Online action detection is the task of predicting the action as soon as it
happens in a streaming video. A major challenge is that the model does not have
access to the future and has to solely rely on the history, i.e., the frames
observed so far, to make predictions. It is therefore important to accentuate
parts of the history that are more informative to the prediction of the current
frame. We present GateHUB, Gated History Unit with Background Suppression, that
comprises a novel position-guided gated cross-attention mechanism to enhance or
suppress parts of the history as per how informative they are for current frame
prediction. GateHUB further proposes Future-augmented History (FaH) to make
history features more informative by using subsequently observed frames when
available. In a single unified framework, GateHUB integrates the transformer's
ability of long-range temporal modeling and the recurrent model's capacity to
selectively encode relevant information. GateHUB also introduces a background
suppression objective to further mitigate false positive background frames that
closely resemble the action frames. Extensive validation on three benchmark
datasets, THUMOS, TVSeries, and HDD, demonstrates that GateHUB significantly
outperforms all existing methods and is also more efficient than the existing
best work. Furthermore, a flow-free version of GateHUB is able to achieve
higher or close accuracy at 2.8x higher frame rate compared to all existing
methods that require both RGB and optical flow information for prediction.
</summary>
    <author>
      <name>Junwen Chen</name>
    </author>
    <author>
      <name>Gaurav Mittal</name>
    </author>
    <author>
      <name>Ye Yu</name>
    </author>
    <author>
      <name>Yu Kong</name>
    </author>
    <author>
      <name>Mei Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.04668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.04668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1809.01607v1</id>
    <updated>2018-09-05T16: 24: 51Z</updated>
    <published>2018-09-05T16: 24: 51Z</published>
    <title>Synthesizing Adaptive Test Strategies from Temporal Logic Specifications</title>
    <summary>  Constructing good test cases is difficult and time-consuming, especially if
the system under test is still under development and its exact behavior is not
yet fixed. We propose a new approach to compute test strategies for reactive
systems from a given temporal logic specification using formal methods. The
computed strategies are guaranteed to reveal certain simple faults in every
realization of the specification and for every behavior of the uncontrollable
part of the system's environment. The proposed approach supports different
assumptions on occurrences of faults (ranging from a single transient fault to
a persistent fault) and by default aims at unveiling the weakest one. Based on
well-established hypotheses from fault-based testing, we argue that such tests
are also sensitive for more complex bugs. Since the specification may not
define the system behavior completely, we use reactive synthesis algorithms
with partial information. The computed strategies are adaptive test strategies
that react to behavior at runtime. We work out the underlying theory of
adaptive test strategy synthesis and present experiments for a safety-critical
component of a real-world satellite system. We demonstrate that our approach
can be applied to industrial specifications and that the synthesized test
strategies are capable of detecting bugs that are hard to detect with random
testing.
</summary>
    <author>
      <name>Roderick Bloem</name>
    </author>
    <author>
      <name>Goerschwin Fey</name>
    </author>
    <author>
      <name>Fabian Greif</name>
    </author>
    <author>
      <name>Robert Koenighofer</name>
    </author>
    <author>
      <name>Ingo Pill</name>
    </author>
    <author>
      <name>Heinz Riener</name>
    </author>
    <author>
      <name>Franz Roeck</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1301.0326v2</id>
    <updated>2013-04-11T06: 03: 29Z</updated>
    <published>2013-01-02T21: 00: 05Z</published>
    <title>The nature of the near-infrared interline sky background using fibre
  Bragg grating OH suppression</title>
    <summary>  We analyse the near-infrared interline sky background, OH and O2 emission in
19 hours of H band observations with the GNOSIS OH suppression unit and the
IRIS2 spectrograph at the 3.9-m AAT. We find that the temporal behaviour of OH
emission is best described by a gradual decrease during the first half of the
night followed by a gradual increase during the second half of the night
following the behaviour of the solar elevation angle. We measure the interline
background at 1.520 microns where the instrumental thermal background is very
low and study its variation with zenith distance, time after sunset, ecliptic
latitude, lunar zenith angle and lunar distance to determine the presence of
non-thermal atmospheric emission, zodiacal scattered light and scattered
moonlight. Zodiacal scattered light is too faint to be detected in the summed
observations. Scattered moonlight due to Mie scattering by atmospheric aerosols
is seen at small lunar distances (&lt; 11 deg), but is otherwise too faint to
detect. Except at very small lunar distances the interline background at a
resolving power of R~2400 when using OH suppression fibres is dominated by a
non-thermal atmospheric source with a temporal behaviour that resembles
atmospheric OH emission suggesting that the interline background contains
instrumentally-scattered OH. However, the interline background dims more
rapidly than OH early in the night suggesting contributions from rapid dimming
molecules. The absolute interline background is 560 +/- 120 photons s^-1 m^-2
micron^-1 arcsec^-2 under dark conditions. This value is similar to previous
measurements without OH suppression suggesting that non-suppressed atmospheric
emission is responsible for the interline background. Future OH suppression
fibre designs may address this by the suppression of more sky lines using more
accurate sky line measurements taken from high resolution spectra.
</summary>
    <author>
      <name>Christopher Q. Trinh</name>
    </author>
    <author>
      <name>Simon C. Ellis</name>
    </author>
    <author>
      <name>Joss Bland-Hawthorn</name>
    </author>
    <author>
      <name>Anthony J. Horton</name>
    </author>
    <author>
      <name>Jon S. Lawrence</name>
    </author>
    <author>
      <name>Sergio G. Leon-Saval</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/mnras/stt677</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/mnras/stt677" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages,
                                    12 figures, submitted to MNRAS</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.0326v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0326v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/0911.0136v1</id>
    <updated>2009-11-01T07: 20: 55Z</updated>
    <published>2009-11-01T07: 20: 55Z</published>
    <title>Checking Behavioral Consistency Constraints for Pervasive Context in
  Asynchronous Environments</title>
    <summary>  Context consistency checking, the checking of specified constraint on
properties of contexts, is essential to context-aware applications. In order to
delineate and adapt to dynamic changes in the pervasive computing environment,
context-aware applications often need to specify and check behavioral
consistency constraints over the contexts. This problem is challenging mainly
due to the distributed and asynchronous nature of pervasive computing
environments. Specifically, the critical issue in checking behavioral
constraints is the temporal ordering of contextual activities. The contextual
activities usually involve multiple context collecting devices, which are
fully-decentralized and interact in an asynchronous manner. However, existing
context consistency checking schemes do not work in asynchronous environments,
since they implicitly assume the availability of a global clock or relay on
synchronized interactions.
  To this end, we propose the Ordering Global Activity (OGA) algorithm, which
detects the ordering of the global activities based on predicate detection in
asynchronous environments. The essence of our approach is the message causality
and its on-the-fly coding as logic vector clocks in asynchronous environments.
We implement the Middleware Infrastructure for Predicate detection in
Asynchronous environments (MIPA), over which the OGA algorithm is implemented
and evaluated. The evaluation results show the impact of asynchrony on the
checking of behavioral consistency constraints, which justifies the primary
motivation of our work. They also show that OGA can achieve accurate checking
of behavioral consistency constraints in dynamic pervasive computing
environments.
</summary>
    <author>
      <name>Yu Huang</name>
    </author>
    <author>
      <name>Jianping Yu</name>
    </author>
    <author>
      <name>Jiannong Cao</name>
    </author>
    <author>
      <name>Xiaoxing Ma</name>
    </author>
    <author>
      <name>Xianping Tao</name>
    </author>
    <author>
      <name>Jian Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages,
                                    9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0911.0136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.0136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2310.05041v3</id>
    <updated>2024-04-04T21: 33: 35Z</updated>
    <published>2023-10-08T06: 51: 05Z</published>
    <title>An Anomaly Behavior Analysis Framework for Securing Autonomous Vehicle
  Perception</title>
    <summary>  As a rapidly growing cyber-physical platform, Autonomous Vehicles (AVs) are
encountering more security challenges as their capabilities continue to expand.
In recent years, adversaries are actively targeting the perception sensors of
autonomous vehicles with sophisticated attacks that are not easily detected by
the vehicles' control systems. This work proposes an Anomaly Behavior Analysis
approach to detect a perception sensor attack against an autonomous vehicle.
The framework relies on temporal features extracted from a physics-based
autonomous vehicle behavior model to capture the normal behavior of vehicular
perception in autonomous driving. By employing a combination of model-based
techniques and machine learning algorithms, the proposed framework
distinguishes between normal and abnormal vehicular perception behavior. To
demonstrate the application of the framework in practice, we performed a depth
camera attack experiment on an autonomous vehicle testbed and generated an
extensive dataset. We validated the effectiveness of the proposed framework
using this real-world data and released the dataset for public access. To our
knowledge, this dataset is the first of its kind and will serve as a valuable
resource for the research community in evaluating their intrusion detection
techniques effectively.
</summary>
    <author>
      <name>Murad Mehrab Abrar</name>
    </author>
    <author>
      <name>Salim Hariri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20th ACS/IEEE International Conference on Computer Systems and
  Applications (IEEE AICCSA 2023)</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.05041v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.05041v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2301.00985v2</id>
    <updated>2024-12-28T01: 22: 26Z</updated>
    <published>2023-01-03T07: 33: 33Z</published>
    <title>DFME: A New Benchmark for Dynamic Facial Micro-expression Recognition</title>
    <summary>  One of the most important subconscious reactions, micro-expression (ME), is a
spontaneous, subtle, and transient facial expression that reveals human beings'
genuine emotion. Therefore, automatically recognizing ME (MER) is becoming
increasingly crucial in the field of affective computing, providing essential
technical support for lie detection, clinical psychological diagnosis, and
public safety. However, the ME data scarcity has severely hindered the
development of advanced data-driven MER models. Despite the recent efforts by
several spontaneous ME databases to alleviate this problem, there is still a
lack of sufficient data. Hence, in this paper, we overcome the ME data scarcity
problem by collecting and annotating a dynamic spontaneous ME database with the
largest current ME data scale called DFME (Dynamic Facial Micro-expressions).
Specifically, the DFME database contains 7,
                                    526 well-labeled ME videos spanning
multiple high frame rates, elicited by 671 participants and annotated by more
than 20 professional annotators over three years. Furthermore, we
comprehensively verify the created DFME, including using influential
spatiotemporal video feature learning models and MER models as baselines, and
conduct emotion classification and ME action unit classification experiments.
The experimental results demonstrate that the DFME database can facilitate
research in automatic MER, and provide a new benchmark for this field. DFME
will be published via https: //mea-lab-421.github.io.
</summary>
    <author>
      <name>Sirui Zhao</name>
    </author>
    <author>
      <name>Huaying Tang</name>
    </author>
    <author>
      <name>Xinglong Mao</name>
    </author>
    <author>
      <name>Shifeng Liu</name>
    </author>
    <author>
      <name>Yiming Zhang</name>
    </author>
    <author>
      <name>Hao Wang</name>
    </author>
    <author>
      <name>Tong Xu</name>
    </author>
    <author>
      <name>Enhong Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TAFFC.2023.3341918</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TAFFC.2023.3341918" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages,
                                    5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.00985v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.00985v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2301.09424v1</id>
    <updated>2023-01-17T02: 05: 29Z</updated>
    <published>2023-01-17T02: 05: 29Z</published>
    <title>Multifunctional nanostructures for intracellular delivery and sensing in
  electrogenic cells</title>
    <summary>  In electrophysiology, multielectrode array devices (MEA) are the gold
standard for the study of large ensambles of electrogenic cells. In the last
decades, thanks to the adoption of nanotechnologies, the study of physiological
and pathological conditions of electro-active cells in culture have becomes
increasingly accurate. In parallel, studies exploited the integration of
nanostructures with delivering capabilities with single-cell specificity and
high throughput in biosensing platforms. Delivery and recording have
independently led to great advances in neurobiology, however, their integration
on a single chip would give complete insights into pathologies development and
fundamental advancements in drug screening methods. In this work, we
demonstrate how a microfluidic-MEA technology may be used to record both
spontaneous and chemically induced activity in vitro. We propose a device that
can deliver molecules to only a few chosen cells and detecting the response in
cellular activity at multiple sites simultaneously. In addition, will be
discussed how the adoption of nanoporous metamaterial in place of
nanostructures might lower costs and speed up production. Furthermore, this
same material, will be identified for the first time in this work as
photoelectrical modulating material for eliciting electrogenic cells firing
activity. Specifically, by converting NIR laser pulses into stimulatory
currents, plasmonic metamaterials may be employed to induce action potentials.
This method enables remote access to optical pacing with precise spatiotemporal
control, allowing to be used as a valid alternative of the traditional
genetic-based optical stimulation techniques. Therefore, in addition to
pharmaceutical applications, these final characteristics may pave the way for a
new generation of minimally invasive, cellular type-independent all-optical
plasmonic pacemakers and muscle actuators.
</summary>
    <author>
      <name>Giulia Bruno</name>
    </author>
    <link href="http://arxiv.org/abs/2301.09424v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.09424v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2105.09909v1</id>
    <updated>2021-05-06T08: 10: 35Z</updated>
    <published>2021-05-06T08: 10: 35Z</published>
    <title>PLSM: A Parallelized Liquid State Machine for Unintentional Action
  Detection</title>
    <summary>  Reservoir Computing (RC) offers a viable option to deploy AI algorithms on
low-end embedded system platforms. Liquid State Machine (LSM) is a bio-inspired
RC model that mimics the cortical microcircuits and uses spiking neural
networks (SNN) that can be directly realized on neuromorphic hardware. In this
paper, we present a novel Parallelized LSM (PLSM) architecture that
incorporates spatio-temporal read-out layer and semantic constraints on model
output. To the best of our knowledge, such a formulation has been done for the
first time in literature, and it offers a computationally lighter alternative
to traditional deep-learning models. Additionally, we also present a
comprehensive algorithm for the implementation of parallelizable SNNs and LSMs
that are GPU-compatible. We implement the PLSM model to classify
unintentional/accidental video clips, using the Oops dataset. From the
experimental results on detecting unintentional action in video, it can be
observed that our proposed model outperforms a self-supervised model and a
fully supervised traditional deep learning model. All the implemented codes can
be found at our repository
https: //github.com/anonymoussentience2020/Parallelized_LSM_for_Unintentional_Action_Recognition.
</summary>
    <author>
      <name>Dipayan Das</name>
    </author>
    <author>
      <name>Saumik Bhattacharya</name>
    </author>
    <author>
      <name>Umapada Pal</name>
    </author>
    <author>
      <name>Sukalpa Chanda</name>
    </author>
    <link href="http://arxiv.org/abs/2105.09909v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.09909v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1912.04465v4</id>
    <updated>2020-09-08T13: 27: 22Z</updated>
    <published>2019-12-10T02: 57: 28Z</published>
    <title>SoccerDB: A Large-Scale Database for Comprehensive Video Understanding</title>
    <summary>  Soccer videos can serve as a perfect research object for video understanding
because soccer games are played under well-defined rules while complex and
intriguing enough for researchers to study. In this paper, we propose a new
soccer video database named SoccerDB, comprising 171,
                                    191 video segments from
346 high-quality soccer games. The database contains 702,
                                    096 bounding boxes,
                                    37,
                                    709 essential event labels with time boundary and 17,
                                    115 highlight
annotations for object detection, action recognition, temporal action
localization, and highlight detection tasks. To our knowledge, it is the
largest database for comprehensive sports video understanding on various
aspects. We further survey a collection of strong baselines on SoccerDB, which
have demonstrated state-of-the-art performances on independent tasks. Our
evaluation suggests that we can benefit significantly when jointly considering
the inner correlations among those tasks. We believe the release of SoccerDB
will tremendously advance researches around comprehensive video understanding.
{\itshape Our dataset and code published on
https: //github.com/newsdata/SoccerDB.}
</summary>
    <author>
      <name>Yudong Jiang</name>
    </author>
    <author>
      <name>Kaixu Cui</name>
    </author>
    <author>
      <name>Leilei Chen</name>
    </author>
    <author>
      <name>Canjin Wang</name>
    </author>
    <author>
      <name>Changliang Xu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3422844.3423051</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3422844.3423051" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by MM2020 sports workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.04465v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.04465v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2008.10534v1</id>
    <updated>2020-08-24T16: 16: 38Z</updated>
    <published>2020-08-24T16: 16: 38Z</published>
    <title>Decision Support for Video-based Detection of Flu Symptoms</title>
    <summary>  The development of decision support systems is a growing domain that can be
applied in the area of disease control and diagnostics. Using video-based
surveillance data, skeleton features are extracted to perform action
recognition, specifically the detection and recognition of coughing and
sneezing motions. Providing evidence of flu-like symptoms, a decision support
system based on causal networks is capable of providing the operator with vital
information for decision-making. A modified residual temporal convolutional
network is proposed for action recognition using skeleton features. This paper
addresses the capability of using results from a machine-learning model as
evidence for a cognitive decision support system. We propose risk and trust
measures as a metric to bridge between machine-learning and machine-reasoning.
We provide experiments on evaluating the performance of the proposed network
and how these performance measures can be combined with risk to generate trust.
</summary>
    <author>
      <name>Kenneth Lai</name>
    </author>
    <author>
      <name>Svetlana N. Yanushkevich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,
                                        7 figures, submitted to IEEE SMC</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.10534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.10534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2104.07921v2</id>
    <updated>2022-06-12T14: 13: 09Z</updated>
    <published>2021-04-16T06: 47: 41Z</published>
    <title>VGNMN: Video-grounded Neural Module Network to Video-Grounded Language
  Tasks</title>
    <summary>  Neural module networks (NMN) have achieved success in image-grounded tasks
such as Visual Question Answering (VQA) on synthetic images. However, very
limited work on NMN has been studied in the video-grounded dialogue tasks.
These tasks extend the complexity of traditional visual tasks with the
additional visual temporal variance and language cross-turn dependencies.
Motivated by recent NMN approaches on image-grounded tasks, we introduce
Video-grounded Neural Module Network (VGNMN) to model the information retrieval
process in video-grounded language tasks as a pipeline of neural modules. VGNMN
first decomposes all language components in dialogues to explicitly resolve any
entity references and detect corresponding action-based inputs from the
question. The detected entities and actions are used as parameters to
instantiate neural module networks and extract visual cues from the video. Our
experiments show that VGNMN can achieve promising performance on a challenging
video-grounded dialogue benchmark as well as a video QA benchmark.
</summary>
    <author>
      <name>Hung Le</name>
    </author>
    <author>
      <name>Nancy F. Chen</name>
    </author>
    <author>
      <name>Steven C. H. Hoi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at NAACL 2022 (Oral)</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.07921v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.07921v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1803.00162v1</id>
    <updated>2018-03-01T01: 53: 52Z</updated>
    <published>2018-03-01T01: 53: 52Z</published>
    <title>Towards Cooperation in Sequential Prisoner's Dilemmas: a Deep Multiagent
  Reinforcement Learning Approach</title>
    <summary>  The Iterated Prisoner's Dilemma has guided research on social dilemmas for
decades. However, it distinguishes between only two atomic actions: cooperate
and defect. In real-world prisoner's dilemmas, these choices are temporally
extended and different strategies may correspond to sequences of actions,
reflecting grades of cooperation. We introduce a Sequential Prisoner's Dilemma
(SPD) game to better capture the aforementioned characteristics. In this work,
we propose a deep multiagent reinforcement learning approach that investigates
the evolution of mutual cooperation in SPD games. Our approach consists of two
phases. The first phase is offline: it synthesizes policies with different
cooperation degrees and then trains a cooperation degree detection network. The
second phase is online: an agent adaptively selects its policy based on the
detected degree of opponent cooperation. The effectiveness of our approach is
demonstrated in two representative SPD 2D games: the Apple-Pear game and the
Fruit Gathering game. Experimental results show that our strategy can avoid
being exploited by exploitative opponents and achieve cooperation with
cooperative opponents.
</summary>
    <author>
      <name>Weixun Wang</name>
    </author>
    <author>
      <name>Jianye Hao</name>
    </author>
    <author>
      <name>Yixi Wang</name>
    </author>
    <author>
      <name>Matthew Taylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages,
                                        21 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.00162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2108.11559v1</id>
    <updated>2021-08-26T02: 34: 55Z</updated>
    <published>2021-08-26T02: 34: 55Z</published>
    <title>Identity-aware Graph Memory Network for Action Detection</title>
    <summary>  Action detection plays an important role in high-level video understanding
and media interpretation. Many existing studies fulfill this spatio-temporal
localization by modeling the context, capturing the relationship of actors,
objects, and scenes conveyed in the video. However, they often universally
treat all the actors without considering the consistency and distinctness
between individuals, leaving much room for improvement. In this paper, we
explicitly highlight the identity information of the actors in terms of both
long-term and short-term context through a graph memory network, namely
identity-aware graph memory network (IGMN). Specifically, we propose the
hierarchical graph neural network (HGNN) to comprehensively conduct long-term
relation modeling within the same identity as well as between different ones.
Regarding short-term context, we develop a dual attention module (DAM) to
generate identity-aware constraint to reduce the influence of interference by
the actors of different identities. Extensive experiments on the challenging
AVA dataset demonstrate the effectiveness of our method, which achieves
state-of-the-art results on AVA v2.1 and v2.2.
</summary>
    <author>
      <name>Jingcheng Ni</name>
    </author>
    <author>
      <name>Jie Qin</name>
    </author>
    <author>
      <name>Di Huang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3474085.3475503</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3474085.3475503" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACM MM2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.11559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.11559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2304.11975v1</id>
    <updated>2023-04-24T10: 15: 31Z</updated>
    <published>2023-04-24T10: 15: 31Z</published>
    <title>MRSN: Multi-Relation Support Network for Video Action Detection</title>
    <summary>  Action detection is a challenging video understanding task, requiring
modeling spatio-temporal and interaction relations. Current methods usually
model actor-actor and actor-context relations separately, ignoring their
complementarity and mutual support. To solve this problem, we propose a novel
network called Multi-Relation Support Network (MRSN). In MRSN, Actor-Context
Relation Encoder (ACRE) and Actor-Actor Relation Encoder (AARE) model the
actor-context and actor-actor relation separately. Then Relation Support
Encoder (RSE) computes the supports between the two relations and performs
relation-level interactions. Finally, Relation Consensus Module (RCM) enhances
two relations with the long-term relations from the Long-term Relation Bank
(LRB) and yields a consensus. Our experiments demonstrate that modeling
relations separately and performing relation-level interactions can achieve and
outperformer state-of-the-art results on two challenging video datasets: AVA
and UCF101-24.
</summary>
    <author>
      <name>Yin-Dong Zheng</name>
    </author>
    <author>
      <name>Guo Chen</name>
    </author>
    <author>
      <name>Minglei Yuan</name>
    </author>
    <author>
      <name>Tong Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.11975v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.11975v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2309.00696v1</id>
    <updated>2023-09-01T18: 35: 31Z</updated>
    <published>2023-09-01T18: 35: 31Z</published>
    <title>AAN: Attributes-Aware Network for Temporal Action Detection</title>
    <summary>  The challenge of long-term video understanding remains constrained by the
efficient extraction of object semantics and the modelling of their
relationships for downstream tasks. Although the CLIP visual features exhibit
discriminative properties for various vision tasks, particularly in object
encoding, they are suboptimal for long-term video understanding. To address
this issue, we present the Attributes-Aware Network (AAN), which consists of
two key components: the Attributes Extractor and a Graph Reasoning block. These
components facilitate the extraction of object-centric attributes and the
modelling of their relationships within the video. By leveraging CLIP features,
AAN outperforms state-of-the-art approaches on two popular action detection
datasets: Charades and Toyota Smarthome Untrimmed datasets.
</summary>
    <author>
      <name>Rui Dai</name>
    </author>
    <author>
      <name>Srijan Das</name>
    </author>
    <author>
      <name>Michael S. Ryoo</name>
    </author>
    <author>
      <name>Francois Bremond</name>
    </author>
    <link href="http://arxiv.org/abs/2309.00696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.00696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2504.00058v1</id>
    <updated>2025-03-31T10: 11: 31Z</updated>
    <published>2025-03-31T10: 11: 31Z</published>
    <title>GAL-MAD: Towards Explainable Anomaly Detection in Microservice
  Applications Using Graph Attention Networks</title>
    <summary>  The transition to microservices has revolutionized software architectures,
offering enhanced scalability and modularity. However, the distributed and
dynamic nature of microservices introduces complexities in ensuring system
reliability, making anomaly detection crucial for maintaining performance and
functionality. Anomalies stemming from network and performance issues must be
swiftly identified and addressed. Existing anomaly detection techniques often
rely on statistical models or machine learning methods that struggle with the
high-dimensional, interdependent data inherent in microservice applications.
Current techniques and available datasets predominantly focus on system traces
and logs, limiting their ability to support advanced detection models. This
paper addresses these gaps by introducing the RS-Anomic dataset generated using
the open-source RobotShop microservice application. The dataset captures
multivariate performance metrics and response times under normal and anomalous
conditions, encompassing ten types of anomalies. We propose a novel anomaly
detection model called Graph Attention and LSTM-based Microservice Anomaly
Detection (GAL-MAD), leveraging Graph Attention and Long Short-Term Memory
architectures to capture spatial and temporal dependencies in microservices. We
utilize SHAP values to localize anomalous services and identify root causes to
enhance explainability. Experimental results demonstrate that GAL-MAD
outperforms state-of-the-art models on the RS-Anomic dataset, achieving higher
accuracy and recall across varying anomaly rates. The explanations provide
actionable insights into service anomalies, which benefits system
administrators.
</summary>
    <author>
      <name>Lahiru Akmeemana</name>
    </author>
    <author>
      <name>Chamodya Attanayake</name>
    </author>
    <author>
      <name>Husni Faiz</name>
    </author>
    <author>
      <name>Sandareka Wickramanayake</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, preprint,
                                        10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.00058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.00058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1901.08093v1</id>
    <updated>2019-01-23T19: 29: 27Z</updated>
    <published>2019-01-23T19: 29: 27Z</published>
    <title>Decoding multimodal behavior using time differences of MEG events</title>
    <summary>  Multimodal behavior involves multiple processing stations distributed across
distant brain regions, but our understanding of how such distributed processing
is coordinated in the brain is limited. Here we take a decoding approach to
this problem, aiming to quantify how temporal aspects of brain-wide neural
activity may be used to infer specific multimodal behaviors. Using high
temporal resolution measurements by MEG, we detect bursts of activity from
hundreds of locations across the surface of the brain at millisecond
resolution. We then compare decoding using three characteristics of neural
activity bursts, decoding with event counts, with latencies and with time
differences between pairs of events. Training decoders in this regime is
particularly challenging because the number of samples is smaller by orders of
magnitude than the input dimensionality. We develop a new decoding approach for
this regime that combines non-parametric modelling with aggressive feature
selection. Surprisingly, we find that decoding using time-differences, based on
thousands of region pairs, is significantly more accurate than using other
activity characteristics, reaching 90% accuracy consistently across subjects.
These results suggest that relevant information about multimodal brain function
is provided by subtle time differences across remote brain areas.
</summary>
    <author>
      <name>Ohad Felsenstein</name>
    </author>
    <author>
      <name>Idan Tal</name>
    </author>
    <author>
      <name>Michal Ben-Shachar</name>
    </author>
    <author>
      <name>Moshe Abeles</name>
    </author>
    <author>
      <name>Gal Chechik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages,
                                        6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.08093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.08093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2101.05415v1</id>
    <updated>2021-01-14T01: 54: 31Z</updated>
    <published>2021-01-14T01: 54: 31Z</published>
    <title>Analysis of E-commerce Ranking Signals via Signal Temporal Logic</title>
    <summary>  The timed position of documents retrieved by learning to rank models can be
seen as signals. Signals carry useful information such as drop or rise of
documents over time or user behaviors. In this work, we propose to use the
logic formalism called Signal Temporal Logic (STL) to characterize document
behaviors in ranking accordingly to the specified formulas. Our analysis shows
that interesting document behaviors can be easily formalized and detected
thanks to STL formulas. We validate our idea on a dataset of 100K product
signals. Through the presented framework, we uncover interesting patterns, such
as cold start, warm start, spikes, and inspect how they affect our learning to
ranks models.
</summary>
    <author>
      <name>Tommaso Dreossi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Amazon Search</arxiv:affiliation>
    </author>
    <author>
      <name>Giorgio Ballardin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Amazon Search</arxiv:affiliation>
    </author>
    <author>
      <name>Parth Gupta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Amazon Search</arxiv:affiliation>
    </author>
    <author>
      <name>Jan Bakus</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Amazon Search</arxiv:affiliation>
    </author>
    <author>
      <name>Yu-Hsiang Lin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Amazon Search</arxiv:affiliation>
    </author>
    <author>
      <name>Vamsi Salaka</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Amazon Search</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.331.3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.331.3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings SNR 2020, arXiv: 2101.05256</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 331,
                                        2021, pp. 33-42</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2101.05415v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.05415v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1312.2047v1</id>
    <updated>2013-12-07T01: 20: 58Z</updated>
    <published>2013-12-07T01: 20: 58Z</published>
    <title>Diagnosis of Switching Systems using Hybrid Bond Graph</title>
    <summary>  Hybrid Bond Graph (HBG) is a Bond Graph-based modelling approach which
provides an effective tool not only for dynamic modeling but also for fault
detection and isolation (FDI) of switching systems. Bond graph (BG) has been
proven useful for FDI for continuous systems. In addition, BG provides the
causal relations between systems variables which allow FDI algorithms to be
developed systematically from the graph. There are many methods that exploit
structural relations and functional redundancy in the system model to find
efficient solutions for the residual generation and residual evaluation steps
in FDI of switching systems. This paper describes two different techniques,
quantitative and qualitative, based on common modelling approach that employs
HBG. In quantitative approach, global analytical redundancy relationships
(GARRs) are derived from the HBG model with a specified causality assignment
procedure. GARRs describe the system behaviour at all of its operating modes.
In qualitative approach, functional redundancy can be captured by a Temporal
Causal Graph (TCG), a directed graph that may include temporal information
</summary>
    <author>
      <name>Taher Mekki</name>
    </author>
    <author>
      <name>Slim Triki</name>
    </author>
    <author>
      <name>Anas Kamoun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues - IJCSI Volume
  10, Issue 1, January 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1312.2047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1909.09020v4</id>
    <updated>2019-11-10T22: 10: 14Z</updated>
    <published>2019-09-19T14: 35: 30Z</published>
    <title>Shape and Time Distortion Loss for Training Deep Time Series Forecasting
  Models</title>
    <summary>  This paper addresses the problem of time series forecasting for
non-stationary signals and multiple future steps prediction. To handle this
challenging task, we introduce DILATE (DIstortion Loss including shApe and
TimE), a new objective function for training deep neural networks. DILATE aims
at accurately predicting sudden changes, and explicitly incorporates two terms
supporting precise shape and temporal change detection. We introduce a
differentiable loss function suitable for training deep neural nets, and
provide a custom back-prop implementation for speeding up optimization. We also
introduce a variant of DILATE, which provides a smooth generalization of
temporally-constrained Dynamic Time Warping (DTW). Experiments carried out on
various non-stationary datasets reveal the very good behaviour of DILATE
compared to models trained with the standard Mean Squared Error (MSE) loss
function, and also to DTW and variants. DILATE is also agnostic to the choice
of the model, and we highlight its benefit for training fully connected
networks as well as specialized recurrent architectures, showing its capacity
to improve over state-of-the-art trajectory forecasting approaches.
</summary>
    <author>
      <name>Vincent Le Guen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CNAM, EDF R&amp;D</arxiv:affiliation>
    </author>
    <author>
      <name>Nicolas Thome</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CNAM</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1909.09020v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.09020v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1805.03627v1</id>
    <updated>2018-05-09T17: 28: 40Z</updated>
    <published>2018-05-09T17: 28: 40Z</published>
    <title>Dynamics of a bistable VCSEL subject to optical feedback from a
  vibrating rough surface</title>
    <summary>  The paper presents an experimental study of the temporal behaviour of a
bistable vertical cavity surface emitting laser (VCSEL) under the effect of
optical feedback coming from vibrating diffusely-reflecting surface. We
demonstrate that a VCSEL operating in the regime of polarization switchings
caused by self-mixing effects can greatly enhance a responsivity of the
detection of microvibrations. For small amplitudes of microvibrations (less
than $\lambda$/2, where $\lambda$ is the operating wavelength of the laser),
which cause only harmonic (or close to harmonic) oscillations in the laser
output outside of the bistability region, the use of polarization switching can
increase the responsivity up to 45 times. For the amplitudes larger then
$\lambda$/2 the response of a bistable VCSEL well reproduces a fine temporal
structure of the self-mixing signal. A procedure of the data processing in the
switching regime for the correct determination of the direction of the
displacement and for the reconstruction of the waveform of surface vibrations
with a basic resolution of $\lambda$/2 is also shown.
</summary>
    <author>
      <name>V. N. Chizhevsky</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JQE.2018.2875563</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JQE.2018.2875563" rel="related"/>
    <link href="http://arxiv.org/abs/1805.03627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2304.11858v1</id>
    <updated>2023-04-24T07: 14: 46Z</updated>
    <published>2023-04-24T07: 14: 46Z</published>
    <title>Fitness-for-Duty Classification using Temporal Sequences of Iris
  Periocular images</title>
    <summary>  Fitness for Duty (FFD) techniques detects whether a subject is Fit to perform
their work safely, which means no reduced alertness condition and security, or
if they are Unfit, which means alertness condition reduced by sleepiness or
consumption of alcohol and drugs. Human iris behaviour provides valuable
information to predict FFD since pupil and iris movements are controlled by the
central nervous system and are influenced by illumination, fatigue, alcohol,
and drugs. This work aims to classify FFD using sequences of 8 iris images and
to extract spatial and temporal information using Convolutional Neural Networks
(CNN) and Long Short Term Memory Networks (LSTM). Our results achieved a
precision of 81.4\% and 96.9\% for the prediction of Fit and Unfit subjects,
respectively. The results also show that it is possible to determine if a
subject is under alcohol, drug, and sleepiness conditions. Sleepiness can be
identified as the most difficult condition to be determined. This system opens
a different insight into iris biometric applications.
</summary>
    <author>
      <name>Pamela C. Zurita</name>
    </author>
    <author>
      <name>Daniel P. Benalcazar</name>
    </author>
    <author>
      <name>Juan E. Tapia</name>
    </author>
    <link href="http://arxiv.org/abs/2304.11858v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.11858v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2504.09463v1</id>
    <updated>2025-04-13T07: 30: 55Z</updated>
    <published>2025-04-13T07: 30: 55Z</published>
    <title>Comorbidity-Informed Transfer Learning for Neuro-developmental Disorder
  Diagnosis</title>
    <summary>  Neuro-developmental disorders are manifested as dysfunctions in cognition,
communication, behaviour and adaptability, and deep learning-based
computer-aided diagnosis (CAD) can alleviate the increasingly strained
healthcare resources on neuroimaging. However, neuroimaging such as fMRI
contains complex spatio-temporal features, which makes the corresponding
representations susceptible to a variety of distractions, thus leading to less
effective in CAD. For the first time, we present a Comorbidity-Informed
Transfer Learning(CITL) framework for diagnosing neuro-developmental disorders
using fMRI. In CITL, a new reinforced representation generation network is
proposed, which first combines transfer learning with pseudo-labelling to
remove interfering patterns from the temporal domain of fMRI and generates new
representations using encoder-decoder architecture. The new representations are
then trained in an architecturally simple classification network to obtain CAD
model. In particular, the framework fully considers the comorbidity mechanisms
of neuro-developmental disorders and effectively integrates them with
semi-supervised learning and transfer learning, providing new perspectives on
interdisciplinary. Experimental results demonstrate that CITL achieves
competitive accuracies of 76.32% and 73.15% for detecting autism spectrum
disorder and attention deficit hyperactivity disorder, respectively, which
outperforms existing related transfer learning work for 7.2% and 0.5%
respectively.
</summary>
    <author>
      <name>Xin Wen</name>
    </author>
    <author>
      <name>Shijie Guo</name>
    </author>
    <author>
      <name>Wenbo Ning</name>
    </author>
    <author>
      <name>Rui Cao</name>
    </author>
    <author>
      <name>Jie Xiang</name>
    </author>
    <author>
      <name>Xiaobo Liu</name>
    </author>
    <author>
      <name>Jintai Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2504.09463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.09463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1601.05632v1</id>
    <updated>2016-01-21T13: 47: 34Z</updated>
    <published>2016-01-21T13: 47: 34Z</published>
    <title>Femtoscopic measurements in $p$+Pb collisions at
  $\sqrt{s_{_{\text{NN
                                                    }
                                                }
                                            }
                                        }= 5.02$~TeV with ATLAS at the LHC</title>
    <summary>  Recent measurements in two-particle correlations in $p$+Pb collisions suggest
collective behavior reminiscent of that observed in Pb+Pb. Femtoscopic
measurements may provide useful insight on this behavior because they image the
spatio-temporal size of the particle emitting region. This proceeding presents
identical-pion Hanbury Brown and Twiss (HBT) measurements from ATLAS using one-
and three-dimensional correlation functions. Pions are identified using
$\text{d
                                        }E/\text{d
                                        }x$ measured in the pixel detector. Correlation functions and
the resulting HBT radii are shown as a function of pair momentum ($k_{\rm T
                                        }$)
and collision centrality.
</summary>
    <author>
      <name>Markus K. Koehler</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">for the ATLAS Collaboration</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.nuclphysa.2016.02.056</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.nuclphysa.2016.02.056" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages,
                                        8 figures, to appear in the proceedings for the Quark Matter
  2015 conference, Kobe, Japan</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nucl. Phys. A956 (2016) 377-380</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.05632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.05632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nucl-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nucl-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1203.1228v3</id>
    <updated>2012-10-27T13: 27: 29Z</updated>
    <published>2012-03-06T15: 34: 15Z</published>
    <title>Correlated Spectral And Temporal Behaviour Of Late-Time Afterglows Of
  Gamma Ray Bursts</title>
    <summary>  The cannonball (CB) model of gamma ray bursts (GRBs) predicts that the
asymptotic behaviour of the spectral energy density of the X-ray afterglow of
GRBs is a power-law in time and in frequency where the difference between the
temporal and spectral power-law indexes, $\alpha_X-\beta_X$, is restricted to
the values 0,
                                        1/2 and 1. Here we report the distributions of the values
$\alpha_X$, $\beta_X$ and their difference for a sample of 315 Swift GRBs. This
sample includes all Swift GRBs that were detected before August 1,
                                        2012, whose
X-ray afterglow extended well beyond 1 day and the estimated error in
$\alpha_X-\beta_X$ was $\leq 0.25$. The values of $\alpha_X$ were extracted
from the CB model fits to the entire light curves of their X-ray afterglow
while the spectral index was extracted by the Swift team from the time
integrated X-ray afterglow of these GRBs. We found that the distribution of the
difference $\alpha_X-\beta_X$ for these 315 Swift GRBs has three narrow peaks
around 0,
                                        1/2 and 1 whose widths are consistent with being due to the
measurement errors, in agreement with the CB model prediction.
</summary>
    <author>
      <name>Shlomo Dado</name>
    </author>
    <author>
      <name>Arnon Dar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/0004-637X/761/2/148</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/0004-637X/761/2/148" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 figures, added references, used data reported in the Swift/XRT
  light-curve repository</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ApJ,
                                        761,
                                        148 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.1228v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.1228v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.HE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/1410.6463v1</id>
    <updated>2014-10-23T19: 36: 44Z</updated>
    <published>2014-10-23T19: 36: 44Z</published>
    <title>Probing the Structure of the Accretion Region in a Sample of Magnetic
  Herbig Ae/Be Stars</title>
    <summary>  We present the results of a study of the temporal behaviour of several
diagnostic lines formed in the region of the accretion-disk/star interaction in
the three magnetic Herbig Ae stars HD101412, HD104237, and HD190073. More than
100 spectra acquired with the ISAAC, X-shooter, and CRIRES spectrographs
installed at the VLT-8m telescope (ESO, Chile), as well as at other
observatories (OHP, Crimean AO) were analyzed. The spectroscopic data were
obtained in the He I lambda10830, Pa gamma and He I lambda5876 lines. We found
that the temporal behaviour of the diagnostic lines in the spectra of all
program stars can be widely explained by a rotational modulation of the line
profiles generated by a local accretion flow. This result is in good agreement
with the predictions of the magnetospheric accretion model. For the first time,
the rotation period of HD104237 (P_rot = 5.37+-0.03 days), as well as the
inclination angle (i = 21+-4deg) were determined. Additional analysis of the
HARPSpol spectra of HD104237 and HD190073, taken from the ESO archive, with the
use of the SVD method shows that the magnetic field structure of HD190073 is
likely more complex than a simple dipole and contains a circumstellar
component. For the first time, the magnetic field of the secondary component of
the binary system HD104237 was also detected (&lt;B_z&gt; = 128+-10G).
</summary>
    <author>
      <name>M. A. Pogodin</name>
    </author>
    <author>
      <name>J. A. Cahuasqui</name>
    </author>
    <author>
      <name>N. A. Drake</name>
    </author>
    <author>
      <name>S. Hubrig</name>
    </author>
    <author>
      <name>M. Schoeller</name>
    </author>
    <author>
      <name>M. Petr-Gotzens</name>
    </author>
    <author>
      <name>G. A. P. Franco</name>
    </author>
    <author>
      <name>D. F. Lopes</name>
    </author>
    <author>
      <name>O. V. Kozlova</name>
    </author>
    <author>
      <name>B. Wolff</name>
    </author>
    <author>
      <name>J. F. Gonzalez</name>
    </author>
    <author>
      <name>T. A. Carroll</name>
    </author>
    <author>
      <name>S. Mysore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,
                                        5 figures, to appear in the proceedings of the conference
  "Physics and Evolution of Magnetic and Related Stars"</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.6463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.6463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.SR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2308.10334v1</id>
    <updated>2023-08-20T18: 23: 07Z</updated>
    <published>2023-08-20T18: 23: 07Z</published>
    <title>Coordinate Transformer: Achieving Single-stage Multi-person Mesh
  Recovery from Videos</title>
    <summary>  Multi-person 3D mesh recovery from videos is a critical first step towards
automatic perception of group behavior in virtual reality, physical therapy and
beyond. However, existing approaches rely on multi-stage paradigms, where the
person detection and tracking stages are performed in a multi-person setting,
while temporal dynamics are only modeled for one person at a time.
Consequently, their performance is severely limited by the lack of inter-person
interactions in the spatial-temporal mesh recovery, as well as by detection and
tracking defects. To address these challenges, we propose the Coordinate
transFormer (CoordFormer) that directly models multi-person spatial-temporal
relations and simultaneously performs multi-mesh recovery in an end-to-end
manner. Instead of partitioning the feature map into coarse-scale patch-wise
tokens, CoordFormer leverages a novel Coordinate-Aware Attention to preserve
pixel-level spatial-temporal coordinate information. Additionally, we propose a
simple, yet effective Body Center Attention mechanism to fuse position
information. Extensive experiments on the 3DPW dataset demonstrate that
CoordFormer significantly improves the state-of-the-art, outperforming the
previously best results by 4.2%,
                                        8.8% and 4.7% according to the MPJPE, PAMPJPE,
and PVE metrics, respectively, while being 40% faster than recent video-based
approaches. The released code can be found at
https: //github.com/Li-Hao-yuan/CoordFormer.
</summary>
    <author>
      <name>Haoyuan Li</name>
    </author>
    <author>
      <name>Haoye Dong</name>
    </author>
    <author>
      <name>Hanchao Jia</name>
    </author>
    <author>
      <name>Dong Huang</name>
    </author>
    <author>
      <name>Michael C. Kampffmeyer</name>
    </author>
    <author>
      <name>Liang Lin</name>
    </author>
    <author>
      <name>Xiaodan Liang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICCV 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.10334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.10334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2005.11203v2</id>
    <updated>2021-12-30T07: 35: 50Z</updated>
    <published>2020-05-22T14: 29: 51Z</published>
    <title>Towards a Neural Model for Serial Order in Frontal Cortex: a Brain
  Theory from Memory Development to Higher-Level Cognition</title>
    <summary>  In order to keep trace of information and grow up, the infant brain has to
resolve the problem about where old information is located and how to index new
ones. We propose that the immature prefrontal cortex (PFC) use its primary
functionality of detecting hierarchical patterns in temporal signals as a
second purpose to organize the spatial ordering of the cortical networks in the
developing brain itself. Our hypothesis is that the PFC detects the
hierarchical structure in temporal sequences in the form of ordinal patterns
and use them to index information hierarchically in different parts of the
brain. Henceforth, we propose that this mechanism for detecting patterns
participates in the ordinal organization development of the brain itself; i.e.,
the bootstrapping of the connectome. By doing so, it gives the tools to the
language-ready brain for manipulating abstract knowledge and planning
temporally ordered information; i.e., the emergence of symbolic thinking and
language. We will review neural models that can support such mechanisms and
propose new ones. We will confront then our ideas with evidence from
developmental, behavioral and brain results and make some hypotheses, for
instance, on the construction of the mirror neuron system, on embodied
cognition, and on the capacity of learning-to-learn.
</summary>
    <author>
      <name>Alexandre Pitti</name>
    </author>
    <author>
      <name>Mathias Quoy</name>
    </author>
    <author>
      <name>Catherine Lavandier</name>
    </author>
    <author>
      <name>Sofiane Boucenna</name>
    </author>
    <author>
      <name>Wassim Swaileh</name>
    </author>
    <author>
      <name>Claudio Weidmann</name>
    </author>
    <link href="http://arxiv.org/abs/2005.11203v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.11203v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2004.09596v1</id>
    <updated>2020-04-20T19: 41: 55Z</updated>
    <published>2020-04-20T19: 41: 55Z</published>
    <title>On-the-fly Detection of User Engagement Decrease in Spontaneous
  Human-Robot Interaction, International Journal of Social Robotics,
                                        2019</title>
    <summary>  In this paper, we consider the detection of a decrease of engagement by users
spontaneously interacting with a socially assistive robot in a public space. We
first describe the UE-HRI dataset that collects spontaneous Human-Robot
Interactions following the guidelines provided by the Affective Computing
research community to collect data "in-the-wild". We then analyze the users'
behaviors, focusing on proxemics, gaze, head motion, facial expressions and
speech during interactions with the robot. Finally, we investigate the use of
deep learning techniques (Recurrent and Deep Neural Networks) to detect user
engagement decrease in realtime. The results of this work highlight, in
particular, the relevance of taking into account the temporal dynamics of a
user's behavior. Allowing 1 to 2 seconds as buffer delay improves the
performance of taking a decision on user engagement.
</summary>
    <author>
      <name>Atef Ben Youssef</name>
    </author>
    <author>
      <name>Giovanna Varni</name>
    </author>
    <author>
      <name>Slim Essid</name>
    </author>
    <author>
      <name>Chloé Clavel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s12369-019-00591-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s12369-019-00591-2" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Social Robotics December 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2004.09596v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.09596v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2011.14968v1</id>
    <updated>2020-11-30T16: 39: 31Z</updated>
    <published>2020-11-30T16: 39: 31Z</published>
    <title>Big Data-driven Automated Anomaly Detection and Performance Forecasting
  in Mobile Networks</title>
    <summary>  The massive amount of data available in operational mobile networks offers an
invaluable opportunity for operators to detect and analyze possible anomalies
and predict network performance. In particular, application of advanced machine
learning (ML) techniques on data aggregated from multiple sources can lead to
important insights, not only for the detection of anomalous behavior but also
for performance forecasting, thereby complementing classic network operation
and maintenance solutions with intelligent monitoring tools. In this paper, we
propose a novel framework that aggregates diverse data sets (e.g.
configuration, performance, inventory, locations, user speeds) from an
operational LTE network and applies ML algorithms to diagnose network issues
and analyze their impact on key performance indicators. To this end, pattern
identification and time-series forecasting algorithms are used on the ingested
data. Results show that proposed framework can indeed be leveraged to automate
the identification of anomalous behaviors associated with the spatial-temporal
characteristics, and predict customer impact in an accurate manner.
</summary>
    <author>
      <name>Jessica Moysen</name>
    </author>
    <author>
      <name>Furqan Ahmed</name>
    </author>
    <author>
      <name>Mario García-Lozano</name>
    </author>
    <author>
      <name>Jarno Niemelä</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in IEEE Globecomm 2020 workshop on AI-Enabled 5G/6G
  Networks: Automation, Openness, and Radio Access</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.14968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.14968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2205.08249v1</id>
    <updated>2022-05-17T11: 45: 03Z</updated>
    <published>2022-05-17T11: 45: 03Z</published>
    <title>Learnable Optimal Sequential Grouping for Video Scene Detection</title>
    <summary>  Video scene detection is the task of dividing videos into temporal semantic
chapters. This is an important preliminary step before attempting to analyze
heterogeneous video content. Recently, Optimal Sequential Grouping (OSG) was
proposed as a powerful unsupervised solution to solve a formulation of the
video scene detection problem. In this work, we extend the capabilities of OSG
to the learning regime. By giving the capability to both learn from examples
and leverage a robust optimization formulation, we can boost performance and
enhance the versatility of the technology. We present a comprehensive analysis
of incorporating OSG into deep learning neural networks under various
configurations. These configurations include learning an embedding in a
straight-forward manner, a tailored loss designed to guide the solution of OSG,
and an integrated model where the learning is performed through the OSG
pipeline. With thorough evaluation and analysis, we assess the benefits and
behavior of the various configurations, and show that our learnable OSG
approach exhibits desirable behavior and enhanced performance compared to the
state of the art.
</summary>
    <author>
      <name>Daniel Rotman</name>
    </author>
    <author>
      <name>Yevgeny Yaroker</name>
    </author>
    <author>
      <name>Elad Amrani</name>
    </author>
    <author>
      <name>Udi Barzelay</name>
    </author>
    <author>
      <name>Rami Ben-Ari</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3394171.3413612</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3394171.3413612" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 28th ACM International Conference on
  Multimedia, pp. 1958-1966. 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2205.08249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.08249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http: //arxiv.org/abs/2407.00140v1</id>
    <updated>2024-06-28T14: 46: 17Z</updated>
    <published>2024-06-28T14: 46: 17Z</published>
    <title>ModeConv: A Novel Convolution for Distinguishing Anomalous and Normal
  Structural Behavior</title>
    <summary>  External influences such as traffic and environmental factors induce
vibrations in structures, leading to material degradation over time. These
vibrations result in cracks due to the material's lack of plasticity
compromising structural integrity. Detecting such damage requires the
installation of vibration sensors to capture the internal dynamics. However,
distinguishing relevant eigenmodes from external noise necessitates the use of
Deep Learning models. The detection of changes in eigenmodes can be used to
anticipate these shifts in material properties and to discern between normal
and anomalous structural behavior. Eigenmodes, representing characteristic
vibration patterns, provide insights into structural dynamics and deviations
from expected states. Thus, we propose ModeConv to automatically capture and
analyze changes in eigenmodes, facilitating effective anomaly detection in
structures and material properties. In the conducted experiments, ModeConv
demonstrates computational efficiency improvements, resulting in reduced
runtime for model calculations. The novel ModeConv neural network layer is
tailored for temporal graph neural networks, in which every node represents one
sensor. ModeConv employs a singular value decomposition based convolutional
filter design for complex numbers and leverages modal transformation in lieu of
Fourier or Laplace transformations in spectral graph convolutions. We include a
mathematical complexity analysis illustrating the runtime reduction.
</summary>
    <author>
      <name>Melanie Schaller</name>
    </author>
    <author>
      <name>Daniel Schlör</name>
    </author>
    <author>
      <name>Andreas Hotho</name>
    </author>
    <link href="http://arxiv.org/abs/2407.00140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.00140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
