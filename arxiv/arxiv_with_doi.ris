TY  - GEN
AU  - Song, S.
AU  - Chandrasekhar, V.
AU  - Cheung, N.-M.
AU  - Mandal, B.
TI  - Deep adaptive temporal pooling for activity recognition
AB  - Deep neural networks have recently achieved competitive accuracy for human activity recognition. However, there is room for improvement, especially in modeling of long-term temporal importance and determining the activity relevance of different temporal segments in a video. To address this problem, we propose a learnable and differentiable module: Deep Adaptive Temporal Pooling (DATP). DATP applies a self-attention mechanism to adaptively pool the classification scores of different video segments. Specifically, using frame-level features, DATP regresses importance of different temporal segments, and generates weights for them. Remarkably, DATP is trained using only the video-level label. There is no need of additional supervision except video-level activity class label. We conduct extensive experiments to investigate various input features and different weight models. Experimental results show that DATP can learn to assign large weights to key video segments. More importantly, DATP can improve training of frame-level feature extractor. This is because relevant temporal segments are assigned large weights during back-propagation. Overall, we achieve state-of-the-art performance on UCF101, HMDB51 and Kinetics datasets.
PB  - arXiv
PY  - 2018
ST  - Deep adaptive temporal pooling for activity recognition
Y2  - 2025/05/05/21:54:28
DO  - 10.1145/3240508.3240713
ER  -


TY  - GEN
AU  - Zheng, M.
AU  - Karanam, S.
AU  - Radke, R.J.
TI  - Measuring the temporal behavior of real-world person re-identification
AB  - Designing real-world person re-identification (re-id) systems requires attention to operational aspects not typically considered in academic research. Typically, the probe image or image sequence is matched to a gallery set with a fixed candidate list. On the other hand, in real-world applications of re-id, we would search for a person of interest in a gallery set that is continuously populated by new candidates over time. A key question of interest for the operator of such a system is: how long is a correct match to a probe likely to remain in a rank-k shortlist of candidates? In this paper, we propose to distill this information into what we call a Rank Persistence Curve (RPC), which unlike a conventional cumulative match characteristic (CMC) curve helps directly compare the temporal performance of different re-id algorithms. To carefully illustrate the concept, we collected a new multi-shot person re-id dataset called RPIfield. The RPIfield dataset is constructed using a network of 12 cameras with 112 explicitly time-stamped actor paths among about 4000 distractors. We then evaluate the temporal performance of different re-id algorithms using the proposed RPCs using single and pairwise camera videos from RPIfield, and discuss considerations for future research.
PB  - arXiv
PY  - 2018
ST  - Measuring the temporal behavior of real-world person re-identification
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/cvpr42600.2020.01409
ER  -


TY  - GEN
AU  - Zhang, R.
AU  - Zheng, F.
AU  - Min, W.
TI  - Sequential behavioral data processing using deep learning and the markov transition field in online fraud detection
AB  - Due to the popularity of the Internet and smart mobile devices, more and more financial transactions and activities have been digitalized. Compared to traditional financial fraud detection strategies using credit-related features, customers are generating a large amount of unstructured behavioral data every second. In this paper, we propose an Recurrent Neural Netword (RNN) based deep-learning structure integrated with Markov Transition Field (MTF) for predicting online fraud behaviors using customer's interactions with websites or smart-phone apps as a series of states. In practice, we tested and proved that the proposed network structure for processing sequential behavioral data could significantly boost fraud predictive ability comparing with the multilayer perceptron network and distance based classifier with Dynamic Time Warping(DTW) as distance metric.
PB  - arXiv
PY  - 2018
ST  - Sequential behavioral data processing using deep learning and the markov transition field in online fraud detection
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/icicet59348.2024.10616357
ER  -


TY  - GEN
AU  - Zhang, Z.
AU  - Kuang, Z.
AU  - Luo, P.
AU  - Feng, L.
AU  - Zhang, W.
TI  - Temporal sequence distillation: Towards few-frame action recognition in videos
AB  - Video Analytics Software as a Service (VA SaaS) has been rapidly growing in recent years. VA SaaS is typically accessed by users using a lightweight client. Because the transmission bandwidth between the client and cloud is usually limited and expensive, it brings great benefits to design cloud video analysis algorithms with a limited data transmission requirement. Although considerable research has been devoted to video analysis, to our best knowledge, little of them has paid attention to the transmission bandwidth limitation in SaaS. As the first attempt in this direction, this work introduces a problem of few-frame action recognition, which aims at maintaining high recognition accuracy, when accessing only a few frames during both training and test. Unlike previous work that processed dense frames, we present Temporal Sequence Distillation (TSD), which distills a long video sequence into a very short one for transmission. By end-to-end training with 3D CNNs for video action recognition, TSD learns a compact and discriminative temporal and spatial representation of video frames. On Kinetics dataset, TSD+I3D typically requires only 50% of the number of frames compared to I3D [1], a state-of-the-art video action recognition algorithm, to achieve almost the same accuracies. The proposed TSD has three appealing advantages. Firstly, TSD has a lightweight architecture, and can be deployed in the client, e.g., mobile devices, to produce compressed representative frames to save transmission bandwidth. Secondly, TSD significantly reduces the computations to run video action recognition with compressed frames on the cloud, while maintaining high recognition accuracies. Thirdly, TSD can be plugged in as a preprocessing module of any existing 3D CNNs. Extensive experiments show the effectiveness and characteristics of TSD.
PB  - arXiv
PY  - 2018
ST  - Temporal sequence distillation
Y2  - 2025/05/05/21:54:28
DO  - 10.1609/aaai.v39i17.34004
ER  -


TY  - GEN
AU  - Zhong, Y.
AU  - Xu, B.
AU  - Zhou, G.-T.
AU  - Bornn, L.
AU  - Mori, G.
TI  - Time perception machine: Temporal point processes for the when, where and what of activity prediction
AB  - Numerous powerful point process models have been developed to understand temporal patterns in sequential data from fields such as health-care, electronic commerce, social networks, and natural disaster forecasting. In this paper, we develop novel models for learning the temporal distribution of human activities in streaming data (e.g., videos and person trajectories). We propose an integrated framework of neural networks and temporal point processes for predicting when the next activity will happen. Because point processes are limited to taking event frames as input, we propose a simple yet effective mechanism to extract features at frames of interest while also preserving the rich information in the remaining frames. We evaluate our model on two challenging datasets. The results show that our model outperforms traditional statistical point process approaches significantly, demonstrating its effectiveness in capturing the underlying temporal dynamics as well as the correlation within sequential activities. Furthermore, we also extend our model to a joint estimation framework for predicting the timing, spatial location, and category of the activity simultaneously, to answer the when, where, and what of activity prediction.
PB  - arXiv
PY  - 2018
ST  - Time perception machine
Y2  - 2025/05/05/21:54:28
DO  - 10.1111/j.1756-8765.2009.01024.x
ER  -


TY  - GEN
AU  - Zhang, D.
AU  - Dai, X.
AU  - Wang, Y.-F.
TI  - Dynamic temporal pyramid network: A closer look at multi-scale modeling for activity detection
AB  - Recognizing instances at varying scales simultaneously is a fundamental challenge in visual detection problems. While spatial multi-scale modeling has been well studied in object detection, how to effectively apply a multi-scale architecture to temporal models for activity detection is still under-explored. In this paper, we identify three unique challenges that need to be specifically handled for temporal activity detection. To address all these issues, we propose Dynamic Temporal Pyramid Network (DTPN), a new activity detection framework with a multi-scale pyramidal architecture featuring three novel designs: (1) We sample frame sequence dynamically with different frame per seconds (FPS) to construct a natural pyramidal representation for arbitrary-length input videos. (2) We design a two-branch multi-scale temporal feature hierarchy to deal with the inherent temporal scale variation of activity instances. (3) We further exploit the temporal context of activities by appropriately fusing multi-scale feature maps, and demonstrate that both local and global temporal contexts are important. By combining all these components into a uniform network, we end up with a single-shot activity detector involving single-pass inferencing and end-to-end training. Extensive experiments show that the proposed DTPN achieves state-of-the-art performance on the challenging ActvityNet dataset.
PB  - arXiv
PY  - 2018
ST  - Dynamic temporal pyramid network
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/978-3-030-20870-7_44
ER  -


TY  - GEN
AU  - Du, Y.
AU  - Yuan, C.
AU  - Li, B.
AU  - Li, Y.
AU  - Hu, W.
TI  - Interaction-aware spatio-temporal pyramid attention networks for action classification
AB  - Local features at neighboring spatial positions in feature maps have high correlation since their receptive fields are often over-lapped. Self-attention usually uses the weighted sum (or other functions) with internal elements of each local feature to obtain its weight score, which ignores interactions among local features. To address this, we propose an effective interaction-aware self-attention model inspired by PCA to learn attention maps. Furthermore, since different layers in a deep network capture feature maps of different scales, we use these feature maps to construct a spatial pyramid and then utilize multi-scale information to obtain more accurate attention scores, which are used to weight the local features in all spatial positions of feature maps to calculate attention maps. Moreover, our spatial pyramid attention is unrestricted to the number of its input feature maps so it is easily extended to a spatio-temporal version. Finally, our model is embedded in general CNNs to form end-to-end attention networks for action classification. Experimental results show that our method achieves the state-of-the-art results on the UCF101, HMDB51 and untrimmed Charades.
PB  - arXiv
PY  - 2018
ST  - Interaction-aware spatio-temporal pyramid attention networks for action classification
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/tpami.2021.3100277
ER  -


TY  - GEN
AU  - Kornysheva, K.
AU  - Bush, D.
AU  - S Meyer, S.
AU  - Barnes, G.
AU  - Burgess, N.
TI  - Neural competitive queuing of ordinal structure underlies skilled sequential action
AB  - The fluent retrieval and production of movement sequences is essential for a variety of daily activities such as speech, tool-use, musical and athletic performance, but the neural mechanisms underlying sequence planning remain elusive. Here, participants learned sequences of finger presses with different timings and different finger orders, and reproduced them in a magneto-encephalography (MEG) scanner. We classified the MEG patterns immediately preceding each press in the sequence, and examined their dynamics over the production of the whole sequence. Our results confirm a role for the ‘competitive queuing’ of upcoming action representations in the production of learned motor sequences, extending previous computational and non-human primate recording studies to non-invasive measures in humans. In addition, we show that competitive queuing does not simply reflect specific motor actions, but representations of higher-level sequential order that generalise across different motor sequences. Finally, we show that the quality of competitive queuing predicts participants’ production accuracy, and originates from parahippocampal and cerebellar sources. These results suggest that the brain learns and produces multiple behavioural sequences by flexibly combining representations of specific actions with more abstract, parallel representations of sequential structure.
PB  - bioRxiv
PY  - 2018
ST  - Neural competitive queuing of ordinal structure underlies skilled sequential action
Y2  - 2025/05/05/21:54:28
DO  - 10.1101/383364
ER  -


TY  - GEN
AU  - Elices, I.
AU  - Levi, R.
AU  - Arroyo, D.
AU  - Rodriguez, F.B.
AU  - Varona, P.
TI  - Robust dynamical invariants in sequential neural activity
AB  - By studying different sources of temporal variability in central pattern generator circuits, in this paper we unveil distinct aspects of the instantaneous balance between flexibility and robustness in sequential dynamics –a property that characterizes many systems that display neural rhythms. The level of irregularity and coordination was characterized using intrinsic time references and intervals in long recordings of the pyloric central pattern generator. The analysis demonstrated strong robustness of transient dynamics in keeping not only the activation sequences but also specific cycle-by-cycle temporal relationships in the form of dynamical invariants. The rich dynamics of neurons and connections balance flexibility and coordination to readily negotiate the interactions between neurons and produce the resultant rhythm. In particular, two dynamical invariants were identified between time intervals that build the sequence, existing even outside steady states. We suggest that invariant temporal sequence relationships could be present in other networks, including those related to brain rhythms, and underlie rhythm programming and functionality.
PB  - bioRxiv
PY  - 2018
ST  - Robust dynamical invariants in sequential neural activity
Y2  - 2025/05/05/21:54:28
DO  - 10.1101/379909
ER  -


TY  - GEN
AU  - Alwassel, H.
AU  - Heilbron, F.C.
AU  - Escorcia, V.
AU  - Ghanem, B.
TI  - Diagnosing error in temporal action detectors
AB  - Despite the recent progress in video understanding and the continuous rate of improvement in temporal action localization throughout the years, it is still unclear how far (or close?) we are to solving the problem. To this end, we introduce a new diagnostic tool to analyze the performance of temporal action detectors in videos and compare different methods beyond a single scalar metric. We exemplify the use of our tool by analyzing the performance of the top rewarded entries in the latest ActivityNet action localization challenge. Our analysis shows that the most impactful areas to work on are: strategies to better handle temporal context around the instances, improving the robustness w.r.t. the instance absolute and relative size, and strategies to reduce the localization errors. Moreover, our experimental analysis finds the lack of agreement among annotator is not a major roadblock to attain progress in the field. Our diagnostic tool is publicly available to keep fueling the minds of other researchers with additional insights about their algorithms.
PB  - arXiv
PY  - 2018
ST  - Diagnosing error in temporal action detectors
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/978-3-030-01219-9_16
ER  -


TY  - GEN
AU  - Neofotistos, G.
AU  - Mattheakis, M.
AU  - Barmparis, G.D.
AU  - Tsironis, G.P.
AU  - Kaxiras, E.
TI  - Machine learning with observers predicts complex spatiotemporal behavior
AB  - Chimeras and branching are two archetypical complex phenomena that appear in many physical systems; because of their different intrinsic dynamics, they delineate opposite non-trivial limits in the complexity of wave motion and present severe challenges in predicting chaotic and singular behavior in extended physical systems. We report on the long-term forecasting capability of Long Short-Term Memory (LSTM) and reservoir computing (RC) recurrent neural networks, when they are applied to the spatiotemporal evolution of turbulent chimeras in simulated arrays of coupled superconducting quantum interference devices (SQUIDs) or lasers, and branching in the electronic flow of two-dimensional graphene with random potential. We propose a new method in which we assign one LSTM network to each system node except for “observer” nodes which provide continual “ground truth” measurements as input; we refer to this method as “Observer LSTM” (OLSTM). We demonstrate that even a small number of observers greatly improves the data-driven (model-free) long-term forecasting capability of the LSTM networks and provide the framework for a consistent comparison between the RC and LSTM methods. We find that RC requires smaller training datasets than OLSTMs, but the latter require fewer observers. Both methods are benchmarked against Feed-Forward neural networks (FNNs), also trained to make predictions with observers (OFNNs).
PB  - arXiv
PY  - 2018
ST  - Machine learning with observers predicts complex spatiotemporal behavior
Y2  - 2025/05/05/21:54:28
DO  - 10.3389/fphy.2019.00024
ER  -


TY  - GEN
AU  - Paul, S.
AU  - Roy, S.
AU  - Roy-Chowdhury, A.K.
TI  - W-TALC: Weakly-supervised temporal activity localization and classification
AB  - Most activity localization methods in the literature suffer from the burden of frame-wise annotation requirement. Learning from weak labels may be a potential solution towards reducing such manual labeling effort. Recent years have witnessed a substantial influx of tagged videos on the Internet, which can serve as a rich source of weakly-supervised training data. Specifically, the correlations between videos with similar tags can be utilized to temporally localize the activities. Towards this goal, we present W-TALC, a Weakly-supervised Temporal Activity Localization and Classification framework using only video-level labels. The proposed network can be divided into two sub-networks, namely the Two-Stream based feature extractor network and a weakly-supervised module, which we learn by optimizing two complimentary loss functions. Qualitative and quantitative results on two challenging datasets - Thumos14 and ActivityNet1.2, demonstrate that the proposed method is able to detect activities at a fine granularity and achieve better performance than current state-of-the-art methods.
PB  - arXiv
PY  - 2018
ST  - W-TALC
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/978-3-030-01225-0_35
ER  -


TY  - GEN
AU  - Broekens, J.
TI  - A temporal difference reinforcement learning theory of emotion: Unifying emotion, cognition and adaptive behavior
AB  - Emotions are intimately tied to motivation and the adaptation of behavior, and many animal species show evidence of emotions in their behavior. Therefore, emotions must be related to powerful mechanisms that aid survival, and, emotions must be evolutionary continuous phenomena. How and why did emotions evolve in nature, how do events get emotionally appraised, how do emotions relate to cognitive complexity, and, how do they impact behavior and learning? In this article I propose that all emotions are manifestations of reward processing, in particular Temporal Difference (TD) error assessment. Reinforcement Learning (RL) is a powerful computational model for the learning of goal oriented tasks by exploration and feedback. Evidence indicates that RL-like processes exist in many animal species. Key in the processing of feedback in RL is the notion of TD error, the assessment of how much better or worse a situation just became, compared to what was previously expected (or, the estimated gain or loss of utility – or well-being – resulting from new evidence). I propose a TDRL Theory of Emotion and discuss its ramifications for our understanding of emotions in humans, animals and machines, and present psychological, neurobiological and computational evidence in its support.
PB  - arXiv
PY  - 2018
ST  - A temporal difference reinforcement learning theory of emotion
Y2  - 2025/05/05/21:54:28
DO  - 10.4324/9781315746135
ER  -


TY  - GEN
AU  - Yudistira, N.
AU  - Kurita, T.
TI  - Correlation Net : Spatiotemporal multimodal deep learning for action recognition
AB  - This letter describes a network that is able to capture multimodal correlations over arbitrary timestamps. The proposed scheme operates as a complementary, extended network over multimodal CNN. For action recognition, the spatial and temporal streams are vital components of deep Convolutional Neural Network (CNNs), but reducing the occurrence of overfitting and fusing these two streams remain open problems. The existing fusion approach is to average the two streams. To this end, we propose a correlation network with a Shannon fusion to learn a CNN that has already been trained. Long-range video may consist of spatiotemporal correlation over arbitrary times. This correlation can be captured using simple fully connected layers to form the correlation network. This is found to be complementary to the existing network fusion methods. We evaluate our approach on the UCF-101 and HMDB-51 datasets, and the resulting improvement in accuracy demonstrates the importance of multimodal correlation.
PB  - arXiv
PY  - 2018
ST  - Correlation Net 
Y2  - 2025/05/05/21:54:28
DO  - 10.1016/j.image.2019.115731
ER  -


TY  - GEN
AU  - Shou, Z.
AU  - Gao, H.
AU  - Zhang, L.
AU  - Miyazawa, K.
AU  - Chang, S.-F.
TI  - AutoLoc: Weakly-supervised Temporal action localization in untrimmed videos
AB  - Temporal Action Localization (TAL) in untrimmed video is important for many applications. But it is very expensive to annotate the segment-level ground truth (action class and temporal boundary). This raises the interest of addressing TAL with weak supervision, namely only video-level annotations are available during training). However, the state-of-the-art weakly-supervised TAL methods only focus on generat-ing good Class Activation Sequence (CAS) over time but conduct simple thresholding on CAS to localize actions. In this paper, we rst develop a novel weakly-supervised TAL framework called AutoLoc to directly predict the temporal boundary of each action instance. We propose a novel Outer-Inner-Contrastive (OIC) loss to automatically discover the needed segment-level supervision for training such a boundary predictor. Our method achieves dramatically improved performance: Under the IoU threshold 0.5, our method improves mAP on THUMOS'14 from 13.7% to 21.2% and mAP on ActivityNet from 7.4% to 27.3%. It is also very en-couraging to see that our weakly-supervised method achieves comparable results with some fully-supervised methods.
PB  - arXiv
PY  - 2018
ST  - AutoLoc
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/978-3-030-01270-0_10
ER  -


TY  - GEN
AU  - Gao, J.
AU  - Chen, K.
AU  - Nevatia, R.
TI  - CTAP: Complementary temporal action proposal generation
AB  - Temporal action proposal generation is an important task, akin to object proposals, temporal action proposals are intended to capture "clips" or temporal intervals in videos that are likely to contain an action. Previous methods can be divided to two groups: sliding window ranking and actionness score grouping. Sliding windows uniformly cover all segments in videos, but the temporal boundaries are imprecise; group-ing based method may have more precise boundaries but it may omit some proposals when the quality of actionness score is low. Based on the complementary characteristics of these two methods, we propose a novel Complementary Temporal Action Proposal (CTAP) generator. Specifically, we apply a Proposal-level Actionness Trustworthiness Estimator (PATE) on the sliding windows proposals to generate the probabilities indicating whether the actions can be correctly detected by actionness scores, the windows with high scores are collected. The collected sliding windows and actionness proposals are then processed by a temporal convolutional neural network for proposal ranking and boundary adjustment. CTAP outperforms state-of-the-art methods on average recall (AR) by a large margin on THUMOS-14 and ActivityNet 1.3 datasets. We further apply CTAP as a proposal generation method in an existing action detector, and show consistent significant improvements.
PB  - arXiv
PY  - 2018
ST  - CTAP
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/978-3-030-01216-8_5
ER  -


TY  - GEN
AU  - Mettes, P.
AU  - Snoek, C.G.M.
TI  - Spatio-temporal instance learning: Action tubes from class supervision
AB  - The goal of this work is spatio-temporal action localization in videos, using only the supervision from videolevel class labels. The state-of-the-art casts this weaklysupervised action localization regime as a Multiple Instance Learning problem, where instances are a priori computed spatio-temporal proposals. Rather than disconnecting the spatio-temporal learning from the training, we propose Spatio-Temporal Instance Learning, which enables action localization directly from box proposals in video frames. We outline the assumptions of our model and propose a maxmargin objective and optimization with latent variables that enable spatio-temporal learning of actions from video labels. We also provide an efficient linking algorithm and two reranking strategies to facilitate and further improve the action localization. Experimental evaluation on four action datasets demonstrate the effectiveness of our approach for localization from weak supervision. Moreover, we show how to incorporate other supervision levels and mixtures, as a step towards determining optimal supervision strategies for action localization.
PB  - arXiv
PY  - 2018
ST  - Spatio-temporal instance learning
Y2  - 2025/05/05/21:54:28
DO  - 10.1016/j.jvcir.2018.12.019
ER  -


TY  - GEN
AU  - Zhong, J.-X.
AU  - Li, N.
AU  - Kong, W.
AU  - Li, T.H.
AU  - Li, G.
TI  - Step-by-step erasion, one-by-one collection: Aweakly supervised temporal action detector
AB  - Weakly supervised temporal action detection is a Herculean task in understanding untrimmed videos, since no supervisory signal except the video-level category label is available on training data. Under the supervision of category labels, weakly supervised detectors are usually built upon classifiers. However, there is an inherent contradiction between classifier and detector; i.e., a classifier in pursuit of high classification performance prefers top-level discriminative video clips that are extremely fragmentary, whereas a detector is obliged to discover the whole action instance without missing any relevant snippet. To reconcile this contradiction, we train a detector by driving a series of classifiers to find new actionness clips progressively, via step-by-step erasion from a complete video. During the test phase, all we need to do is to collect detection results from the one-by-one trained classifiers at various erasing steps. To assist in the collection process, a fully connected conditional random field is established to refine the temporal localization outputs. We evaluate our approach on two prevailing datasets, THUMOS'14 and ActivityNet. The experiments show that our detector advances state-of-the-art weakly supervised temporal action detection results, and even compares with quite a few strongly supervised methods.
PB  - arXiv
PY  - 2018
ST  - Step-by-step erasion, one-by-one collection
Y2  - 2025/05/05/21:54:28
DO  - 10.1145/3240508.3240511
ER  -


TY  - GEN
AU  - Wurm, M.F.
AU  - Caramazza, A.
TI  - Distinct profiles of temporal and frontoparietal cortex in representing actions across vision and language
AB  - Both temporal and frontoparietal brain areas are associated with the representation of knowledge about the world, in particular about actions. However, what these brain regions represent and precisely how they differ remains unknown. Here, we reveal fundamentally distinct functional profiles of lateral temporal and frontoparietal cortex: Using fMRI-based MVPA we found that frontoparietal areas encode representations of observed actions and corresponding written sentences in an overlapping way, but these representations did not generalize across stimulus type. By contrast, only left lateral posterior temporal cortex (LPTC) encoded action representations that generalize across observed action scenes and sentences. The representational organization of stimulus-general action information in LPTC could be predicted from models that describe basic agent-patient relations (object- and person-directedness) and the general semantic similarity between actions. The match between action videos and sentences in LPTC and its representational profile indicate that this region encodes general, conceptual aspects of actions whereas frontoparietal representations appear to be tied to specific stimulus types.
PB  - bioRxiv
PY  - 2018
ST  - Distinct profiles of temporal and frontoparietal cortex in representing actions across vision and language
Y2  - 2025/05/05/21:54:28
DO  - 10.1101/361220
ER  -


TY  - GEN
AU  - Chéron, G.
AU  - Osokin, A.
AU  - Laptev, I.
AU  - Schmid, C.
TI  - Modeling spatio-temporal human track structure for action localization
AB  - This paper addresses spatio-temporal localization of human actions in video. In order to localize actions in time, we propose a recurrent localization network (RecLNet) designed to model the temporal structure of actions on the level of person tracks. Our model is trained to simultaneously recognize and localize action classes in time and is based on two layer gated recurrent units (GRU) applied separately to two streams, i.e. appearance and optical flow streams. When used together with state-of-the-art person detection and tracking, our model is shown to improve substantially spatio-temporal action localization in videos. The gain is shown to be mainly due to improved temporal localization. We evaluate our method on two recent datasets for spatiotemporal action localization, UCF101-24 and DALY, demonstrating a significant improvement of the state of the art.
PB  - arXiv
PY  - 2018
ST  - Modeling spatio-temporal human track structure for action localization
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/iccv.2015.362
ER  -


TY  - GEN
AU  - He, D.
AU  - Li, F.
AU  - Zhao, Q.
AU  - Fu, Y.
AU  - Wen, S.
TI  - Exploiting spatial-temporal modelling and multi-modal fusion for human action recognition
AB  - In this report, our approach to tackling the task of ActivityNet 2018 Kinetics-600 challenge is described in detail. Though spatial-temporal modelling methods, which adopt either such end-to-end framework as I3D [1] or two-stage frameworks (i.e., CNN+RNN), have been proposed in existing state-of-the-arts for this task, video modelling is far from being well solved. In this challenge, we propose spatial-temporal network (StNet) for better joint spatial-temporal modelling and comprehensively video understanding. Besides, given that multimodal information is contained in video source, we manage to integrate both early-fusion and later-fusion strategy of multi-modal information via our proposed improved temporal Xception network (iTXN) for video understanding. Our StNet RGB single model achieves 78.99% top-1 precision in the Kinetics-600 validation set and that of our improved temporal Xception network which integrates RGB, flow and audio modalities is up to 82.35%. After model ensemble, we achieve top-1 precision as high as 85.0% on the validation set and rank No.1 among all submissions.
PB  - arXiv
PY  - 2018
ST  - Exploiting spatial-temporal modelling and multi-modal fusion for human action recognition
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/978-981-96-2468-3_12
ER  -


TY  - GEN
AU  - Long, J.
AU  - Zhu, L.
AU  - Yang, Z.
AU  - Zhang, C.
AU  - Yuan, X.
TI  - Temporal activity path based character correction in social networks
AB  - Vast amount of multimedia data contains massive and multifarious social in-formation which is used to construct large-scale social networks. In a complex social network, a character should be ideally denoted by one and only one vertex. However, it is pervasive that a character is denoted by two or more vertices with different names, thus it is usually considered as multiple, different characters. This problem causes incorrectness of results in network analysis and mining. The factual challenge is that character unique-ness is hard to correctly confirm due to lots of complicated factors, e.g. name changing and anonymization, leading to character duplication. Early, limited research has shown that previous methods depended overly upon supplementary attribute information from databases. In this paper, we propose a novel method to merge the character vertices which refer to as the same entity but are denoted with different names. With this method, we firstly build the relationship network among characters based on records of social activities participated, which are extracted from multimedia sources. Then define temporal activity paths (TAPs) for each character over time. After that, we measure similarity of the TAPs for any two characters. If the similarity is high enough, the two vertices should be con-sidered to the same character. Based on TAPs, we can determine whether to merge the two character vertices. Our experiments shown that this solution can accurately confirm character uniqueness in large-scale social network.
PB  - arXiv
PY  - 2018
ST  - Temporal activity path based character correction in social networks
Y2  - 2025/05/05/21:54:28
DO  - 10.1155/2018/2058670
ER  -


TY  - GEN
AU  - Diba, A.
AU  - Fayyaz, M.
AU  - Sharma, V.
AU  - Gall, J.
AU  - van Gool, L.
TI  - Spatio-temporal channel correlation networks for action classification
AB  - The work in this paper is driven by the question if spatio-temporal correlations are enough for 3D convolutional neural networks (CNN)? Most of the traditional 3D networks use local spatio-temporal features. We introduce a new block that models correlations between channels of a 3D CNN with respect to temporal and spatial features. This new block can be added as a residual unit to different parts of 3D CNNs. We name our novel block’Spatio-Temporal Channel Correlation’ (STC). By embedding this block to the current state-of-the-art architectures such as ResNext and ResNet, we improved the performance by 2-3% on Kinetics dataset. Our experiments show that adding STC blocks to current state-of-the-art architectures outperforms the state-of-the-art methods on the HMDB51, UCF101 and Kinetics datasets. The other issue in training 3D CNNs is about training them from scratch with a huge labeled dataset to get a reasonable performance. So the knowledge learned in 2D CNNs is completely ignored. Another contribution in this work is a simple and effective technique to transfer knowledge from a pre-trained 2D CNN to a randomly initialized 3D CNN for a stable weight initialization. This allows us to significantly reduce the number of training samples for 3D CNNs. Thus, by fine-tuning this network, we beat the performance of generic and recent methods in 3D CNNs, which were trained on large video datasets, e.g. Sports-1M, and fine-tuned on the target datasets, e.g. HMDB51/UCF101. 1
PB  - arXiv
PY  - 2018
ST  - Spatio-temporal channel correlation networks for action classification
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/978-3-030-01225-0_18
ER  -


TY  - GEN
AU  - Gauy, M.M.
AU  - Lengler, J.
AU  - Einarsson, H.
AU  - Yanik, M.F.
AU  - Steger, A.
TI  - A hippocampal model for behavioral time acquisition and fast bidirectional replay of spatio-temporal memory sequences
AB  - The hippocampus is known to play a crucial role in the formation of long-term memory. For this, fast replays of previously experienced activities during sleep or after reward experiences are believed to be crucial. But how such replays are generated is still completely unclear. In this paper we propose a possible mechanism for this: we present a model that can store experienced trajectories on a behavioral timescale after a single run, and can subsequently bidirectionally replay such trajectories, thereby omitting any specifics of the previous behavior like speed, etc, but allowing repetitions of events, even with different subsequent events. Our solution builds on well-known concepts, one-shot learning and synfire chains, enhancing them by additional mechanisms using global inhibition and disinhibition. For replays our approach relies on dendritic spikes and cholinergic modulation, as supported by experimental data. We also hypothesize a functional role of disinhibition as a pacemaker during behavioral time.
PB  - bioRxiv
PY  - 2018
ST  - A hippocampal model for behavioral time acquisition and fast bidirectional replay of spatio-temporal memory sequences
Y2  - 2025/05/05/21:54:28
DO  - 10.1101/343988
ER  -


TY  - GEN
AU  - Zich, C.
AU  - Woolrich, M.W.
AU  - Becker, R.
AU  - Quinn, A.J.
AU  - Stagg, C.J.
TI  - Motor learning shapes temporal activity in human sensorimotor cortex
AB  - Although neuroimaging techniques have provided vital insights into the anatomical regions involved in motor learning, the underlying changes in temporal dynamics are not well understood. Using magnetoencephalography and Hidden Markov Modelling to model the dynamics of neural oscillations on data-adaptive time-scales, we detected specific changes in movement-related sensorimotor β-activity during practice of a self-paced sequential visuo-motor task. The behaviourally-relevant neural signature generalised to another motor task, emphasising the centrality of β-activity in motor plasticity.
PB  - bioRxiv
PY  - 2018
ST  - Motor learning shapes temporal activity in human sensorimotor cortex
Y2  - 2025/05/05/21:54:28
DO  - 10.1101/345421
ER  -


TY  - GEN
AU  - Kikumoto, A.
AU  - Mayr, U.
TI  - Decoding Hierarchical Control of Sequential Behavior in Oscillatory EEG Activity
AB  - Despite strong theoretical reasons for assuming that abstract representations organize complex action sequences in terms of subplans (chunks) and sequential positions, we lack methods to directly track such content-independent, hierarchical representations in humans. We applied time-resolved, multivariate decoding analysis to the pattern of rhythmic EEG activity that was registered while participants planned and executed individual elements from pre-learned, structured sequences. Across three experiments, the theta and alpha-band activity independently coded basic elements and abstract control representations, in particular the ordinal position of basic elements, but also the identity and position of chunks. Further, a robust representation of higher-level, chunk identity information was only found in individuals with above-median working memory capacity, potentially providing a neural-level explanation for working-memory differences in sequential performance. Our results suggest that by decoding oscillations we can track how the cognitive system traverses through the states of a hierarchical control structure.
PB  - bioRxiv
PY  - 2018
ST  - Decoding Hierarchical Control of Sequential Behavior in Oscillatory EEG Activity
Y2  - 2025/05/05/21:54:28
DO  - 10.7554/elife.38550
ER  -


TY  - GEN
AU  - Sekara, V.
AU  - Mones, E.
AU  - Jonsson, H.
TI  - Temporal limits of privacy in human behavior
AB  - Large-scale collection of human behavioral data by companies raises serious privacy concerns. We show that behavior captured in the form of application usage data collected from smartphones is highly unique even in very large datasets encompassing millions of individuals. This makes behavior-based re-identification of users across datasets possible. We study 12 months of data from 3.5 million users and show that four apps are enough to uniquely re-identify 91.2% of users using a simple strategy based on public information. Furthermore, we show that there is seasonal variability in uniqueness and that application usage fingerprints drift over time at an average constant rate.
PB  - arXiv
PY  - 2018
ST  - Temporal limits of privacy in human behavior
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/978-3-540-79856-9_18
ER  -


TY  - GEN
AU  - Lin, T.
AU  - Zhao, X.
AU  - Su, H.
AU  - Wang, C.
AU  - Yang, M.
TI  - BSN: Boundary sensitive network for temporal action proposal generation
AB  - Temporal action proposal generation is an important yet challenging problem, since temporal proposals with rich action content are indispensable for analysing real-world videos with long duration and high proportion irrelevant content. This problem requires methods not only generating proposals with precise temporal boundaries, but also retrieving proposals to cover truth action instances with high recall and high overlap using relatively fewer proposals. To address these difficulties, we introduce an effective proposal generation method, named Boundary-Sensitive Network (BSN), which adopts "local to global" fashion. Locally, BSN first locates temporal boundaries with high probabilities, then directly combines these boundaries as proposals. Globally, with Boundary-Sensitive Proposal feature, BSN retrieves proposals by evaluating the confidence of whether a proposal contains an action within its region. We conduct experiments on two challenging datasets: ActivityNet-1.3 and THUMOS14, where BSN outperforms other state-of-the-art temporal action proposal generation methods with high recall and high temporal precision. Finally, further experiments demonstrate that by combining existing action classifiers, our method significantly improves the state-of-the-art temporal action detection performance.
PB  - arXiv
PY  - 2018
ST  - BSN
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/978-3-030-01225-0_1
ER  -


TY  - GEN
AU  - D'Silva, K.
AU  - Noulas, A.
AU  - Musolesi, M.
AU  - Mascolo, C.
AU  - Sklar, M.
TI  - Predicting the temporal activity patterns of new venues
AB  - Estimating revenue and business demand of a newly opened venue is paramount as these early stages often involve critical decisions such as first rounds of staffing and resource allocation. Traditionally, this estimation has been performed through coarse-grained measures such as observing numbers in local venues or venues at similar places (e.g., coffee shops around another station in the same city). The advent of crowdsourced data from devices and services carried by individuals on a daily basis has opened up the possibility of performing better predictions of temporal visitation patterns for locations and venues. In this paper, using mobility data from Foursquare, a location-centric platform, we treat venue categories as proxies for urban activities and analyze how they become popular over time. The main contribution of this work is a prediction framework able to use characteristic temporal signatures of places together with k-nearest neighbor metrics capturing similarities among urban regions, to forecast weekly popularity dynamics of a new venue establishment in a city neighborhood. We further show how we are able to forecast the popularity of the new venue after one month following its opening by using locality and temporal similarity as features. For the evaluation of our approach we focus on London. We show that temporally similar areas of the city can be successfully used as inputs of predictions of the visit patterns of new venues, with an improvement of 41% compared to a random selection of wards as a training set for the prediction task. We apply these concepts of temporally similar areas and locality to the real-time predictions related to new venues and show that these features can effectively be used to predict the future trends of a venue. Our findings have the potential to impact the design of location-based technologies and decisions made by new business owners.
PB  - arXiv
PY  - 2018
ST  - Predicting the temporal activity patterns of new venues
Y2  - 2025/05/05/21:54:28
DO  - 10.1140/epjds/s13688-018-0142-z
ER  -


TY  - GEN
AU  - An, G.
AU  - Zhou, W.
AU  - Wu, Y.
AU  - Zheng, Z.
AU  - Liu, Y.
TI  - Squeeze-and-excitation on spatial and temporal deep feature space for action recognition
AB  - Spatial and temporal features are two key and complementary information for human action recognition. In order to make full use of the intra-frame spatial characteristics and inter-frame temporal relationships, we propose the Squeezeand-Excitation Long-term Recurrent Convolutional Networks (SE-LRCN) for human action recognition. The Squeeze and Excitation operations are used to implement the feature recalibration. In SE-LRCN, Squeeze-and-Excitation ResNet-34 (SE-ResNet-34) network is adopted to extract spatial features to enhance the dependencies and importance of feature channels of pixel granularity. We also propose the Squeeze-and-Excitation Long Short-Term Memory (SE-LSTM) network to model the temporal relationship, and to enhance the dependencies and importance of feature channels of frame granularity. We evaluate the proposed model on two challenging benchmarks, HMDB51 and UCF101, and the proposed SE-LRCN achieves the competitive results with the state-of-the-art.
PB  - arXiv
PY  - 2018
ST  - Squeeze-and-excitation on spatial and temporal deep feature space for action recognition
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/icsp.2018.8652287
ER  -


TY  - GEN
AU  - Sanchez, R.G.
AU  - Ryley Parrish, R.
AU  - Rich, M.
AU  - Riley, K.
AU  - Lubin, F.D.
TI  - HUMAN AND RODENT TEMPORAL LOBE EPILEPSY IS CHARACTERIZED BY CHANGES IN O-GLCNAC HOMEOSTASIS THAT CAN BE REVERSED TO DAMPEN EPILEPTIFORM ACTIVITY
AB  - We propose a learning approach for mapping context-dependent sequential instructions to actions. We address the problem of discourse and state dependencies with an attention-based model that considers both the history of the interaction and the state of the world. To train from start and goal states without access to demonstrations, we propose SESTRA, a learning algorithm that takes advantage of single-step reward observations and immediate expected reward maximization. We evaluate on the SCONE domains, and show absolute accuracy improvements of 9.8%-25.3% across the domains over approaches that use high-level logical representations.
PB  - bioRxiv
PY  - 2018
ST  - HUMAN AND RODENT TEMPORAL LOBE EPILEPSY IS CHARACTERIZED BY CHANGES IN O-GLCNAC HOMEOSTASIS THAT CAN BE REVERSED TO DAMPEN EPILEPTIFORM ACTIVITY
Y2  - 2025/05/05/21:54:28
DO  - 10.1101/330738
ER  -


TY  - GEN
AU  - Suhr, A.
AU  - Artzi, Y.
TI  - Situated mapping of sequential instructions to actions with single-step reward observation
AB  - We propose a learning approach for mapping context-dependent sequential instructions to actions. We address the problem of discourse and state dependencies with an attention-based model that considers both the history of the interaction and the state of the world. To train from start and goal states without access to demonstrations, we propose SESTRA, a learning algorithm that takes advantage of single-step reward observations and immediate expected reward maximization. We evaluate on the SCONE domains, and show absolute accuracy improvements of 9.8%-25.3% across the domains over approaches that use high-level logical representations.
PB  - arXiv
PY  - 2018
ST  - Situated mapping of sequential instructions to actions with single-step reward observation
Y2  - 2025/05/05/21:54:28
DO  - 10.18653/v1/p18-1193
ER  -


TY  - GEN
AU  - Heidarivincheh, F.
AU  - Mirmehdi, M.
AU  - Damen, D.
TI  - Action completion: A temporal model for moment detection
AB  - We introduce completion moment detection for actions - the problem of locating the moment of completion, when the action’s goal is confidently considered achieved. The paper proposes a joint classification-regression recurrent model that predicts completion from a given frame, and then integrates frame-level contributions to detect sequence-level completion moment. We introduce a recurrent voting node that predicts the frame’s relative position of the completion moment by either classification or regression. The method is also capable of detecting incompletion. For example, the method is capable of detecting a missed ball-catch, as well as the moment at which the ball is safely caught. We test the method on 16 actions from three public datasets, covering sports as well as daily actions. Results show that when combining contributions from frames prior to the completion moment as well as frames post completion, the completion moment is detected within one second in 89% of all tested sequences.
PB  - arXiv
PY  - 2018
ST  - Action completion
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/iccvw.2019.00150
ER  -


TY  - GEN
AU  - Bannier, D.
AU  - Wearden, J.
AU  - Le Dantec, C.C.
AU  - Rebaï, M.
TI  - Differences in the temporal processing between identification and categorization of durations: A behavioral and ERP study
AB  - This study examined how different forms of decision-making modulate time perception. Participants performed temporal bisection and generalization tasks, requiring them to either categorize a stimulus duration as more similar to short or long standards (bisection), or identify whether or not a duration was the same as a previously-presented standard (generalization). They responded faster in the bisection task than in the generalization one for long durations. This behavioral effect was accompanied by modulation of event-related potentials (ERPs). More specifically, between 500 ms and 600 ms after stimulus offset, a late positive component (LPC), appearing in the centro-parietal region, showed lower amplitude in the bisection task than in the generalization one, for long durations, mirroring the behavioral result. Before (200-500 ms) and after (600-800 ms) this window, the amplitude of the LPC was globally larger in the generalization paradigm, independently of the presented duration. Finally, the latency of the LPC’s peak was earlier for long durations than for the short ones, indicating that the decision about the former stimuli was made earlier than for the latter ones. Taken together, these results indicate that the categorization of durations engages fewer cognitive resources than their identification.
PB  - bioRxiv
PY  - 2018
ST  - Differences in the temporal processing between identification and categorization of durations
Y2  - 2025/05/05/21:54:28
DO  - 10.1101/315887
ER  -


TY  - GEN
AU  - Si, C.
AU  - Jing, Y.
AU  - Wang, W.
AU  - Wang, L.
AU  - Tan, T.
TI  - Skeleton-based action recognition with spatial reasoning and temporal stack learning
AB  - Skeleton-based action recognition has made great progress recently, but many problems still remain unsolved. For example, the representations of skeleton sequences captured by most of the previous methods lack spatial structure information and detailed temporal dynamics features. In this paper, we propose a novel model with spatial reasoning and temporal stack learning (SR-TSL) for skeleton-based action recognition, which consists of a spatial reasoning network (SRN) and a temporal stack learning network (TSLN). The SRN can capture the high-level spatial structural information within each frame by a residual graph neural network, while the TSLN can model the detailed temporal dynamics of skeleton sequences by a composition of multiple skip-clip LSTMs. During training, we propose a clip-based incremental loss to optimize the model. We perform extensive experiments on the SYSU 3D Human-Object Interaction dataset and NTU RGB+D dataset and verify the effectiveness of each network of our model. The comparison results illustrate that our approach achieves much better results than the state-of-the-art methods.
PB  - arXiv
PY  - 2018
ST  - Skeleton-based action recognition with spatial reasoning and temporal stack learning
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/978-3-030-01246-5_7
ER  -


TY  - GEN
AU  - Chao, Y.-W.
AU  - Vijayanarasimhan, S.
AU  - Seybold, B.
AU  - Deng, J.
AU  - Sukthankar, R.
TI  - Rethinking the faster R-CNN architecture for temporal action localization
AB  - We propose TAL-Net, an improved approach to temporal action localization in video that is inspired by the Faster RCNN object detection framework. TAL-Net addresses three key shortcomings of existing approaches: (1) we improve receptive field alignment using a multi-scale architecture that can accommodate extreme variation in action durations; (2) we better exploit the temporal context of actions for both proposal generation and action classification by appropriately extending receptive fields; and (3) we explicitly consider multi-stream feature fusion and demonstrate that fusing motion late is important. We achieve state-ofthe-art performance for both action proposal and localization on THUMOS'14 detection benchmark and competitive performance on ActivityNet challenge.
PB  - arXiv
PY  - 2018
ST  - Rethinking the faster R-CNN architecture for temporal action localization
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/cvpr.2018.00124
ER  -


TY  - GEN
AU  - Qiu, H.
AU  - Zheng, Y.
AU  - Ye, H.
AU  - Wang, F.
AU  - He, L.
TI  - Precise Temporal Action Localization by Evolving Temporal Proposals
AB  - Locating actions in long untrimmed videos has been a challenging problem in video content analysis. The performances of existing action localization approaches remain unsatisfactory in precisely determining the beginning and the end of an action. Imitating the human perception procedure with observations and refinements, we propose a novel three-phase action localization framework. Our framework is embedded with an Actionness Network to generate initial proposals through frame-wise similarity grouping, and then a Refinement Network to conduct boundary adjustment on these proposals. Finally, the refined proposals are sent to a Localization Network for further fine-grained location regression. The whole process can be deemed as multi-stage refinement using a novel non-local pyramid feature under various temporal granularities. We evaluate our framework on THUMOS14 benchmark and obtain a significant improvement over the state-of-the-arts approaches. Specifically, the performance gain is remarkable under precise localization with high IoU thresholds. Our proposed framework achieves mAP@IoU=0.5 of 34.2%.
PB  - arXiv
PY  - 2018
ST  - Precise Temporal Action Localization by Evolving Temporal Proposals
Y2  - 2025/05/05/21:54:28
DO  - 10.1145/3206025.3206029
ER  -


TY  - GEN
AU  - Knöll, J.
AU  - Pillow, J.W.
AU  - Huk, A.C.
TI  - An Efficient and Naturalistic Experimental Paradigm for Spatiotemporal Assessment of Visual Behavior and Neurophysiology
AB  - We introduce a novel paradigm eliciting a continuous, naturalistic behavior in multiple primate species, and which supports quantitative characterization of sensory function. The stimulus comprises a large field of moving dots, globally specifying optic flow, but locally perturbed to allow spatiotemporal quantification of the goal-directed shifting of gaze towards the focus of flow-field expansion. We provide proof of concept that humans, macaques, and marmosets engage in this task with minimal training and for long durations, supporting collection of extensive neurophysiological data within an experimental session. An extension to selection between alternatives is demonstrated, and a large range of further generalizations across species and aspects of sensorimotor and cognitive function is described. This paradigm provides a rich, quantitative, and efficient means of assessing behavior, complementing classical trialbased tasks. It allows for direct comparisons across species with different cognitive capacities, and provides additional power for concurrent neurophysiological measures.
PB  - SSRN
PY  - 2018
ST  - An Efficient and Naturalistic Experimental Paradigm for Spatiotemporal Assessment of Visual Behavior and Neurophysiology
Y2  - 2025/05/05/21:54:28
DO  - 10.2139/ssrn.3155786
ER  -


TY  - GEN
AU  - Escorcia, V.
AU  - Dao, C.D.
AU  - Jain, M.
AU  - Ghanem, B.
AU  - Snoek, C.
TI  - Guess where? Actor-supervision for spatiotemporal action localization
AB  - This paper addresses the problem of spatiotemporal localization of actions in videos. Compared to leading approaches, which all learn to localize based on carefully annotated boxes on training video frames, we adhere to a weakly-supervised solution that only requires a video class label. We introduce an actor-supervised architecture that exploits the inherent compositionality of actions in terms of actor transformations, to localize actions. We make two contributions. First, we propose actor proposals derived from a detector for human and non-human actors intended for images, which is linked over time by Siamese similarity matching to account for actor deformations. Second, we propose an actor-based attention mechanism that enables the localization of the actions from action class labels and actor proposals and is end-to-end trainable. Experiments on three human and non-human action datasets show actor supervision is state-of-the-art for weakly-supervised action localization and is even competitive to some fully-supervised alternatives.
PB  - arXiv
PY  - 2018
ST  - Guess where? Actor-supervision for spatiotemporal action localization
Y2  - 2025/05/05/21:54:28
DO  - 10.1016/j.cviu.2019.102886
ER  -


TY  - GEN
AU  - Capone, C.
AU  - Gigante, G.
AU  - Giudice, P.D.
TI  - Spontaneous activity emerging from an inferred network model captures complex spatio-temporal dynamics of spike data
AB  - Inference methods are widely used to recover effective models from observed data. However, few studies attempted to investigate the dynamics of inferred models in neuroscience, and none, to our knowledge, at the network level. We introduce a principled modification of a widely used generalized linear model (GLM), and learn its structural and dynamic parameters from in-vitro spike data. The spontaneous activity of the new model captures prominent features of the non-stationary and non-linear dynamics displayed by the biological network, where the reference GLM largely fails, and also reflects fine-grained spatio-temporal dynamical features. Two ingredients were key for success. The first is a saturating transfer function: beyond its biological plausibility, it limits the neurons information transfer, improving robustness against endogenous and external noise. The second is a super-Poisson spikes generative mechanism; it accounts for the undersampling of the network, and allows the model neuron to flexibly incorporate the observed activity fluctuations.
PB  - bioRxiv
PY  - 2018
ST  - Spontaneous activity emerging from an inferred network model captures complex spatio-temporal dynamics of spike data
Y2  - 2025/05/05/21:54:28
DO  - 10.1038/s41598-018-35433-0
ER  -


TY  - GEN
AU  - Farha, Y.A.
AU  - Richard, A.
AU  - Gall, J.
TI  - When will you do what? Anticipating temporal occurrences of activities
AB  - Analyzing human actions in videos has gained increased attention recently. While most works focus on classifying and labeling observed video frames or anticipating the very recent future, making long-term predictions over more than just a few seconds is a task with many practical applications that has not yet been addressed. In this paper, we propose two methods to predict a considerably large amount of future actions and their durations. Both, a CNN and an RNN are trained to learn future video labels based on previously seen content. We show that our methods generate accurate predictions of the future even for long videos with a huge amount of different actions and can even deal with noisy or erroneous input information.
PB  - arXiv
PY  - 2018
ST  - When will you do what? Anticipating temporal occurrences of activities
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/cvpr.2018.00560
ER  -


TY  - GEN
AU  - Kleinepier, T.
AU  - van Ham, M.
TI  - The Temporal Dynamics of Neighborhood Disadvantage in Childhood and Subsequent Problem Behavior in Adolescence
AB  - Research on neighborhood effects has increasingly focused on how long children have lived in a deprived neighborhood during childhood (duration), but has typically ignored when in childhood the exposure occurred (timing) and whether circumstances were improving or deteriorating (sequencing). Using Dutch register data, we applied sequence analysis to simultaneously capture duration, timing, and sequencing of exposure to neighborhood (dis)advantage in childhood. Compared to children who lived in a deprived neighborhood throughout childhood, we found that children who were exposed to neighborhood deprivation only during adolescence were equally likely to become a teenage parent and were more likely to drop out of school. Unexpectedly, children who lived in an affluent neighbor-hood throughout childhood were most likely to engage in delinquent behavior.
PB  - SSRN
PY  - 2018
ST  - The Temporal Dynamics of Neighborhood Disadvantage in Childhood and Subsequent Problem Behavior in Adolescence
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/s10964-018-0878-6
ER  -


TY  - GEN
AU  - Pandhe, N.
AU  - Rada, B.
AU  - Quinn, S.
TI  - Generative spatiotemporal modeling of neutrophil behavior
AB  - Cell motion and appearance have a strong correlation with cell cycle and disease progression. Many contemporary efforts in machine learning utilize spatio-temporal models to predict a cell's physical state and, consequently, the advancement of disease. Alternatively, generative models learn the underlying distribution of the data, creating holistic representations that can be used in learning. In this work, we propose an aggregate model that combine Generative Adversarial Networks (GANs) and Autoregressive (AR) models to predict cell motion and appearance in human neutrophils imaged by differential interference contrast (DIC) microscopy. We bifurcate the task of learning cell statistics by leveraging GANs for the spatial component and AR models for the temporal component. The aggregate model learned results offer a promising computational environment for studying changes in organellar shape, quantity, and spatial distribution over large sequences.
PB  - arXiv
PY  - 2018
ST  - Generative spatiotemporal modeling of neutrophil behavior
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/isbi.2018.8363732
ER  -


TY  - GEN
AU  - Capone, C.
AU  - Gigante, G.
AU  - Del Giudice, P.
TI  - Spontaneous activity emerging from an inferred network model captures complex temporal dynamics of spiking data
AB  - The combination of new recording techniques in neuroscience and powerful inference methods recently held the promise to recover useful effective models, at the single neuron or network level, directly from observed data. The value of a model of course should critically depend on its ability to reproduce the dynamical behavior of the modeled system; however, few attempts have been made to inquire into the dynamics of inferred models in neuroscience, and none, to our knowledge, at the network level. Here we introduce a principled modification of a widely used generalized linear model (GLM), and learn its structural and dynamic parameters from ex-vivo spiking data. We show that the new model is able to capture the most prominent features of the highly non-stationary and non-linear dynamics displayed by the biological network, where the reference GLM largely fails. Two ingredients turn out to be key for success. The first one is a bounded transfer function that makes the single neuron able to respond to its input in a saturating fashion; beyond its biological plausibility such property, by limiting the capacity of the neuron to transfer information, makes the coding more robust in the face of the highly variable network activity, and noise. The second ingredient is a super-Poisson spikes generative probabilistic mechanism; this feature, that accounts for the fact that observations largely undersample the network, allows the model neuron to more flexibly incorporate the observed activity fluctuations. Taken together, the two ingredients, without increasing complexity, allow the model to capture the key dynamic elements. When left free to generate its spontaneous activity, the inferred model proved able to reproduce not only the non-stationary population dynamics of the network, but also part of the fine-grained structure of the dynamics at the single neuron level.
PB  - arXiv
PY  - 2018
ST  - Spontaneous activity emerging from an inferred network model captures complex temporal dynamics of spiking data
Y2  - 2025/05/05/21:54:28
DO  - 10.1038/s41598-018-35433-0
ER  -


TY  - GEN
AU  - Levanova, T.A.
AU  - Kazakov, A.O.
AU  - Korotkov, A.G.
AU  - Osipov, G.V.
TI  - The impact of electrical couplings on the sequential bursting activity in the ensemble of inhibitory coupled Van der Pol elements
AB  - The new phenomenological model of the ensemble of three neurons with chemical (synaptic) and electrical couplings has been studied. One neuron is modeled by a single Van der Pol oscillator. The influence of the electrical coupling strength and the frequency mismatch between the elements to the regime of sequential activity is investigated.
PB  - arXiv
PY  - 2018
ST  - The impact of electrical couplings on the sequential bursting activity in the ensemble of inhibitory coupled Van der Pol elements
Y2  - 2025/05/05/21:54:28
DO  - 10.1140/epjst/e2013-02026-7
ER  -


TY  - GEN
AU  - Cherian, A.
AU  - Sra, S.
AU  - Gould, S.
AU  - Hartley, R.
TI  - Non-linear temporal subspace representations for activity recognition
AB  - Representations that can compactly and effectively capture the temporal evolution of semantic content are important to computer vision and machine learning algorithms that operate on multi-variate time-series data. We investigate such representations motivated by the task of human action recognition. Here each data instance is encoded by a multivariate feature (such as via a deep CNN) where action dynamics are characterized by their variations in time. As these features are often non-linear, we propose a novel pooling method, kernelized rank pooling, that represents a given sequence compactly as the pre-image of the parameters of a hyperplane in a reproducing kernel Hilbert space, projections of data onto which captures their temporal order. We develop this idea further and show that such a pooling scheme can be cast as an order-constrained kernelized PCA objective. We then propose to use the parameters of a kernelized low-rank feature subspace as the representation of the sequences. We cast our formulation as an optimization problem on generalized Grassmann manifolds and then solve it efficiently using Riemannian optimization techniques. We present experiments on several action recognition datasets using diverse feature modalities and demonstrate state-of-the-art results.
PB  - arXiv
PY  - 2018
ST  - Non-linear temporal subspace representations for activity recognition
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/cvpr.2018.00234
ER  -


TY  - GEN
AU  - Zang, J.
AU  - Wang, L.
AU  - Liu, Z.
AU  - Hua, G.
AU  - Zheng, N.
TI  - Attention-based Temporal Weighted Convolutional Neural Network for Action Recognition
AB  - Research in human action recognition has accelerated significantly since the introduction of powerful machine learning tools such as Convolutional Neural Networks (CNNs). However, effective and efficient methods for incorporation of temporal information into CNNs are still being actively explored in the recent literature. Motivated by the popular recurrent attention models in the research area of natural language processing, we propose the Attention-based Temporal Weighted CNN (ATW), which embeds a visual attention model into a temporal weighted multi-stream CNN. This attention model is simply implemented as temporal weighting yet it effectively boosts the recognition performance of video representations. Besides, each stream in the proposed ATW framework is capable of end-to-end training, with both network parameters and temporal weights optimized by stochastic gradient descent (SGD) with backpropagation. Our experiments show that the proposed attention mechanism contributes substantially to the performance gains with the more discriminative snippets by focusing on more relevant video segments.
PB  - arXiv
PY  - 2018
ST  - Attention-based Temporal Weighted Convolutional Neural Network for Action Recognition
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/978-3-319-92007-8_9
ER  -


TY  - GEN
AU  - Zhang, Y.
AU  - Sun, H.
AU  - Tang, S.
AU  - Neumann, H.
TI  - Temporal human action segmentation via dynamic clustering
AB  - We present an effective dynamic clustering algorithm for the task of temporal human action segmentation, which has comprehensive applications such as robotics, motion analysis, and patient monitoring. Our proposed algorithm is unsupervised, fast, generic to process various types of features, and applicable in both the online and offline settings. We perform extensive experiments of processing data streams, and show that our algorithm achieves the state-of-the-art results for both online and offline settings.
PB  - arXiv
PY  - 2018
ST  - Temporal human action segmentation via dynamic clustering
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/978-3-031-78354-8_20
ER  -


TY  - GEN
AU  - Böhme, M.
TI  - STADS: Software Testing as Species Discovery Spatial and Temporal Extrapolation from Tested Program Behaviors
AB  - A fundamental challenge of software testing is the statistically well-grounded extrapolation from program behaviors observed during testing. For instance, a security researcher who has run the fuzzer for a week has currently no means (i) to estimate the total number of feasible program branches, given that only a fraction has been covered so far, (ii) to estimate the additional time required to cover 10% more branches (or to estimate the coverage achieved in one more day, resp.), or (iii) to assess the residual risk that a vulnerability exists when no vulnerability has been discovered. Failing to discover a vulnerability, does not mean that none exists—even if the fuzzer was run for a week (or a year). Hence, testing provides no formal correctness guarantees. In this article, I establish an unexpected connection with the otherwise unrelated scientific field of ecology, and introduce a statistical framework that models Software Testing and Analysis as Discovery of Species (STADS). For instance, in order to study the species diversity of arthropods in a tropical rain forest, ecologists would first sample a large number of individuals from that forest, determine their species, and extrapolate from the properties observed in the sample to properties of the whole forest. The estimation (i) of the total number of species, (ii) of the additional sampling effort required to discover 10% more species, or (iii) of the probability to discover a new species are classical problems in ecology. The STADS framework draws from over three decades of research in ecological biostatistics to address the fundamental extrapolation challenge for automated test generation. Our preliminary empirical study demonstrates a good estimator performance even for a fuzzer with adaptive sampling bias—AFL, a state-of-the-art vulnerability detection tool. The STADS framework provides statistical correctness guarantees with quantifiable accuracy.
PB  - arXiv
PY  - 2018
ST  - STADS
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/978-3-030-38961-1_6
ER  -


TY  - GEN
AU  - Thomas, R.M.
AU  - Sanctis, T.D.
AU  - Gazzola, V.
AU  - Keysers, C.
TI  - Where and how our brain represents the temporal structure of observed action
AB  - Reacting faster to the behavior of others provides evolutionary advantages. Reacting to unpredictable events takes hundreds of milliseconds. Understanding where and how the brain represents what actions are likely to follow one another is therefore important. Everyday actions are predictable sequences of acts, yet neuroscientists focus on how brains responds to unexpected, individual motor acts. Using fMRI we show the brain encodes sequence-specific information in the motor system. Using EEG, we show visual responses were faster and smaller for predictable sequences that recruit the motor system. This study shifts the study of action observation from single acts to motor sequences, informs how we adapt to the actions of others and suggests the motor system may implement perceptual predictive coding.
PB  - bioRxiv
PY  - 2018
ST  - Where and how our brain represents the temporal structure of observed action
Y2  - 2025/05/05/21:54:28
DO  - 10.1016/j.neuroimage.2018.08.056
ER  -


TY  - GEN
AU  - Li, C.
AU  - Cui, Z.
AU  - Zheng, W.
AU  - Xu, C.
AU  - Yang, J.
TI  - Spatio-temporal graph convolution for skeleton based action recognition
AB  - Variations of human body skeletons may be considered as dynamic graphs, which are generic data representation for numerous real-world applications. In this paper, we propose a spatio-temporal graph convolution (STGC) approach for assembling the successes of local convolutional filtering and sequence learning ability of autoregressive moving average. To encode dynamic graphs, the constructed multi-scale local graph convolution filters, consisting of matrices of local receptive fields and signal mappings, are recursively performed on structured graph data of temporal and spatial domain. The proposed model is generic and principled as it can be generalized into other dynamic models. We theoretically prove the stability of STGC and provide an upper-bound of the signal transformation to be learnt. Further, the proposed recursive model can be stacked into a multi-layer architecture. To evaluate our model, we conduct extensive experiments on four benchmark skeleton-based action datasets, including the large-scale challenging NTU RGB+D. The experimental results demonstrate the effectiveness of our proposed model and the improvement over the state-of-the-art.
PB  - arXiv
PY  - 2018
ST  - Spatio-temporal graph convolution for skeleton based action recognition
Y2  - 2025/05/05/21:54:28
DO  - 10.1609/aaai.v32i1.11776
ER  -


TY  - GEN
AU  - Ogura, M.
AU  - Preciado, V.M.
AU  - Masuda, N.
TI  - Optimal containment of epidemics over temporal activity-driven networks
AB  - In this paper, we study the dynamics of epidemic processes taking place in tempo- ral and adaptive networks. Building on the activity-driven network model, we propose an adaptive model of epidemic processes, where the network topology dynamically changes due to both exogenous factors independent of the epidemic dynamics as well as endogenous preventive measures adopted by individuals in response to the state of the infection. A direct analysis of the model using Markov processes involves the spectral analysis of a transition probability matrix whose size grows expo- nentially with the number of nodes. To overcome this limitation, we derive an upper-bound on the decay rate of the number of infected nodes in terms of the eigenvalues of a 2 2 matrix. Using this upper bound, we propose an efficient algorithm to tune the parameters describing the endogenous preventive measures in order to contain epidemics over time. We confirm our theoretical results via numerical simulations.
PB  - arXiv
PY  - 2018
ST  - Optimal containment of epidemics over temporal activity-driven networks
Y2  - 2025/05/05/21:54:28
DO  - 10.1137/18m1172740
ER  -


TY  - GEN
AU  - Javidani, A.
AU  - Mahmoudi-Aznaveh, A.
TI  - Learning representative temporal features for action recognition
AB  - In this paper, a novel video classification method is presented that aims to recognize different categories of third-person videos efficiently. Our motivation is to achieve a light model that could be trained with insufficient training data. With this intuition, the processing of the 3-dimensional video input is broken to 1D in temporal dimension on top of the 2D in spatial. The processes related to 2D spatial frames are being done by utilizing pre-trained networks with no training phase. The only step which involves training is to classify the 1D time series resulted from the description of the 2D signals. As a matter of fact, optical flow images are first calculated from consecutive frames and described by pre-trained CNN networks. Their dimension is then reduced using PCA. By stacking the description vectors beside each other, a multi-channel time series is created for each video. Each channel of the time series represents a specific feature and follows it over time. The main focus of the proposed method is to classify the obtained time series effectively. Towards this, the idea is to let the machine learn temporal features. This is done by training a multi-channel one dimensional Convolutional Neural Network (1D-CNN). The 1D-CNN learns the features along the only temporal dimension. Hence, the number of training parameters decreases significantly which would result in the trainability of the method on even smaller datasets. It is illustrated that the proposed method could reach the state-of-the-art results on two public datasets UCF11, jHMDB and competitive results on HMDB51.
PB  - arXiv
PY  - 2018
ST  - Learning representative temporal features for action recognition
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/s11042-021-11022-8
ER  -


TY  - GEN
AU  - Das, S.
AU  - Koperski, M.
AU  - Bremond, F.
AU  - Francesca, G.
TI  - Deep-temporal LSTM for daily living action recognition
AB  - In this paper, we propose to improve the traditional use of RNNs by employing a many to many model for video classification. We analyze the importance of modeling spatial layout and temporal encoding for daily living action recognition. Many RGB methods focus only on short term temporal information obtained from optical flow. Skeleton based methods on the other hand show that modeling long term skeleton evolution improves action recognition accuracy. In this work, we propose a deep-temporal LSTM architecture which extends standard LSTM and allows better encoding of temporal information. In addition, we propose to fuse 3D skeleton geometry with deep static appearance. We validate our approach on public available CAD60, MSRDailyActivity3D and NTU-RGB+D, achieving competitive performance as compared to the state-of-the art.
PB  - arXiv
PY  - 2018
ST  - Deep-temporal LSTM for daily living action recognition
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/avss.2018.8639122
ER  -


TY  - GEN
AU  - Yang, Z.
AU  - Li, Y.
AU  - Yang, J.
AU  - Luo, J.
TI  - Action recognition with spatio-temporal visual attention on skeleton image sequences
AB  - Action recognition with 3D skeleton sequences is becoming popular due to its speed and robustness. The recently proposed Convolutional Neural Networks \(CNN\) based methods have shown good performance in learning spatio-temporal representations for skeleton sequences. Despite the good recognition accuracy achieved by previous CNN based methods, there exist two problems that potentially limit the performance. First, previous skeleton representations are generated by chaining joints with a fixed order. The corresponding semantic meaning is unclear and the structural information among the joints is lost. Second, previous models do not have an ability to focus on informative joints. The attention mechanism is important for skeleton based action recognition because there exist spatio-temporal key stages while the joint predictions can be inaccurate. To solve these two problems, we propose a novel CNN based method for skeleton based action recognition. We first redesign the skeleton representations with a depth-first tree traversal order, which enhances the semantic meaning of skeleton images and better preserves the associated structural information. We then propose the idea of a two-branch attention architecture that focuses on spatio-temporal key stages and filters out unreliable joint predictions. A base attention model with the simplest structure is first introduced to illustrate the two-branch attention architecture. By improving the structures in both branches, we further propose a Global Long-sequence Attention Network \(GLAN\). Furthermore, in order to adjust the kernel's spatio-temporal aspect ratios and better capture long term dependencies, we propose a Sub-Sequence Attention Network \(SSAN\) that takes sub-image sequences as inputs. We show that the two-branch attention architecture can be combined with the SSAN to further improve the performance. Our experiment results on the NTU RGB+D dataset and the SBU Kinetic Interaction dataset outperforms the state-of-the-art. The model is further validated on noisy estimated poses from the UCF101 dataset and the Kinetics dataset.
PB  - arXiv
PY  - 2018
ST  - Action recognition with spatio-temporal visual attention on skeleton image sequences
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/tcsvt.2018.2864148
ER  -


TY  - GEN
AU  - Mellor, A.
TI  - Analysing Collective Behaviour in Temporal Networks Using Event Graphs and Temporal Motifs
AB  - Historically studies of behaviour on networks have focused on the behaviour of individuals (node-based) or on the aggregate behaviour of the entire network. We propose a new method to decompose a temporal network into macroscale components and to analyse the behaviour of these components, or collectives of nodes, across time. This method utilises all available information in the temporal network (i.e. no temporal aggregation), combining both topological and temporal structure using temporal motifs and inter-event times. This allows us create an embedding of a temporal network in order to describe behaviour over time and at different timescales. We illustrate this method using an example of digital communication data collected from an online social network.
PB  - arXiv
PY  - 2018
ST  - Analysing Collective Behaviour in Temporal Networks Using Event Graphs and Temporal Motifs
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/978-3-030-23495-9_6
ER  -


TY  - GEN
AU  - Yan, S.
AU  - Xiong, Y.
AU  - Lin, D.
TI  - Spatial temporal graph convolutional networks for skeleton-based action recognition
AB  - Dynamics of human body skeletons convey significant information for human action recognition. Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we propose a novel model of dynamic skeletons called Spatial- Temporal Graph Convolutional Networks (ST-GCN), which moves beyond the limitations of previous methods by automatically learning both the spatial and temporal patterns from data. This formulation not only leads to greater expressive power but also stronger generalization capability. On two large datasets, Kinetics and NTU-RGBD, it achieves substantial improvements over mainstream methods.
PB  - arXiv
PY  - 2018
ST  - Spatial temporal graph convolutional networks for skeleton-based action recognition
Y2  - 2025/05/05/21:54:28
DO  - 10.1609/aaai.v32i1.12328
ER  -


TY  - GEN
AU  - Xu, M.
AU  - Sharghi, A.
AU  - Chen, X.
AU  - Crandall, D.J.
TI  - Fully-coupled two-stream spatiotemporal networks for extremely low resolution action recognition
AB  - A major emerging challenge is how to protect people's privacy as cameras and computer vision are increasingly integrated into our daily lives, including in smart devices inside homes. A potential solution is to capture and record just the minimum amount of information needed to perform a task of interest. In this paper, we propose a fully-coupled two-stream spatiotemporal architecture for reliable human action recognition on extremely low resolution (e.g., 12×16 pixel) videos. We provide an efficient method to extract spatial and temporal features and to aggregate them into a robust feature representation for an entire action video sequence. We also consider how to incorporate high resolution videos during training in order to build better low resolution action recognition models. We evaluate on two publicly-available datasets, showing significant improvements over the state-of-the-art.
PB  - arXiv
PY  - 2018
ST  - Fully-coupled two-stream spatiotemporal networks for extremely low resolution action recognition
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/wacv.2018.00178
ER  -


TY  - GEN
AU  - Zhao, H.
AU  - Torralba, A.
AU  - Torresani, L.
AU  - Yan, Z.
TI  - HACS: Human action clips and segments dataset for recognition and temporal localization
AB  - This paper presents a new large-scale dataset for recognition and temporal localization of human actions collected from Web videos. We refer to it as HACS (Human Action Clips and Segments). We leverage both consensus and disagreement among visual classifiers to automatically mine candidate short clips from unlabeled videos, which are subsequently validated by human annotators. The resulting dataset is dubbed HACS Clips. Through a separate process we also collect annotations defining action segment boundaries. This resulting dataset is called HACS Segments. Overall, HACS Clips consists of 1.5M annotated clips sampled from 504K untrimmed videos, and HACS Segments contains 139K action segments densely annotated in 50K untrimmed videos spanning 200 action categories. HACS Clips contains more labeled examples than any existing video benchmark. This renders our dataset both a large-scale action recognition benchmark and an excellent source for spatiotemporal feature learning. In our transfer learning experiments on three target datasets, HACS Clips outperforms Kinetics-600, Moments-In-Time and Sports1M as a pretraining source. On HACS Segments, we evaluate state-of-the-art methods of action proposal generation and action localization, and highlight the new challenges posed by our dense temporal annotations.
PB  - arXiv
PY  - 2017
ST  - HACS
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/iccv.2019.00876
ER  -


TY  - GEN
AU  - Nguyen, P.
AU  - Liu, T.
AU  - Prasad, G.
AU  - Han, B.
TI  - Weakly supervised action localization by sparse temporal pooling network
AB  - We propose a weakly supervised temporal action localization algorithm on untrimmed videos using convolutional neural networks. Our algorithm learns from video-level class labels and predicts temporal intervals of human actions with no requirement of temporal localization annotations. We design our network to identify a sparse subset of key segments associated with target actions in a video using an attention module and fuse the key segments through adaptive temporal pooling. Our loss function is comprised of two terms that minimize the video-level action classification error and enforce the sparsity of the segment selection. At inference time, we extract and score temporal proposals using temporal class activations and class-agnostic attentions to estimate the time intervals that correspond to target actions. The proposed algorithm attains state-of-the-art results on the THUMOS14 dataset and outstanding performance on ActivityNet1.3 even with its weak supervision.
PB  - arXiv
PY  - 2017
ST  - Weakly supervised action localization by sparse temporal pooling network
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/cvpr.2018.00706
ER  -


TY  - GEN
AU  - Tran, D.
AU  - Wang, H.
AU  - Torresani, L.
AU  - LeCun, Y.
AU  - Paluri, M.
TI  - A closer look at spatiotemporal convolutions for action recognition
AB  - In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observa-tion that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advan-tages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly gains in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block "R(2+1)D" which produces CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101, and HMDB51.
PB  - arXiv
PY  - 2017
ST  - A closer look at spatiotemporal convolutions for action recognition
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/cvpr.2018.00675
ER  -


TY  - GEN
AU  - Bankson, B.B.
AU  - Hebart, M.N.
AU  - Groen, I.I.A.
AU  - Baker, C.I.
TI  - The temporal evolution of conceptual object representations revealed through models of behavior, semantics and deep neural networks
AB  - Visual object representations are commonly thought to emerge rapidly, yet it has remained unclear to what extent early brain responses reflect purely low-level visual features of these objects and how strongly those features contribute to later categorical or conceptual representations. Here, we aimed to estimate a lower temporal bound for the emergence of conceptual representations by defining two criteria that characterize such representations: 1) conceptual object representations should generalize across different exemplars of the same object, and 2) these representations should reflect high-level behavioral judgments. To test these criteria, we compared magnetoencephalography (MEG) recordings between two groups of participants (n = 16 per group) exposed to different exemplar images of the same object concepts. Further, we disentangled low-level from high-level MEG responses by estimating the unique and shared contribution of models of behavioral judgments, semantics, and different layers of deep neural networks of visual object processing. We find that 1) both generalization across exemplars as well as generalization of object-related signals across time increase after 150 ms, peaking around 230 ms; 2) behavioral judgments explain the most unique variance in the response after 150 ms. Collectively, these results suggest a lower bound for the emergence of conceptual object representations around 150 ms following stimulus onset.
PB  - bioRxiv
PY  - 2017
ST  - The temporal evolution of conceptual object representations revealed through models of behavior, semantics and deep neural networks
Y2  - 2025/05/05/21:54:28
DO  - 10.1101/223990
ER  -


TY  - GEN
AU  - Kim, H.
AU  - Ha, M.
AU  - Jeong, H.
TI  - Dynamic topologies of activity-driven temporal networks with memory
AB  - We propose dynamic scaling in temporal networks with heterogeneous activities and memory, and provide a comprehensive picture for the dynamic topologies of such networks, in terms of the modified activity-driven network model [H. Kim et al., Eur. Phys. J. B 88, 315 (2015)]. Particularly, we focus on the interplay of the time resolution and memory in dynamic topologies. Through the random walk (RW) process, we investigate diffusion properties and topological changes as the time resolution increases. Our results with memory are compared to those of the memoryless case. Based on the temporal percolation concept, we derive scaling exponents in the dynamics of the largest cluster and the coverage of the RW process in time-varying networks. We find that the time resolution in the time-accumulated network determines the effective size of the network, while memory affects relevant scaling properties at the crossover from the dynamic regime to the static one. The origin of memory-dependent scaling behaviors is the dynamics of the largest cluster, which depends on temporal degree distributions. Finally, we conjecture of the extended finite-size scaling ansatz for dynamic topologies and the fundamental property of temporal networks, which are numerically confirmed.
PB  - arXiv
PY  - 2017
ST  - Dynamic topologies of activity-driven temporal networks with memory
Y2  - 2025/05/05/21:54:28
DO  - 10.1103/physreve.97.062148
ER  -


TY  - GEN
AU  - Liu, J.
AU  - Akhtar, N.
AU  - Mian, A.
TI  - Skepxels: Spatio-temporal image representation of human skeleton joints for action recognition
AB  - Human skeleton joints are popular for action analysis since they can be easily extracted from videos to discard background noises. However, current skeleton representations do not fully benefit from machine learning with Convolutional Neural Networks (CNNs). We propose "Skepxels" a spatiotemporal representation for skeleton sequences to fully exploit the correlations between joints using the kernels of CNNs. We transform skeleton videos into images of flexible dimensions using Skepxels and develop a CNN-based framework for effective human action recognition using the resulting images. Skepxels encode rich spatio-temporal information about the skeleton joints in the frames by maximizing a unique distance metric, defined collaboratively over the distinct joint arrangements used in the skeletal images. Moreover, they are flexible in encoding compound semantic notions such as location and speed of the joints. The proposed action recognition exploits the representation in a hierarchical manner by first capturing the micro-temporal relations between the skeleton joints with the Skepxels and then exploiting their macro-temporal relations by computing the Fourier Temporal Pyramids over the CNN features of the skeletal images. We extend the Inception-ResNet CNN architecture with the proposed method and improve the state-of-the-art accuracy by 4:4% on the large scale NTU human activity dataset. On the medium-sized NUCLA and UTD-MHAD datasets, our method outperforms the existing results by 5:7% and 9:3% respectively.
PB  - arXiv
PY  - 2017
ST  - Skepxels
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/mapr49794.2020.9237766
ER  -


TY  - GEN
AU  - Ushakova, A.
AU  - Mikhaylov, S.J.
TI  - Learning to predict with highly granular temporal data: Estimating individual behavioral profiles with smart meter data
AB  - Big spatio-temporal datasets, available through both open and administrative data sources, offer significant potential for social science research. The magnitude of the data allows for increased resolution and analysis at individual level. While there are recent advances in forecasting techniques for highly granular temporal data, little attention is given to segmenting the time series and finding homogeneous patterns. In this paper, it is proposed to estimate behavioral profiles of individuals' activities over time using Gaussian Process-based models. In particular, the aim is to investigate how individuals or groups may be clustered according to the model parameters. Such a Bayesian non-parametric method is then tested by looking at the predictability of the segments using a combination of models to fit different parts of the temporal profiles. Model validity is then tested on a set of holdout data. The dataset consists of half hourly energy consumption records from smart meters from more than 100,000 households in the UK and covers the period from 2015 to 2016. The methodological approach developed in the paper may be easily applied to datasets of similar structure and granularity, for example social media data, and may lead to improved accuracy in the prediction of social dynamics and behavior.
PB  - arXiv
PY  - 2017
ST  - Learning to predict with highly granular temporal data
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/upec.2019.8893572
ER  -


TY  - GEN
AU  - Lazarov, E.
AU  - Dannemeyer, M.
AU  - Feulner, B.
AU  - Wolf, F.
AU  - Neef, A.
TI  - An axon initial segment is required for temporal precision in action potential encoding by neuronal populations
AB  - Central neurons initiate action potentials (APs) in the axon initial segment (AIS), a compartment characterized by a high concentration of voltage-dependent ion channels and specialized cytoskeletal anchoring proteins arranged in a regular nanoscale pattern. Although the AIS was a key evolutionary innovation in neurons, the functional benefits it confers are not clear. Using a mutation of the AIS cytoskeletal protein βIV-spectrin, we here establish an in vitro model of neurons with a perturbed AIS architecture that retains nanoscale order but loses the ability to maintain a high NaV density. Combining experiments and simulations we show that a high NaV density in the AIS is not required for axonal AP initiation; it is however crucial for a high bandwidth of information encoding and AP timing precision. Our results provide the first experimental demonstration of axonal AP initiation without high axonal channel density and suggest that increasing the bandwidth of the neuronal code and hence the computational efficiency of network function was a major benefit of the evolution of the AIS.
PB  - arXiv
PY  - 2017
ST  - An axon initial segment is required for temporal precision in action potential encoding by neuronal populations
Y2  - 2025/05/05/21:54:28
DO  - 10.1126/sciadv.aau8621
ER  -


TY  - GEN
AU  - Sun, L.
AU  - Wang, Y.
AU  - Cao, B.
AU  - Srisa-An, W.
AU  - Leow, A.D.
TI  - Sequential keystroke behavioral biometrics for mobile user identification via multi-view deep learning
AB  - With the rapid growth in smartphone usage, more organizations begin to focus on providing better services for mobile users. User identification can help these organizations to identify their customers and then cater services that have been customized for them. Currently, the use of cookies is the most common form to identify users. However, cookies are not easily transportable (e.g., when a user uses a different login account, cookies do not follow the user). This limitation motivates the need to use behavior biometric for user identification. In this paper, we propose DEEPSERVICE, a new technique that can identify mobile users based on user's keystroke information captured by a special keyboard or web browser. Our evaluation results indicate that DEEPSERVICE is highly accurate in identify-ing mobile users (over 93% accuracy). The technique is also efficient and only takes less than 1 ms to perform identification.
PB  - arXiv
PY  - 2017
ST  - Sequential keystroke behavioral biometrics for mobile user identification via multi-view deep learning
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/978-3-319-71273-4_19
ER  -


TY  - GEN
AU  - Sanchez-Diaz, E.
AU  - Rouillard, A.P.
AU  - Davies, J.A.
AU  - Pinto, R.F.
AU  - Kilpua, E.
TI  - The temporal and spatial scales of density structures released in the slow solar wind during solar activity maximum
AB  - In a recent study, we took advantage of a highly tilted coronal neutral sheet to show that density structures, extending radially over several solar radii (Rs), are released in the forming slow solar wind approximately 4-5 Rs above the solar surface (Sanchez-Diaz et al. 2017). We related the signatures of this formation process to intermittent magnetic reconnection occurring continuously above helmet streamers. We now exploit the heliospheric imagery from the Solar Terrestrial Relation Observatory (STEREO) to map the spatial and temporal distribution of the ejected structures. We demonstrate that streamers experience quasi-periodic bursts of activity with the simultaneous outpouring of small transients over a large range of latitudes in the corona. This cyclic activity leads to the emergence of well-defined and broad structures. Derivation of the trajectories and kinematic properties of the individual small transients that make up these large-scale structures confirms their association with the forming Slow Solar Wind (SSW). We find that these transients are released, on average, every 19.5 hours, simultaneously at all latitudes with a typical radial size of 12 Rs. Their spatial distribution, release rate and three-dimensional extent are used to estimate the contribution of this cyclic activity to the mass flux carried outward by the SSW. Our results suggest that, in interplanetary space, the global structure of the heliospheric current sheet is dominated by a succession of blobs and associated flux ropes. We demonstrated this with an example event using STEREO-A in-situ measurements.
PB  - arXiv
PY  - 2017
ST  - The temporal and spatial scales of density structures released in the slow solar wind during solar activity maximum
Y2  - 2025/05/05/21:54:28
DO  - 10.3847/1538-4357/aa98e2
ER  -


TY  - GEN
AU  - Watson, B.O.
AU  - Ding, M.
AU  - Buzsáki, G.
TI  - Temporal coupling of field potentials and action potentials in the neocortex
AB  - The local field potential (LFP) is an aggregate measure of group neuronal activity and is often correlated with the action potentials of single neurons. In recent years investigators have found that action potential firing rates increase during elevations in power high-frequency band oscillations (50-200 Hz range). However action potentials also contribute to the LFP signal itself, making the spike–LFP relationship complex. Here we examine the relationship between spike rates and LFPs in varying frequency bands in rat neocortical recordings. We find that 50-180Hz oscillations correlate most consistently with high firing rates, but that other LFPs bands also carry information relating to spiking, including in some cases anti-correlations. Relatedly, we find that spiking itself and electromyographic activity contribute to LFP power in these bands. The relationship between spike rates and LFP power varies between brain states and between individual cells. Finally, we create an improved oscillation-based predictor of action potential activity by specifically utilizing information from across the entire recorded frequency spectrum of LFP. The findings illustrate both caveats and improvements to be taken into account in attempts to infer spiking activity from LFP.
PB  - bioRxiv
PY  - 2017
ST  - Temporal coupling of field potentials and action potentials in the neocortex
Y2  - 2025/05/05/21:54:28
DO  - 10.1101/214650
ER  -


TY  - GEN
AU  - Schultz, P.
AU  - Spivak, D.I.
TI  - Temporal type theory: A topos-theoretic approach to systems and behavior
AB  - The integration of multiple sensory modalities is a key aspect of brain function, allowing animals to take advantage of concurrent sources of information to make more accurate perceptual judgments. For many years, multisensory integration in the cerebral cortex was deemed to occur only in high-level “polysensory” association areas. However, more recent studies have suggested that cross-modal stimulation can also influence neural activity in areas traditionally considered to be unimodal. In particular, several human neuroimaging studies have reported that extrastriate areas involved in visual motion perception are also activated by auditory motion, and may integrate audio-visual motion cues. However, the exact nature and extent of the effects of auditory motion on the visual cortex have not been studied at the single neuron level. We recorded the spiking activity of neurons in the middle temporal (MT) and medial superior temporal (MST) areas of anesthetized marmoset monkeys upon presentation of unimodal stimuli (moving auditory or visual patterns), as well as bimodal stimuli (concurrent audio-visual motion). Despite robust, direction selective responses to visual motion, none of the sampled neurons responded to auditory motion stimuli. Moreover, concurrent moving auditory stimuli had no significant effect on the ability of single MT and MST neurons, or populations of simultaneously recorded neurons, to discriminate the direction of motion of visual stimuli (moving random dot patterns with varying levels of motion noise). Our findings do not support the hypothesis that direct interactions between MT, MST and areas low in the hierarchy of auditory areas underlie audiovisual motion integration.
PB  - arXiv
PY  - 2017
ST  - Temporal type theory
Y2  - 2025/05/05/21:54:28
DO  - 10.1016/j.entcs.2009.11.004
ER  -


TY  - GEN
AU  - Chaplin, T.A.
AU  - Allitt, B.J.
AU  - Hagan, M.A.
AU  - Rajan, R.
AU  - Lui, L.L.
TI  - Auditory Motion Does Not Modulate Spiking Activity in the Middle Temporal and Medial Superior Temporal Visual Areas
AB  - The integration of multiple sensory modalities is a key aspect of brain function, allowing animals to take advantage of concurrent sources of information to make more accurate perceptual judgments. For many years, multisensory integration in the cerebral cortex was deemed to occur only in high-level “polysensory” association areas. However, more recent studies have suggested that cross-modal stimulation can also influence neural activity in areas traditionally considered to be unimodal. In particular, several human neuroimaging studies have reported that extrastriate areas involved in visual motion perception are also activated by auditory motion, and may integrate audio-visual motion cues. However, the exact nature and extent of the effects of auditory motion on the visual cortex have not been studied at the single neuron level. We recorded the spiking activity of neurons in the middle temporal (MT) and medial superior temporal (MST) areas of anesthetized marmoset monkeys upon presentation of unimodal stimuli (moving auditory or visual patterns), as well as bimodal stimuli (concurrent audio-visual motion). Despite robust, direction selective responses to visual motion, none of the sampled neurons responded to auditory motion stimuli. Moreover, concurrent moving auditory stimuli had no significant effect on the ability of single MT and MST neurons, or populations of simultaneously recorded neurons, to discriminate the direction of motion of visual stimuli (moving random dot patterns with varying levels of motion noise). Our findings do not support the hypothesis that direct interactions between MT, MST and areas low in the hierarchy of auditory areas underlie audiovisual motion integration.
PB  - bioRxiv
PY  - 2017
ST  - Auditory Motion Does Not Modulate Spiking Activity in the Middle Temporal and Medial Superior Temporal Visual Areas
Y2  - 2025/05/05/21:54:28
DO  - 10.1111/ejn.14071
ER  -


TY  - GEN
AU  - Lin, T.
AU  - Zhao, X.
AU  - Shou, Z.
TI  - Single shot temporal action detection
AB  - Temporal action detection is a very important yet challenging problem, since videos in real applications are usually long, untrimmed and contain multiple action instances. This problem requires not only recognizing action categories but also detecting start time and end time of each action instance. Many state-of-the-art methods adopt the "detection by classification" framework: first do proposal, and then classify proposals. The main drawback of this framework is that the boundaries of action instance proposals have been fixed during the classification step. To address this issue, we propose a novel Single Shot Action Detector (SSAD) network based on 1D temporal convolutional layers to skip the proposal generation step via directly detecting action instances in untrimmed video. On pursuit of designing a particular SSAD network that can work effectively for temporal action detection, we empirically search for the best network architecture of SSAD due to lacking existing models that can be directly adopted. Moreover, we investigate into input feature types and fusion strategies to further improve detection accuracy. We conduct extensive experiments on two challenging datasets: THUMOS 2014 and MEXaction2. When setting Intersection-over-Union threshold to 0.5 during evaluation, SSAD significantly outperforms other state-of-the-art systems by increasing mAP from 19.0% to 24.6% on THUMOS 2014 and from 7.4% to 11.0% on MEXaction2.
PB  - arXiv
PY  - 2017
ST  - Single shot temporal action detection
Y2  - 2025/05/05/21:54:28
DO  - 10.1145/3123266.3123343
ER  -


TY  - GEN
AU  - Chadha, A.
AU  - Abbas, A.
AU  - Andreopoulos, Y.
TI  - Video classification with CNNs: Using the codec as a spatio-temporal activity sensor
AB  - We investigate video classification via a two-stream convolutional neural network (CNN) design that directly ingests information extracted from compressed video bitstreams. Our approach begins with the observation that all modern video codecs divide the input frames into macroblocks (MBs). We demonstrate that selective access to MB motion vector (MV) information within compressed video bitstreams can also provide for selective, motion-adaptive, MB pixel decoding (a.k.a., MB texture decoding). This in turn allows for the derivation of spatio-temporal video activity regions at extremely high speed in comparison to conventional full-frame decoding followed by optical flow estimation. In order to evaluate the accuracy of a video classification framework based on such activity data, we independently train two CNN architectures on MB texture and MV correspondences and then fuse their scores to derive the final classification of each test video. Evaluation on two standard datasets shows that the proposed approach is competitive to the best two-stream video classification approaches found in the literature. At the same time: (i) a CPU-based realization of our MV extraction is over 977 times faster than GPU-based optical flow methods; (ii) selective decoding is up to 12 times faster than full-frame decoding; (iii) our proposed spatial and temporal CNNs perform inference at 5 to 49 times lower cloud computing cost than the fastest methods from the literature.
PB  - arXiv
PY  - 2017
ST  - Video classification with CNNs
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/tcsvt.2017.2786999
ER  -


TY  - GEN
AU  - Heidarivincheh, F.
AU  - Mirmehdi, M.
AU  - Damen, D.
TI  - Detecting the moment of completion: Temporal models for localising action completion
AB  - Action completion detection is the problem of modelling the action's progression towards localising the moment of completion - when the action's goal is confidently considered achieved. In this work, we assess the ability of two temporal models, namely Hidden Markov Models (HMM) and Long-Short Term Memory (LSTM), to localise completion for six object interactions: switch, plug, open, pull, pick and drink. We use a supervised approach, where annotations of pre-completion and post-completion frames are available per action, and fine-tuned CNN features are used to train temporal models. Tested on the Action-Completion-2016 dataset from [1], we detect completion within 10 frames of annotations for ∼75% of completed action sequences using both temporal models. Results show that fine-tuned CNN features outperform hand-crafted features for localisation, and that observing incomplete instances is necessary when incomplete sequences are also present in the test set.
PB  - arXiv
PY  - 2017
ST  - Detecting the moment of completion
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/iccvw.2019.00150
ER  -


TY  - GEN
AU  - Martelli, V.
AU  - Cai, A.
AU  - Nica, E.M.
AU  - Si, Q.
AU  - Paschen, S.
TI  - Sequential localization and strange-metal behavior of a complex electron fluid
AB  - Complex and correlated quantum systems with promise for new functionality often involve entwined electronic degrees of freedom1–5. In such materials, highly unusual properties emerge4–11 and could be the result of electron localization12. Here, a cubic heavy fermion metal governed by spins and orbitals is chosen as a model system for this physics. Its properties are found to originate from surprisingly simple low-energy behavior, with two distinct localization tranitions driven by a single degree of freedom at a time. This result is unexpected, but we are able to understand it by advancing the notion of sequential destruction of an SU(4) spin-orbital-coupled Kondo entanglement. Our results establish electron localization as a unified framework for strongly correlated materials and suggest ways to exploit multiple degrees of freedom for quantum engineering.
PB  - arXiv
PY  - 2017
ST  - Sequential localization and strange-metal behavior of a complex electron fluid
Y2  - 2025/05/05/21:54:28
DO  - 10.1073/pnas.1908101116
ER  -


TY  - GEN
AU  - Rezazadegan, F.
AU  - Shirazi, S.
AU  - Baktashmotlagh, M.
AU  - Davis, L.S.
TI  - On encoding temporal evolution for real-Time action prediction
AB  - Anticipating future actions is a key component of intelligence, specifically when it applies to real-Time systems, such as robots or autonomous cars. While recent works have addressed prediction of raw RGB pixel values, we focus on anticipating the motion evolution in future video frames. To this end, we construct dynamic images (DIs) by sum-marising moving pixels through a sequence of fu-Ture frames. We train a convolutional LSTMs to predict the next DIs based on an unsupervised learning process, and then recognise the activity as-sociated with the predicted DI. We demonstrate the effectiveness of our approach on 3 benchmark ac-Tion datasets showing that despite running on vide-os with complex activities, our approach is able to anticipate the next human action with high accura-cy and obtain better results than the state-of-The-Art methods.
PB  - arXiv
PY  - 2017
ST  - On encoding temporal evolution for real-Time action prediction
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/tcsvt.2020.2984569
ER  -


TY  - GEN
AU  - Allen, C.
TI  - The separation of auditory experience and the temporal structure of MEG recorded brain activity
AB  - Do brain oscillations limit the temporal dynamics of experience? This pre-registered study used the separation of auditory stimuli to track perceptual experience and related this to oscillatory activity using magnetoencephalography. The rates at which auditory stimuli could be individuated matched the rates of oscillatory brain activity. Stimuli also entrained brain activity at the frequencies at which they were presented and a progression of high frequency gamma band events appeared to predict successful separation. These findings support a generalised function for brain oscillations, across frequency bands, in the alignment of activity to delineate representations.
PB  - bioRxiv
PY  - 2017
ST  - The separation of auditory experience and the temporal structure of MEG recorded brain activity
Y2  - 2025/05/05/21:54:28
DO  - 10.1101/189480
ER  -


TY  - GEN
AU  - Mamakoukas, G.
AU  - MacIver, M.A.
AU  - Murphey, T.D.
TI  - Feedback synthesis for controllable underactuated systems using sequential second order actions
AB  - This paper derives nonlinear feedback control synthesis for general control affine systems using second-order actions-the needle variations of optimal control-as the basis for choosing each control response to the current state. A second result of the paper is that the method provably exploits the nonlinear controllability of a system by virtue of an explicit dependence of the second-order needle variation on the Lie bracket between vector fields. As a result, each control decision necessarily decreases the objective when the system is nonlinearly controllable using first-order Lie brackets. Simulation results using a differential drive cart, an underactuated kinematic vehicle in three dimensions, and an underactuated dynamic model of an underwater vehicle demonstrate that the method finds control solutions when the first-order analysis is singular. Moreover, the simulated examples demonstrate superior convergence when compared to synthesis based on first-order needle variations. Lastly, the underactuated dynamic underwater vehicle model demonstrates the convergence even in the presence of a velocity field.
PB  - arXiv
PY  - 2017
ST  - Feedback synthesis for controllable underactuated systems using sequential second order actions
Y2  - 2025/05/05/21:54:28
DO  - 10.15607/rss.2017.xiii.066
ER  -


TY  - GEN
AU  - Ansari, A.
AU  - Murphey, T.
TI  - Sequential action control: Closed-form optimal control for nonlinear and nonsmooth systems
AB  - This paper presents a new model-based algorithm that computes predictive optimal controls on-line and in closed loop for traditionally challenging nonlinear systems. Examples demonstrate the same algorithm controlling hybrid impulsive, underactuated, and constrained systems using only high-level models and trajectory goals. Rather than iteratively optimize finite horizon control sequences to minimize an objective, this paper derives a closed-form expression for individual control actions, i.e., control values that can be applied for short duration, that optimally improve a tracking objective over a long time horizon. Under mild assumptions, actions become linear feedback laws near equilibria that permit stability analysis and performance-based parameter selection. Globally, optimal actions are guaranteed existence and uniqueness. By sequencing these actions on-line, in receding horizon fashion, the proposed controller provides a min-max constrained response to state that avoids the overhead typically required to impose control constraints. Benchmark examples show the approach can avoid local minima and outperform nonlinear optimal controllers and recent, case-specific methods in terms of tracking performance, and at speeds orders of magnitude faster than traditionally achievable.
PB  - arXiv
PY  - 2017
ST  - Sequential action control
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/tro.2016.2596768
ER  -


TY  - GEN
AU  - Hara, K.
AU  - Kataoka, H.
AU  - Satoh, Y.
TI  - Learning spatio-temporal features with 3D residual networks for action recognition
AB  - Convolutional neural networks with spatio-temporal 3D kernels (3D CNNs) have an ability to directly extract spatiotemporal features from videos for action recognition. Although the 3D kernels tend to overfit because of a large number of their parameters, the 3D CNNs are greatly improved by using recent huge video databases. However, the architecture of 3D CNNs is relatively shallow against to the success of very deep neural networks in 2D-based CNNs, such as residual networks (ResNets). In this paper, we propose a 3D CNNs based on ResNets toward a better action representation. We describe the training procedure of our 3D ResNets in details. We experimentally evaluate the 3D ResNets on the ActivityNet and Kinetics datasets. The 3D ResNets trained on the Kinetics did not suffer from overfitting despite the large number of parameters of the model, and achieved better performance than relatively shallow networks, such as C3D. Our code and pretrained models (e.g. Kinetics and ActivityNet) are publicly available at https://github.com/kenshohara/3D-ResNets.
PB  - arXiv
PY  - 2017
ST  - Learning spatio-temporal features with 3D residual networks for action recognition
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/iccvw.2017.373
ER  -


TY  - GEN
AU  - Emmott, E.
AU  - de Rougemont, A.
AU  - Haas, J.
AU  - Goodfellow, I.
TI  - Spatial and temporal control of norovirus protease activity is determined by polyprotein processing and intermolecular interactions within the viral replication complex
AB  - Norovirus infections are a major cause of acute viral gastroenteritis and a significant burden to human health globally. A vital process for norovirus replication is the processing of the nonstructural polyprotein, by an internal protease, into the necessary viral components required to form the viral replication complex. This cleavage occurs at different rates resulting in the accumulation of stable precursor forms. In this report, we characterized how precursor forms of the norovirus protease accumulate during infection. Using stable forms of the protease precursors we demonstrated that these are all proteolytically active in vitro, but that when expressed in cells, activity is determined by both substrate and protease localization. Whilst all precursors could cleave a replication complex-associated substrate, only a subset of precursors lacking NS4 were capable of efficiently cleaving a cytoplasmic substrate. For the first time, the full range of protein-protein interactions between murine and human norovirus proteins were mapped by LUMIER assay, with conserved interactions between replication complex members, modifying the localization of a subset of precursors. Finally, we demonstrate that re-targeting of a poorly cleaved artificial cytoplasmic substrate to the replication complex is sufficient to permit efficient cleavage in the context of norovirus infection. This offers a model for how norovirus can regulate the timing of substrate cleavage throughout the replication cycle. The norovirus protease represents a key target in the search for effective antiviral treatments for norovirus infection. An improved understanding of protease function and regulation, as well as identification of interactions between the other non-structural proteins, offers new avenues for antiviral drug design.
PB  - bioRxiv
PY  - 2017
ST  - Spatial and temporal control of norovirus protease activity is determined by polyprotein processing and intermolecular interactions within the viral replication complex
Y2  - 2025/05/05/21:54:28
DO  - 10.1101/175463
ER  -


TY  - GEN
AU  - Yang, K.
AU  - Qiao, P.
AU  - Li, D.
AU  - Lv, S.
AU  - Dou, Y.
TI  - Exploring temporal preservation networks for precise temporal action localization
AB  - Temporal action localization is an important task of computer vision. Though a variety of methods have been proposed, it still remains an open question how to predict the temporal boundaries of action segments precisely. Most works use segment-level classifiers to select video segments predetermined by action proposal or dense sliding windows. However, in order to achieve more precise action boundaries, a temporal localization system should make dense predictions at a fine granularity. A newly proposed work exploits Convolutional-Deconvolutional-Convolutional (CDC) filters to upsample the predictions of 3D ConvNets, making it possible to perform per-frame action predictions and achieving promising performance in terms of temporal action localization. However, CDC network loses temporal information partially due to the temporal downsampling operation. In this paper, we propose an elegant and powerful Temporal Preservation Convolutional (TPC) Network that equips 3D ConvNets with TPC filters. TPC network can fully preserve temporal resolution and downsample the spatial resolution simultaneously, enabling frame-level granularity action localization with minimal loss of time information. TPC network can be trained in an end-to-end manner. Experiment results on public datasets show that TPC network achieves significant improvement on per-frame action prediction and competing results on segment-level temporal action localization.
PB  - arXiv
PY  - 2017
ST  - Exploring temporal preservation networks for precise temporal action localization
Y2  - 2025/05/05/21:54:28
DO  - 10.1609/aaai.v32i1.12234
ER  -


TY  - GEN
AU  - Dai, X.
AU  - Singh, B.
AU  - Zhang, G.
AU  - Davis, L.S.
AU  - Chen, Y.Q.
TI  - Temporal context network for activity localization in videos
AB  - We present a Temporal Context Network (TCN) for precise temporal localization of human activities. Similar to the Faster-RCNN architecture, proposals are placed at equal intervals in a video which span multiple temporal scales. We propose a novel representation for ranking these proposals. Since pooling features only inside a segment is not sufficient to predict activity boundaries, we construct a representation which explicitly captures context around a proposal for ranking it. For each temporal segment inside a proposal, features are uniformly sampled at a pair of scales and are input to a temporal convolutional neural network for classification. After ranking proposals, non-maximum suppression is applied and classification is performed to obtain final detections. TCN outperforms state-of-the-art methods on the ActivityNet dataset and the THUMOS14 dataset.
PB  - arXiv
PY  - 2017
ST  - Temporal context network for activity localization in videos
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/iccv.2017.610
ER  -


TY  - GEN
AU  - Yuan, Y.
AU  - Liang, X.
AU  - Wang, X.
AU  - Yeung, D.-Y.
AU  - Gupta, A.
TI  - Temporal dynamic graph LSTM for action-driven video object detection
AB  - In this paper, we investigate a weakly-supervised object detection framework. Most existing frameworks focus on using static images to learn object detectors. However, these detectors often fail to generalize to videos because of the existing domain shift. Therefore, we investigate learning these detectors directly from boring videos of daily activities. Instead of using bounding boxes, we explore the use of action descriptions as supervision since they are relatively easy to gather. A common issue, however, is that objects of interest that are not involved in human actions are often absent in global action descriptions known as “missing label”. To tackle this problem, we propose a novel temporal dynamic graph Long Short-Term Memory network (TD-Graph LSTM). TD-Graph LSTM enables global temporal reasoning by constructing a dynamic graph that is based on temporal correlations of object proposals and spans the entire video. The missing label issue for each individual frame can thus be significantly alleviated by transferring knowledge across correlated objects proposals in the whole video. Extensive evaluations on a large-scale daily-life action dataset (i.e., Charades) demonstrates the superiority of our proposed method. We also release object bounding-box annotations for more than 5,000 frames in Charades. We believe this annotated data can also benefit other research on video-based object recognition in the future.
PB  - arXiv
PY  - 2017
ST  - Temporal dynamic graph LSTM for action-driven video object detection
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/iccv.2017.200
ER  -


TY  - GEN
AU  - Yang, Z.
AU  - Gao, J.
AU  - Nevatia, R.
TI  - Spatio-temporal action detection with cascade proposal and location anticipation 2017
AB  - In this work, we address the problem of spatio-temporal action detection in temporally untrimmed videos. It is an important and challenging task as finding accurate human actions in both temporal and spatial space is important for analyzing large-scale video data. To tackle this problem, we propose a cascade proposal and location anticipation (CPLA) model for frame-level action detection. There are several salient points of our model: (1) a cascade region proposal network (casRPN) is adopted for action proposal generation and shows better localization accuracy compared with single region proposal network (RPN); (2) action spatio-temporal consistencies are exploited via a location anticipation network (LAN) and thus frame-level action detection is not conducted independently. Frame-level detections are then linked by solving an linking score maximization problem, and temporally trimmed into spatio-temporal action tubes. We demonstrate the effectiveness of our model on the challenging UCF101 and LIRIS-HARL datasets, both achieving state-of-the-art performance.
PB  - arXiv
PY  - 2017
ST  - Spatio-temporal action detection with cascade proposal and location anticipation 2017
Y2  - 2025/05/05/21:54:28
DO  - 10.5244/c.31.95
ER  -


TY  - GEN
AU  - Matsui, T.
AU  - Murakami, T.
AU  - Ohki, K.
TI  - Neuronal origin of the temporal dynamics of spontaneous BOLD activity correlation
AB  - Resting-state functional connectivity (FC) has become a major fMRI method to study network organization of human brains. There has been recent interest in the temporal fluctuations of FC calculated using short time windows (“dynamic FC”) because this method could provide information inaccessible with conventional “static” FC, which is typically calculated using the entire scan lasting several tens of minutes. Although multiple studies have revealed considerable temporal fluctuations in FC, it is still unclear whether the fluctuations of FC measured in hemodynamics reflect the dynamics of underlying neural activity. We addressed this question using simultaneous imaging of neuronal calcium and hemodynamic signals in mice and found coordinated temporal dynamics of calcium FC and hemodynamic FC measured in the same short time windows. Moreover, we found that variation in transient neuronal coactivation patterns (CAPs) was significantly related to temporal fluctuations of sliding window FC in hemodynamics. Finally, we show that the observed dynamics of FC cannot be fully accounted for by simulated data assuming stationary FC. These results provide evidence for the neuronal origin of dynamic FC and further suggest that information relevant to FC is condensed in temporally sparse events that can be extracted using a small number of time points.
PB  - bioRxiv
PY  - 2017
ST  - Neuronal origin of the temporal dynamics of spontaneous BOLD activity correlation
Y2  - 2025/05/05/21:54:28
DO  - 10.1101/169698
ER  -


TY  - GEN
AU  - Colman, E.
AU  - Spies, K.
AU  - Bansal, S.
TI  - The reachability of contagion in temporal contact networks: How disease latency can exploit the rhythm of human behavior
AB  - Background: The symptoms of many infectious diseases influence their host to withdraw from social activity limiting their own potential to spread. Successful transmission therefore requires the onset of infectiousness to coincide with a time when its host is socially active. Since social activity and infectiousness are both temporal phenomena, we hypothesize that diseases are most pervasive when these two processes are synchronized. Methods: We consider disease dynamics that incorporate a behavioral response that effectively shortens the infectious period of the disease. We apply this model to data collected from face-to-face social interactions and look specifically at how the duration of the latent period effects the reachability of the disease. We then simulate the spread of the model disease on the network to test the robustness of our results. Results: Diseases with latent periods that synchronize with the temporal social behavior of people, i.e. latent periods of 24 hours or 7 days, correspond to peaks in the number of individuals who are potentially at risk of becoming infected. The effect of this synchronization is present for a range of disease models with realistic parameters. Conclusions: The relationship between the latent period of an infectious disease and its pervasiveness is non-linear and depends strongly on the social context in which the disease is spreading.
PB  - arXiv
PY  - 2017
ST  - The reachability of contagion in temporal contact networks
Y2  - 2025/05/05/21:54:28
DO  - 10.1186/s12879-018-3117-6
ER  -


TY  - GEN
AU  - Saha, S.
AU  - Singh, G.
AU  - Sapienza, M.
AU  - Torr, P.H.S.
AU  - Cuzzolin, F.
TI  - Spatio-temporal human action localisation and instance segmentation in temporally untrimmed videos
AB  - Current state-of-the-art human action recognition is focused on the classification of temporally trimmed videos in which only one action occurs per frame. In this work we address the problem of action localisation and instance segmentation in which multiple concurrent actions of the same class may be segmented out of an image sequence. We cast the action tube extraction as an energy maximisation problem in which configurations of region proposals in each frame are assigned a cost and the best action tubes are selected via two passes of dynamic programming. One pass associates region proposals in space and time for each action category, and another pass is used to solve for the tube's temporal extent and to enforce a smooth label sequence through the video. In addition, by taking advantage of recent work on action foreground-background segmentation, we are able to associate each tube with class-specific segmentations. We demonstrate the performance of our algorithm on the challenging LIRIS-HARL dataset and achieve a new state-of-the-art result which is 14.3 times better than previous methods.
PB  - arXiv
PY  - 2017
ST  - Spatio-temporal human action localisation and instance segmentation in temporally untrimmed videos
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/978-3-030-46732-6_8
ER  -


TY  - GEN
AU  - Ma, Z.
AU  - Zhang, N.
TI  - Temporal transitions of spontaneous brain activity
AB  - Spontaneous brain activity, typically investigated using resting-state fMRI (rsfMRI), provides a measure of inter-areal resting-state functional connectivity (RSFC). Previous rsfMRI studies mainly focused on spatial characteristics of RSFC, but the temporal relationship between RSFC patterns is still elusive. Particularly, it remains unknown whether separate RSFC patterns temporally fluctuate in a random manner, or transit in specific orders. Here we investigated temporal transitions between characteristic RSFC patterns in awake rats and humans. We found that transitions between RSFC patterns were reproducible and significantly above chance, suggesting that RSFC pattern transitions were nonrandom. The organization of RSFC pattern transitions in rats was analyzed using graph theory. Pivotal RSFC patterns in transitions were identified including hippocampal, thalamic and striatal networks. This study has revealed nonrandom temporal relationship between characteristic RSFC patterns in both rats and humans. It offers new insights into understanding the spatiotemporal dynamics of spontaneous activity in the mammalian brain.
PB  - bioRxiv
PY  - 2017
ST  - Temporal transitions of spontaneous brain activity
Y2  - 2025/05/05/21:54:28
DO  - 10.1101/166512
ER  -


TY  - GEN
AU  - Lin, T.
AU  - Zhao, X.
AU  - Shou, Z.
TI  - Temporal Convolution Based Action Proposal: Submission to ActivityNet 2017
AB  - In this notebook paper, we describe our approach in the submission to the temporal action proposal (task 3) and temporal action localization (task 4) of ActivityNet Challenge hosted at CVPR 2017. Since the accuracy in action classification task is already very high (nearly 90% in ActivityNet dataset), we believe that the main bottleneck for temporal action localization is the quality of action proposals. Therefore, we mainly focus on the temporal action proposal task and propose a new proposal model based on temporal convolutional network. Our approach achieves the state-of-the-art performances on both temporal action proposal task and temporal action localization task.
PB  - arXiv
PY  - 2017
ST  - Temporal Convolution Based Action Proposal
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/itia50152.2020.9312328
ER  -


TY  - GEN
AU  - Lehmann, M.P.
AU  - Xu, H.A.
AU  - Liakoni, V.
AU  - Gerstner, W.
AU  - Preuschoff, K.
TI  - One-shot learning and behavioral eligibility traces in sequential decision making
AB  - In many daily tasks we make multiple decisions before reaching a goal. In order to learn such sequences of decisions, a mechanism to link earlier actions to later reward is necessary. Reinforcement learning theory suggests two classes of algorithms solving this credit assignment problem: In classic temporal-difference learning, earlier actions receive reward information only after multiple repetitions of the task, whereas models with eligibility traces reinforce entire sequences of actions from a single experience (one-shot). Here we asked whether humans use eligibility traces. We developed a novel paradigm to directly observe which actions and states along a multi-step sequence are reinforced after a single reward. By focusing our analysis on those states for which RL with and without eligibility trace make qualitatively distinct predictions, we find direct behavioral (choice probability) and physiological (pupil dilation) signatures of reinforcement learning with eligibility trace across multiple sensory modalities.
PB  - arXiv
PY  - 2017
ST  - One-shot learning and behavioral eligibility traces in sequential decision making
Y2  - 2025/05/05/21:54:28
DO  - 10.7554/elife.47463
ER  -


TY  - GEN
AU  - Csomós, G.
TI  - Mapping spatial and temporal changes of global corporate research and development activities by conducting a bibliometric analysis
AB  - Corporate research and development (R&D) activities have long been highly concentrated in a handful of world cities. This is due to the fact that these cities (e.g., Tokyo, New York, London, and Paris) are home to the largest and most powerful transnational corporations and are globally important sites for innovative start-up firms that operate in the fastest growing industries. However, in tandem with the rapid technological changes of our age, corporate R&D activities have shifted towards newly emerging and now globally significant R&D centres, like San Jose, San Francisco, and Boston in the United States, and Beijing, Seoul, and Shenzhen in East Asia. In this paper, I will conduct a bibliometric analysis to define which cities are centres of corporate R&D activities, how different industries influence their performance, and what spatial tendencies characterise the period from 1980 to 2014. The bibliometric analysis is based upon an assumption that implies there is a close connection between the number of scientific articles published by a given firm and the volume of its R&D activity. Results show that firms headquartered in Tokyo, New York, London, and Paris published the largest combined number of scientific articles in the period from 1980 to 2014, but that the growth rate of the annual output of scientific articles was much greater in Boston, San Jose, Beijing, and Seoul, as well as some Taiwanese cities. Furthermore, it can also be seen that those cities that have the largest number of articles; i.e., that can be considered as the most significant sites of corporate R&D in which firms operate in fast-growing industries, are primarily in the pharmaceutical and information technology industries. For these reasons, some mid-sized cities that are home to globally significant pharmaceutical or information technology firms are also top corporate R&D hubs.
PB  - arXiv
PY  - 2017
ST  - Mapping spatial and temporal changes of global corporate research and development activities by conducting a bibliometric analysis
Y2  - 2025/05/05/21:54:28
DO  - 10.1515/quageo-2017-0005
ER  -


TY  - GEN
AU  - Kelley, D.R.
AU  - Reshef, Y.A.
AU  - Bileschi, M.
AU  - McLean, C.Y.
AU  - Snoek, J.
TI  - Sequential regulatory activity prediction across chromosomes with convolutional neural networks
AB  - Models for predicting phenotypic outcomes from genotypes have important applications to understanding genomic function and improving human health. Here, we develop a machine-learning system to predict cell type-specific epigenetic and transcriptional profiles in large mammalian genomes from DNA sequence alone. Using convolutional neural networks, this system identifies promoters and distal regulatory elements and synthesizes their content to make effective gene expression predictions. We show that model predictions for the influence of genomic variants on gene expression align well to causal variants underlying eQTLs in human populations and can be useful for generating mechanistic hypotheses to enable fine mapping of disease loci.
PB  - bioRxiv
PY  - 2017
ST  - Sequential regulatory activity prediction across chromosomes with convolutional neural networks
Y2  - 2025/05/05/21:54:28
DO  - 10.1101/gr.227819.117
ER  -


TY  - GEN
AU  - Tzorakoleftherakis, E.
AU  - Murphey, T.D.
TI  - Iterative sequential action control for stable, model-based control of nonlinear systems
AB  - This paper presents iterative Sequential Action Control (iSAC), a receding horizon approach for control of nonlinear systems. The iSAC method has a closed-form open-loop solution, which is iteratively updated between time steps by introducing constant control values applied for short duration. Application of a contractive constraint on the cost is shown to lead to closedloop asymptotic stability under mild assumptions. The effect of asymptotically decaying disturbances on system trajectories is also examined. To demonstrate the applicability of iSAC, we employ a variety of systems and conditions, including a 13-dimensional quaternion-based quadrotor and NASA's TRACE spacecraft. Each system is tested in different scenarios, ranging from feasible and infeasible trajectory tracking, to setpoint stabilization, with or without the presence of external disturbances. Finally, limitations of this work are discussed.
PB  - arXiv
PY  - 2017
ST  - Iterative sequential action control for stable, model-based control of nonlinear systems
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/tac.2018.2885477
ER  -


TY  - GEN
AU  - Liu, J.
AU  - Shahroudy, A.
AU  - Xu, D.
AU  - Kot, A.C.
AU  - Wang, G.
TI  - Skeleton-based action recognition using spatio-temporal LSTM network with trust gates
AB  - Skeleton-based human action recognition has attracted a lot of research attention during the past few years. Recent works attempted to utilize recurrent neural networks to model the temporal dependencies between the 3D positional configurations of human body joints for better analysis of human activities in the skeletal data. The proposed work extends this idea to spatial domain as well as temporal domain to better analyze the hidden sources of action-related information within the human skeleton sequences in both of these domains simultaneously. Based on the pictorial structure of Kinect's skeletal data, an effective tree-structure based traversal framework is also proposed. In order to deal with the noise in the skeletal data, a new gating mechanism within LSTM module is introduced, with which the network can learn the reliability of the sequential data and accordingly adjust the effect of the input data on the updating procedure of the long-term context representation stored in the unit's memory cell. Moreover, we introduce a novel multi-modal feature fusion strategy within the LSTM unit in this paper. The comprehensive experimental results on seven challenging benchmark datasets for human action recognition demonstrate the effectiveness of the proposed method.
PB  - arXiv
PY  - 2017
ST  - Skeleton-based action recognition using spatio-temporal LSTM network with trust gates
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/tpami.2017.2771306
ER  -


TY  - GEN
AU  - Huang, J.
AU  - Li, N.
AU  - Zhang, T.
AU  - Li, G.
TI  - A self-adaptive proposal model for temporal action detection based on reinforcement learning
AB  - Existing action detection algorithms usually generate action proposals through an extensive search over the video at multiple temporal scales, which brings about huge computational overhead and deviates from the human perception procedure. We argue that the process of detecting actions should be naturally one of observation and refinement: Observe the current window and refine the span of attended window to cover true action regions. In this paper, we propose an active action proposal model that learns to find actions through continuously adjusting the temporal bounds in a self-adaptive way. The whole process can be deemed as an agent, which is firstly placed at a position in the video at random, adopts a sequence of transformations on the current attended region to discover actions according to a learned policy. We utilize reinforcement learning, especially the Deep Q-learning algorithm to learn the agent's decision policy. In addition, we use temporal pooling operation to extract more effective feature representation for the long temporal window, and design a regression network to adjust the position offsets between predicted results and the ground truth. Experiment results on THUMOS 2014 validate the effectiveness of the proposed approach, which can achieve competitive performance with current action detection algorithms via much fewer proposals.
PB  - arXiv
PY  - 2017
ST  - A self-adaptive proposal model for temporal action detection based on reinforcement learning
Y2  - 2025/05/05/21:54:28
DO  - 10.1609/aaai.v32i1.12229
ER  -


TY  - GEN
AU  - Alwassel, H.
AU  - Caba Heilbron, F.
AU  - Ghanem, B.
TI  - Action search: Spotting actions in videos and its application to temporal action localization
AB  - State-of-the-art temporal action detectors inefficiently search the entire video for specific actions. Despite the encouraging progress these methods achieve, it is crucial to design automated approaches that only explore parts of the video which are the most relevant to the actions being searched for. To address this need, we propose the new problem of action spotting in video, which we define as finding a specific action in a video while observing a small portion of that video. Inspired by the observation that humans are extremely efficient and accurate in spotting and finding action instances in video, we propose Action Search, a novel Recurrent Neural Network approach that mimics the way humans spot actions. Moreover, to address the absence of data recording the behavior of human annotators, we put forward the Human Searches dataset, which compiles the search sequences employed by human annotators spotting actions in the AVA and THUMOS14 datasets. We consider temporal action localization as an application of the action spotting problem. Experiments on the THUMOS14 dataset reveal that our model is not only able to explore the video efficiently (observing on average 17.3% of the video) but it also accurately finds human activities with 30.8% mAP.
PB  - arXiv
PY  - 2017
ST  - Action search
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/978-3-030-01240-3_16
ER  -


TY  - GEN
AU  - Biehl, M.
AU  - Polani, D.
TI  - Action and perception for spatiotemporal patterns
AB  - This is a contribution to the formalization of the concept of agents in multivariate Markov chains. Agents are commonly defined as entities that act, perceive, and are goal-directed. In a multivariate Markov chain (e.g. a cellular automaton) the transition matrix completely determines the dynamics. This seems to contradict the possibility of acting entities within such a system. Here we present definitions of actions and perceptions within multivariate Markov chains based on entity- sets. Entity-sets represent a largely independent choice of a set of spatiotemporal patterns that are considered as all the entities within the Markov chain. For example, the entityset can be chosen according to operational closure conditions or complete specific integration. Importantly, the perceptionaction loop also induces an entity-set and is a multivariate Markov chain. We then show that our definition of actions leads to non-heteronomy and that of perceptions specialize to the usual concept of perception in the perception-action loop. MSC Codes 92B20
PB  - arXiv
PY  - 2017
ST  - Action and perception for spatiotemporal patterns
Y2  - 2025/05/05/21:54:28
DO  - 10.7551/ecal_a_015
ER  -


TY  - GEN
AU  - Porto, D.A.
AU  - Giblin, J.
AU  - Zhao, Y.
AU  - Lu, H.
TI  - Reverse-Correlation Analysis of the Mechanosensation Circuit and Behavior in C. elegans Reveals Temporal and Spatial Encoding
AB  - Animals must integrate the activity of multiple mechanoreceptors to navigate complex environments. In Caenorhabditis elegans, the general roles of the mechanosensory neurons have been defined, but most studies involve end-point or single-time-point measurements, and thus lack dynamical information. Here, we formulate a set of unbiased quantitative characterizations of the mechanosensory system by using reverse correlation analysis on behavior. We use a custom tracking, selective illumination, and optogenetics platform to compare two mechanosensory systems: the gentle-touch (TRNs) and harsh-touch (PVD) circuits. This method yields characteristic linear filters that allow for prediction of behavioral responses. The resulting filters are consistent with previous findings, and further provide new insights on the dynamics and spatial encoding of the systems. Our results suggest that the tiled network of the gentle-touch neurons has better resolution for spatial encoding than the harsh-touch neurons. Additionally, linear-nonlinear models can predict behavioral responses based only on sensory neuron activity. Our results capture the overall dynamics of behavior induced by the activation of sensory neurons, providing simple transformations that quantitatively characterize these systems. Furthermore, this platform can be extended to capture the behavioral dynamics induced by any neuron or other excitable cells in the animal.
PB  - bioRxiv
PY  - 2017
ST  - Reverse-Correlation Analysis of the Mechanosensation Circuit and Behavior in C. elegans Reveals Temporal and Spatial Encoding
Y2  - 2025/05/05/21:54:28
DO  - 10.1038/s41598-019-41349-0
ER  -


TY  - GEN
AU  - Duh, A.
AU  - Slak Rupnik, M.
AU  - Korošak, D.
TI  - Collective behaviour of social bots is encoded in their temporal Twitter activity
AB  - Action segmentation as a milestone towards building automatic systems to understand untrimmed videos has received considerable attention in the recent years. It is typically being modeled as a sequence labeling problem but contains intrinsic and sufficient differences than text parsing or speech processing. In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal convolutional kernels that capture the local motion changes of different actions; the decoder is a hierarchy of recurrent neural networks that are able to learn and memorize long-term action dependencies after the encoding stage. Our model is simple but extremely effective in terms of video sequence labeling. The experimental results on three public action segmentation datasets have shown that the proposed model achieves superior performance over the state of the art.
PB  - arXiv
PY  - 2017
ST  - Collective behaviour of social bots is encoded in their temporal Twitter activity
Y2  - 2025/05/05/21:54:28
DO  - 10.1089/big.2017.0041
ER  -


TY  - GEN
AU  - Ding, L.
AU  - Xu, C.
TI  - Tricornet: A hybrid temporal convolutional and recurrent network for video action segmentation
AB  - Action segmentation as a milestone towards building automatic systems to understand untrimmed videos has received considerable attention in the recent years. It is typically being modeled as a sequence labeling problem but contains intrinsic and sufficient differences than text parsing or speech processing. In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal convolutional kernels that capture the local motion changes of different actions; the decoder is a hierarchy of recurrent neural networks that are able to learn and memorize long-term action dependencies after the encoding stage. Our model is simple but extremely effective in terms of video sequence labeling. The experimental results on three public action segmentation datasets have shown that the proposed model achieves superior performance over the state of the art.
PB  - arXiv
PY  - 2017
ST  - Tricornet
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/icip.2019.8803088
ER  -


TY  - GEN
AU  - Jiang, Z.
AU  - Rozgic, V.
AU  - Adali, S.
TI  - Learning spatiotemporal features for infrared action recognition with 3D convolutional neural networks
AB  - Infrared (IR) imaging has the potential to enable more robust action recognition systems compared to visible spectrum cameras due to lower sensitivity to lighting conditions and appearance variability. While the action recognition task on videos collected from visible spectrum imaging has received much attention, action recognition in IR videos is significantly less explored. Our objective is to exploit imaging data in this modality for the action recognition task. In this work, we propose a novel two-stream 3D convolutional neural network (CNN) architecture by introducing the discriminative code layer and the corresponding discriminative code loss function. The proposed network processes IR image and the IR-based optical flow field sequences. We pretrain the 3D CNN model on the visible spectrum Sports-1M action dataset and finetune it on the Infrared Action Recognition (InfAR) dataset. To our best knowledge, this is the first application of the 3D CNN to action recognition in the IR domain. We conduct an elaborate analysis of different fusion schemes (weighted average, single and double-layer neural nets) applied to different 3D CNN outputs. Experimental results demonstrate that our approach can achieve state-of-the-art average precision (AP) performances on the InfAR dataset: (1) the proposed two-stream 3D CNN achieves the best reported 77.5% AP, and (2) our 3D CNN model applied to the optical flow fields achieves the best reported single stream 75.42% AP.
PB  - arXiv
PY  - 2017
ST  - Learning spatiotemporal features for infrared action recognition with 3D convolutional neural networks
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/cvprw.2017.44
ER  -


TY  - GEN
AU  - Metz, L.
AU  - Ibarz, J.
AU  - Jaitly, N.
AU  - Davidson, J.
TI  - Discrete sequential prediction of continuous actions for deep rl
AB  - It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that predict one dimension at a time. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG. We apply the technique to off-policy (Q-learning) methods and show that our method can achieve the state-of-the-art for off-policy methods on several continuous control tasks.
PB  - arXiv
PY  - 2017
ST  - Discrete sequential prediction of continuous actions for deep rl
Y2  - 2025/05/05/21:54:28
DO  - 10.5220/0006772003140319
ER  -


TY  - GEN
AU  - Li, C.
AU  - Chen, C.
AU  - Zhang, B.
AU  - Han, J.
AU  - Ji, R.
TI  - Deep spatio-temporal manifold network for action recognition
AB  - Visual data such as videos are often sampled from complex manifold. We propose leveraging the manifold structure to constrain the deep action feature learning, thereby minimizing the intra-class variations in the feature space and alleviating the over-fitting problem. Considering that manifold can be transferred, layer by layer, from the data domain to the deep features, the manifold priori is posed from the top layer into the back propagation learning procedure of convolutional neural network (CNN). The resulting algorithm -Spatio-Temporal Manifold Network- is solved with the efficient Alternating Direction Method of Multipliers and Backward Propagation (ADMM-BP). We theoretically show that STMN recasts the problem as projection over the manifold via an embedding method. The proposed approach is evaluated on two benchmark datasets, showing significant improvements to the baselines.
PB  - arXiv
PY  - 2017
ST  - Deep spatio-temporal manifold network for action recognition
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/s41870-024-01799-w
ER  -


TY  - GEN
AU  - Wang, L.
AU  - Xiong, Y.
AU  - Wang, Z.
AU  - Tang, X.
AU  - Van Gool, L.
TI  - Temporal segment networks for action recognition in videos
AB  - Deep convolutional networks have achieved great success for image recognition. However, for action recognition in videos, their advantage over traditional methods is not so evident. We present a general and flexible video-level framework for learning action models in videos. This method, called temporal segment network (TSN), aims to model long-range temporal structures with a new segment-based sampling and aggregation module. This unique design enables our TSN to efficiently learn action models by using the whole action videos. The learned models could be easily adapted for action recognition in both trimmed and untrimmed videos with simple average pooling and multi-scale temporal window integration, respectively. We also study a series of good practices for the instantiation of temporal segment network framework given limited training samples. Our approach obtains the state-the-of-art performance on four challenging action recognition benchmarks: HMDB51 (71:0%), UCF101 (94:9%), THUMOS14 (80:1%), and ActivityNet v1.2 (89:6%). Using the proposed RGB difference for motion models, our method can still achieve competitive accuracy on UCF101 (91:0%) while running at 340 FPS. Furthermore, based on the temporal segment networks, we won the video classification track at the ActivityNet challenge 2016 among 24 teams, which demonstrates the effectiveness of temporal segment network and the proposed good practices.
PB  - arXiv
PY  - 2017
ST  - Temporal segment networks for action recognition in videos
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/tpami.2018.2868668
ER  -


TY  - GEN
AU  - Song, Y.
AU  - Wang, R.
AU  - Wang, Y.
AU  - Viventi, J.
TI  - Long Term Prediction Of Neural Activity Using LSTM With Multiple Temporal Resolution
AB  - Epileptic seizures are caused by abnormal, overly synchronized, electrical activity in the brain. The abnormal electrical activity manifests as waves, propagating across the brain. Accurate prediction of the propagation velocity and direction of these waves could enable real-time responsive brain stimulation to suppress or prevent the seizures entirely. However, this problem is very challenging because the algorithm must be able to predict the neural signals in a sufficiently long time horizon to allow enough time for medical intervention. We consider how to accomplish long term prediction using a LSTM network. To alleviate the vanishing gradient problem, we propose two encoder-decoder-predictor structures, both using multi-resolution representation. The novel LSTM structure with multi-resolution layers could significantly outperform the single-resolution benchmark with similar number of parameters. To overcome the blurring effect associated with video prediction in the pixel domain using standard mean square error (MSE) loss, we use energy-based adversarial training to improve the long-term prediction. We demonstrate and analyze how a discriminative model with an encoder-decoder structure using 3D CNN model improves long term prediction.
PB  - arXiv
PY  - 2017
ST  - Long Term Prediction Of Neural Activity Using LSTM With Multiple Temporal Resolution
Y2  - 2025/05/05/21:54:28
ER  -


TY  - GEN
AU  - Gao, J.
AU  - Sun, C.
AU  - Yang, Z.
AU  - Nevatia, R.
TI  - TALL: Temporal activity localization via language query
AB  - This paper focuses on temporal localization of actions in untrimmed videos. Existing methods typically train classifiers for a pre-defined list of actions and apply them in a sliding window fashion. However, activities in the wild consist of a wide combination of actors, actions and objects; it is difficult to design a proper activity list that meets users' needs. We propose to localize activities by natural language queries. Temporal Activity Localization via Language (TALL) is challenging as it requires: (1) suitable design of text and video representations to allow cross-modal matching of actions and language queries; (2) ability to locate actions accurately given features from sliding windows of limited granularity. We propose a novel Cross-modal Temporal Regression Localizer (CTRL) to jointly model text query and video clips, output alignment scores and action boundary regression results for candidate clips. For evaluation, we adopt TaCoS dataset, and build a new dataset for this task on top of Charades by adding sentence temporal annotations, called Charades-STA. We also build complex sentence queries in Charades-STA for test. Experimental results show that CTRL outperforms previous methods significantly on both datasets.
PB  - arXiv
PY  - 2017
ST  - TALL
Y2  - 2025/05/05/21:54:28
ER  -


TY  - GEN
AU  - Gao, J.
AU  - Yang, Z.
AU  - Nevatia, R.
TI  - Cascaded boundary regression for temporal action detection Y 201
AB  - Temporal action detection in long videos is an important problem. State-of-the-art methods address this problem by applying action classifiers on sliding windows. Although sliding windows may contain an identifiable portion of the actions, they may not necessarily cover the entire action instance, which would lead to inferior performance. We adapt a two-stage temporal action detection pipeline with Cascaded Boundary Regression (CBR) model. Class-agnostic proposals and specific actions are detected respectively in the first and the second stage. CBR uses temporal coordinate regression to refine the temporal boundaries of the sliding windows. The salient aspect of the refinement process is that, inside each stage, the temporal boundaries are adjusted in a cascaded way by feeding the refined windows back to the system for further boundary refinement. We test CBR on THUMOS-14 and TVSeries, and achieve state-of-the-art performance on both datasets. The performance gain is especially remarkable under high IoU thresholds, e.g. map@tIoU=0.5 on THUMOS-14 is improved from 19.0% to 31.0%.
PB  - arXiv
PY  - 2017
ST  - Cascaded boundary regression for temporal action detection Y 201
Y2  - 2025/05/05/21:54:28
ER  -


TY  - GEN
AU  - Bouton, S.
AU  - Chambon, V.
AU  - Tyrand, R.
AU  - van de Ville, D.
AU  - Giraud, A.-L.
TI  - Focal versus distributed temporal cortex activity for speech sound category assignment
AB  - Percepts and words can be decoded from largely distributed neural activity measures. The existence of widespread representations might, however, conflict with the fundamental notions of hierarchical processing and efficient coding. Using fMRI and MEG during syllable identification, we first show that sensory and decisional activity co-localize to a restricted part of the posterior superior temporal cortex. Next, using intracortical recordings we demonstrate that early and focal neural activity in this region distinguishes correct from incorrect decisions and can be machine-decoded to classify syllables. Crucially, significant machine-decoding was possible from neuronal activity sampled across widespread regions, despite weak or absent sensory or decision-related responses. These findings show that a complex behavior like speech sound categorization relies on an efficient readout of focal neural activity, while distributed activity, although decodable by machine-learning, reflects collateral processes of sensory perception and decision.
PB  - bioRxiv
PY  - 2017
ST  - Focal versus distributed temporal cortex activity for speech sound category assignment
Y2  - 2025/05/05/21:54:28
ER  -


TY  - GEN
AU  - Cherian, A.
AU  - Gould, S.
TI  - Second-order temporal pooling for action recognition
AB  - Deep learning models for video-based action recognition usually generate features for short clips (consisting of a few frames); such clip-level features are aggregated to video-level representations by computing statistics on these features. Typically zero-th (max) or the first-order (average) statistics are used. In this paper, we explore the benefits of using second-order statistics. Specifically, we propose a novel end-to-end learnable feature aggregation scheme, dubbed temporal correlation pooling that generates an action descriptor for a video sequence by capturing the similarities between the temporal evolution of clip-level CNN features computed across the video. Such a descriptor, while being computationally cheap, also naturally encodes the co-activations of multiple CNN features, thereby providing a richer characterization of actions than their first-order counterparts. We also propose higher-order extensions of this scheme by computing correlations after embedding the CNN features in a reproducing kernel Hilbert space. We provide experiments on benchmark datasets such as HMDB-51 and UCF-101, fine-grained datasets such as MPII Cooking activities and JHMDB, as well as the recent Kinetics-600. Our results demonstrate the advantages of higher-order pooling schemes that when combined with hand-crafted features (as is standard practice) achieves state-of-the-art accuracy.
PB  - arXiv
PY  - 2017
ST  - Second-order temporal pooling for action recognition
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/s11263-018-1111-5
ER  -


TY  - GEN
AU  - Zhao, Y.
AU  - Xiong, Y.
AU  - Wang, L.
AU  - Tang, X.
AU  - Lin, D.
TI  - Temporal action detection with structured segment networks
AB  - Detecting actions in untrimmed videos is an important yet challenging task. In this paper, we present the structured segment network (SSN), a novel framework which models the temporal structure of each action instance via a structured temporal pyramid. On top of the pyramid, we further introduce a decomposed discriminative model comprising two classifiers, respectively for classifying actions and determining completeness. This allows the framework to effectively distinguish positive proposals from background or incomplete ones, thus leading to both accurate recognition and localization. These components are integrated into a unified network that can be efficiently trained in an end-to-end fashion. Additionally, a simple yet effective temporal action proposal scheme, dubbed temporal actionness grouping (TAG) is devised to generate high quality action proposals. On two challenging benchmarks, THUMOS14 and ActivityNet, our method remarkably outperforms previous state-of-the-art methods, demonstrating superior accuracy and strong adaptivity in handling actions with various temporal structures.
PB  - arXiv
PY  - 2017
ST  - Temporal action detection with structured segment networks
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/s11263-019-01211-2
ER  -


TY  - GEN
AU  - Yuan, Z.
AU  - Stroud, J.C.
AU  - Lu, T.
AU  - Deng, J.
TI  - Temporal action localization by structured maximal sums
AB  - We address the problem of temporal action localization in videos. We pose action localization as a structured prediction over arbitrary-length temporal windows, where each window is scored as the sum of frame-wise classification scores. Additionally, our model classifies the start, middle, and end of each action as separate components, allowing our system to explicitly model each action's temporal evolution and take advantage of informative temporal dependencies present in this structure. In this framework, we localize actions by searching for the structured maximal sum, a problem for which we develop a novel, provablyefficient algorithmic solution. The frame-wise classification scores are computed using features from a deep Convolutional Neural Network (CNN), which are trained end-toend to directly optimize for a novel structured objective. We evaluate our system on the THUMOS '14 action detection benchmark and achieve competitive performance.
PB  - arXiv
PY  - 2017
ST  - Temporal action localization by structured maximal sums
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/cvpr.2017.342
ER  -


TY  - GEN
AU  - Kim, T.S.
AU  - Reiter, A.
TI  - Interpretable 3D human action analysis with temporal convolutional networks
AB  - The discriminative power of modern deep learning models for 3D human action recognition is growing ever so potent. In conjunction with the recent resurgence of 3D human action representation with 3D skeletons, the quality and the pace of recent progress have been significant. However, the inner workings of state-of-the-art learning based methods in 3D human action recognition still remain mostly black-box. In this work, we propose to use a new class of models known as Temporal Convolutional Neural Networks (TCN) for 3D human action recognition. Compared to popular LSTM-based Recurrent Neural Network models, given interpretable input such as 3D skeletons, TCN provides us a way to explicitly learn readily interpretable spatio-temporal representations for 3D human action recognition. We provide our strategy in re-designing the TCN with interpretability in mind and how such characteristics of the model is leveraged to construct a powerful 3D activity recognition method. Through this work, we wish to take a step towards a spatio-temporal model that is easier to understand, explain and interpret. The resulting model, Res-TCN, achieves state-of-the-art results on the largest 3D human action recognition dataset, NTU-RGBD. 68T45, 68T10 (Primary)
PB  - arXiv
PY  - 2017
ST  - Interpretable 3D human action analysis with temporal convolutional networks
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/cvprw.2017.207
ER  -


TY  - GEN
AU  - Li, W.
AU  - Abtahi, F.
AU  - Zhu, Z.
TI  - Action Unit Detection with Region Adaptation, Multi-labeling Learning and Optimal Temporal Fusing
AB  - Action Unit (AU) detection becomes essential for facial analysis. Many proposed approaches face challenging problems in dealing with the alignments of different face regions, in the effective fusion of temporal information, and in training a model for multiple AU labels. To better address these problems, we propose a deep learning framework for AU detection with region of interest (ROI) adaptation, integrated multi-label learning, and optimal LSTM-based temporal fusing. First, ROI cropping nets (ROI Nets) are designed to make sure specifically interested regions of faces are learned independently; each sub-region has a local convolutional neural network (CNN) - an ROI Net, whose convolutional filters will only be trained for the corresponding region. Second, multi-label learning is employed to integrate the outputs of those individual ROI cropping nets, which learns the inter-relationships of various AUs and acquires global features across sub-regions for AU detection. Finally, the optimal selection of multiple LSTM layers to form the best LSTM Net is carried out to best fuse temporal features, in order to make the AU prediction the most accurate. The proposed approach is evaluated on two popular AU detection datasets, BP4D and DISFA, outperforming the state of the art significantly, with an average improvement of around 13% on BP4D and 25% on DISFA, respectively.
PB  - arXiv
PY  - 2017
ST  - Action Unit Detection with Region Adaptation, Multi-labeling Learning and Optimal Temporal Fusing
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/cvpr.2017.716
ER  -


TY  - GEN
AU  - Girdhar, R.
AU  - Ramanan, D.
AU  - Gupta, A.
AU  - Sivic, J.
AU  - Russell, B.
TI  - ActionVLAD: Learning spatio-temporal aggregation for action classification
AB  - In this work, we introduce a new video representation for action classification that aggregates local convolutional features across the entire spatio-temporal extent of the video. We do so by integrating state-of-the-art two-stream networks [42] with learnable spatio-temporal feature aggregation [6]. The resulting architecture is end-to-end trainable for whole-video classification. We investigate different strategies for pooling across space and time and combining signals from the different streams. We find that: (i) it is important to pool jointly across space and time, but (ii) appearance and motion streams are best aggregated into their own separate representations. Finally, we show that our representation outperforms the two-stream base architecture by a large margin (13% relative) as well as outperforms other baselines with comparable base architectures on HMDB51, UCF101, and Charades video classification benchmarks.
PB  - arXiv
PY  - 2017
ST  - ActionVLAD
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/cvpr.2017.337
ER  -


TY  - GEN
AU  - Wang, H.
AU  - Wang, L.
TI  - Modeling temporal dynamics and spatial configurations of actions using two-stream recurrent neural networks
AB  - Recently, skeleton based action recognition gains more popularity due to cost-effective depth sensors coupled with real-time skeleton estimation algorithms. Traditional approaches based on handcrafted features are limited to represent the complexity of motion patterns. Recent methods that use Recurrent Neural Networks (RNN) to handle raw skeletons only focus on the contextual dependency in the temporal domain and neglect the spatial configurations of articulated skeletons. In this paper, we propose a novel two-stream RNN architecture to model both temporal dynamics and spatial configurations for skeleton based action recognition. We explore two different structures for the temporal stream: stacked RNN and hierarchical RNN. Hierarchical RNN is designed according to human body kinematics. We also propose two effective methods to model the spatial structure by converting the spatial graph into a sequence of joints. To improve generalization of our model, we further exploit 3D transformation based data augmentation techniques including rotation and scaling transformation to transform the 3D coordinates of skeletons during training. Experiments on 3D action recognition benchmark datasets show that our method brings a considerable improvement for a variety of actions, i.e., generic actions, interaction activities and gestures.
PB  - arXiv
PY  - 2017
ST  - Modeling temporal dynamics and spatial configurations of actions using two-stream recurrent neural networks
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/cvpr.2017.387
ER  -


TY  - GEN
AU  - Ma, C.-Y.
AU  - Chen, M.-H.
AU  - Kira, Z.
AU  - AlRegib, G.
TI  - TS-LSTM and temporal-inception: Exploiting spatiotemporal dynamics for activity recognition
AB  - Recent two-stream deep Convolutional Neural Networks (ConvNets) have made significant progress in recognizing human actions in videos. Despite their success, methods extending the basic two-stream ConvNet have not systematically explored possible network architectures to further exploit spatiotemporal dynamics within video sequences. Further, such networks often use different baseline two-stream networks. Therefore, the differences and the distinguishing factors between various methods using Recurrent Neural Networks (RNN) or convolutional networks on temporally-constructed feature vectors (Temporal-ConvNet) are unclear. In this work, we first demonstrate a strong baseline two-stream ConvNet using ResNet-101. We use this baseline to thoroughly examine the use of both RNNs and Temporal-ConvNets for extracting spatiotemporal information. Building upon our experimental results, we then propose and investigate two different networks to further integrate spatiotemporal information: 1) temporal segment RNN and 2) Inception-style Temporal-ConvNet. We demonstrate that using both RNNs (using LSTMs) and Temporal-ConvNets on spatiotemporal feature matrices are able to exploit spatiotemporal dynamics to improve the overall performance. However, each of these methods require proper care to achieve state-of-the-art performance; for example, LSTMs require pre-segmented data or else they cannot fully exploit temporal information. Our analysis identifies specific limitations for each method that could form the basis of future work. Our experimental results on UCF101 and HMDB51 datasets achieve state-of-the-art performances, 94.1% and 69.0%, respectively, without requiring extensive temporal augmentation.
PB  - arXiv
PY  - 2017
ST  - TS-LSTM and temporal-inception
Y2  - 2025/05/05/21:54:28
DO  - 10.1016/j.image.2018.09.003
ER  -


TY  - GEN
AU  - Baradel, F.
AU  - Wolf, C.
AU  - Mille, J.
TI  - Pose-conditioned Spatio-Temporal Attention for Human Action Recognition
AB  - We address human action recognition from multi-modal video data involving articulated pose and RGB frames and propose a two-stream approach. The pose stream is processed with a convolutional model taking as input a 3D tensor holding data from a sub-sequence. A specific joint ordering, which respects the topology of the human body, ensures that different convolutional layers correspond to meaningful levels of abstraction. The raw RGB stream is handled by a spatio-temporal soft-attention mechanism conditioned on features from the pose network. An LSTM network receives input from a set of image locations at each instant. A trainable glimpse sensor extracts features on a set of predefined locations specified by the pose stream, namely the 4 hands of the two people involved in the activity. Appearance features give important cues on hand motion and on objects held in each hand. We show that it is of high interest to shift the attention to different hands at different time steps depending on the activity itself. Finally a temporal attention mechanism learns how to fuse LSTM features over time. We evaluate the method on 3 datasets. State-of-the-art results are achieved on the largest dataset for human activity recognition, namely NTU-RGB+D, as well as on the SBU Kinect Interaction dataset. Performance close to state-of-the-art is achieved on the smaller MSR Daily Activity 3D dataset.
PB  - arXiv
PY  - 2017
ST  - Pose-conditioned Spatio-Temporal Attention for Human Action Recognition
Y2  - 2025/05/05/21:54:28
DO  - 10.3724/sp.j.1089.2018.16848
ER  -


TY  - GEN
AU  - Clark, N.J.
AU  - Dixon, P.M.
TI  - Modeling and estimation for self-exciting spatio-temporal models of terrorist activity
AB  - Spatio-temporal hierarchical modeling is an extremely attractive way to model the spread of crime or terrorism data over a given region, especially when the observations are counts and must be modeled discretely. The spatio-temporal diffusion is placed, as a matter of convenience, in the process model allowing for straightforward estimation of the diffusion parameters through Bayesian techniques. However, this method of modeling does not allow for the existence of self-excitation, or a temporal data model dependency, that has been shown to exist in criminal and terrorism data. In this manuscript we will use existing theories on how violence spreads to create models that allow for both spatio-temporal diffusion in the process model as well as temporal diffusion, or self-excitation, in the data model. We will further demonstrate how Laplace approximations similar to their use in Integrated Nested Laplace Approximation can be used to quickly and accurately conduct inference of self-exciting spatio-temporal models allowing practitioners a new way of fitting and comparing multiple process models. We will illustrate this approach by fitting a self-exciting spatio-temporal model to terrorism data in Iraq and demonstrate how choice of process model leads to differing conclusions on the existence of self-excitation in the data and differing conclusions on how violence spread spatially-temporally in that country from 2003-2010.
PB  - arXiv
PY  - 2017
ST  - Modeling and estimation for self-exciting spatio-temporal models of terrorist activity
Y2  - 2025/05/05/21:54:28
DO  - 10.1214/17-aoas1112
ER  -


TY  - GEN
AU  - Xu, H.
AU  - Das, A.
AU  - Saenko, K.
TI  - R-C3D: Region convolutional 3D network for temporal activity detection
AB  - We address the problem of activity detection in continuous, untrimmed video streams. This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity. We introduce a new model, Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities, and finally classifies selected regions into specific activities. Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines. The entire model is trained end-to-end with jointly optimized localization and classification losses. R-C3D is faster than existing methods (569 frames per second on a single Titan X Maxwell GPU) and achieves state-of-the-art results on THUMOS’14. We further demonstrate that our model is a general activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on ActivityNet and Charades. Our code is available at http://ai.bu.edu/r-c3d/
PB  - arXiv
PY  - 2017
ST  - R-C3D
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/iccv.2017.617
ER  -


TY  - GEN
AU  - Gao, J.
AU  - Yang, Z.
AU  - Sun, C.
AU  - Chen, K.
AU  - Nevatia, R.
TI  - Turn tap: Temporal unit regression network for temporal action proposals
AB  - Temporal Action Proposal (TAP) generation is an important problem, as fast and accurate extraction of semantically important (e.g. human actions) segments from untrimmed videos is an important step for large-scale video analysis. We propose a novel Temporal Unit Regression Network (TURN) model. There are two salient aspects of TURN: (1) TURN jointly predicts action proposals and refines the temporal boundaries by temporal coordinate regression; (2) Fast computation is enabled by unit feature reuse: a long untrimmed video is decomposed into video units, which are reused as basic building blocks of temporal proposals. TURN outperforms the previous state-of-the-art methods under average recall (AR) by a large margin on THUMOS-14 and ActivityNet datasets, and runs at over 880 frames per second (FPS) on a TITAN X GPU. We further apply TURN as a proposal generation stage for existing temporal action localization pipelines, it outperforms state-of-the-art performance on THUMOS-14 and ActivityNet.
PB  - arXiv
PY  - 2017
ST  - Turn tap
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/iccv.2017.392
ER  -


TY  - GEN
AU  - Xiong, Y.
AU  - Zhao, Y.
AU  - Wang, L.
AU  - Lin, D.
AU  - Tang, X.
TI  - A pursuit of temporal accuracy in general activity detection
AB  - Detecting activities in untrimmed videos is an important but challenging task. The performance of existing methods remains unsatisfactory, e.g. they often meet difficulties in locating the beginning and end of a long complex action. In this paper, we propose a generic framework that can accurately detect a wide variety of activities from untrimmed videos. Our first contribution is a novel proposal scheme that can efficiently generate candidates with accurate temporal boundaries. The other contribution is a cascaded classification pipeline that explicitly distinguishes between relevance and completeness of a candidate instance. On two challenging temporal activity detection datasets, THUMOS14 and ActivityNet, the proposed framework significantly outperforms the existing state-of-the-art methods, demonstrating superior accuracy and strong adaptivity in handling activities with various temporal structures.
PB  - arXiv
PY  - 2017
ST  - A pursuit of temporal accuracy in general activity detection
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/cvpr42600.2020.00096
ER  -


TY  - GEN
AU  - Shou, Z.
AU  - Chan, J.
AU  - Zareian, A.
AU  - Miyazawa, K.
AU  - Chang, S.-F.
TI  - CDC: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos
AB  - Temporal action localization is an important yet challenging problem. Given a long, untrimmed video consisting of multiple action instances and complex background contents, we need not only to recognize their action categories, but also to localize the start time and end time of each instance. Many state-of-the-art systems use segment-level classifiers to select and rank proposal segments of predetermined boundaries. However, a desirable model should move beyond segment-level and make dense predictions at a fine granularity in time to determine precise temporal boundaries. To this end, we design a novel Convolutional-De-Convolutional (CDC) network that places CDC filters on top of 3D ConvNets, which have been shown to be effective for abstracting action semantics but reduce the temporal length of the input data. The proposed CDC filter performs the required temporal upsampling and spatial downsampling operations simultaneously to predict actions at the frame-level granularity. It is unique in jointly modeling action semantics in space-time and fine-grained temporal dynamics. We train the CDC network in an end-to-end manner efficiently. Our model not only achieves superior performance in detecting actions in every frame, but also significantly boosts the precision of localizing temporal boundaries. Finally, the CDC network demonstrates a very high efficiency with the ability to process 500 frames per second on a single GPU server. Source code and trained models are available online at https://bitbucket.org/columbiadvmm/cdc.
PB  - arXiv
PY  - 2017
ST  - CDC
Y2  - 2025/05/05/21:54:28
DO  - 10.1109/cvpr.2017.155
ER  -


TY  - GEN
AU  - Du, Z.
AU  - Yang, B.
AU  - Liu, J.
TI  - Understanding the spatial and temporal activity patterns of subway mobility flows
AB  - In urban transportation systems, mobility flows in the subway system reflect the spatial and temporal dynamics of working days. To investigate the variability of mobility flows, we analyse the spatial community through a series of snapshots of subway stations over sequential periods. Using Shanghai as a case study, we find that the spatial community snapshots reveal dynamic passenger activities. Adopting a dual-perspective, we apply spatial and temporal models separately to explore where and when individuals travel for entertainment. In the two models, microblog topics and spatial facilities such as food venues and entertainment businesses are used to characterise the spatial popularity of each station and people’s travelling perceptions. In the studied case, the city centre is characterised by greater social influence, and it is better described by the spatial model. In the temporal model, shorter travel distances motivate individuals to start their trips earlier. Interestingly, as the number of food-related facilities near the starting station increases, until it exceeds 1563, the speed of people’s journeys slows down. This study provides a method for modelling the effects of social features on mobility flows and for predicting the spatial-temporal mobility flows of newly built subway stations.
PB  - arXiv
PY  - 2017
ST  - Understanding the spatial and temporal activity patterns of subway mobility flows
Y2  - 2025/05/05/21:54:28
DO  - 10.1007/978-981-16-3021-7
ER  -


TY  - GEN
AU  - Reznik, D.
AU  - Simon, S.
AU  - Mukamel, R.
TI  - Predicted sensory consequences of voluntary actions modulate amplitude and temporal dynamics of preceding readiness potentials
AB  - Self-generated, voluntary actions, are preceded by a slow negativity in the scalp electroencephalography (EEG) signal recorded from frontal regions (termed ‘readiness potential’; RP). This signal, and its lateralized subcomponent (LRP), is mainly regarded as preparatory motor activity associated with the forthcoming motor act. However, it is not clear whether this neural signature is associated with preparatory motor activity, expectation of its associated sensory consequences, or both. Here we recorded EEG data from 12 healthy subjects while they performed self-paced button presses with their right index and middle fingers. In one condition (motor+sound) these button-presses triggered a sound while in another (motor-only) they did not. Additionally, subjects passively listened to sounds delivered in expected timings (sound-only). We found that the RP amplitude (locked to time of button press) was significantly more negative in the motor+sound compared with motor-only conditions starting ~1.4 seconds prior to button press. Importantly, no signal negativity was observed prior to expected sound delivery in the sound-only condition. Thus, the differences in RP amplitude between motor+sound and motor-only conditions are beyond differences in mere expectation of a forthcoming auditory sound. No significant differences between the two conditions were obtained in the LRP component. Our results suggest that expected auditory consequences are encoded in the early phase of the RP preceding the voluntary actions that generate them.
PB  - bioRxiv
PY  - 2017
ST  - Predicted sensory consequences of voluntary actions modulate amplitude and temporal dynamics of preceding readiness potentials
Y2  - 2025/05/05/21:54:28
DO  - 10.1101/101402
ER  -


TY  - GEN
AU  - Jankowiak, M.
AU  - Gomez-Rodriguez, M.
TI  - Uncovering the spatiotemporal patterns of collective social activity
AB  - Social media users and microbloggers post about a wide variety of (off-line) collective social activities as they participate in them, ranging from concerts and sporting events to political rallies and civil protests. In this context, people who take part in the same collective social activ-ity often post closely related content from nearby locations at similar times, resulting in distinctive spatiotemporal patterns. Can we au-tomatically detect these patterns and thus provide insights into the associated activities? In this paper, we propose a modeling frame-work for clustering streaming spatiotemporal data, the Spatial Dirich-let Hawkes Process (SDHP), which allows us to automatically uncover a wide variety of spatiotemporal patterns of collective social activity from geolocated online traces. Moreover, we develop an efficient, on-line inference algorithm based on Sequential Monte Carlo that scales to millions of geolocated posts. Experiments on synthetic data and real data gathered from Twitter show that our framework can recover a wide variety of meaningful social activity patterns in terms of both content and spatiotemporal dynamics, that it yields interesting insights about these patterns, and that it can be used to estimate the location from where a tweet was posted.
PB  - arXiv
PY  - 2017
ST  - Uncovering the spatiotemporal patterns of collective social activity
Y2  - 2025/05/05/21:54:28
DO  - 10.1137/1.9781611974973.92
ER  -


TY  - GEN
AU  - Li, Q.
AU  - Mo, H.
AU  - Zhao, J.
AU  - Hao, H.
AU  - Li, H.
TI  - Spatio-temporal dual affine differential invariant for skeleton-based action recognition
AB  - The dynamics of human skeletons have significant information for the task of action recognition. The similarity between trajectories of corresponding joints is an indicating feature of the same action, while this similarity may subject to some distortions that can be modeled as the combination of spatial and temporal affine transformations. In this work, we propose a novel feature called spatio-temporal dual affine differential invariant (STDADI). Furthermore, in order to improve the generalization ability of neural networks, a channel augmentation method is proposed. On the large scale action recognition dataset NTU-RGB+D, and its extended version NTU-RGB+D 120, it achieves remarkable improvements over previous state-of-the-art methods.
PB  - arXiv
PY  - 2020
ST  - Spatio-temporal dual affine differential invariant for skeleton-based action recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.11834/jig.200453
ER  -


TY  - GEN
AU  - Vaudaux-Ruth, G.
AU  - Chan-Hon-Tong, A.
AU  - Achard, C.
TI  - ActionSpotter: Deep reinforcement learning framework for temporal action spotting in videos
AB  - —Summarizing video content is an important task in many applications. This task can be defined as the computation of the ordered list of actions present in a video. Such a list could be extracted using action detection algorithms. However, it is not necessary to determine the temporal boundaries of actions to know their existence. Moreover, localizing precise boundaries usually requires dense video analysis to be effective. In this work, we propose to directly compute this ordered list by sparsely browsing the video and selecting one frame per action instance, task known as action spotting in literature. To do this, we propose ActionSpotter, a spotting algorithm that takes advantage of Deep Reinforcement Learning to efficiently spot actions while adapting its video browsing speed, without additional supervision. Experiments performed on datasets THUMOS14 and ActivityNet show that our framework outperforms state-of-the-art detection methods. In particular, the spotting mean Average Precision on THUMOS14 is significantly improved from 59.7% to 65.6% while skipping 23% of video.
PB  - arXiv
PY  - 2020
ST  - ActionSpotter
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icpr48806.2021.9413153
ER  -


TY  - GEN
AU  - Han, Y.
AU  - Zhou, Z.
AU  - Zhou, Z.
AU  - Glynn, P.W.
AU  - Ye, Y.
TI  - Sequential batch learning in finite-action linear contextual bandits
AB  - We study the sequential batch learning problem in linear contextual bandits with finite action sets, where the decision maker is constrained to split incoming individuals into (at most) a fixed number of batches and can only observe outcomes for the individuals within a batch at the batch’s end. Compared to both standard online contextual bandits learning or offline policy learning in contexutal bandits, this sequential batch learning problem provides a finer-grained formulation of many personalized sequential decision making problems in practical applications, including medical treatment in clinical trials, product recommendation in e-commerce and adaptive experiment design in crowdsourcing. We study two settings of the problem: one where the contexts are arbitrarily generated and the other where the contexts are iid drawn from some distribution. In each setting, we establish a regret lower bound and provide an algorithm, whose regret upper bound nearly matches the lower bound. As an important insight revealed therefrom, in the former setting, we show that the number of batches required to achieve the fully online performance is polynomial in the time horizon, while for the latter setting, a pure-exploitation algorithm with a judicious batch partition scheme achieves the fully online performance even when the number of batches is less than logarithmic in the time horizon. Together, our results provide a near-complete characterization of sequential decision making in linear contextual bandits when batch constraints are present.
PB  - arXiv
PY  - 2020
ST  - Sequential batch learning in finite-action linear contextual bandits
Y2  - 2025/05/05/21:54:30
DO  - 10.1287/mnsc.2023.4895
ER  -


TY  - GEN
AU  - Wang, K.
AU  - He, J.
AU  - Zhang, L.
TI  - Sequential weakly labeled multi-activity localization and recognition on wearable sensors using recurrent attention networks
AB  - With the popularity and development of the wearable devices such as smartphones, human activity recognition (HAR) based on sensors has become as a key research area in human computer interaction and ubiquitous computing. The emergence of deep learning leads to a recent shift in the research of HAR, which requires massive strictly labeled data. In comparison with video data, activity data recorded from accelerometer or gyroscope is often more difficult to interpret and segment. Recently, several attention mechanisms are proposed to handle the weakly labeled human activity data, which do not require accurate data annotation. However, these attentionbased models can only handle the weakly labeled dataset whose sample includes one target activity, as a result it limits efficiency and practicality. In the paper, we propose a recurrent attention networks (RAN) to handle sequential weakly labeled multiactivity recognition and location tasks. The model can repeatedly perform steps of attention on multiple activities of one sample and each step is corresponding to the current focused activity. The effectiveness of the RAN model is validated on a collected sequential weakly labeled multi-activity dataset and the other two public datasets. The experiment results show that our RAN model can simultaneously infer multi-activity types from the coarsegrained sequential weak labels and determine specific locations of every target activity with only knowledge of which types of activities contained in the long sequence. It will greatly reduce the burden of manual labeling. The code of our work is available at https://github.com/KennCoder7/RAN.
PB  - arXiv
PY  - 2020
ST  - Sequential weakly labeled multi-activity localization and recognition on wearable sensors using recurrent attention networks
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/thms.2021.3086008
ER  -


TY  - GEN
AU  - Yang, C.
AU  - Xu, Y.
AU  - Shi, J.
AU  - Dai, B.
AU  - Zhou, B.
TI  - Temporal pyramid network for action recognition
AB  - Visual tempo characterizes the dynamics and the temporal scale of an action. Modeling such visual tempos of different actions facilitates their recognition. Previous works often capture the visual tempo through sampling raw videos at multiple rates and constructing an input-level frame pyramid, which usually requires a costly multi-branch network to handle. In this work we propose a generic Temporal Pyramid Network (TPN) at the feature-level, which can be flexibly integrated into 2D or 3D backbone networks in a plug-and-play manner. Two essential components of TPN, the source of features and the fusion of features, form a feature hierarchy for the backbone so that it can capture action instances at various tempos. TPN also shows consistent improvements over other challenging baselines on several action recognition datasets. Specifically, when equipped with TPN, the 3D ResNet-50 with dense sampling obtains a 2% gain on the validation set of Kinetics-400. A further analysis also reveals that TPN gains most of its improvements on action classes that have large variances in their visual tempos, validating the effectiveness of TPN.
PB  - arXiv
PY  - 2020
ST  - Temporal pyramid network for action recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvpr42600.2020.00067
ER  -


TY  - GEN
AU  - Carreño-Muñoz, M.I.
AU  - Medrano, M.C.
AU  - Leinekugel, T.
AU  - Grana, M.
AU  - Leinekugel, X.
TI  - Detecting fine and elaborate movements with piezo sensors, from heartbeat to the temporal organization of behavior
AB  - Behavioral phenotyping devices have been successfully used to build ethograms, but studying the temporal dynamics of individual movements during spontaneous, ongoing behavior, remains a challenge. We now report on a novel device, the Phenotypix, which consists in an open-field platform resting on highly sensitive piezoelectric (electro-mechanical) pressure-sensors, with which we could detect the slightest movements from freely moving rats and mice. The combination with video recordings and signal analysis based on time-frequency decomposition, clustering and machine learning algorithms allowed to quantify various behavioral components with unprecedented accuracy, such as individual heartbeats and breathing cycles during rest, shaking in response to pain or fear, and the dynamics of balance within individual footsteps during spontaneous locomotion. We believe that this device represents a significant progress and offers new opportunities for the awaited advance of behavioral phenotyping.
PB  - bioRxiv
PY  - 2020
ST  - Detecting fine and elaborate movements with piezo sensors, from heartbeat to the temporal organization of behavior
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/2020.04.03.024711
ER  -


TY  - GEN
AU  - Li, Y.
AU  - Ji, B.
AU  - Shi, X.
AU  - Kang, B.
AU  - Wang, L.
TI  - TEA: Temporal excitation and aggregation for action recognition
AB  - Temporal modeling is key for action recognition in videos. It normally considers both short-range motions and long-range aggregations. In this paper, we propose a Temporal Excitation and Aggregation (TEA) block, including a motion excitation (ME) module and a multiple temporal aggregation (MTA) module, specifically designed to capture both short- and long-range temporal evolution. In particular, for short-range motion modeling, the ME module calculates the feature-level temporal differences from spatiotemporal features. It then utilizes the differences to excite the motion-sensitive channels of the features. The long-range temporal aggregations in previous works are typically achieved by stacking a large number of local temporal convolutions. Each convolution processes a local temporal window at a time. In contrast, the MTA module proposes to deform the local convolution to a group of sub-convolutions, forming a hierarchical residual architecture. Without introducing additional parameters, the features will be processed with a series of sub-convolutions, and each frame could complete multiple temporal aggregations with neighborhoods. The final equivalent receptive field of temporal dimension is accordingly enlarged, which is capable of modeling the long-range temporal relationship over distant frames. The two components of the TEA block are complementary in temporal modeling. Finally, our approach achieves impressive results at low FLOPs on several action recognition benchmarks, such as Kinetics, Something-Something, HMDB51, and UCF101, which confirms its effectiveness and efficiency.
PB  - arXiv
PY  - 2020
ST  - TEA
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvpr42600.2020.00099
ER  -


TY  - GEN
AU  - Rennig, J.
AU  - Beauchamp, M.S.
TI  - Linking Activity in Human Superior Temporal Cortex to Perception of Noisy Audiovisual Speech
AB  - Regions of the human posterior superior temporal gyrus and sulcus (pSTG/S) respond to the visual mouth movements that constitute visual speech and the auditory vocalizations that constitute auditory speech. We hypothesized that these multisensory responses in pSTG/S underlie the observation that comprehension of noisy auditory speech is improved when it is accompanied by visual speech. To test this idea, we presented audiovisual sentences that contained either a clear auditory component or a noisy auditory component while measuring brain activity using BOLD fMRI. Participants reported the intelligibility of the speech on each trial with a button press. Perceptually, adding visual speech to noisy auditory sentences rendered them much more intelligible. Post-hoc trial sorting was used to examine brain activations during noisy sentences that were more or less intelligible, focusing on multisensory speech regions in the pSTG/S identified with an independent visual speech localizer. Univariate analysis showed that less intelligible noisy audiovisual sentences evoked a weaker BOLD response, while more intelligible sentences evoked a stronger BOLD response that was indistinguishable from clear sentences. To better understand these differences, we conducted a multivariate representational similarity analysis. The pattern of response for intelligible noisy audiovisual sentences was more similar to the pattern for clear sentences, while the response pattern for unintelligible noisy sentences was less similar. These results show that for both univariate and multivariate analyses, successful integration of visual and noisy auditory speech normalizes responses in pSTG/S, providing evidence that multisensory subregions of pSTG/S are responsible for the perceptual benefit of visual speech.
PB  - bioRxiv
PY  - 2020
ST  - Linking Activity in Human Superior Temporal Cortex to Perception of Noisy Audiovisual Speech
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/2020.04.02.021774
ER  -


TY  - GEN
AU  - Fayyaz, M.
AU  - Gall, J.
TI  - SCT: Set Constrained Temporal Transformer for Set Supervised Action Segmentation
AB  - Temporal action segmentation is a topic of increasing interest, however, annotating each frame in a video is cumbersome and costly. Weakly supervised approaches therefore aim at learning temporal action segmentation from videos that are only weakly labeled. In this work, we assume that for each training video only the list of actions is given that occur in the video, but not when, how often, and in which order they occur. In order to address this task, we propose an approach that can be trained end-to-end on such data. The approach divides the video into smaller temporal regions and predicts for each region the action label and its length. In addition, the network estimates the action labels for each frame. By measuring how consistent the frame-wise predictions are with respect to the temporal regions and the annotated action labels, the network learns to divide a video into class-consistent regions. We evaluate our approach on three datasets where the approach achieves state-of-the-art results.
PB  - arXiv
PY  - 2020
ST  - SCT
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvpr42600.2020.00058
ER  -


TY  - GEN
AU  - Xu, H.
AU  - Yang, L.
AU  - Sclaroff, S.
AU  - Saenko, K.
AU  - Darrell, T.
TI  - Spatio-temporal action detection with multi-object interaction
AB  - Spatio-temporal action detection in videos requires localizing the action both spatially and temporally in the form of an “action tube.” Nowadays, most spatio-temporal action detection datasets (e.g. UCF101-24, AVA, DALY) are annotated with action tubes that contain a single person performing the action, thus the predominant action detection models simply employ a person detection and tracking pipeline for localization. However, when the action is defined as an interaction between multiple objects, such methods may fail since each bounding box in the action tube contains multiple objects instead of one person. In this paper, we study the spatio-temporal action detection problem with multi-object interaction. We introduce a new dataset that is annotated with action tubes containing multi-object interactions. Moreover, we propose an end-to-end spatio-temporal action detection model that performs both spatial and temporal regression simultaneously. Our spatial regression may enclose multiple objects participating in the action. During test time, we simply connect the regressed bounding boxes within the predicted temporal duration using a simple heuristic. We report the baseline results of our proposed model on this new dataset, and also show competitive results on the standard benchmark UCF101-24 using only RGB input.
PB  - arXiv
PY  - 2020
ST  - Spatio-temporal action detection with multi-object interaction
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/s00530-025-01796-4
ER  -


TY  - GEN
AU  - Chen, Z.
AU  - Grayden, D.B.
AU  - Burkitt, A.N.
AU  - Cook, M.J.
AU  - Maturana, M.I.
TI  - Spatiotemporal patterns of high-frequency activity (80-170 Hz) in long-term intracranial EEG
AB  - Objective: To assess the variability in the rates and locations of high-frequency activity (HFA) and epileptiform spikes after electrode implantation, and to examine the long-term patterns of HFA using ambulatory intracranial EEG (iEEG) recordings. Methods: Continuous iEEG recordings obtained over an average of 1.4 years from 15 patients with drug-resistant focal epilepsy were used in this study. HFA was defined as high-frequency events with amplitudes clearly larger than the background, which was automatically detected using a custom algorithm. High-frequency oscillations (HFOs) were also visually annotated by three neurologists in randomly sampled segments of the total data. The automatically detected HFA was compared with the visually marked HFOs. The variations of HFA rates were compared with spikes and seizures on patient-specific and electrode-specific bases. Results: HFA was a more general event that encompassed HFOs manually annotated by different reviewers. HFA and spike rates had high amounts of intra- and inter-patient variability. The rates and locations of HFA and spikes took up to weeks to stabilize after electrode implantation in some patients. Both HFA and spike rates showed strong circadian rhythms in all patients and some also showed multiday cycles. Furthermore, the circadian patterns of HFA and spike rates had patient-specific correlations with seizures, which tended to vary across electrodes. Conclusions: Analysis of HFA and epileptiform spikes should account for post-implantation variability. Like seizures, HFA and epileptiform spikes show circadian rhythms. However, the circadian profiles can vary spatially within patients and their correlations to seizures are patient-specific.
PB  - bioRxiv
PY  - 2020
ST  - Spatiotemporal patterns of high-frequency activity (80-170 Hz) in long-term intracranial EEG
Y2  - 2025/05/05/21:54:30
DO  - 10.1212/wnl.0000000000011408
ER  -


TY  - GEN
AU  - Obinata, Y.
AU  - Yamamoto, T.
TI  - Temporal Extension Module for Skeleton-Based Action Recognition
AB  - We present a module that extends the temporal graph of a graph convolutional network (GCN) for action recognition with a sequence of skeletons. Existing methods attempt to represent a more appropriate spatial graph on an intra-frame, but disregard optimization of the temporal graph on the inter-frame. In this work, we focus on adding extra edges to neighboring multiple vertices on the inter-frame and extracting additional features based on the extended temporal graph. Our module is a simple yet effective method to extract correlated features of multiple joints in human movement. Moreover, our module aids in further performance improvements, along with other GCN methods that optimize only the spatial graph. We conduct extensive experiments on two large datasets, NTU RGB+D and Kinetics-Skeleton, and demonstrate that our module is effective for several existing models and our final model achieves competitive or state-of-the-art performance.
PB  - arXiv
PY  - 2020
ST  - Temporal Extension Module for Skeleton-Based Action Recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icpr48806.2021.9412113
ER  -


TY  - GEN
AU  - Li, X.
AU  - Wang, J.
AU  - Ma, L.
AU  - Kang, Z.
AU  - Wang, J.
TI  - STH: Spatio-temporal hybrid convolution for efficient action recognition
AB  - Effective and Efficient spatio-temporal modeling is essential for action recognition. Existing methods suffer from the trade-off between model performance and model complexity. In this paper, we present a novel Spatio-Temporal Hybrid Convolution Network (denoted as “STH”) which simultaneously encodes spatial and temporal video information with a small parameter cost. Different from existing works that sequentially or parallelly extract spatial and temporal information with different convolutional layers, we divide the input channels into multiple groups and interleave the spatial and temporal operations in one convolutional layer, which deeply incorporates spatial and temporal clues. Such a design enables efficient spatio-temporal modeling and maintains a small model scale. STH-Conv is a general building block, which can be plugged into existing 2D CNN architectures such as ResNet and MobileNet by replacing the conventional 2D-Conv blocks (2D convolutions). STH network achieves competitive or even better performance than its competitors on benchmark datasets such as Something-Something (V1 & V2), Jester, and HMDB-51. Moreover, STH enjoys performance superiority over 3D CNNs while maintaining an even smaller parameter cost than 2D CNNs.
PB  - arXiv
PY  - 2020
ST  - STH
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icaica50127.2020.9182498
ER  -


TY  - GEN
AU  - Ma, F.
AU  - Zhu, L.
AU  - Yang, Y.
AU  - Feiszli, M.
AU  - Shou, Z.
TI  - SF-Net: Single-frame supervision for temporal action localization
AB  - In this paper, we study an intermediate form of supervision, i.e., single-frame supervision, for temporal action localization (TAL). To obtain the single-frame supervision, the annotators are asked to identify only a single frame within the temporal window of an action. This can significantly reduce the labor cost of obtaining full supervision which requires annotating the action boundary. Compared to the weak supervision that only annotates the video-level label, the single-frame supervision introduces extra temporal action signals while maintaining low annotation overhead. To make full use of such single-frame supervision, we propose a unified system called SF-Net. First, we propose to predict an actionness score for each video frame. Along with a typical category score, the actionness score can provide comprehensive information about the occurrence of a potential action and aid the temporal boundary refinement during inference. Second, we mine pseudo action and background frames based on the single-frame annotations. We identify pseudo action frames by adaptively expanding each annotated single frame to its nearby, contextual frames and we mine pseudo background frames from all the unannotated frames across multiple videos. Together with the ground-truth labeled frames, these pseudo-labeled frames are further used for training the classifier. In extensive experiments on THUMOS14, GTEA, and BEOID, SF-Net significantly improves upon state-of-the-art weakly-supervised methods in terms of both segment localization and single-frame localization. Notably, SF-Net achieves comparable results to its fully-supervised counterpart which requires much more resource intensive annotations.
PB  - arXiv
PY  - 2020
ST  - SF-Net
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/978-3-030-58548-8_25
ER  -


TY  - GEN
AU  - Subramanian, S.
AU  - Barbieri, R.
AU  - Brown, E.N.
TI  - Point process temporal structure characterizes electrodermal activity
AB  - Electrodermal activity (EDA) is a read-out of the body’s sympathetic nervous system measured as sweat-induced changes in the electrical conductance properties of the skin. There is growing interest in using EDA to track physiological conditions such as stress levels, sleep quality and emotional states. Standardized EDA data analysis methods are readily available. However, none considers two established physiological features of EDA: 1) sympathetically mediated pulsatile changes in skin sweat measured as EDA resemble an integrate-and-fire process; 2) inter-pulse interval times vary depending upon the local physiological state of the skin. Based on the anatomy and physiology that underlie feature 1, we postulate that inverse Gaussian probability models would accurately describe EDA inter-pulse intervals. Given feature 2, we postulate that under fluctuating local physiological states, the inter-pulse intervals would follow mixtures of inverse Gaussian models, that can be represented as lognormal models if the conditions favor longer intervals (heavy tails) or by gamma models if the conditions favor shorter intervals (light tails). To assess the validity of these probability models we recorded and analyzed EDA measurements in 11 healthy volunteers during 1 to 2 hours of quiet wakefulness. We assess the tail behavior of the probability models by computing their settling rates. All data series were accurately described by one or more of the models: two by inverse Gaussian models; five by lognormal models and three by gamma models. These probability models suggest a highly succinct point process framework for real-time tracking of sympathetically-mediated changes in physiological state.
PB  - bioRxiv
PY  - 2020
ST  - Point process temporal structure characterizes electrodermal activity
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/2020.03.11.982843
ER  -


TY  - GEN
AU  - Zhang, L.
AU  - Chang, X.
AU  - Liu, J.
AU  - Ge, Z.
AU  - Hauptmann, A.
TI  - ZSTAD: Zero-shot temporal activity detection
AB  - An integral part of video analysis and surveillance is temporal activity detection, which means to simultaneously recognize and localize activities in long untrimmed videos. Currently, the most effective methods of temporal activity detection are based on deep learning, and they typically perform very well with large scale annotated videos for training. However, these methods are limited in real applications due to the unavailable videos about certain activity classes and the time-consuming data annotation. To solve this challenging problem, we propose a novel task setting called zero-shot temporal activity detection (ZSTAD), where activities that have never been seen in training can still be detected. We design an end-to-end deep network based on R-C3D as the architecture for this solution. The proposed network is optimized with an innovative loss function that considers the embeddings of activity labels and their super-classes while learning the common semantics of seen and unseen activities. Experiments on both the THUMOS14 and the Charades datasets show promising performance in terms of detecting unseen activities.
PB  - arXiv
PY  - 2020
ST  - ZSTAD
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvpr42600.2020.00096
ER  -


TY  - GEN
AU  - Gao, J.
AU  - Shi, Z.
AU  - Li, J.
AU  - Ge, S.
AU  - Zhou, X.
TI  - Accurate temporal action proposal generation with relation-aware pyramid network
AB  - Accurate temporal action proposals play an important role in detecting actions from untrimmed videos. The existing approaches have difficulties in capturing global contextual information and simultaneously localizing actions with different durations. To this end, we propose a Relation-aware pyramid Network (RapNet) to generate highly accurate temporal action proposals. In RapNet, a novel relation-aware module is introduced to exploit bi-directional long-range relations between local features for context distilling. This embedded module enhances the RapNet in terms of its multi-granularity temporal proposal generation ability, given predefined anchor boxes. We further introduce a two-stage adjustment scheme to refine the proposal boundaries and measure their confidence in containing an action with snippet-level actionness. Extensive experiments on the challenging ActivityNet and THUMOS14 benchmarks demonstrate our RapNet generates superior accurate proposals over the existing state-of-the-art methods.
PB  - arXiv
PY  - 2020
ST  - Accurate temporal action proposal generation with relation-aware pyramid network
Y2  - 2025/05/05/21:54:30
DO  - 10.1609/aaai.v34i07.6711
ER  -


TY  - GEN
AU  - Wang, W.
AU  - Peng, X.
AU  - Su, Y.
AU  - Qiao, Y.
AU  - Cheng, J.
TI  - TTPP: Temporal transformer with progressive prediction for efficient action anticipation
AB  - Video action anticipation aims to predict future action categories from observed frames. Current state-of-the-art approaches mainly resort to recurrent neural networks to encode history information into hidden states, and predict future actions from the hidden representations. It is well known that the recurrent pipeline is inefficient in capturing long-term information which may limit its performance in predication task. To address this problem, this paper proposes a simple yet efficient Temporal Transformer with Progressive Prediction (TTPP) framework, which repurposes a Transformer-style architecture to aggregate observed features, and then leverages a light-weight network to progressively predict future features and actions. Specifically, predicted features along with predicted probabilities are accumulated into the inputs of subsequent prediction. We evaluate our approach on three action datasets, namely TVSeries, THUMOS-14, and TV-Human-Interaction. Additionally we also conduct a comprehensive study for several popular aggregation and prediction strategies. Extensive results show that TTPP not only outperforms the state-of-the-art methods but also more efficient.
PB  - arXiv
PY  - 2020
ST  - TTPP
Y2  - 2025/05/05/21:54:30
DO  - 10.1016/j.neucom.2021.01.087
ER  -


TY  - GEN
AU  - Chen, M.-H.
AU  - Li, B.
AU  - Bao, Y.
AU  - AlRegib, G.
AU  - Kira, Z.
TI  - Action segmentation with joint self-supervised temporal domain adaptation
AB  - Despite the recent progress of fully-supervised action segmentation techniques, the performance is still not fully satisfactory. One main challenge is the problem of spatiotemporal variations (e.g. different people may perform the same activity in various ways). Therefore, we exploit unlabeled videos to address this problem by reformulating the action segmentation task as a cross-domain problem with domain discrepancy caused by spatio-temporal variations. To reduce the discrepancy, we propose Self-Supervised Temporal Domain Adaptation (SSTDA), which contains two self-supervised auxiliary tasks (binary and sequential domain prediction) to jointly align cross-domain feature spaces embedded with local and global temporal dynamics, achieving better performance than other Domain Adaptation (DA) approaches. On three challenging benchmark datasets (GTEA, 50Salads, and Breakfast), SSTDA outperforms the current state-of-the-art method by large margins (e.g. for the F1@25 score, from 59.6% to 69.1% on Breakfast, from 73.4% to 81.5% on 50Salads, and from 83.6% to 89.1% on GTEA), and requires only 65% of the labeled training data for comparable performance, demonstrating the usefulness of adapting to unlabeled target videos across variations. The source code is available at https://github.com/cmhungsteve/SSTDA.
PB  - arXiv
PY  - 2020
ST  - Action segmentation with joint self-supervised temporal domain adaptation
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvpr42600.2020.00947
ER  -


TY  - GEN
AU  - Fernandez-Carmona, M.
AU  - Mghames, S.
AU  - Bellotto, N.
TI  - Wavelet-based temporal models of human activity for anomaly detection in smart robot-assisted environments
AB  - Detecting anomalies in patterns of sensor data is important in many practical applications, including domestic activity monitoring for Active Assisted Living (AAL). How to represent and analyse these patterns, however, remains a challenging task, especially when data is relatively scarce and an explicit model is required to be fine-tuned for specific scenarios. This paper, therefore, presents a new approach for temporal modelling of long-term human activities with smart-home sensors, which is used to detect anomalous situations in a robot-assisted environment. The model is based on wavelet transforms and used to forecast smart sensor data, providing a temporal prior to detect unexpected events in human environments. To this end, a new extension of Hybrid Markov Logic Networks has been developed that merges different anomaly indicators, including activities detected by binary sensors, expert logic rules, and wavelet-based temporal models. The latter in particular allows the inference system to discover deviations from long-term activity patterns, which cannot be detected by simpler frequency-based models. Two new publicly available datasets were collected using several smart-sensors to evaluate the approach in office and domestic scenarios. The experimental results demonstrate the effectiveness of the proposed solutions and their successful deployment in complex human environments, showing their potential for future smart-home and robot integrated services. MSC Codes 68T10
PB  - arXiv
PY  - 2020
ST  - Wavelet-based temporal models of human activity for anomaly detection in smart robot-assisted environments
Y2  - 2025/05/05/21:54:30
DO  - 10.3233/ais-230144
ER  -


TY  - GEN
AU  - Nishimura, H.
AU  - Schwager, M.
TI  - SACBP: Belief space planning for continuous-time dynamical systems via stochastic sequential action control
AB  - We propose a novel belief space planning technique for continuous dynamics by viewing the belief system as a hybrid dynamical system with time-driven switching. Our approach is based on the perturbation theory of differential equations and extends Sequential Action Control Ansari and Murphey (2016) to stochastic dynamics. The resulting algorithm, which we name SACBP, does not require discretization of spaces or time and synthesizes control signals in near real-time. SACBP is an anytime algorithm that can handle general parametric Bayesian filters under certain assumptions. We demonstrate the effectiveness of our approach in an active sensing scenario and a model-based Bayesian reinforcement learning problem. In these challenging problems, we show that the algorithm significantly outperforms other existing solution techniques including approximate dynamic programming and local trajectory optimization.
PB  - arXiv
PY  - 2020
ST  - SACBP
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/978-3-030-44051-0_16
ER  -


TY  - GEN
AU  - Sonkusare, S.
AU  - Nguyen, V.T.
AU  - Moran, R.
AU  - Breakspear, M.
AU  - Guo, C.
TI  - Intracranial-EEG evidence for medial temporal pole driving amygdala activity induced by multi-modal emotional stimuli
AB  - The temporal pole (TP) is an associative cortical region required for complex cognitive functions such as social and emotional cognition. However, functional mapping of the TP with functional magnetic resonance imaging is technically challenging and thus understanding of its interaction with other key emotional circuitry, such as the amygdala, remain elusive. We exploited the unique advantages of stereo-electroencephalography (SEEG) to assess the responses of the TP and the amygdala during the perception of emotionally salient stimuli of pictures, music and movies. These stimuli consistently elicited high gamma responses (70-140 Hz) in both the TP and the amygdala, accompanied by functional connectivity in the low frequency range (2-12 Hz). Computational analyses suggested the TP driving this effect in the theta-alpha frequency range and which was modulated by the emotional salience of the stimuli. Of note, cross-frequency analysis indicated the phase of theta-alpha oscillations in the TP modulated the amplitude of high gamma activity in the amygdala. These results were reproducible with three types of stimuli including naturalistic stimuli suggesting a hierarchical influence of the TP over the amygdala in non-threatening stimuli.
PB  - bioRxiv
PY  - 2020
ST  - Intracranial-EEG evidence for medial temporal pole driving amygdala activity induced by multi-modal emotional stimuli
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/2020.02.24.963801
ER  -


TY  - GEN
AU  - Zhao, P.
AU  - Xie, L.
AU  - Ju, C.
AU  - Wang, Y.
AU  - Tian, Q.
TI  - Bottom-up temporal action localization with mutual regularization
AB  - Recently, temporal action localization (TAL), i.e., finding specific action segments in untrimmed videos, has attracted increasing attentions of the computer vision community. State-of-the-art solutions for TAL involves evaluating the frame-level probabilities of three action-indicating phases, i.e. starting, continuing, and ending; and then post-processing these predictions for the final localization. This paper delves deep into this mechanism, and argues that existing methods, by modeling these phases as individual classification tasks, ignored the potential temporal constraints between them. This can lead to incorrect and/or inconsistent predictions when some frames of the video input lack sufficient discriminative information. To alleviate this problem, we introduce two regularization terms to mutually regularize the learning procedure: the Intra-phase Consistency (IntraC) regularization is proposed to make the predictions verified inside each phase; and the Inter-phase Consistency (InterC) regularization is proposed to keep consistency between these phases. Jointly optimizing these two terms, the entire framework is aware of these potential constraints during an end-to-end optimization process. Experiments are performed on two popular TAL datasets, THUMOS14 and ActivityNet1.3. Our approach clearly outperforms the baseline both quantitatively and qualitatively. The proposed regularization also generalizes to other TAL methods (e.g., TSA-Net and PGCN). code: https://github.com/PeisenZhao/Bottom-Up-TAL-with-MR
PB  - arXiv
PY  - 2020
ST  - Bottom-up temporal action localization with mutual regularization
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/978-3-030-58598-3_32
ER  -


TY  - GEN
AU  - Madar, A.D.
AU  - Pfammatter, J.A.
AU  - Bordenave, J.
AU  - Maganti, R.K.
AU  - Jones, M.V.
TI  - Deficits in behavioral and neuronal pattern separation in temporal lobe epilepsy
AB  - In temporal lobe epilepsy, the ability of the dentate gyrus to limit excitatory cortical input to the hippocampus breaks down, leading to seizures. The dentate gyrus is also thought to help discriminate between similar memories by performing pattern separation, but whether epilepsy leads to a breakdown in this neural computation, and thus to mnemonic discrimination impairments, remains unknown. Here we show that temporal lobe epilepsy is characterized by behavioral deficits in mnemonic discrimination tasks, in both humans (females and males) and mice (C57Bl6 males, systemic low-dose kainate model). Using a recently developed assay in brain slices of the same epileptic mice, we reveal a decreased ability of the dentate gyrus to perform certain forms of pattern separation. This is due to a subset of granule cells with abnormal bursting that can develop independently of early EEG abnormalities. Overall, our results linking physiology, computation and cognition in the same mice advance our understanding of episodic memory mechanisms and their dysfunction in epilepsy. Significance Statement: People with temporal lobe epilepsy (TLE) often have learning and memory impairments, sometimes occurring earlier than the first seizure, but those symptoms and their biological underpinnings are poorly understood. We focused on the dentate gyrus, a brain region that is critical to avoid confusion between similar memories and is anatomically disorganized in TLE. We show that both humans and mice with TLE experience confusion between similar situations. This impairment coincides with a failure of the dentate gyrus to disambiguate similar input signals because of pathological bursting in a subset of neurons. Our work bridges seizure-oriented and memory-oriented views of the dentate gyrus function, suggests a mechanism for cognitive symptoms in TLE and supports a long-standing hypothesis of episodic memory theories.
PB  - bioRxiv
PY  - 2020
ST  - Deficits in behavioral and neuronal pattern separation in temporal lobe epilepsy
Y2  - 2025/05/05/21:54:30
DO  - 10.1523/jneurosci.2439-20.2021
ER  -


TY  - GEN
AU  - Liu, Q.
AU  - Wang, T.
AU  - Liu, J.
AU  - Bu, Q.
AU  - Yang, L.
TI  - CTM: Collaborative temporal modeling for action recognition
AB  - With the rapid development of digital multimedia, video understanding has become an important field. For action recognition, temporal dimension plays an important role, and this is quite different from image recognition. In order to learn powerful feature of videos, we propose a Collaborative Temporal Modeling (CTM) block (Figure 1) to learn temporal information for action recognition. Besides a parameter-free identity shortcut, as a separate temporal modeling block, CTM includes two collaborative paths: A spatial-aware temporal modeling path, which we propose the Temporal-Channel Convolution Module (TCCM) with unshared parameters for each spatial position (H x W) to build, and a spatial-unaware temporal modeling path. CTM blocks can seamlessly be inserted into many popular networks to generate CTM Networks and bring the capability of learning temporal information to 2D CNN backbone networks, which only capture spatial information. Experiments on several popular action recognition datasets demonstrate that CTM blocks bring the performance improvements on 2D CNN baselines, and our method achieves the competitive results against the state-ofthe-art methods. Code will be made publicly available.
PB  - arXiv
PY  - 2020
ST  - CTM
Y2  - 2025/05/05/21:54:30
DO  - 10.1016/j.cviu.2024.104013
ER  -


TY  - GEN
AU  - Shabbeer Basha, S.H.
AU  - Pulabaigari, V.
AU  - Mukherjee, S.
TI  - An information-rich sampling technique over spatio-temporal CNN for classification of human actions in videos
AB  - We propose a novel scheme for human action recognition in videos, using a 3-dimensional Convolutional Neural Network (3D CNN) based classifier. Traditionally in deep learning based human activity recognition approaches, either a few random frames or every kth frame of the video is considered for training the 3D CNN, where k is a small positive integer, like 4, 5, or 6. This kind of sampling reduces the volume of the input data, which speeds-up training of the network and also avoids over-fitting to some extent, thus enhancing the performance of the 3D CNN model. In the proposed video sampling technique, consecutive k frames of a video are aggregated into a single frame by computing a Gaussianweighted summation of the k frames. The resulting frame (aggregated frame) preserves the information in a better way than the conventional approaches and experimentally shown to perform better. In this paper, a 3D CNN architecture is proposed to extract the spatio-temporal features and follows Long Short-Term Memory (LSTM) to recognize the human actions. The proposed 3D CNN architecture is capable of handling the videos where the camera is placed at a distance from the performer. Experiments are performed with KTH and WEIZMANN human actions datasets, whereby it is shown to produce comparable results with the state-of-the-art techniques.
PB  - arXiv
PY  - 2020
ST  - An information-rich sampling technique over spatio-temporal CNN for classification of human actions in videos
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/s11042-022-12856-6
ER  -


TY  - GEN
AU  - Lombardi, F.
AU  - Shriki, O.
AU  - Herrmann, H.J.
AU  - de Arcangelis, L.
TI  - Long-range temporal correlations in the broadband resting state activity of the human brain revealed by neuronal avalanches
AB  - Resting-state brain activity is characterized by the presence of neuronal avalanches showing absence of characteristic size. Such evidence has been interpreted in the context of criticality and associated with the normal functioning of the brain. At criticality, a crucial role is played by long-range power-law correlations. Thus, to verify the hypothesis that the brain operates close to a critical point and consequently assess deviations from criticality for diagnostic purposes, it is of primary importance to robustly and reliably characterize correlations in resting-state brain activity. Recent works focused on the analysis of narrow band electroencephalography (EEG) and magnetoencephalography (MEG) signal amplitude envelope, showing evidence of long-range temporal correlations (LRTC) in neural oscillations. However, this approach is not suitable for assessing long-range correlations in broadband resting-state cortical signals. To overcome such limitation, here we propose to characterize the correlations in the broadband brain activity through the lens of neuronal avalanches. To this end, we consider resting-state EEG and long-term MEG recordings, extract the corresponding neuronal avalanche sequences, and study their temporal correlations. We demonstrate that the broadband resting-state brain activity consistently exhibits long-range power-law correlations in both EEG and MEG recordings, with similar values of the scaling exponents. Importantly, although we observe that avalanche size distribution depends on scale parameters, scaling exponents characterizing long-range correlations are quite robust. In particular, they are independent of the temporal binning (scale of analysis), indicating that our analysis captures intrinsic characteristics of the underlying dynamics. Because neuronal avalanches constitute a fundamental feature of neural systems with universal characteristics, the proposed approach may serve as a general, systems- and experiment-independent procedure to infer the existence of underlying long-range correlations in extended neural systems, and identify pathological behaviors in the complex spatio-temporal interplay of cortical rhythms.
PB  - bioRxiv
PY  - 2020
ST  - Long-range temporal correlations in the broadband resting state activity of the human brain revealed by neuronal avalanches
Y2  - 2025/05/05/21:54:30
DO  - 10.1016/j.neucom.2020.05.126
ER  -


TY  - GEN
AU  - Salet, J.M.
AU  - Kruijne, W.
AU  - van Rijn, H.
TI  - Implicit learning of temporal behavior in complex dynamic environments
AB  - To anticipate future events humans exploit predictive patterns in the environment. Such statistical learning is often outside of awareness. Timing research suggests that humans also adapt to temporal regularities. However, in these experiments, the intervals to be timed are isolated and explicitly cued, contrasting with everyday life where intervals are often unnoticed. In the present study, we found implicit adaptation to temporal regularities in an ecological setting. Ninety-eight participants played a game in which they responded to sudden-onset targets. While two targets appeared at random times, one target appeared every three seconds. In two experiments, we found adaptation to the regularity: Response times were lower, hit rates higher, and mouse cursor trajectories revealed anticipatory movements. Crucially, this was observed when participants were informed about the regularity, but also when they were unaware of it. Here, we for the first time, show implicit learning of temporal behavior in a complex environment.
PB  - bioRxiv
PY  - 2020
ST  - Implicit learning of temporal behavior in complex dynamic environments
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/2020.01.29.924472
ER  -


TY  - GEN
AU  - VidalMata, R.G.
AU  - Scheirer, W.J.
AU  - Kuehne, H.
TI  - Joint Visual-Temporal Embedding for Unsupervised Learning of Actions in Untrimmed Sequences
AB  - Understanding the structure of complex activities in videos is one of the many challenges faced by action recognition methods. To overcome this challenge, not only do methods need a solid knowledge of the visual structure of underlying features but also a good interpretation of how they could change over time. Consequently, action segmentation tasks must take into account not only the visual cues from individual frames, but their characteristics as a temporal sequence of features. This work presents our findings on the impact of incorporating both visual and temporal learning on an unsupervised action segmentation pipeline. We introduce a novel approach to extract relevant visual and temporal features from untrimmed sequences for the temporal localization of sub-activities within complex actions without any labeling information. Through extensive experimentation on two benchmark datasets – Breakfast Actions, and YouTube Instructions – we show that the proposed approach is able to provide a meaningful visual and temporal embedding from the visual cues from contiguous video frames and that it indeed helps in temporal segmentation.
PB  - arXiv
PY  - 2020
ST  - Joint Visual-Temporal Embedding for Unsupervised Learning of Actions in Untrimmed Sequences
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/wacv48630.2021.00128
ER  -


TY  - GEN
AU  - Perrodin, C.
AU  - Verzat, C.
AU  - Bendor, D.
TI  - Courtship behaviour reveals temporal regularity is a critical social cue in mouse communication
AB  - While animals navigating the real world face a barrage of complex sensory input, their brains have evolved to perceptually compress multidimensional information by selectively extracting the features relevant for survival. For instance, communication signals supporting social interactions in several mammalian species consist of acoustically complex sequences of vocalizations, however little is known about what information listeners extract from such time-varying sensory streams. Here, we utilize female mice’s natural behavioural response to male courtship songs to evaluate the relevant acoustic dimensions used in their social decisions. We found that females were highly sensitive to disruptions of song temporal regularity, and preferentially approached playbacks of intact male songs over rhythmically irregular versions of the songs. In contrast, female behaviour was invariant to manipulations affecting the songs’ sequential organization, or the spectrotemporal structure of individual syllables. The results reveal temporal regularity as a key acoustic cue extracted by mammalian listeners from complex vocal sequences during goal-directed social behaviour.Natural behaviour is used to probe how mouse listeners encode vocal sequencesListeners are highly sensitive to disruptions of the songs’ rhythmic regularityFemale behaviour is invariant to changes in song sequenceApproach behaviour is robust to the removal of syllable spectrotemporal dynamics
PB  - bioRxiv
PY  - 2020
ST  - Courtship behaviour reveals temporal regularity is a critical social cue in mouse communication
Y2  - 2025/05/05/21:54:30
DO  - 10.7554/elife.86464.2
ER  -


TY  - GEN
AU  - Monaco, H.
AU  - Sereno, T.
AU  - Liu, K.
AU  - Deforet, M.
AU  - Xavier, J.B.
TI  - Spatial-temporal dynamics of a microbial cooperative behavior robust to cheating
AB  - Individuals living in dense populations control their behaviors by sensing, integrating and responding to many cues. How can these processes enable the evolution and stability of cooperative behaviors that could easily be exploited by cheaters? Here we shed light on how bacteria regulate cooperation by studying swarming in Pseudomonas aeruginosa, a behavior requiring cooperative secretions of rhamnolipid surfactants to facilitate collective movement over surfaces. By combining fluorescent imaging and computational analyses we show, counterintuitively, that rhamnolipid expression peaks at swarming edges. We then show that the integration of competing diffusive cues—quorum sensing signals and growth-limiting nutrients—enables P. aeruginosa to communicate across centimeters and adopt expression patterns unseen in well-mixed liquid culture. Integration of multiple cues enables robustness against cheating even when we experimentally perturb the quorum sensing system. Taken together, these results illuminate how the integration of cues in spatially structured communities can stabilize cooperation.
PB  - bioRxiv
PY  - 2020
ST  - Spatial-temporal dynamics of a microbial cooperative behavior robust to cheating
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/2020.01.23.914481
ER  -


TY  - GEN
AU  - Wang, W.
AU  - Peng, X.
AU  - Qiao, Y.
AU  - Cheng, J.
TI  - A comprehensive study on temporal modeling for online action detection
AB  - —Online action detection (OAD) is a practical yet challenging task, which has attracted increasing attention in recent years. A typical OAD system mainly consists of three modules: a frame-level feature extractor which is usually based on pre-trained deep Convolutional Neural Networks (CNNs), a temporal modeling module, and an action classifier. Among them, the temporal modeling module is crucial which aggregates discriminative information from historical and current features. Though many temporal modeling methods have been developed for OAD and other topics, their effects are lack of investigation on OAD fairly. This paper aims to provide a comprehensive study on temporal modeling for OAD including four meta types of temporal modeling methods, i.e. temporal pooling, temporal convolution, recurrent neural networks, and temporal attention, and uncover some good practices to produce a state-of-the-art OAD system. Many of them are explored in OAD for the first time, and extensively evaluated with various hyper parameters. Furthermore, based on our comprehensive study, we present several hybrid temporal modeling methods, which outperform the recent state-of-the-art methods with sizable margins on THUMOS-14 and TVSeries.
PB  - arXiv
PY  - 2020
ST  - A comprehensive study on temporal modeling for online action detection
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/s40747-021-00534-3
ER  -


TY  - GEN
AU  - Islam, A.
AU  - Radke, R.J.
TI  - Weakly supervised temporal action localization using deep metric learning
AB  - Temporal action localization is an important step towards video understanding. Most current action localization methods depend on untrimmed videos with full temporal annotations of action instances. However, it is expensive and time-consuming to annotate both action labels and temporal boundaries of videos. To this end, we propose a weakly supervised temporal action localization method that only requires video-level action instances as supervision during training. We propose a classification module to generate action labels for each segment in the video, and a deep metric learning module to learn the similarity between different action instances. We jointly optimize a balanced binary cross-entropy loss and a metric loss using a standard back propagation algorithm. Extensive experiments demonstrate the effectiveness of both of these components in temporal localization. We evaluate our algorithm on two challenging untrimmed video datasets: THUMOS14 and ActivityNet1.2. Our approach improves the current state-of-the-art result for THUMOS14 by 6.5% mAP at IoU threshold 0.5, and achieves competitive performance for ActivityNet1.2.
PB  - arXiv
PY  - 2020
ST  - Weakly supervised temporal action localization using deep metric learning
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/wacv45572.2020.9093620
ER  -


TY  - GEN
AU  - Shan, K.
AU  - Wang, Y.
AU  - Wang, Z.
AU  - Chen, Y.
AU  - Li, Y.
TI  - Mixtconv: Mixed temporal convolutional kernels for efficient action recogntion
AB  - To efficiently extract spatiotemporal features of video for action recognition, most state-of-the-art methods integrate 1D temporal convolution into a conventional 2D CNN backbone. However, they all exploit 1D temporal convolution of fixed kernel size (i.e., 3) in the network building block, thus have suboptimal temporal modeling capability to handle both long-term and short-term actions. To address this problem, we first investigate the impacts of different kernel sizes for the 1D temporal convolutional filters. Then, we propose a simple yet efficient operation called Mixed Temporal Convolution (MixTConv), which consists of multiple depthwise 1D convolutional filters with different kernel sizes. By plugging MixTConv into the conventional 2D CNN backbone ResNet-50, we further propose an efficient and effective network architecture named MSTNet for action recognition, and achieve state-of-the-art results on multiple benchmarks.
PB  - arXiv
PY  - 2020
ST  - Mixtconv
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icpr48806.2021.9412586
ER  -


TY  - GEN
AU  - Badjatiya, P.
AU  - Sarkar, M.
AU  - Sinha, A.
AU  - Subramanian, J.
AU  - Krishnamurthy, B.
TI  - Inducing cooperative behaviour in sequential-social dilemmas through multi-agent reinforcement learning using status-quo loss
AB  - In social dilemma situations, individual rationality leads to sub-optimal group outcomes. Several human engagements can be modeled as a sequential (multi-step) social dilemmas. However, in contrast to humans, Deep Reinforcement Learning agents trained to optimize individual rewards in sequential social dilemmas converge to selfish, mutually harmful behavior. We introduce a status-quo loss (SQLoss) that encourages an agent to stick to the status-quo, rather than repeatedly changing its policy. We show how agents trained with SQLoss evolve cooperative behavior in several social dilemma matrix games. To work with social dilemma games that have visual input, we propose GameDistill. GameDistill uses self-supervision and clustering to automatically extract cooperative and selfish policies from a social dilemma game. We combine GameDistill and SQLoss to show how agents evolve socially desirable cooperative behavior in the Coin Game.
PB  - arXiv
PY  - 2020
ST  - Inducing cooperative behaviour in sequential-social dilemmas through multi-agent reinforcement learning using status-quo loss
Y2  - 2025/05/05/21:54:30
DO  - 10.1063/5.0246332
ER  -


TY  - GEN
AU  - Jia, X.
AU  - Hong, H.
AU  - DiCarlo, J.J.
TI  - Unsupervised changes in core object recognition behavioral performance are accurately predicted by unsupervised neural plasticity in inferior temporal cortex
AB  - Temporal continuity of object identity is a natural feature of visual input statistics, and it is potentially exploited -- in an unsupervised manner -- by the ventral visual stream to build and re-shape the neural representation in inferior temporal (IT) cortex and IT-dependent core object recognition behavior. Prior psychophysical studies in humans and electrophysiological studies in monkey IT are individually supportive of this hypothesis. However, due to differences in tasks and experience manipulations, it is not yet known if the reported plasticity of individual IT neurons and the reported human behavioral changes are quantitatively consistent. Here we tested that consistency by building an unsupervised plasticity model that captures the previously-reported IT neural plasticity and combined that model with a previously established IT-to-recognition-behavior linking model. We compared the predictions of the overall model with the results of three new human behavioral experiments: in each we delivered a different type of unsupervised temporal contiguity experience and longitudinally measured its effect on performance of targeted object discrimination tasks. We found that, without any parameter tuning, the overall model accurately predicted the mean direction, magnitude and time course of performance changes in all three of these experiments. We also found a previously unreported dependency of the observed human performance change on the initial difficulty of the targeted object discrimination task, which was also largely predicted by the overall model. This result demonstrates the interlocking consistency of a range of prior neural and behavioral work, and thus adds support to the hypothesis that tolerant core object recognition in human and non-human primates is instructed -- at least in part -- by naturally occurring unsupervised temporal contiguity experience.
PB  - bioRxiv
PY  - 2020
ST  - Unsupervised changes in core object recognition behavioral performance are accurately predicted by unsupervised neural plasticity in inferior temporal cortex
Y2  - 2025/05/05/21:54:30
DO  - 10.7554/elife.60830
ER  -


TY  - GEN
AU  - Yuan, F.
AU  - Karatzoglou, A.
AU  - He, X.
AU  - Zhang, L.
TI  - Parameter-efficient transfer from sequential behaviors for user modeling and recommendation
AB  - Inductive transfer learning has had a big impact on computer vision and NLP domains but has not been used in the area of recommender systems. Even though there has been a large body of research on generating recommendations based on modeling user-item interaction sequences, few of them attempt to represent and transfer these models for serving downstream tasks where only limited data exists. In this paper, we delve on the task of effectively learning a single user representation that can be applied to a diversity of tasks, from cross-domain recommendations to user profile predictions. Fine-tuning a large pre-trained network and adapting it to downstream tasks is an effective way to solve such tasks. However, fine-tuning is parameter inefficient considering that an entire model needs to be re-trained for every new task. To overcome this issue, we develop a parameter-efficient transfer learning architecture, termed as PeterRec, which can be configured on-the-fly to various downstream tasks. Specifically, PeterRec allows the pre-trained parameters to remain unaltered during fine-tuning by injecting a series of re-learned neural networks, which are small but as expressive as learning the entire network. We perform extensive experimental ablation to show the effectiveness of the learned user representation in five downstream tasks. Moreover, we show that PeterRec performs efficient transfer learning in multiple domains, where it achieves comparable or sometimes better performance relative to fine-tuning the entire model parameters.
PB  - arXiv
PY  - 2020
ST  - Parameter-efficient transfer from sequential behaviors for user modeling and recommendation
Y2  - 2025/05/05/21:54:30
DO  - 10.1145/3397271.3401156
ER  -


TY  - GEN
AU  - Sarikaya, D.
AU  - Jannin, P.
TI  - Towards generalizable surgical activity recognition using spatial temporal graph convolutional networks
AB  - Modeling and recognition of surgical activities poses an interesting research prob- lem. Although a number of recent works studied automatic recognition of sur- gical activities, generalizability of these works across different tasks and different datasets remains a challenge. We introduce a modality that is robust to scene vari- ation, based on spatial temporal graph representations of surgical tools in videos for surgical activity recognition.
PB  - arXiv
PY  - 2020
ST  - Towards generalizable surgical activity recognition using spatial temporal graph convolutional networks
Y2  - 2025/05/05/21:54:30
DO  - 10.1049/itr2.12025
ER  -


TY  - GEN
AU  - Schewe, K.-D.
AU  - Ferrarotti, F.
TI  - Behavioural theory of reflective algorithms i: Reflective sequential algorithms
AB  - We develop a behavioural theory of reflective sequential algorithms (RSAs), i.e. sequential algorithms that can modify their own behaviour. The theory comprises a set of languageindependent postulates defining the class of RSAs, an abstract machine model, and the proof that all RSAs are captured by this machine model. As in Gurevich's behavioural theory for sequential algorithms RSAs are sequential-time, bounded parallel algorithms, where the bound depends on the algorithm only and not on the input. Different from the class of sequential algorithms every state of an RSA includes a representation of the algorithm in that state, thus enabling linguistic reflection. Bounded exploration is preserved using terms as values. The model of reflective sequential abstract state machines (rsASMs) extends sequential ASMs using extended states that include an updatable representation of the main ASM rule to be executed by the machine in that state. Updates to the representation of ASM signatures and rules are realised by means of a sophisticated tree algebra. 68Q05, 68Q10, 03D10
PB  - arXiv
PY  - 2020
ST  - Behavioural theory of reflective algorithms i
Y2  - 2025/05/05/21:54:30
DO  - 10.1016/j.scico.2022.102864
ER  -


TY  - GEN
AU  - MacDowell, C.J.
AU  - Buschman, T.J.
TI  - Low-Dimensional Spatio-Temporal Dynamics Underlie Cortex-Wide Neural Activity
AB  - Cognition arises from the dynamic flow of neural activity through the brain. To capture these dynamics, we used mesoscale calcium imaging to record neural activity across the dorsal cortex of awake mice. We found that the large majority of variance in cortex-wide activity (∼75%) could be explained by a limited set of ∼14 ‘motifs’ of neural activity. Each motif captured a unique spatio-temporal pattern of neural activity across the cortex. These motifs generalized across animals and were seen in multiple behavioral environments. Motif expression differed across behavioral states and specific motifs were engaged by sensory processing, suggesting the motifs reflect core cortical computations. Together, our results show that cortex-wide neural activity is highly dynamic, but that these dynamics are restricted to a low-dimensional set of motifs, potentially to allow for efficient control of behavior.
PB  - bioRxiv
PY  - 2020
ST  - Low-Dimensional Spatio-Temporal Dynamics Underlie Cortex-Wide Neural Activity
Y2  - 2025/05/05/21:54:30
ER  -


TY  - GEN
AU  - Hiraoka, T.
AU  - Masuda, N.
AU  - Li, A.
AU  - Jo, H.-H.
TI  - Modeling temporal networks with bursty activity patterns of nodes and links
AB  - The concept of temporal networks provides a framework to understand how the interaction between system components changes over time. In empirical communication data, we often detect non-Poissonian, so-called bursty behavior in the activity of nodes as well as in the interaction between nodes. However, such reconciliation between node burstiness and link burstiness cannot be explained if the interaction processes on different links are independent of each other. This is because the activity of a node is the superposition of the interaction processes on the links incident to the node and the superposition of independent bursty point processes is not bursty in general. Here we introduce a temporal network model based on bursty node activation and show that it leads to heavy-tailed inter-event time distributions for both node dynamics and link dynamics. Our analysis indicates that activation processes intrinsic to nodes give rise to dynamical correlations across links. Our framework offers a way to model competition and correlation between links, which is key to understanding dynamical processes in various systems.
PB  - arXiv
PY  - 2019
ST  - Modeling temporal networks with bursty activity patterns of nodes and links
Y2  - 2025/05/05/21:54:30
DO  - 10.1103/physrevresearch.2.023073
ER  -


TY  - GEN
AU  - García-Rosales, F.
AU  - Lopez-Jury, L.
AU  - Gonzalez-Palomares, E.
AU  - Cabral-Calderín, Y.
AU  - Hechavarría, J.C.
TI  - Fronto-temporal coupling dynamics during spontaneous activity and auditory processing
AB  - Most mammals rely on the extraction of acoustic information from the environment in order to survive. However, the mechanisms that support sound representation in auditory neural networks involving sensory and association areas in the brain remain underexplored. In this study, we address the functional connectivity between an auditory region in the frontal cortex (the frontal auditory field, FAF) and the auditory cortex (AC) in the bat Carollia perspicillata. The AC is a classic sensory area central for the processing of acoustic information. The FAF, on the other hand, belongs to the frontal lobe, a brain region involved in the integration of sensory inputs, the modulation of cognitive states, and the coordination of behavioural outputs. The FAF-AC network was examined in terms of oscillatory coherence (local-field potentials, LFPs), and within an information theoretical framework linking FAF and AC spiking activities. We show that in the absence of acoustic stimulation, simultaneously recorded LFPs from FAF and AC are coherent in low frequencies (1-12 Hz). This “default” coupling was strongest in deep AC layers and was unaltered by acoustic stimulation. However, presenting auditory stimuli did trigger the emergence of coherent auditory-evoked gamma-band activity (25-45 Hz) between the FAF and AC. In terms of spiking, our results suggest that FAF and AC engage in distinct coding strategies for representing artificial and natural sounds. Taken together, our findings shed light onto the functional coupling dynamics that could underlie acoustic processing in fronto-temporal networks of the brain.
PB  - bioRxiv
PY  - 2019
ST  - Fronto-temporal coupling dynamics during spontaneous activity and auditory processing
Y2  - 2025/05/05/21:54:30
DO  - 10.37473/dac/10.1101/2019.12.23.886770
ER  -


TY  - GEN
AU  - Su, T.-C.
AU  - Chen, G.-Y.
TI  - ET-USB: Transformer-Based Sequential Behavior Modeling for Inbound Customer Service
AB  - Deep learning models with attention mechanisms have achieved exceptional results for many tasks, including language tasks and recommendation systems. Whereas previous studies have emphasized allocation of phone agents, we focused on inbound call prediction for customer service. A common method of analyzing user history behaviors is to extract all types of aggregated feature over time, but that method may fail to detect users’ behavioral sequences. Therefore, we created a new approach, ET-USB, that incorporates users’ sequential and nonsequential features; we apply the powerful Transformer encoder, a self-attention network model, to capture the information underlying user behavior sequences. ET-USB is helpful in various business scenarios at Cathay Financial Holdings. We conducted experiments to test the proposed network structure’s ability to process various dimensions of behavior data; the results suggest that ET-USB delivers results superior to those of delivered by other deep-learning models.
PB  - arXiv
PY  - 2019
ST  - ET-USB
Y2  - 2025/05/05/21:54:30
DO  - 10.1002/9781118133880.hop212018
ER  -


TY  - GEN
AU  - Materzynska, J.
AU  - Xiao, T.
AU  - Herzig, R.
AU  - Wang, X.
AU  - Darrell, T.
TI  - Something-else: Compositional action recognition with spatial-temporal interaction networks
AB  - Human action is naturally compositional: humans can easily recognize and perform actions with objects that are different from those used in training demonstrations. In this paper, we study the compositionality of action by looking into the dynamics of subject-object interactions. We propose a novel model which can explicitly reason about the geometric relations between constituent objects and an agent performing an action. To train our model, we collect dense object box annotations on the Something-Something dataset. We propose a novel compositional action recognition task where the training combinations of verbs and nouns do not overlap with the test set. The novel aspects of our model are applicable to activities with prominent object interaction dynamics and to objects which can be tracked using state-of-the-art approaches; for activities without clearly defined spatial object-agent interactions, we rely on baseline scene-level spatio-temporal representations. We show the effectiveness of our approach not only on the proposed compositional action recognition task, but also in a few-shot compositional setting which requires the model to generalize across both object appearance and action category.
PB  - arXiv
PY  - 2019
ST  - Something-else
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvpr42600.2020.00113
ER  -


TY  - GEN
AU  - Papadopoulos, K.
AU  - Ghorbel, E.
AU  - Aouada, D.
AU  - Ottersten, B.
TI  - Vertex feature encoding and hierarchical temporal modeling in a spatial-temporal graph convolutional network for action recognition
AB  - This paper extends the Spatial-Temporal Graph Convolutional Network (ST-GCN) for skeleton-based action recognition by introducing two novel modules, namely, the Graph Vertex Feature Encoder (GVFE) and the Dilated Hierarchical Temporal Convolutional Network (DH-TCN). On the one hand, the GVFE module learns appropriate vertex features for action recognition by encoding raw skeleton data into a new feature space. On the other hand, the DH-TCN module is capable of capturing both short-term and long-term temporal dependencies using a hierarchical dilated convolutional network. Experiments have been conducted on the challenging NTU RGB-D-60 and NTU RGB-D 120 datasets. The obtained results show that our method competes with state-of-the-art approaches while using a smaller number of layers and parameters; thus reducing the required training time and memory.
PB  - arXiv
PY  - 2019
ST  - Vertex feature encoding and hierarchical temporal modeling in a spatial-temporal graph convolutional network for action recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icpr48806.2021.9413189
ER  -


TY  - GEN
AU  - Ji, J.
AU  - Krishna, R.
AU  - Fei-Fei, L.
AU  - Niebles, J.C.
TI  - Action genome: Actions as composition of spatio-temporal scene graphs
AB  - Action recognition has typically treated actions and activities as monolithic events that occur in videos. However, there is evidence from Cognitive Science and Neuroscience that people actively encode activities into consistent hierarchical part structures. However in Computer Vision, few explorations on representations encoding event partonomies have been made. Inspired by evidence that the prototypical unit of an event is an action-object interaction, we introduce Action Genome, a representation that decomposes actions into spatio-temporal scene graphs. Action Genome captures changes between objects and their pairwise relationships while an action occurs. It contains 10K videos with 0.4M objects and 1.7M visual relationships annotated. With Action Genome, we extend an existing action recognition model by incorporating scene graphs as spatiotemporal feature banks to achieve better performance on the Charades dataset. Next, by decomposing and learning the temporal changes in visual relationships that result in an action, we demonstrate the utility of a hierarchical event decomposition by enabling few-shot action recognition, achieving 42.7% mAP using as few as 10 examples. Finally, we benchmark existing scene graph models on the new task of spatio-temporal scene graph prediction.
PB  - arXiv
PY  - 2019
ST  - Action genome
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvpr42600.2020.01025
ER  -


TY  - GEN
AU  - Maginn, J.M.F.
TI  - Conservative Critique: The Spatiotemporality of Normative Behaviourism
AB  - Jonathan Floyd's 'Is Political Philosophy Impossible?' is a thoroughly unique and interesting work that looks to get to the heart of Western Political Thought and its mechanics. Floyd's work has, however, remained underexposed to academic critique and exploration. This piece will look to explore Floyd's book regards how it formulates and treats time and space. Using the theories of the likes of Haraway, Hutchings, Spivak and Kuhn, I will seek to show that (despite exhibiting some reflexivity regards spatiotemporal situatedness and attempting to provide an alternative to the orthodoxy of western political philosophy) Floyd reproduces many of the universalising and homogenise tropes that have characterised western political thought.
PB  - SSRN
PY  - 2019
ST  - Conservative Critique
Y2  - 2025/05/05/21:54:30
DO  - 10.2139/ssrn.3491829
ER  -


TY  - GEN
AU  - Knorr, F.G.
AU  - Neukam, P.T.
AU  - Fröhner, J.H.
AU  - Smolka, M.N.
AU  - Marxen, M.
TI  - A comparison of fMRI and behavioral models for predicting inter-temporal choices
AB  - In an inter-temporal choice (IteCh) task, subjects are offered a smaller amount of money immediately or a larger amount at a later time point. Here, we are using trial-by-trial fMRI data from 363 recording sessions and machine learning in an attempt to build a classifier that would ideally outperform established behavioral model given that it has access to brain activity specific to a single trial. Such methods could allow for future investigations of state-like factors that influence IteCh choices.To investigate this, coefficients of a GLM with one regressor per trial were used as features for a support vector machine (SVM) in combination with a searchlight approach for feature selection and cross-validation. We then compare the results to the performance of four different behavioral models.We found that the behavioral models reached mean accuracies of 90% and above, while the fMRI model only reached 54.84% at the best location in the brain with a spatial distribution similar to the well-known value-tracking network. This low, though significant, accuracy is in line with simulations showing that classifying based on signals with realistic correlations with subjective value produces comparable, low accuracies. These results emphasize the limitations of fMRI recordings from single events to predict human choices, especially when compared to conventional behavioral models. Better performance may be obtained with paradigms that allow the construction of miniblocks to improve the available signal-to-noise ratio.
PB  - bioRxiv
PY  - 2019
ST  - A comparison of fMRI and behavioral models for predicting inter-temporal choices
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/866285
ER  -


TY  - GEN
AU  - Tomei, M.
AU  - Baraldi, L.
AU  - Calderara, S.
AU  - Bronzin, S.
AU  - Cucchiara, R.
TI  - Video action detection by learning graph-based spatio-temporal interactions
AB  - Action Detection is a complex task that aims to detect and classify human actions in video clips. Typically, it has been addressed by processing fine-grained features extracted from a video classification backbone. Recently, thanks to the robustness of object and people detectors, a deeper focus has been added on relationship modelling. Following this line, we propose a graph-based framework to learn high-level interactions between people and objects, in both space and time. In our formulation, spatio-temporal relationships are learned through self-attention on a multi-layer graph structure which can connect entities from consecutive clips, thus considering long-range spatial and temporal dependencies. The proposed module is backbone independent by design and does not require end-to-end training. Extensive experiments are conducted on the AVA dataset, where our model demonstrates state-of-the-art results and consistent improvements over baselines built with different backbones. Code is publicly available at https://github.com/aimagelab/STAGE_action_detection.
PB  - arXiv
PY  - 2019
ST  - Video action detection by learning graph-based spatio-temporal interactions
Y2  - 2025/05/05/21:54:30
DO  - 10.1016/j.cviu.2021.103187
ER  -


TY  - GEN
AU  - Zhang, S.
AU  - Peng, H.
AU  - Yang, L.
AU  - Fu, J.
AU  - Luo, J.
TI  - Learning Sparse 2D Temporal Adjacent Networks for Temporal Action Localization
AB  - In this report, we introduce the Winner method for HACS Temporal Action Localization Challenge 2019. Temporal action localization is challenging since a target proposal may be related to several other candidate proposals in an untrimmed video. Existing methods cannot tackle this challenge well since temporal proposals are considered individually and their temporal dependencies are neglected. To address this issue, we propose sparse 2D temporal adjacent networks to model the temporal relationship between candidate proposals. This method is built upon the recent proposed 2D-TAN approach [6]. The sampling strategy in 2D-TAN introduces the unbalanced context problem, where short proposals can perceive more context than long proposals. Therefore, we further propose a Sparse 2D Temporal Adjacent Network (S-2D-TAN). It is capable of involving more context information for long proposals and further learning discriminative features from them. By combining our S-2D-TAN with a simple action classifier, our method achieves a mAP of 23.49 on the test set, which win the first place in the HACS challenge.
PB  - arXiv
PY  - 2019
ST  - Learning Sparse 2D Temporal Adjacent Networks for Temporal Action Localization
Y2  - 2025/05/05/21:54:30
DO  - 10.1609/aaai.v34i07.6984
ER  -


TY  - GEN
AU  - Parsa, B.
AU  - Narayanan, A.
AU  - Dariush, B.
TI  - Spatio-temporal pyramid graph convolutions for human action recognition and postural assessment
AB  - Recognition of human actions and associated interactions with objects and the environment is an important problem in computer vision due to its potential applications in a variety of domains. The most versatile methods can generalize to various environments and deal with cluttered backgrounds, occlusions, and viewpoint variations. Among them, methods based on graph convolutional networks that extract features from the skeleton have demonstrated promising performance. In this paper, we propose a novel Spatio-Temporal Pyramid Graph Convolutional Network (ST-PGN) for online action recognition for ergonomic risk assessment that enables the use of features from all levels of the skeleton feature hierarchy. The proposed algorithm outperforms state-of-art action recognition algorithms tested on two public benchmark datasets typically used for postural assessment (TUM and UW-IOM). We also introduce a pipeline to enhance postural assessment methods with online action recognition techniques. Finally, the proposed algorithm is integrated with a traditional ergonomic risk index (REBA) to demonstrate the potential value for assessment of musculoskeletal disorders in occupational safety.
PB  - arXiv
PY  - 2019
ST  - Spatio-temporal pyramid graph convolutions for human action recognition and postural assessment
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/wacv45572.2020.9093368
ER  -


TY  - GEN
AU  - Dhiman, C.
AU  - Vishwakarma, D.K.
AU  - Aggarwal, P.
TI  - Skeleton based activity recognition by fusing part-wise spatio-temporal and attention driven residues
AB  - There exist a wide range of intra-class variations of the same actions and inter-class similarity among the actions, at the same time, which makes the action recognition in videos very challenging. In this paper, we present a novel skeleton-based part-wise Spatio-temporal CNN - RIAC Network-based 3D human action recognition framework to visualise the action dynamics in part wise manner and utilise each part for action recognition by applying weighted late fusion mechanism. Part-wise skeleton-based motion dynamics helps to highlight local features of the skeleton which is performed by partitioning the complete skeleton in five parts- Head to Spine (HS), Left Leg (LL), Right Leg (RL), Left Hand (LH), Right Hand (RH). The RIAFNet architecture is greatly inspired by the InceptionV4 architecture which unified the ResNet and Inception based Spatio-temporal feature representation concept and achieving the highest top-1 accuracy till date. To extract and learn salient features for action recognition, attention driven residues are used which enhance the performance of residual components for effective 3D skeleton-based Spatio-temporal action representation. The robustness of the proposed framework is evaluated by performing extensive experiments on three challenging datasets such as UT Kinect Action 3D, Florence 3D action Dataset, and MSR Daily Action3D datasets, which consistently demonstrate the superiority of our method.
PB  - arXiv
PY  - 2019
ST  - Skeleton based activity recognition by fusing part-wise spatio-temporal and attention driven residues
Y2  - 2025/05/05/21:54:30
DO  - 10.1145/3441628
ER  -


TY  - GEN
AU  - Xu, M.
AU  - Zhao, C.
AU  - Rojas, D.S.
AU  - Thabet, A.
AU  - Ghanem, B.
TI  - G-TAD: Sub-graph localization for temporal action detection
AB  - Temporal action detection is a fundamental yet challenging task in video understanding. Video context is a critical cue to effectively detect actions, but current works mainly focus on temporal context, while neglecting semantic context as well as other important context properties. In this work, we propose a graph convolutional network (GCN) model to adaptively incorporate multi-level semantic context into video features and cast temporal action detection as a sub-graph localization problem. Specifically, we formulate video snippets as graph nodes, snippet-snippet correlations as edges, and actions associated with context as target sub-graphs. With graph convolution as the basic operation, we design a GCN block called GCNeXt, which learns the features of each node by aggregating its context and dynamically updates the edges in the graph. To localize each sub-graph, we also design a SGAlign layer to embed each sub-graph into the Euclidean space. Extensive experiments show that G-TAD is capable of finding effective video context without extra supervision and achieves state-of-the-art performance on two detection benchmarks. On ActityNet-1.3, we obtain an average mAP of 34.09%; on THUMOS14, we obtain 40.16% in mAP@0.5, beating all the other one-stage methods.
PB  - arXiv
PY  - 2019
ST  - G-TAD
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvpr42600.2020.01017
ER  -


TY  - GEN
AU  - Eun, H.
AU  - Lee, S.
AU  - Moon, J.
AU  - Jung, C.
AU  - Kim, C.
TI  - SRG: Snippet Relatedness-based Temporal Action Proposal Generator
AB  - Recent temporal action proposal generation approaches have suggested integrating segment- and snippet score-based methodologies to produce proposals with high recall and accurate boundaries. In this paper, different from such a hybrid strategy, we focus on the potential of the snippet score-based approach. Specifically, we propose a new snippet score-based method, named Snippet Relatedness-based Generator (SRG), with a novel concept of “snippet relatedness”. Snippet relatedness represents which snippets are related to a specific action instance. To effectively learn this snippet relatedness, we present “pyramid non-local operations” for locally and globally capturing long-range dependencies among snippets. By employing these components, SRG first produces a 2D relatedness score map that enables the generation of various temporal intervals reliably covering most action instances with high overlap. Then, SRG evaluates the action confidence scores of these temporal intervals and refines their boundaries to obtain temporal action proposals. On THUMOS-14 and ActivityNet-1.3 datasets, SRG outperforms state-of-the-art methods for temporal action proposal generation. Furthermore, compared to competing proposal generators, SRG leads to significant improvements in temporal action detection.
PB  - arXiv
PY  - 2019
ST  - SRG
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/tcsvt.2019.2953187
ER  -


TY  - GEN
AU  - Lee, P.
AU  - Uh, Y.
AU  - Byun, H.
TI  - Background suppression network for weakly-supervised temporal action localization
AB  - Weakly-supervised temporal action localization is a very challenging problem because frame-wise labels are not given in the training stage while the only hint is video-level labels: whether each video contains action frames of interest. Previous methods aggregate frame-level class scores to produce video-level prediction and learn from video-level action labels. This formulation does not fully model the problem in that background frames are forced to be misclassified as action classes to predict video-level labels accurately. In this paper, we design Background Suppression Network (BaSNet) which introduces an auxiliary class for background and has a two-branch weight-sharing architecture with an asymmetrical training strategy. This enables BaS-Net to suppress activations from background frames to improve localization performance. Extensive experiments demonstrate the effectiveness of BaS-Net and its superiority over the state-of-the-art methods on the most popular benchmarks - THUMOS'14 and ActivityNet. Our code and the trained model are available at https://github.com/Pilhyeon/BaSNet-pytorch.
PB  - arXiv
PY  - 2019
ST  - Background suppression network for weakly-supervised temporal action localization
Y2  - 2025/05/05/21:54:30
DO  - 10.1609/aaai.v34i07.6793
ER  -


TY  - GEN
AU  - Wu, H.
AU  - Hu, Z.
AU  - Jia, J.
AU  - He, X.
AU  - Chua, T.-S.
TI  - Mining unfollow behavior in large-scale online social networks via spatial-temporal interaction
AB  - Online Social Networks (OSNs) evolve through two pervasive behaviors: follow and unfollow, which respectively signify relationship creation and relationship dissolution. Researches on social network evolution mainly focus on the follow behavior, while the unfollow behavior has largely been ignored. Mining unfollow behavior is challenging because user's decision on unfollow is not only affected by the simple combination of user's attributes like informativeness and reciprocity, but also affected by the complex interaction among them. Meanwhile, prior datasets seldom contain sufficient records for inferring such complex interaction. To address these issues, we first construct a large-scale real-world Weibo1 dataset, which records detailed post content and relationship dynamics of 1.8 million Chinese users. Next, we define user's attributes as two categories: spatial attributes (e.g., social role of user) and temporal attributes (e.g., post content of user). Leveraging the constructed dataset, we systematically study how the interaction effects between user's spatial and temporal attributes contribute to the unfollow behavior. Afterwards, we propose a novel unified model with heterogeneous information (UMHI) for unfollow prediction. Specifically, our UMHI model: 1) captures user's spatial attributes through social network structure; 2) infers user's temporal attributes through user-posted content and unfollow history; and 3) models the interaction between spatial and temporal attributes by the nonlinear MLP layers. Comprehensive evaluations on the constructed dataset demonstrate that the proposed UMHI model outperforms baseline methods by 16.44% on average in terms of precision. In addition, factor analyses verify that both spatial attributes and temporal attributes are essential for mining unfollow behavior.
PB  - arXiv
PY  - 2019
ST  - Mining unfollow behavior in large-scale online social networks via spatial-temporal interaction
Y2  - 2025/05/05/21:54:30
DO  - 10.1609/aaai.v34i01.5358
ER  -


TY  - GEN
AU  - Köpüklü, O.
AU  - Wei, X.
AU  - Rigoll, G.
TI  - You only watch once: A unified CNN architecture for real-time spatiotemporal action localization
AB  - Spatiotemporal action localization requires the incorporation of two sources of information into the designed architecture: (1) temporal information from the previous frames and (2) spatial information from the key frame. Current state-of-the-art approaches usually extract these information with separate networks and use an extra mechanism for fusion to get detections. In this work, we present YOWO, a unified CNN architecture for real-time spatiotemporal action localization in video streams. YOWO is a single-stage architecture with two branches to extract temporal and spatial information concurrently and predict bounding boxes and action probabilities directly from video clips in one evaluation. Since the whole architecture is unified, it can be optimized end-to-end. The YOWO architecture is fast providing 34 frames-per-second on 16-frames input clips and 62 frames-per-second on 8-frames input clips, which is currently the fastest state-of-the-art architecture on spatiotemporal action localization task. Remarkably, YOWO outperforms the previous state-of-the art results on J-HMDB-21 and UCF101-24 with an impressive improvement of ∼3% and ∼12%, respectively. Moreover, YOWO is the first and only single-stage architecture that provides competitive results on AVA dataset. We make our code and pretrained models publicly available.
PB  - arXiv
PY  - 2019
ST  - You only watch once
Y2  - 2025/05/05/21:54:30
DO  - 10.21203/rs.3.rs-3163610/v1
ER  -


TY  - GEN
AU  - Bogadhi, A.R.
AU  - Katz, L.N.
AU  - Bollimunta, A.
AU  - Leopold, D.A.
AU  - Krauzlis, R.J.
TI  - Midbrain activity supports high-level visual properties in primate temporal cortex
AB  - The evolution of the primate brain is marked by a dramatic increase in the number of neocortical areas that process visual information 1. This cortical expansion supports two hallmarks of high-level primate vision - the ability to selectively attend to particular visual features 2 and the ability to recognize a seemingly limitless number of complex visual objects 3. Given their prominent roles in high-level vision for primates, it is commonly assumed that these cortical processes supersede the earlier versions of these functions accomplished by the evolutionarily older brain structures that lie beneath the cortex. Contrary to this view, here we show that the superior colliculus (SC), a midbrain structure conserved across all vertebrates 4, is necessary for the normal expression of attention-related modulation and object selectivity in a newly identified region of macaque temporal cortex. Using a combination of psychophysics, causal perturbations and fMRI, we identified a localized region in the temporal cortex that is functionally dependent on the SC. Targeted electrophysiological recordings in this cortical region revealed neurons with strong attention-related modulation that was markedly reduced during attention deficits caused by SC inactivation. Many of these neurons also exhibited selectivity for particular visual objects, and this selectivity was also reduced during SC inactivation. Thus, the SC exerts a causal influence on high-level visual processing in cortex at a surprisingly late stage where attention and object selectivity converge, perhaps determined by the elemental forms of perceptual processing the SC has supported since before there was a neocortex.
PB  - bioRxiv
PY  - 2019
ST  - Midbrain activity supports high-level visual properties in primate temporal cortex
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/841155
ER  -


TY  - GEN
AU  - Hu, Y.
AU  - Jin, Y.
AU  - Li, R.
AU  - Zhang, X.
TI  - CMSN: Continuous multi-stage network and variable margin cosine loss for temporal action proposal generation
AB  - Accurately locating the start and end time of an action in untrimmed videos is a challenging task. One of the important reasons is the boundary of an action is not highly distinguishable, and the features around the boundary are difficult to discriminate. To address this problem, we propose a novel framework for temporal action proposal generation, namely Continuous Multi-stage Network (CMSN), which divides a video that contains a complete action instance into six stages, namely Background, Ready, Start, Confirm, End, Follow. To distinguish between Ready and Start, End and Follow more accurately, we propose a novel loss function, Variable Margin Cosine Loss (VMCL), which allows for different margins between different categories. Our experiments on THUMOS14 show that the proposed method for temporal proposal generation performs better than the state-of-the-art methods using the same network architecture and training dataset.
PB  - arXiv
PY  - 2019
ST  - CMSN
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/access.2019.2933360
ER  -


TY  - GEN
AU  - Gwilliams, L.
AU  - King, J.-R.
TI  - Recurrent processes emulate a cascade of hierarchical decisions: Evidence from spatio-temporal decoding of human brain activity
AB  - Mounting evidence suggests that perception depends on a largely-feedforward brain network. However, the discrepancy between (i) the latency of the corresponding feedforward responses (150-200 ms) and (ii) the time it takes human subjects to recognize brief images (often >500 ms) suggests that recurrent neuronal activity is critical to visual processing. Here, we use magneto-encephalography to localize, track and decode the feedforward and recurrent responses elicited by brief presentations of variably-ambiguous letters and digits. We first confirm that these stimuli trigger, within the first 200 ms, a feedforward response in the ventral and dorsal cortical pathways. The subsequent activity is distributed across temporal, parietal and prefrontal cortices and leads to a slow and incremental cascade of representations culminating in action-specific motor signals. We introduce an analytical framework to show that these brain responses are best accounted for by a hierarchy of recurrent neural assemblies. An accumulation of computational delays across specific processing stages explains subjects' reaction times. Finally, the slow convergence of neural representations towards perceptual categories is quickly followed by all-or-none motor decision signals. Together, these results show how recurrent processes generate, over extended time periods, a cascade of hierarchical decisions that ultimately predicts subjects' perceptual reports.
PB  - bioRxiv
PY  - 2019
ST  - Recurrent processes emulate a cascade of hierarchical decisions
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/840074
ER  -


TY  - GEN
AU  - Lund, A.M.
AU  - Gouripeddi, R.
AU  - Facelli, J.C.
TI  - Generation and classification of activity sequences for spatiotemporal modeling of human populations
AB  - Human activity encompasses a series of complex spatiotemporal processes that are difficult to model, but represents an essential component of human exposure assessment. A significant empirical data source like the American Time Use Survey (ATUS) can be leveraged to model human activity, but tractable models require a better stratification of activity data to inform about different, but classifiable groups of individuals that exhibit similar activities and mobility patterns. We have developed a simple unsupervised classification and sequence generation method from existing machine learning algorithms that is capable of generating coherent and stochastic sequences of activity from the data in the ATUS. This classification, when combined with any spatiotemporal exposure profile, allows the development of stochastic models of exposure patterns for groups of individuals exhibiting similar activity behaviors.
PB  - arXiv
PY  - 2019
ST  - Generation and classification of activity sequences for spatiotemporal modeling of human populations
Y2  - 2025/05/05/21:54:30
DO  - 10.5210/ojphi.v12i1.10588
ER  -


TY  - GEN
AU  - Lin, C.
AU  - Li, J.
AU  - Wang, Y.
AU  - Huang, F.
AU  - Ji, R.
TI  - Fast learning of temporal action proposal via dense boundary generator
AB  - Generating temporal action proposals remains a very challenging problem, where the main issue lies in predicting precise temporal proposal boundaries and reliable action confidence in long and untrimmed real-world videos. In this paper, we propose an efficient and unified framework to generate temporal action proposals named Dense Boundary Generator (DBG), which draws inspiration from boundary-sensitive methods and implements boundary classification and action completeness regression for densely distributed proposals. In particular, the DBG consists of two modules: Temporal boundary classification (TBC) and Action-aware completeness regression (ACR). The TBC aims to provide two temporal boundary confidence maps by low-level two-stream features, while the ACR is designed to generate an action completeness score map by high-level action-aware features. Moreover, we introduce a dual stream BaseNet (DSB) to encode RGB and optical flow information, which helps to capture discriminative boundary and actionness features. Extensive experiments on popular benchmarks ActivityNet-1.3 and THUMOS14 demonstrate the superiority of DBG over the state-of-the-art proposal generator (e.g., MGG and BMN).
PB  - arXiv
PY  - 2019
ST  - Fast learning of temporal action proposal via dense boundary generator
Y2  - 2025/05/05/21:54:30
DO  - 10.1609/aaai.v34i07.6815
ER  -


TY  - GEN
AU  - Lugones, R.
AU  - Dmitruk, P.
AU  - Mininni, P.D.
AU  - Pouquet, A.
AU  - Matthaeus, W.H.
TI  - Spatio-temporal behavior of magnetohydrodynamic fluctuations with cross-helicity and background magnetic field
AB  - We study the spatio-temporal behavior of the Elsässer variables describing magnetic and velocity field fluctuations, using direct numerical simulations of three-dimensional magnetohydrodynamic turbulence. We consider cases with relatively small, intermediate, and large values of a mean background magnetic field, and with null, small, and high cross-helicity (correlations between the velocity and the magnetic field). Wavenumber-dependent time correlation functions are computed for the different simulations. From these correlation functions, the decorrelation time is computed and compared with different theoretical characteristic times: the local non-linear time, the random-sweeping time, and the Alfvénic time. It is found that decorrelation times are dominated by sweeping effects for low values of the mean magnetic field and for low values of the cross-helicity, while for large values of the background field or of the cross-helicity and for wave vectors sufficiently aligned with the guide field, decorrelation times are controlled by Alfvénic effects. Finally, we observe counter-propagation of Alfvénic fluctuations due to reflections produced by inhomogeneities in the total magnetic field. This effect becomes more prominent in flows with large cross-helicity, strongly modifying the propagation of waves in turbulent magnetohydrodynamic flows.
PB  - arXiv
PY  - 2019
ST  - Spatio-temporal behavior of magnetohydrodynamic fluctuations with cross-helicity and background magnetic field
Y2  - 2025/05/05/21:54:30
DO  - 10.1063/1.5129655
ER  -


TY  - GEN
AU  - Liu, Y.-C.
AU  - Hsieh, Y.-A.
AU  - Chen, M.-H.
AU  - Tegner, J.
AU  - James Tsai, Y.-C.
TI  - Interpretable self-attention temporal reasoning for driving behavior understanding
AB  - Performing driving behaviors based on causal reasoning is essential to ensure driving safety. In this work, we investigated how state-of-the-art 3D Convolutional Neural Networks (CNNs) perform on classifying driving behaviors based on causal reasoning. We proposed a perturbation-based visual explanation method to inspect the models' performance visually. By examining the video attention saliency, we found that existing models could not precisely capture the causes (e.g., traffic light) of the specific action (e.g., stopping). Therefore, the Temporal Reasoning Block (TRB) was proposed and introduced to the models. With the TRB models, we achieved the accuracy of 86.3%, which outperform the state-of-the-art 3D CNNs from previous works. The attention saliency also demonstrated that TRB helped models focus on the causes more precisely. With both numerical and visual evaluations, we concluded that our proposed TRB models were able to provide accurate driving behavior prediction by learning the causal reasoning of the behaviors.
PB  - arXiv
PY  - 2019
ST  - Interpretable self-attention temporal reasoning for driving behavior understanding
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icassp40776.2020.9053783
ER  -


TY  - GEN
AU  - Zhou, Y.
AU  - Li, H.
AU  - Kung, S.-Y.
TI  - Temporal action localization using long short-term dependency
AB  - Temporal action localization in untrimmed videos is an important but difficult task. Difficulties are encountered in the application of existing methods when modeling temporal structures of videos. In the present study, we developed a novel method, referred to as Gemini Network, for effective modeling of temporal structures and achieving high-performance temporal action localization. The significant improvements afforded by the proposed method are attributable to three major factors. First, the developed network utilizes two subnets for effective modeling of temporal structures. Second, three parallel feature extraction pipelines are used to prevent interference between the extractions of different stage features. Third, the proposed method utilizes auxiliary supervision, with the auxiliary classifier losses affording additional constraints for improving the modeling capability of the network. As a demonstration of its effectiveness, the Gemini Network was used to achieve state-of-the-art temporal action localization performance on two challenging datasets, namely, THUMOS14 and ActivityNet.
PB  - arXiv
PY  - 2019
ST  - Temporal action localization using long short-term dependency
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/tmm.2020.3042077
ER  -


TY  - GEN
AU  - Rozanski, G.M.
AU  - Huntley, A.H.
AU  - Crosby, L.D.
AU  - Mansfield, A.
AU  - Patterson, K.K.
TI  - Lower limb muscle activity underlying temporal gait asymmetry post-stroke
AB  - Objective: Asymmetric walking after stroke is common, detrimental, and difficult to treat, but current knowledge of underlying physiological mechanisms is limited. This study investigated electromyographic (EMG) features of temporal gait asymmetry (TGA). Methods: Participants post-stroke with or without TGA and control adults (n=27, 8, and 9, respectively) performed self-paced overground gait trials. EMG, force plate, and motion capture data were collected. Lower limb muscle activity was compared across groups and sides (more/less affected). Correlation between burst timing variables and asymmetry ratios was examined. Results: Significant group by side interaction effects were found: fewer TGA group dorsiflexor bursts during swing (p=.0009), more affected plantarflexor stance activity ended early (p=.0006) and less affected dorsiflexor on/off time was delayed (p<.01) in persons with asymmetry compared to symmetric and normative controls. Less affected side EMG timing correlated most with swing time ratio (r=0.68-0.90, p<.001). Conclusions: Temporal patterns of muscular activation, particularly about the ankle around the stance-to-swing transition period, are associated with TGA. The results may reflect specific impairments or compensations that affect locomotor coordination. Significance: Neuromuscular underpinnings of spatiotemporal asymmetry have not been previously characterized. These novel findings may inform targeted therapeutic strategies to improve gait quality after stroke.
PB  - medRxiv
PY  - 2019
ST  - Lower limb muscle activity underlying temporal gait asymmetry post-stroke
Y2  - 2025/05/05/21:54:30
DO  - 10.1016/j.clinph.2020.04.171
ER  -


TY  - GEN
AU  - Gillett, M.
AU  - Pereira, U.
AU  - Brunel, N.
TI  - Characteristics of sequential activity in networks with temporally asymmetric Hebbian learning
AB  - Sequential activity has been observed in multiple neuronal circuits across species, neural structures, and behaviors. It has been hypothesized that sequences could arise from unsupervised learning processes. However, it is still unclear whether biologically plausible synaptic plasticity rules can organize neuronal activity to form sequences whose statistics match experimental observations. Here we investigate temporally asymmetric Hebbian rules in sparsely connected recurrent rate networks, and develop a theory of the transient sequential activity observed after learning. These rules transform a sequence of random input patterns into synaptic weight updates. After learning, recalled sequential activity is reflected in the transient correlation of network activity with each of the stored input patterns. Using mean-field theory, we derive a low-dimensional description of the network dynamics and compute the storage capacity of these networks. Multiple temporal characteristics of the recalled sequential activity are consistent with experimental observations. We find that the degree of sparseness of the recalled sequences can be controlled by non-linearities in the learning rule. Furthermore, sequences maintain robust decoding, but display highly labile dynamics, when synaptic connectivity is continuously modified due to noise or storage of other patterns, similar to recent observations in hippocampus and parietal cortex. Finally, we demonstrate that our results also hold in recurrent networks of spiking neurons with separate excitatory and inhibitory populations.
PB  - bioRxiv
PY  - 2019
ST  - Characteristics of sequential activity in networks with temporally asymmetric Hebbian learning
Y2  - 2025/05/05/21:54:30
DO  - 10.1073/pnas.1918674117
ER  -


TY  - GEN
AU  - Lin, X.
AU  - Shou, Z.
AU  - Chang, S.-F.
TI  - LPAT: Learning to Predict Adaptive Threshold for Weakly-supervised Temporal Action Localization
AB  - Recently, Weakly-supervised Temporal Action Localization (WTAL) has been densely studied because it can free us from costly annotating temporal boundaries of actions. One prevalent strategy is obtaining action score sequences over time and then truncating segments of scores higher than a fixed threshold at every kept snippet. However, the threshold is not modeled in the training process and manually setting the threshold introduces expert knowledge, which damages the coherence of systems and makes it unfair for comparisons. In this paper, we propose to adaptively set the threshold at each snippet to be its background score, which can be learned to predict (LPAT).1 In both training and testing time, the predicted threshold is leveraged to localize action segments and the scores of these segments are allocated for video classification. We also identify an important constraint to improve the confidence of generated proposals, and model it as a novel loss term, which facilitates the video classification loss to improve models’ localization ability. As such, our LPAT model is able to generate accurate action proposals with only video-level supervision. Extensive experiments on two standard yet challenging datasets, i.e., THUMOS’14 and ActivityNet1.2, show significant improvement over state-of-the-art methods.
PB  - arXiv
PY  - 2019
ST  - LPAT
Y2  - 2025/05/05/21:54:30
DO  - 10.2139/ssrn.4690808
ER  -


TY  - GEN
AU  - Silva, A.
AU  - Karunasekera, S.
AU  - Leckie, C.
AU  - Luo, L.
TI  - USTAR: Online multimodal embedding for modeling user-guided spatiotemporal activity
AB  - Building spatiotemporal activity models for people's activities in urban spaces is important for understanding the ever-increasing complexity of urban dynamics. With the emergence of Geo-Tagged Social Media (GTSM) records, previous studies demonstrate the potential of GTSM records for spatiotemporal activity modeling. State-of-the-art methods for this task embed different modalities (location, time, and text) of GTSM records into a single embedding space. However, they ignore Non-GeoTagged Social Media (NGTSM) records, which generally account for the majority of posts (e.g., more than 95% in Twitter), and could represent a great source of information to alleviate the sparsity of GTSM records. Furthermore, in the current spatiotemporal embedding techniques, less focus has been given to the users, who exhibit spatially motivated behaviors. To bridge this research gap, this work proposes USTAR, a novel online learning method for User-guided SpatioTemporal Activity Representation, which (1) embeds locations, time, and text along with users into the same embedding space to capture their correlations; (2) uses a novel collaborative filtering approach based on two different empirically studied user behaviors to incorporate both NGTSM and GTSM records in learning; and (3) introduces a novel sampling technique to learn spatiotemporal representations in an online fashion to accommodate recent information into the embedding space, while avoiding overfitting to recent records and frequently appearing units in social media streams. Our results show that USTAR substantially improves the state-of-the-art for region retrieval and keyword retrieval and its potential to be applied to other downstream applications such as local event detection.
PB  - arXiv
PY  - 2019
ST  - USTAR
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/bigdata47090.2019.9005461
ER  -


TY  - GEN
AU  - Benitez, E.M.
AU  - Estallo, E.L.
AU  - Grech, M.
AU  - Almirón, W.R.
AU  - Ludueña-Almeida, F.F.
TI  - Temporal models using environmental variables to predict Aedes aegypti oviposition activity in a temperate region of Argentina
AB  - Environmental variables are some of the factors that more impact on Aedes aegypti, vector of Dengue, Chikungunya and Zika viruses. In this study, the Ae. aegypti oviposition activity was related to satellite and meteorological variables, in Córdoba City (Argentina). Eggs were collected using ovitraps placed throughout the city from 2009 to 2012, replaced them weekly. Generalized Linear Mixed Models were developed with negative binomial distributions of errors, modeling average number of eggs collected weekly as a function of satellite and meteorological variables with time lags. The best model included a vegetation index, vapor pressure of water, precipitation and photoperiod, lagged between 3 and 4 weeks. By each increment unit in vegetation index, vapor pressure (hPa) and light hour, the average number of eggs increases by 72%, 20% and 29%, respectively, in the following 3 or 4 weeks. In addition, the average number of eggs decreases a 50% in the following month by each millimeter of rain. The results evince the relevant importance of the vegetation, maybe due to the shade that provide to the containers as breeding habitat of this species and the shelter for adults. On the other hand, the negative effect of precipitation could be a consequence of abundant rainfalls that fulfill containers avoiding females to lay eggs in there. Although no significant effect of the photoperiod on the vector abundance has been detected in the north of the country, in Córdoba City an important effect is observed when it is presented together with other variables, producing different effects of these variables in different regions. Lastly, it is necessary to emphasize the importance of the minimum temperature in temperate zones, since it would be a limiting factor that prevents the Ae. aegypti oviposition activity.
PB  - bioRxiv
PY  - 2019
ST  - Temporal models using environmental variables to predict Aedes aegypti oviposition activity in a temperate region of Argentina
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/816421
ER  -


TY  - GEN
AU  - Malaia, E.A.
AU  - Ahn, S.
AU  - Rubchinsky, L.L.
TI  - Dysregulation of temporal dynamics of synchronous neural activity in adolescents on autism spectrum
AB  - Autism spectrum disorder is increasingly understood to be based on atypical signal transfer among multiple interconnected networks in the brain. Relative temporal patterns of neural activity have been shown to underlie both the altered neurophysiology and the altered behaviors in a variety of neurogenic disorders. We assessed brain network dynamics variability in Autism Spectrum Disorders (ASD) using measures of synchronization (phase-locking) strength, and timing of synchronization and desynchronization of neural activity (desynchronization ratio) across frequency bands of resting state EEG. Our analysis indicated that fronto-parietal synchronization is higher in ASD, but with more short periods of desynchronization. It also indicates that the relationship between the properties of neural synchronization and behavior is different in ASD and typically developing populations. Recent theoretical studies suggest that neural networks with high desynchronization ratio have increased sensitivity to inputs. Our results point to the potential significance of this phenomenon to autistic brain. This sensitivity may disrupt production of an appropriate neural and behavioral responses to external stimuli. Cognitive processes dependent on integration of activity from multiple networks may be, as a result, particularly vulnerable to disruption.
PB  - bioRxiv
PY  - 2019
ST  - Dysregulation of temporal dynamics of synchronous neural activity in adolescents on autism spectrum
Y2  - 2025/05/05/21:54:30
DO  - 10.1002/aur.2219
ER  -


TY  - GEN
AU  - Tang, Y.
AU  - Niu, C.
AU  - Dong, M.
AU  - Ren, S.
AU  - Liang, J.
TI  - AFO-TAD: Anchor-free one-stage detector for temporal action detection
AB  - Temporal action detection is a fundamental yet challenging task in video understanding. Many of the state-of-the-art methods predict the boundaries of action instances based on predetermined anchors akin to the two-dimensional object detection detectors. However, it is hard to detect all the action instances with predetermined temporal scales because the durations of instances in untrimmed videos can vary from few seconds to several minutes. In this paper, we propose a novel action detection architecture named anchor-free one-stage temporal action detector (AFO-TAD). AFO-TAD achieves better performance for detecting action instances with arbitrary lengths and high temporal resolution, which can be attributed to two aspects. First, we design a receptive field adaption module which dynamically adjusts the receptive field for precise action detection. Second, AFO-TAD directly predicts the categories and boundaries at every temporal locations without predetermined anchors. Extensive experiments show that AFO-TAD improves the state-of-the-art performance on THUMOS'14.
PB  - arXiv
PY  - 2019
ST  - AFO-TAD
Y2  - 2025/05/05/21:54:30
DO  - 10.1117/12.2572966
ER  -


TY  - GEN
AU  - Karvounas, G.
AU  - Oikonomidis, I.
AU  - Argyros, A.
TI  - ReActNet: Temporal localization of repetitive activities in real-world videos
AB  - We address the problem of temporal localization of repetitive activities in a video, i.e., the problem of identifying all segments of a video that contain some sort of repetitive or periodic motion. To do so, the proposed method represents a video by the matrix of pairwise frame distances. These distances are computed on frame representations obtained with a convolutional neural network. On top of this representation, we design, implement and evaluate ReAct-Net, a lightweight convolutional neural network that classifies a given frame as belonging (or not) to a repetitive video segment. An important property of the employed representation is that it can handle repetitive segments of arbitrary number and duration. Furthermore, the proposed training process requires a relatively small number of annotated videos. Our method raises several of the limiting assumptions of existing approaches regarding the contents of the video and the types of the observed repetitive activities. Experimental results on recent, publicly available datasets validate our design choices, verify the generalization potential of ReActNet and demonstrate its superior performance in comparison to the current state of the art.
PB  - arXiv
PY  - 2019
ST  - ReActNet
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/ism.2020.00018
ER  -


TY  - GEN
AU  - Girdhar, R.
AU  - Ramanan, D.
TI  - CATER: A diagnostic dataset for compositional actions & temporal reasoning
AB  - Computer vision has undergone a dramatic revolution in performance, driven in large part through deep features trained on large-scale supervised datasets. However, much of these improvements have focused on static image analysis; video understanding has seen rather modest improvements. Even though new datasets and spatiotemporal models have been proposed, simple frame-by-frame classification methods often still remain competitive. We posit that current video datasets are plagued with implicit biases over scene and object structure that can dwarf variations in temporal structure. In this work, we build a video dataset with fully observable and controllable object and scene bias, and which truly requires spatiotemporal understanding in order to be solved. Our dataset, named CATER, is rendered synthetically using a library of standard 3D objects, and tests the ability to recognize compositions of object movements that require long-term reasoning. In addition to being a challenging dataset, CATER also provides a plethora of diagnostic tools to analyze modern spatiotemporal video architectures by being completely observable and controllable. Using CATER, we provide insights into some of the most recent state of the art deep video architectures.
PB  - arXiv
PY  - 2019
ST  - CATER
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvpr.2017.215
ER  -


TY  - GEN
AU  - Peng, T.-Q.
AU  - Zhu, J.J.H.
TI  - Mobile phone use as sequential processes: From discrete behaviors to sessions of behaviors and trajectories of sessions
AB  - Mobile phone use is an unfolding process by nature. In this study, it is explicated as two sequential processes: mobile sessions composed of an uninterrupted set of behaviors and mobile trajectories composed of mobile sessions and mobile-off time. A dataset of a five-month behavioral logfile of mobile application use by approximately 2,500 users in Hong Kong is used. Mobile sessions are constructed and mined to uncover sequential characteristics and patterns in mobile phone use. Mobile trajectories are analyzed to examine intraindividual change and interindividual differences on mobile re-engagement as indicators of behavioral dynamics in mobile phone use. The study provides empirical support for and expands the boundaries of existing theories about combinatorial use of information and communication technologies (ICTs). Finally, the understanding on mobile temporality is enhanced, that is, mobile temporality is homogeneous across social sectors. Furthermore, mobile phones redefine, rather than blur, the boundary between private and public time.
PB  - arXiv
PY  - 2019
ST  - Mobile phone use as sequential processes
Y2  - 2025/05/05/21:54:30
DO  - 10.1093/jcmc/zmz029
ER  -


TY  - GEN
AU  - Ji, J.
AU  - Cao, K.
AU  - Niebles, J.C.
TI  - Learning temporal action proposalswith fewer labels
AB  - Temporal action proposals are a common module in action detection pipelines today. Most current methods for training action proposal modules rely on fully supervised approaches that require large amounts of annotated temporal action intervals in long video sequences. The large cost and effort in annotation that this entails motivate us to study the problem of training proposal modules with less supervision. In this work, we propose a semi-supervised learning algorithm specifically designed for training temporal action proposal networks. When only a small number of labels are available, our semi-supervised method generates significantly better proposals than the fully-supervised counterpart and other strong semi-supervised baselines. We validate our method on two challenging action detection video datasets, ActivityNet v1.3 and THUMOS14. We show that our semi-supervised approach consistently matches or outperforms the fully supervised state-of-The-Art approaches.
PB  - arXiv
PY  - 2019
ST  - Learning temporal action proposalswith fewer labels
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/iccv.2019.00717
ER  -


TY  - GEN
AU  - Narayanan, A.
AU  - Siravuru, A.
AU  - Dariush, B.
TI  - Temporal multimodal fusion for driver behavior prediction tasks using gated recurrent fusion units
AB  - The Tactical Driver Behavior modeling problem requires understanding of driver actions in complicated urban scenarios from a rich multi modal signals including video, LiDAR and CAN bus data streams. However, the majority of deep learning research is focused either on learning the vehicle/environment state (sensor fusion) or the driver policy (from temporal data), but not both. Learning both tasks end-To-end offers the richest distillation of knowledge, but presents challenges in formulation and successful training. In this work, we propose promising first steps in this direction. Inspired by the gating mechanisms in LSTM, we propose gated recurrent fusion units (GRFU) that learn fusion weighting and temporal weighting simultaneously. We demonstrate it's superior performance over multimodal and temporal baselines in supervised regression and classification tasks, all in the realm of autonomous navigation. We note a 10% improvement in the mAP score over state-of-The-Art for tactical driver behavior classification in HDD dataset and a 20% drop in overall Mean squared error for steering action regression on TORCS dataset.
PB  - arXiv
PY  - 2019
ST  - Temporal multimodal fusion for driver behavior prediction tasks using gated recurrent fusion units
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/lra.2020.2967738
ER  -


TY  - GEN
AU  - Stergiou, A.
AU  - Poppe, R.
TI  - Spatio-Temporal FAST 3D convolutions for human action recognition
AB  - —Effective processing of video input is essential for the recognition of temporally varying events such as human actions. Motivated by the often distinctive temporal characteristics of actions in either horizontal or vertical direction, we introduce a novel convolution block for CNN architectures with video input. Our proposed Fractioned Adjacent Spatial and Temporal (FAST) 3D convolutions are a natural decomposition of a regular 3D convolution. Each convolution block consist of three sequential convolution operations: a 2D spatial convolution followed by spatio-temporal convolutions in the horizontal and vertical direction, respectively. Additionally, we introduce a FAST variant that treats horizontal and vertical motion in parallel. Experiments on benchmark action recognition datasets UCF-101 and HMDB-51 with ResNet architectures demonstrate consistent increased performance of FAST 3D convolution blocks over traditional 3D convolutions. The lower validation loss indicates better generalization, especially for deeper networks. We also evaluate the performance of CNN architectures with similar memory requirements, based either on Two-stream networks or with 3D convolution blocks. DenseNet-121 with FAST 3D convolutions was shown to perform best, giving further evidence of the merits of the decoupled spatio-temporal convolutions.
PB  - arXiv
PY  - 2019
ST  - Spatio-Temporal FAST 3D convolutions for human action recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icmla.2019.00036
ER  -


TY  - GEN
AU  - Karimi Abadchi, J.
AU  - Nazari-Ahangarkolaee, M.
AU  - Gattas, S.
AU  - McNaughton, B.L.
AU  - Mohajerani, M.H.
TI  - Spatiotemporal patterns of neocortical activity around hippocampal sharp-wave ripples
AB  - A prevalent model is that sharp-wave ripples (SWR) arise ‘spontaneously’ in CA3 and propagate recent memory traces outward to the neocortex to facilitate memory consolidation there. Using voltage and extracellular glutamate transient recording over widespread regions of mice dorsal neocortex in relation to CA1 multiunit activity (MUA) and SWR, we find that the largest SWR-related modulation occurs in retrosplenial cortex; however, contrary to the unidirectional hypothesis, neocortical activation exhibited a continuum of activation timings relative to SWRs, varying from leading to lagging. Thus, contrary to the model in which SWRs arise ‘spontaneously’ in the hippocampus, neocortical activation often precedes SWRs and may thus constitute a trigger event in which neocortical information seeds associative reactivation of hippocampal ‘indices’. This timing continuum is consistent with a dynamics in which older, more consolidated memories may in fact initiate the hippocampal-neocortical dialog, whereas reactivation of newer memories may be initiated predominantly in the hippocampus.
PB  - bioRxiv
PY  - 2019
ST  - Spatiotemporal patterns of neocortical activity around hippocampal sharp-wave ripples
Y2  - 2025/05/05/21:54:30
DO  - 10.7554/elife.51972
ER  -


TY  - GEN
AU  - Luo, C.
AU  - Yuille, A.
TI  - Grouped spatial-temporal aggregation for efficient action recognition
AB  - Temporal reasoning is an important aspect of video analysis. 3D CNN shows good performance by exploring spatial-temporal features jointly in an unconstrained way, but it also increases the computational cost a lot. Previous works try to reduce the complexity by decoupling the spatial and temporal filters. In this paper, we propose a novel decomposition method that decomposes the feature channels into spatial and temporal groups in parallel. This decomposition can make two groups focus on static and dynamic cues separately. We call this grouped spatial-temporal aggregation (GST). This decomposition is more parameter-efficient and enables us to quantitatively analyze the contributions of spatial and temporal features in different layers. We verify our model on several action recognition tasks that require temporal reasoning and show its effectiveness.
PB  - arXiv
PY  - 2019
ST  - Grouped spatial-temporal aggregation for efficient action recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/iccv.2019.00561
ER  -


TY  - GEN
AU  - Gong, J.
AU  - Chen, G.
AU  - Jia, Y.
AU  - Huang, L.
AU  - Wang, Y.
TI  - Characteristics of Temporal Dynamics of Intrinsic Brain Activity in Unmedicated Bipolar II Disorder with Suicidality
AB  - Background: Bipolar disorder (BD) is associated with a high risk of suicide. Routine neuroimaging examination exhibited that BD with suicidality was associated with brain structural and functional changes. However, the alterations of brain dynamics are still unknown. Purpose: To investigate the alterations of brain dynamics in unmedicated BD II depression with suicidality and to predict the severity of suicidality. Materials and Methods: The prospective study included 106 BD II participants (20 with suicidal attempt (SA), 35 with suicidal ideation (SI), 51 without SI (NSI)) and 50 healthy controls (HCs) who underwent resting-state functional magnetic resonance imaging (rs-fMRI) between February 2016 and December 2017. We first used sliding window analysis to evaluate the dynamic amplitude of low-frequency fluctuations (dALFF). Then we predicted the severity of suicidality using a multivariate regression model. Results: One-Way ANOVA analyses revealed that the dALFF in the right temporal pole (TP), inferior temporal gyrus (ITG), superior temporal gyrus (STG), and the bilateral precuneus/posterior cingulate cortex (PCC) was significantly different among the 4 groups. Post hoc comparisons revealed all BD groups showed decreased dALFF in the bilateral precuneus/PCC compared with HCs. Increased dALFF was found in the right STG and ITG in the SA group compared with the others, and in the right TP in the SA group compared with SI and HCs groups. Importantly, these temporal variabilities could be used to predict the severity of suicidality (r = 0.330, p = 0.036), whereas static ALFF couldn't (r = -0.050, p = 0.532). Conclusion: Our findings suggest that alterations of temporal variability in the precuneus/PCC is a common feature of BD participants, the right temporal lobe involved in impulsivity, social and emotional processing are associated with suicidality in BD II depression participants. This predictive model using the dynamics of intrinsic brain activity may be helpful for clinical applications. Funding: The study was supported by grants from the National Natural Science Foundation of China (81971597, 81671670, and 81501456); Project in Basic Research and Applied Basic Research in General Colleges and Universities of Guangdong, China (2018KZDXM009); Planned Science and Technology Project of Guangzhou, China (20160402007 and 201604020184). Declaration of Interest: The authors declare no other competing interests. Ethical Approval: This retrospective study was approved by the Ethics Committee of the First Affiliated Hospital of Jinan University, Guangzhou, China.
PB  - SSRN
PY  - 2019
ST  - Characteristics of Temporal Dynamics of Intrinsic Brain Activity in Unmedicated Bipolar II Disorder with Suicidality
Y2  - 2025/05/05/21:54:30
DO  - 10.2139/ssrn.3458502
ER  -


TY  - GEN
AU  - Gheidi, A.
AU  - Kumar, V.
AU  - Fitzpatrick, C.J.
AU  - Atkinson, R.L.
AU  - Morrow, J.D.
TI  - Cellular compartment analysis of temporal activity by fluorescent in situ hybridization (catFISH) in the transcardially perfused rat brain
AB  - Cellular compartment analysis of temporal activity by fluorescent in situ hybridization (catFISH) allows high spatiotemporal resolution mapping of immediate early genes in the brain in response to internal/external stimuli. One caveat of this technique and indeed other methods of in situ hybridization is the necessity of flash-freezing the brain prior to staining. Often however, the mammalian brain is transcardially perfused to use the brain tissue for immunohistochemistry, the most widely-used technique to study gene expression. The present study illustrates how the original catFISH protocol can be modified for use in adult rats that have been transcardially perfused with 4% paraformaldehyde. c-Fos activity induced by either an auditory tone or status epilepticus was visualized using the catFISH procedure. Analysis of the rat prefrontal cortex, hippocampus and amygdala shows that a clear distinction can be made between the compartmental distribution of c-Fos mRNA in the nuclei and cytoplasmic regions. Furthermore, the qualitative proportion of c-Fos compartmentalization is similar to previous reports of c-Fos expression pattern in rodents navigating novel environments. c-Fos catFISH on perfused rodent brains is an valuable addition to the traditional histological methods using fluorescently labeled riboprobes, and opens several avenues for future investigations.
PB  - bioRxiv
PY  - 2019
ST  - Cellular compartment analysis of temporal activity by fluorescent in situ hybridization (catFISH) in the transcardially perfused rat brain
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/767111
ER  -


TY  - GEN
AU  - Wang, J.
TI  - LEARNING COUPLED SPATIAL-TEMPORAL ATTENTION FOR SKELETON-BASED ACTION RECOGNITION
AB  - In this paper, we propose a coupled spatial-temporal attention (CSTA) model for skeleton-based action recognition, which aims to figure out the most discriminative joints and frames in spatial and temporal domains simultaneously. Conventional approaches usually consider all the joints or frames in a skeletal sequence equally important, which are unrobust to ambiguous and redundant information. To address this, we first learn two sets of weights for different joints and frames through two subnetworks respectively, which enable the model to have the ability of “paying attention to” the relatively informative section. Then, we calculate the cross product based on the weights of joints and frames for the coupled spatial-temporal attention. Moreover, our CSTA mechanisms can be easily plugged into existing hierarchical CNN models (CSTA-CNN) to realize their function. Extensive experimental results on the recently collected UESTC dataset and the currently largest NTU dataset have shown the effectiveness of our proposed method for skeleton-based action recognition.
PB  - arXiv
PY  - 2019
ST  - LEARNING COUPLED SPATIAL-TEMPORAL ATTENTION FOR SKELETON-BASED ACTION RECOGNITION
Y2  - 2025/05/05/21:54:30
DO  - 10.1117/1.jei.29.5.053003
ER  -


TY  - GEN
AU  - Liu, Y.
AU  - Lu, Z.
AU  - Li, J.
AU  - Yang, T.
AU  - Yao, C.
TI  - Global temporal representation based CNNs for infrared action recognition
AB  - Infrared human action recognition has many advantages, i.e., it is insensitive to illumination change, appearance variability, and shadows. Existing methods for infrared action recognition are either based on spatial or local temporal information, however, the global temporal information, which can better describe the movements of body parts across the whole video, is not considered. In this letter, we propose a novel global temporal representation named optical-flow stacked difference image (OFSDI) and extract robust and discriminative feature from the infrared action data by considering the local, global, and spatial temporal information together. Due to the small size of the infrared action dataset, we first apply convolutional neural networks on local, spatial, and global temporal stream respectively to obtain efficient convolutional feature maps from the raw data rather than train a classifier directly. Then these convolutional feature maps are aggregated into effective descriptors named three-stream trajectory-pooled deep-convolutional descriptors by trajectory-constrained pooling. Furthermore, we improve the robustness of these features by using the locality-constrained linear coding (LLC) method. With these features, a linear support vector machine (SVM) is adopted to classify the action data in our scheme. We conduct the experiments on infrared action recognition datasets InfAR and NTU RGB+D. The experimental results show that the proposed approach outperforms the representative state-of-the-art handcrafted features and deep learning features based methods for the infrared action recognition.
PB  - arXiv
PY  - 2019
ST  - Global temporal representation based CNNs for infrared action recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/lsp.2018.2823910
ER  -


TY  - GEN
AU  - Li, L.
AU  - Kong, T.
AU  - Sun, F.
AU  - Liu, H.
TI  - Deep point-wise prediction for action temporal proposal
AB  - Detecting actions in videos is an important yet challenging task. Previous works usually utilize (a) sliding window paradigms, or (b) per-frame action scoring and grouping to enumerate the possible temporal locations. Their performances are also limited to the designs of sliding windows or grouping strategies. In this paper, we present a simple and effective method for temporal action proposal generation, named Deep Point-wise Prediction (DPP). DPP simultaneously predicts the action existing possibility and the corresponding temporal locations, without the utilization of any handcrafted sliding window or grouping. The whole system is end-to-end trained with joint loss of temporal action proposal classification and location prediction. We conduct extensive experiments to verify its effectiveness, generality and robustness on standard THUMOS14 dataset. DPP runs more than 1000 frames per second, which largely satisfies the real-time requirement. The code is available at https://github.com/liluxuan1997/DPP.
PB  - arXiv
PY  - 2019
ST  - Deep point-wise prediction for action temporal proposal
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/978-3-030-36718-3_40
ER  -


TY  - GEN
AU  - Long, F.
AU  - Yao, T.
AU  - Qiu, Z.
AU  - Luo, J.
AU  - Mei, T.
TI  - Gaussian temporal awareness networks for action localization
AB  - Temporally localizing actions in a video is a fundamental challenge in video understanding. Most existing approaches have often drawn inspiration from image object detection and extended the advances, e.g., SSD and Faster R-CNN, to produce temporal locations of an action in a 1D sequence. Nevertheless, the results can suffer from robustness problem due to the design of predetermined temporal scales, which overlooks the temporal structure of an action and limits the utility on detecting actions with complex variations. In this paper, we propose to address the problem by introducing Gaussian kernels to dynamically optimize temporal scale of each action proposal. Specifically, we present Gaussian Temporal Awareness Networks (GTAN) - a new architecture that novelly integrates the exploitation of temporal structure into an one-stage action localization framework. Technically, GTAN models the temporal structure through learning a set of Gaussian kernels, each for a cell in the feature maps. Each Gaussian kernel corresponds to a particular interval of an action proposal and a mixture of Gaussian kernels could further characterize action proposals with various length. Moreover, the values in each Gaussian curve reflect the contextual contributions to the localization of an action proposal. Extensive experiments are conducted on both THUMOS14 and ActivityNet v1.3 datasets, and superior results are reported when comparing to state-of-the-art approaches. More remarkably, GTAN achieves 1.9% and 1.1% improvements in mAP on testing set of the two datasets.
PB  - arXiv
PY  - 2019
ST  - Gaussian temporal awareness networks for action localization
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvpr.2019.00043
ER  -


TY  - GEN
AU  - Bai, Y.
AU  - Zou, Q.
AU  - Chen, X.
AU  - Ding, Z.
AU  - Chen, L.
TI  - Extreme low-resolution activity recognition with confident spatial-temporal attention transfer
AB  - Activity recognition on extreme low-resolution videos, e.g., a resolution of 12×16 pixels, plays a vital role in far-view surveillance and privacy-preserving multimedia analysis. Low-resolution videos only contain limited information. Given the fact that one same activity may be represented by videos in both high resolution (HR) and extreme low resolution (eLR), it is worth studying to utilize the relevant HR data to improve the eLR activity recognition. In this work, we propose a novel Confident Spatial-Temporal Attention Transfer (CSTAT) for eLR activity recognition. CSTAT can acquire information from HR data by reducing the attention differences with a transfer-learning strategy. Besides, the confidence of the supervisory signal is also taken into consideration for a more reliable transferring process. Experimental results demonstrate that, the proposed method can effectively improve the accuracy of eLR activity recognition and achieve an accuracy of 59.41% on 12×16 videos in HMDB51, a state-of-the-art performance.
PB  - arXiv
PY  - 2019
ST  - Extreme low-resolution activity recognition with confident spatial-temporal attention transfer
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/s11263-023-01771-4
ER  -


TY  - GEN
AU  - Zeng, R.
AU  - Huang, W.
AU  - Tan, M.
AU  - Huang, J.
AU  - Gan, C.
TI  - Graph convolutional networks for temporal action localization
AB  - Most state-of-the-art action localization systems process each action proposal individually, without explicitly exploiting their relations during learning. However, the relations between proposals actually play an important role in action localization, since a meaningful action always consists of multiple proposals in a video. In this paper, we propose to exploit the proposal-proposal relations using Graph Convolutional Networks (GCNs). First, we construct an action proposal graph, where each proposal is represented as a node and their relations between two proposals as an edge. Here, we use two types of relations, one for capturing the context information for each proposal and the other one for characterizing the correlations between distinct actions. Then we apply the GCNs over the graph to model the relations among different proposals and learn powerful representations for the action classification and localization. Experimental results show that our approach significantly outperforms the state-of-the-art on THUMOS14 (49.1% versus 42.8%). Moreover, augmentation experiments on ActivityNet also verify the efficacy of modeling action proposal relationships. Codes are available at https://github.com/Alvin-Zeng/PGCN.
PB  - arXiv
PY  - 2019
ST  - Graph convolutional networks for temporal action localization
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/iccv.2019.00719
ER  -


TY  - GEN
AU  - Lee, X.Y.
AU  - Ghadai, S.
AU  - Tan, K.L.
AU  - Hegde, C.
AU  - Sarkar, S.
TI  - Spatiotemporally constrained action space attacks on deep reinforcement learning agents
AB  - Robustness of Deep Reinforcement Learning (DRL) algorithms towards adversarial attacks in real world applications such as those deployed in cyber-physical systems (CPS) are of increasing concern. Numerous studies have investigated the mechanisms of attacks on the RL agent's state space. Nonetheless, attacks on the RL agent's action space (corresponding to actuators in engineering systems) are equally perverse, but such attacks are relatively less studied in the ML literature. In this work, we first frame the problem as an optimization problem of minimizing the cumulative reward of an RL agent with decoupled constraints as the budget of attack. We propose the white-box Myopic Action Space (MAS) attack algorithm that distributes the attacks across the action space dimensions. Next, we reformulate the optimization problem above with the same objective function, but with a temporally coupled constraint on the attack budget to take into account the approximated dynamics of the agent. This leads to the white-box Look-ahead Action Space (LAS) attack algorithm that distributes the attacks across the action and temporal dimensions. Our results showed that using the same amount of resources, the LAS attack deteriorates the agent's performance significantly more than the MAS attack. This reveals the possibility that with limited resource, an adversary can utilize the agent's dynamics to malevolently craft attacks that causes the agent to fail. Additionally, we leverage these attack strategies as a possible tool to gain insights on the potential vulnerabilities of DRL agents.
PB  - arXiv
PY  - 2019
ST  - Spatiotemporally constrained action space attacks on deep reinforcement learning agents
Y2  - 2025/05/05/21:54:30
DO  - 10.1609/aaai.v34i04.5887
ER  -


TY  - GEN
AU  - Oliveira, G.M.D.
AU  - Cunha, A.L.
TI  - A calibration method structured on Bayesian Inference of the HCM speed-flow relationship for freeways and multilane highways and a temporal analysis of traffic behavior
AB  - This paper presents a calibration method for the speed-flow model of the HCM 2016 for freeways and multilane highways allied to temporal analysis of traffic stream. The proposed method was developed using a sample of more than one million observations collected by 23 traffic sensors on four highways in the state of São Paulo. The method is structured on Bayesian inference and provided for each model parameters a probability distribution function. The free-flow speed and capacity presented a probability density function that approximates a Normal distribution. The segment in which the speed of traffic stream remain constant with the increase of the traffic flow is lower than described in HCM 2016, being in some cases close to zero. Along with the proposed calibration method an analysis of temporal variation is performed which shows a significant variation in traffic behavior for different periods. The free-flow speed, capacity and breakpoint distributions obtained through monthly and annual calibration were considered equal by means of Kolmogorov-Smirnov test, different for the model calibration coefficient.
PB  - arXiv
PY  - 2019
ST  - A calibration method structured on Bayesian Inference of the HCM speed-flow relationship for freeways and multilane highways and a temporal analysis of traffic behavior
Y2  - 2025/05/05/21:54:30
DO  - 10.14311/cej.2017.03.0026
ER  -


TY  - GEN
AU  - Zhang, J.
AU  - Shen, F.
AU  - Xu, X.
AU  - Shen, H.T.
TI  - Temporal reasoning graph for activity recognition
AB  - Despite great success has been achieved in activity analysis, it still has many challenges. Most existing work in activity recognition pay more attention to design efficient architecture or video sampling strategy. However, due to the property of fine-grained action and long term structure in video, activity recognition is expected to reason temporal relation between video sequences. In this paper, we propose an efficient temporal reasoning graph (TRG) to simultaneously capture the appearance features and temporal relation between video sequences at multiple time scales. Specifically, we construct learnable temporal relation graphs to explore temporal relation on the multi-scale range. Additionally, to facilitate multi-scale temporal relation extraction, we design a multi-head temporal adjacent matrix to represent multi-kinds of temporal relations. Eventually, a multi-head temporal relation aggregator is proposed to extract the semantic meaning of those features convolving through the graphs. Extensive experiments are performed on widely-used large-scale datasets, such as Something-Something and Charades, and the results show that our model can achieve state-of-the-art performance. Further analysis shows that temporal relation reasoning with our TRG can extract discriminative features for activity recognition.
PB  - arXiv
PY  - 2019
ST  - Temporal reasoning graph for activity recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/tip.2020.2985219
ER  -


TY  - GEN
AU  - Li, X.
AU  - Lin, T.
AU  - Liu, X.
AU  - Li, F.
AU  - Wen, S.
TI  - Deep concept-wise temporal convolutional networks for action localization
AB  - Existing action localization approaches adopt shallow temporal convolutional networks (i.e., TCN) on 1D feature map extracted from video frames. In this paper, we empirically find that stacking more conventional temporal convolution layers actually deteriorates action classification performance, possibly ascribing to that all channels of 1D feature map, which generally are highly abstract and can be regarded as latent concepts, are excessively recombined in temporal convolution. To address this issue, we introduce a novel concept-wise temporal convolution (CTC) layer as an alternative to conventional temporal convolution layer for training deeper action localization networks. Instead of recombining latent concepts, CTC layer deploys a number of temporal filters to each concept separately with shared filter parameters across concepts. Thus can capture common temporal patterns of different concepts and significantly enrich representation ability. Via stacking CTC layers, we proposed a deep concept-wise temporal convolutional network (C-TCN), which boosts the state-of-the-art action localization performance on THUMOS'14 from 42.8 to 52.1 in terms of mAP(%), achieving a relative improvement of 21.7%. Favorable result is also obtained on ActivityNet.
PB  - arXiv
PY  - 2019
ST  - Deep concept-wise temporal convolutional networks for action localization
Y2  - 2025/05/05/21:54:30
DO  - 10.1145/3394171.3413860
ER  -


TY  - GEN
AU  - Kazakos, E.
AU  - Nagrani, A.
AU  - Zisserman, A.
AU  - Damen, D.
TI  - EPIC-Fusion: Audio-visual temporal binding for egocentric action recognition
AB  - We focus on multi-modal fusion for egocentric action recognition, and propose a novel architecture for multimodal temporal-binding, i.e. the combination of modalities within a range of temporal offsets. We train the architecture with three modalities – RGB, Flow and Audio – and combine them with mid-level fusion alongside sparse temporal sampling of fused representations. In contrast with previous works, modalities are fused before temporal aggregation, with shared modality and fusion weights over time. Our proposed architecture is trained end-to-end, outperforming individual modalities as well as late-fusion of modalities. We demonstrate the importance of audio in egocentric vision, on per-class basis, for identifying actions as well as interacting objects. Our method achieves state of the art results on both the seen and unseen test sets of the largest egocentric dataset: EPIC-Kitchens, on all metrics using the public leaderboard.
PB  - arXiv
PY  - 2019
ST  - EPIC-Fusion
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/iccv.2019.00559
ER  -


TY  - GEN
AU  - Zhang, P.
AU  - Cao, Y.
AU  - Liu, B.
TI  - MULTI-STREAM SINGLE SHOT SPATIAL-TEMPORAL ACTION DETECTION
AB  - We present a 3D Convolutional Neural Networks (CNNs) based single shot detector for spatial-temporal action detection tasks. Our model includes: (i) two short-term appearance and motion streams, with single RGB and optical flow image input separately, in order to capture the spatial and temporal information for the current frame; (ii) two long-term 3D ConvNet based stream, working on sequences of continuous RGB and optical flow images to capture the context from past frames. Our model achieves strong performance for action detection in video and can be easily integrated into any current two-stream action detection methods. We report a frame-mAP of 71.30% on the challenging UCF101-24 [1] actions dataset, achieving the state-of-the-art result of the one-stage methods. To the best of our knowledge, our work is the first system that combined 3D CNN and SSD in action detection tasks.
PB  - arXiv
PY  - 2019
ST  - MULTI-STREAM SINGLE SHOT SPATIAL-TEMPORAL ACTION DETECTION
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icip.2019.8803564
ER  -


TY  - GEN
AU  - Martinez, B.
AU  - Modolo, D.
AU  - Xiong, Y.
AU  - Tighe, J.
TI  - Action recognition with spatial-temporal discriminative filter banks
AB  - Action recognition has seen a dramatic performance improvement in the last few years. Most of the current state-of-the-art literature either aims at improving performance through changes to the backbone CNN network, or they explore different trade-offs between computational efficiency and performance, again through altering the backbone network. However, almost all of these works maintain the same last layers of the network, which simply consist of a global average pooling followed by a fully connected layer. In this work we focus on how to improve the representation capacity of the network, but rather than altering the backbone, we focus on improving the last layers of the network, where changes have low impact in terms of computational cost. In particular, we show that current architectures have poor sensitivity to finer details and we exploit recent advances in the fine-grained recognition literature to improve our model in this aspect. With the proposed approach, we obtain state-of-the-art performance on Kinetics-400 and Something-Something-V1, the two major large-scale action recognition benchmarks.
PB  - arXiv
PY  - 2019
ST  - Action recognition with spatial-temporal discriminative filter banks
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/iccv.2019.00558
ER  -


TY  - GEN
AU  - Sinha, K.
AU  - Viswanathan, A.
AU  - Bunn, J.
TI  - Tracking temporal evolution of network activity for botnet detection
AB  - Botnets are becoming increasingly prevalent as the primary enabling technology in a variety of malicious campaigns such as email spam, click fraud, distributed denial-of-service (DDoS) attacks, and cryptocurrency mining. Botnet technology has continued to evolve rapidly making detection a very challenging problem. There is a fundamental need for robust detection methods that are insensitive to characteristics of a specific botnet and are generalizable across different botnet types. We propose a novel supervised approach to detect malicious botnet hosts by tracking a host's network activity over time using a Long Short-Term Memory (LSTM) based neural network architecture. We build a prototype to demonstrate the feasibility of our approach, evaluate it on the CTU-13 dataset, and compare our performance against existing detection methods. We show that our approach results in a more generalizable, botnet-agnostic detection methodology, is amenable to real-time implementation, and performs well compared to existing approaches, with an overall accuracy score of 96.2%.
PB  - arXiv
PY  - 2019
ST  - Tracking temporal evolution of network activity for botnet detection
Y2  - 2025/05/05/21:54:30
DO  - 10.12677/csa.2022.124108
ER  -


TY  - GEN
AU  - Gao, J.
AU  - Shi, Z.
AU  - Li, J.
AU  - Li, J.
AU  - Zhou, X.
TI  - Relation-aware pyramid network (RapNet) for temporal action proposal: Submission to activitynet challenge 2019
AB  - In this technical report, we describe our solution to temporal action proposal (task 1) in ActivityNet Challenge 2019. First, we fine-tune a ResNet-50-C3D CNN on ActivityNet v1.3 based on Kinetics pretrained model to extract snippet-level video representations and then we design a Relation-Aware Pyramid Network (RapNet) to generate temporal multiscale proposals with confidence score. After that, we employ a two-stage snippet-level boundary adjustment scheme to re-rank the order of generated proposals. Ensemble methods are also been used to improve the performance of our solution, which helps us achieve 2nd place.
PB  - arXiv
PY  - 2019
ST  - Relation-aware pyramid network (RapNet) for temporal action proposal
Y2  - 2025/05/05/21:54:30
DO  - 10.1609/aaai.v34i07.6711
ER  -


TY  - GEN
AU  - Jiang, B.
AU  - Wang, M.
AU  - Gan, W.
AU  - Wu, W.
AU  - Yan, J.
TI  - STM: SpatioTemporal and motion encoding for action recognition
AB  - Spatiotemporal and motion features are two complementary and crucial information for video action recognition. Recent state-of-the-art methods adopt a 3D CNN stream to learn spatiotemporal features and another flow stream to learn motion features. In this work, we aim to efficiently encode these two features in a unified 2D framework. To this end, we first propose an STM block, which contains a Channel-wise SpatioTemporal Module (CSTM) to present the spatiotemporal features and a Channel-wise Motion Module (CMM) to efficiently encode motion features. We then replace original residual blocks in the ResNet architecture with STM blcoks to form a simple yet effective STM network by introducing very limited extra computation cost. Extensive experiments demonstrate that the proposed STM network outperforms the state-of-the-art methods on both temporal-related datasets (i.e., Something-Something v1 & v2 and Jester) and scene-related datasets (i.e., Kinetics-400, UCF-101, and HMDB-51) with the help of encoding spatiotemporal and motion features together.
PB  - arXiv
PY  - 2019
ST  - STM
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/iccv.2019.00209
ER  -


TY  - GEN
AU  - Zhang, C.
AU  - Xu, Y.
AU  - Cheng, Z.
AU  - Wu, F.
AU  - Zou, F.
TI  - Adversarial seeded sequence growing for weakly-supervised temporal action localization
AB  - Temporal action localization is an important yet challenging research topic due to its various applications. Since the frame-level or segment-level annotations of untrimmed videos require amounts of labor expenditure, studies on the weakly-supervised action detection have been springing up. However, most of existing frameworks rely on Class Activation Sequence (CAS) to localize actions by minimizing the video-level classification loss, which exploits the most discriminative parts of actions but ignores the minor regions. In this paper, we propose a novel weakly-supervised framework by adversarial learning of two modules for eliminating such demerits. Specifically, the first module is designed as a well-designed Seeded Sequence Growing (SSG) Network for progressively extending seed regions (namely the highly reliable regions initialized by a CAS-based framework) to their expected boundaries. The second module is a specific classifier for mining trivial or incomplete action regions, which is trained on the shared features after erasing the seeded regions activated by SSG. In this way, a whole network composed of these two modules can be trained in an adversarial manner. The goal of the adversary is to mine features that are difficult for the action classifier. That is, erasion from SSG will force the classifier to discover minor or even new action regions on the input feature sequence, and the classifier will drive the seeds to grow, alternately. At last, we could obtain the action locations and categories from the well-trained SSG and the classifier. Extensive experiments on two public benchmarks THUMOS'14 and ActivityNet1.3 demonstrate the impressive performance of our proposed method compared with the state-of-the-arts.
PB  - arXiv
PY  - 2019
ST  - Adversarial seeded sequence growing for weakly-supervised temporal action localization
Y2  - 2025/05/05/21:54:30
DO  - 10.1145/3343031.3351044
ER  -


TY  - GEN
AU  - Hiley, L.
AU  - Taylor, H.
AU  - Preece, A.
AU  - Hicks, Y.
AU  - Marshall, D.
TI  - Discriminating spatial and temporal relevance in deep taylor decompositions for explainable activity recognition
AB  - Current techniques for explainable AI have been applied with some success to image processing. The recent rise of research in video processing has called for similar work in deconstructing and explaining spatio-temporal models. While many techniques are designed for 2D convolutional models, others are inherently applicable to any input domain. One such body of work, deep Taylor decomposition, propagates relevance from the model output distributively onto its input and thus is not restricted to image processing models. However, by exploiting a simple technique that removes motion information, we show that it is not the case that this technique is effective as-is for representing relevance in non-image tasks. We instead propose a discriminative method that produces a naïve representation of both the spatial and temporal relevance of a frame as two separate objects. This new discriminative relevance model exposes relevance in the frame attributed to motion, that was previously ambiguous in the original explanation. We observe the effectiveness of this technique on a range of samples from the UCF-101 action recognition dataset, two of which are demonstrated in this paper.
PB  - arXiv
PY  - 2019
ST  - Discriminating spatial and temporal relevance in deep taylor decompositions for explainable activity recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/jsen.2023.3335449
ER  -


TY  - GEN
AU  - Ding, J.
AU  - Chen, K.
AU  - Liu, H.
AU  - Han, Z.
AU  - Lambon Ralph, M.A.
TI  - A unified neurocognitive model of the anterior temporal lobe contributions to semantics, language, social behaviour & face recognition
AB  - The anterior temporal lobes (ATL) have become a key brain region of interest in cognitive and clinical neuroscience. Contemporary explorations are founded upon neuropsychological investigations of semantic dementia (SD) that describe the patients’ selective semantic impairment and the variations in their language, behavioural and face recognition abilities. The purpose of this investigation was to generate a single unified model which captures the known cognitive-behavioural variations in SD, and integrates with the considerable database on healthy semantic function and other patient groups. A new analytical approach was able to capture the graded neuropsychological differences and map these to the patients’ distribution of frontotemporal atrophy. Multiple regression and principal component analyses confirmed that the degree of generalised semantic impairment was related to the patients’ total, bilateral ATL atrophy. Verbal production and word-finding abilities were related to total ATL atrophy as well as to the balance of left>right ATL atrophy. Behavioural apathy was found to relate positively to the degree of orbitofrontal atrophy and negatively to total temporal volumes. Disinhibited behaviour was related to right ATL and orbitofrontal atrophy and face recognition to right ATL volumes. Rather than positing mutually-exclusive sub-categories, the data-driven model repositions semantics, language, social behaviour and face recognition into a continuous frontotemporal neurocognitive space.
PB  - bioRxiv
PY  - 2019
ST  - A unified neurocognitive model of the anterior temporal lobe contributions to semantics, language, social behaviour & face recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/725515
ER  -


TY  - GEN
AU  - Gong, G.
AU  - Zheng, L.
AU  - Bai, K.
AU  - Mu, Y.
TI  - Scale matters: Temporal scale aggregation network for precise action localization in untrimmed videos
AB  - Temporal action localization is a recently-emerging task, aiming to localize video segments from untrimmed videos that contain specific actions. Despite the remarkable recent progress, most two-stage action localization methods still suffer from imprecise temporal boundaries of action proposals. This work proposes a novel integrated temporal scale aggregation network (TSA-Net). Our main insight is that ensembling convolution filters with different dilation rates can effectively enlarge the receptive field with low computational cost, which inspires us to devise multi-dilation temporal convolution (MDC) block. Furthermore, to tackle video action instances with different durations, TSA-Net consists of multiple branches of sub-networks. Each of them adopts stacked MDC blocks with different dilation parameters, accomplishing a temporal receptive field specially optimized for specific-duration actions. We follow the formulation of boundary point detection, novelly detecting three kinds of critical points (i.e., starting / mid-point / ending) and pairing them for proposal generation. Comprehensive evaluations are conducted on two challenging video benchmarks, THUMOS14 and ActivityNet-1.3. Our proposed TSA-Net demonstrates clear and consistent better performances and re-calibrates new state-of-the-art on both benchmarks. For example, our new record on THUMOS14 is 46.9% while the previous best is 42.8% under mAP@0.5.
PB  - arXiv
PY  - 2019
ST  - Scale matters
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icme46284.2020.9102850
ER  -


TY  - GEN
AU  - Sultan, K.
AU  - Ali, H.
AU  - Ahmad, A.
AU  - Anwaar, H.
AU  - Zhang, Z.
TI  - Understanding and partitioning mobile traffic using internet activity records data - A spatiotemporal approach
AB  - —The internet activity records (IARs) of a mobile cellular network posses significant information which can be exploited to identify the network’s efficacy and the mobile users’ behavior. In this work, we extract useful information from the IAR data and identify a healthy predictability of spatio-temporal pattern within the network traffic. The information extracted is helpful for network operators to plan effective network configuration and perform management and optimization of network’s resources. We report experimentation on spatiotemporal analysis of IAR data of the Telecom Italia. Based on this, we present mobile traffic partitioning scheme. Experimental results of the proposed model is helpful in modelling and partitioning of network traffic patterns.
PB  - arXiv
PY  - 2019
ST  - Understanding and partitioning mobile traffic using internet activity records data - A spatiotemporal approach
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/wocc.2019.8770653
ER  -


TY  - GEN
AU  - Lempert, K.M.
AU  - Mechanic-Hamilton, D.J.
AU  - Xie, L.
AU  - Wolk, D.A.
AU  - Kable, J.W.
TI  - Neural and behavioral correlates of declarative memory are associated with temporal discounting in older adults
AB  - When facing intertemporal choices, or decisions involving trade-offs between smaller, sooner and larger, delayed rewards, people tend to discount the value of future rewards. There are substantial individual differences in this tendency toward temporal discounting, however. The neurocognitive mechanisms underlying these individual differences remain unclear. One possibility is that individual differences in declarative memory ability underlie individual differences in temporal discounting. Here we tested this hypothesis in older adults (both cognitively normal and with mild cognitive impairment), given that declarative memory declines with age, at rates that vary across individuals. We related temporal discounting to performance on neuropsychological measures of declarative memory, as well as to structural measures of medial temporal lobe regions that are critical for memory. Performance on episodic and semantic memory retrieval tasks was associated with temporal discounting, such that people with better memory discounted delayed rewards less. This relationship held even when controlling for executive function abilities that might influence memory retrieval. Underlining the specificity of this association, performance on measures of executive function (Trail Making Test, Digit Span Backwards, lexical fluency) was unrelated to temporal discounting, and declarative memory ability was unrelated to risk preferences. Finally, cortical thickness in the entorhinal cortex was associated with reduced temporal discounting, with episodic memory performance partially mediating this association. These findings suggest that individual differences in temporal discounting are primarily driven by memory ability, and not executive function, and that a decline in medial temporal lobe structural integrity with aging may impact time preferences.
PB  - bioRxiv
PY  - 2019
ST  - Neural and behavioral correlates of declarative memory are associated with temporal discounting in older adults
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/720250
ER  -


TY  - GEN
AU  - Ma, C.
AU  - Choi, J.
AU  - Lee, B.
AU  - Yang, S.
TI  - Submission to activitynet challenge 2019: Task B spatio-temporal action localization
AB  - This technical report present an overview of our system proposed for the spatio-temporal action localization(SAL) task in ActivityNet Challenge 2019. Unlike previous two-streams-based works, we focus on exploring the end-to-end trainable architecture using only RGB sequential images. To this end, we employ a previously proposed simple yet effective two-branches network called SlowFast Networks which is capable of capturing both short- and long-term spatiotemporal features. Moreover, to handle the severe class imbalance and overfitting problems, we propose a correlation-preserving data augmentation method and a random label subsampling method which have been proven to be able to reduce overfitting and improve the performance.
PB  - arXiv
PY  - 2019
ST  - Submission to activitynet challenge 2019
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/978-981-15-3250-4_171
ER  -


TY  - GEN
AU  - Lin, T.
AU  - Liu, X.
AU  - Li, X.
AU  - Ding, E.
AU  - Wen, S.
TI  - BMN: Boundary-matching network for temporal action proposal generation
AB  - Temporal action proposal generation is an challenging and promising task which aims to locate temporal regions in real-world videos where action or event may occur. Current bottom-up proposal generation methods can generate proposals with precise boundary, but cannot efficiently generate adequately reliable confidence scores for retrieving proposals. To address these difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed proposals, which denote a proposal as a matching pair of starting and ending boundaries and combine all densely distributed BM pairs into the BM confidence map. Based on BM mechanism, we propose an effective, efficient and end-to-end proposal generation method, named Boundary-Matching Network (BMN), which generates proposals with precise temporal boundaries as well as reliable confidence scores simultaneously. The two-branches of BMN are jointly trained in an unified framework. We conduct experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where BMN shows significant performance improvement with remarkable efficiency and generalizability. Further, combining with existing action classifier, BMN can achieve state-of-the-art temporal action detection performance.
PB  - arXiv
PY  - 2019
ST  - BMN
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/iccv.2019.00399
ER  -


TY  - GEN
AU  - Lee, W.
AU  - Fu, J.
AU  - Bouwman, N.
AU  - Farago, P.
AU  - Curley, J.P.
TI  - Temporal microstructure of dyadic social behavior during relationship formation in mice
AB  - Understanding the temporal dynamics of how unfamiliar animals establish dominant-subordinate relationships and learn to modify their behavior in response to their social partner in context-appropriate manners is critical in biomedical research concerning social competence. Here we observe and analyze the microstructure of social and non-social behaviors as 21 pairs of outbred CD-1 male mice (Mus Musculus) establish dominant-subordinate relationships during daily 20-minute interaction for five consecutive days. Using Kleinberg burst detection algorithm, we demonstrate aggressive and subordinate interactions occur in bursting patterns followed by quiescence period rather than in uniformly distributed across social interactions. Further, we identify three phases of dominant-subordinate relationship development (pre-, middle-, and post-resolution) by combining phi-coefficient and difference methods used to determine at which bursting event mice resolve dominant-subordinate relationships. Using First Order Markov Chains within individuals we show dominant and subordinate animals establish significantly different behavioral repertoire once they resolve the relationships. In both dominant and subordinate mice, the transitions between investigative and agonistic behavior states are not common. Lastly, we introduce Forward Spike Time Tiling Coefficient, the strength of association between the given behavior of one individual with the target behavior of the other individual within a specified time window. With this method, we describe the likelihood of a mouse responding to a behavior with another behavior differ in pre- and post-resolution phases. The data suggest that subordinate mice learn to exhibit subordinate behavior in response to dominant partner’s behaviors while dominant mice become less likely to show subordinate behaviors in response to their partners’ action. Overall, with the tool we present in this study, the data suggest CD-1 male mice are able to establish dominance relationships and modify their behaviors even to the same social cues under different social contexts competently.
PB  - bioRxiv
PY  - 2019
ST  - Temporal microstructure of dyadic social behavior during relationship formation in mice
Y2  - 2025/05/05/21:54:30
DO  - 10.1371/journal.pone.0220596
ER  -


TY  - GEN
AU  - Bishay, M.
AU  - Zoumpourlis, G.
AU  - Patras, I.
TI  - TARN: Temporal attentive relation network for few-shot and zero-shot action recognition
AB  - In this paper we propose a novel Temporal Attentive Relation Network (TARN) for the problems of few-shot and zero-shot action recognition. At the heart of our network is a meta-learning approach that learns to compare representations of variable temporal length, that is, either two videos of different length (in the case of few-shot action recognition) or a video and a semantic representation such as word vector (in the case of zero-shot action recognition). By contrast to other works in few-shot and zero-shot action recognition, we a) utilise attention mechanisms so as to perform temporal alignment, and b) learn a deep-distance measure on the aligned representations at video segment level. We adopt an episode-based training scheme and train our network in an end-toend manner. The proposed method does not require any fine-tuning in the target domain or maintaining additional representations as is the case of memory networks. Experimental results show that the proposed architecture outperforms the state of the art in few-shot action recognition, and achieves competitive results in zero-shot action recognition.
PB  - arXiv
PY  - 2019
ST  - TARN
Y2  - 2025/05/05/21:54:30
DO  - 10.1016/j.adhoc.2020.102380
ER  -


TY  - GEN
AU  - Antunes, J.
AU  - Abreu, P.
AU  - Bernardino, A.
AU  - Smailagic, A.
AU  - Siewiorek, D.
TI  - Attention filtering for multi-person spatiotemporal action detection on deep two-stream CNN architectures
AB  - Action detection and recognition tasks have been the target of much focus in the computer vision community due to their many applications, namely, security, robotics and recommendation systems. Recently, datasets like AVA, provide multi-person, multi-label, spatiotemporal action detection and recognition challenges. Being unable to discern which portions of the input to use for classification is a limitation of two-stream CNN approaches, once the vision task involves several people with several labels. We address this limitation and improve the state-of-the-art performance of two-stream CNNs. In this paper we present four contributions: our fovea attention filtering that highlights targets for classification without discarding background; a generalized binary loss function designed for the AVA dataset; miniAVA, a partition of AVA that maintains temporal continuity and class distribution with only one tenth of the dataset size; and ablation studies on alternative attention filters. Our method, using fovea attention filtering and our generalized binary loss, achieves a relative video mAP improvement of 20% over the two-stream baseline in AVA, and is competitive with the state-of-the-art in the UCF101-24. We also show a relative video mAP improvement of 12.6% when using our generalized binary loss over the standard sum-of-sigmoids.
PB  - arXiv
PY  - 2019
ST  - Attention filtering for multi-person spatiotemporal action detection on deep two-stream CNN architectures
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/978-3-319-46493-0_45
ER  -


TY  - GEN
AU  - van der Weegen, Y.
AU  - Berman, H.G.
AU  - Mevissen, T.E.T.
AU  - Adar, S.
AU  - Luijsterburg, M.S.
TI  - The sequential and cooperative action of CSB, CSA and UVSSA targets the TFIIH complex to DNA damage-stalled RNA polymerase II
AB  - The response to DNA damage-stalled RNA polymerase II (RNAPIIo) involves the assembly of the transcription-coupled repair (TCR) complex on actively transcribed strands. The function of the TCR proteins CSB, CSA and UVSSA and the manner in which the core DNA repair complex, including transcription factor IIH (TFIIH), is recruited are largely unknown. Here, we define the assembly mechanism of the TCR complex in human isogenic knockout cells. We show that TCR is initiated by RNAPIIo-bound CSB, which recruits CSA through a newly identified CSA-interaction motif (CIM). Once recruited, CSA facilitates the association of UVSSA with stalled RNAPIIo. Importantly, we find that UVSSA is the key factor that recruits the TFIIH complex in a manner that is stimulated by CSB and CSA. Together these findings reveal a sequential and highly cooperative assembly mechanism of TCR proteins and reveal the mechanism for TFIIH recruitment to DNA damage-stalled RNAPIIo to initiate repair.
PB  - bioRxiv
PY  - 2019
ST  - The sequential and cooperative action of CSB, CSA and UVSSA targets the TFIIH complex to DNA damage-stalled RNA polymerase II
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/707216
ER  -


TY  - GEN
AU  - Pierpaoli, P.
AU  - Li, A.
AU  - Srinivasan, M.
AU  - Coogan, S.
AU  - Egerstedt, M.
TI  - A sequential composition framework for coordinating multi-robot behaviors
AB  - A number of coordinated behaviors have been proposed for achieving specific tasks for multi-robot systems. However, since most applications require more than one such behavior, one needs to be able to compose together sequences of behaviors while respecting local information flow constraints. Specifically, when the inter-agent communication depends on inter-robot distances, these constraints translate into particular configurations that must be reached in finite time in order for the system to be able to transition between the behaviors. To this end, we develop a framework based on finite-time convergence control barrier functions that drives the robots to the required configurations. In order to demonstrate the proposed framework, we consider a scenario where a team of eight planar robots explore an urban environment in order to localize and rescue a subject. The results are presented in the form of a case study, which is implemented on a multi-agent robotic test-bed.
PB  - arXiv
PY  - 2019
ST  - A sequential composition framework for coordinating multi-robot behaviors
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/tro.2020.3036628
ER  -


TY  - GEN
AU  - Alfeo, A.L.
AU  - Cimino, M.G.C.A.
AU  - Lepri, B.
AU  - Pentland, A.
AU  - Vaglini, G.
TI  - Assessing refugees' integration via spatio-temporal similarities of mobility and calling behaviors
AB  - — In Turkey the increasing tension, due to the presence of 3.4 million Syrian refugees, demands the formulation of effective integration policies. Moreover, their design requires tools aimed at understanding the integration of refugees despite the complexity of this phenomenon. In this work, we propose a set of metrics aimed at providing insights and assessing the integration of Syrians refugees, by analyzing a real-world Call Details Records (CDRs) dataset including calls from refugees and locals in Turkey throughout 2017. Specifically, we exploit the similarity between refugees’ and locals’ spatial and temporal behaviors, in terms of communication and mobility in order to assess integration dynamics. Together with the already known methods for data analysis, we use a novel computational approach to analyze spatiotemporal patterns: Computational Stigmergy, a bio-inspired scalar and temporal aggregation of samples. Computational Stigmergy associates each sample to a virtual pheromone deposit (mark). Marks in spatiotemporal proximity are aggregated into functional structures called trails, which summarize the spatiotemporal patterns in data and allows computing the similarity between different patterns. According to our results, collective mobility and behavioral similarity with locals have great potential as measures of integration, since they are: (i) correlated with the amount of interaction with locals; (ii) an effective proxy for refugee's economic capacity, thus refugee's potential employment; and (iii) able to capture events that may disrupt the integration phenomena, such as social tensions.
PB  - arXiv
PY  - 2019
ST  - Assessing refugees' integration via spatio-temporal similarities of mobility and calling behaviors
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/tcss.2019.2923216
ER  -


TY  - GEN
AU  - Duncan, N.W.
AU  - Hsu, T.-Y.
AU  - Cheng, P.Z.
AU  - Lee, H.-C.
AU  - Lane, T.J.
TI  - Intrinsic activity temporal structure reactivity to behavioural state change is correlated with depressive rumination
AB  - Patients with major depressive disorder (MDD) tend to focus their thoughts on their personal problems and negative self-reflection. This negative self-focus has a major impact on patient quality of life and is associated with a range of cognitive deficits. Self-related thoughts, such as those seen in rumination, have been associated with intrinsic activity within cortical midline structures. In normal conditions this intrinsic activity is responsive to behavioural state, changing as an individual switches from an internally to externally oriented context. It was hypothesised that this responsiveness would be blunted in patients with MDD (n = 26; control n = 37) and that the degree to which an individual was fixed in a particular intrinsic state would be correlated with reported ruminatory behaviours. This was tested by measuring intrinsic EEG activity temporal structure, quantified with detrended fluctuation analysis (DFA), in eyes-closed and eyes-open task-free states and contrasting between the conditions. The difference between the internally oriented eyes-closed and externally oriented eyes-open states was then correlated with ruminatory behaviour, measured with the Rumination Response Scale. Healthy controls showed a robust beta band DFA change between states, moving from a critical to sub-critical activity state with the opening of the eyes. This change was not seen in MDD patients, with the eyes-closed DFA remaining similar to that in eyes-open. A negative correlation was seen between the DFA difference and rumination scores over the midline electrodes. These results identify a reduced reactivity of intrinsic activity properties in MDD that is related to greater ruminative symptoms.
PB  - bioRxiv
PY  - 2019
ST  - Intrinsic activity temporal structure reactivity to behavioural state change is correlated with depressive rumination
Y2  - 2025/05/05/21:54:30
DO  - 10.1111/ejn.14858
ER  -


TY  - GEN
AU  - Jana, S.
AU  - Hannah, R.
AU  - Muralidharan, V.
AU  - Aron, A.R.
TI  - Temporal cascade of frontal, motor and muscle processes underlying human action-stopping
AB  - Action-stopping is a canonical executive function thought to involve top-down control over the motor system. Here we aimed to validate this stopping system using high temporal resolution methods in humans. We show that, following the requirement to stop, there was an increase of right frontal beta (∼13 to 30 Hz) at ∼120 ms, likely a proxy of right inferior frontal gyrus; then, at 140 ms, there was a broad skeletomotor suppression, likely reflecting the impact of the subthalamic nucleus on basal ganglia output; then, at ∼160 ms, suppression was detected in the muscle, and, finally, the behavioral time of stopping was ∼220 ms. This temporal cascade confirms a detailed model of action-stopping, and partitions it into subprocesses that are isolable to different nodes and are more precise than the behavioral speed of stopping. Variation in these subprocesses, including at the single-trial level, could better explain individual differences in impulse control.
PB  - bioRxiv
PY  - 2019
ST  - Temporal cascade of frontal, motor and muscle processes underlying human action-stopping
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/700088
ER  -


TY  - GEN
AU  - Li, Y.
AU  - Huang, X.
AU  - Zhao, G.
TI  - Micro-expression action unit detection with spatio-temporal adaptive pooling
AB  - Action Unit (AU) detection plays an important role for facial expression recognition. To the best of our knowledge, there is little research about AU analysis for micro-expressions. In this paper, we focus on AU detection in micro-expressions. Micro-expression AU detection is challenging due to the small quantity of micro-expression databases, low intensity, short duration of facial muscle change, and class imbalance. In order to alleviate the problems, we propose a novel Spatio-Temporal Adaptive Pooling (STAP) network for AU detection in micro-expressions. Firstly, STAP is aggregated by a series of convolutional filters of different sizes. In this way, STAP can obtain multi-scale information on spatial and temporal domains. On the other hand, STAP contains less parameters, thus it has less computational cost and is suitable for micro-expression AU detection on very small databases. Furthermore, STAP module is designed to pool discriminative information for micro-expression AUs on spatial and temporal domains.Finally, Focal loss is employed to prevent the vast number of negatives from overwhelming the micro-expression AU detector. In experiments, we firstly polish the AU annotations on three commonly used databases. We conduct intensive experiments on three micro-expression databases, and provide several baseline results on micro-expression AU detection. The results show that our proposed approach outperforms the basic Inflated inception-v1 (I3D) in terms of an average of F1-score. We also evaluate the performance of our proposed method on cross-database protocol. It demonstrates that our proposed approach is feasible for cross-database micro-expression AU detection. Importantly, the results on three micro-expression databases and cross-database protocol provide extensive baseline results for future research on micro-expression AU detection. MSC Codes 68T10
PB  - arXiv
PY  - 2019
ST  - Micro-expression action unit detection with spatio-temporal adaptive pooling
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icassp48485.2024.10446702
ER  -


TY  - GEN
AU  - Nystrom, S.L.
AU  - Niederhuber, M.J.
AU  - McKay, D.J.
TI  - The temporal transcription factor E93 controls dynamic enhancer activity and chromatin accessibility during development
AB  - How temporal cues combine with spatial inputs to control gene expression during development is poorly understood. Here, we test the hypothesis that the Drosophila transcription factor E93 controls temporal gene expression by regulating chromatin accessibility. Precocious expression of E93 early in wing development reveals that it can simultaneously activate and deactivate different target enhancers. Notably, the precocious patterns of enhancer activity resemble the wild-type patterns that occur later in development, suggesting that provision of E93 alters the competence of enhancers to respond to spatial cues. Genomic profiling reveals that precocious E93 expression is sufficient to regulate chromatin accessibility at a subset of its targets. These accessibility changes mimic those that normally occur later in development, indicating that precocious E93 accelerates the wild-type developmental program. Further, we find that target enhancers that do not respond to precocious E93 in early wings become responsive after a developmental transition, suggesting that parallel temporal pathways work alongside E93. These findings support a model wherein E93 expression functions as an instructive cue that defines a broad window of developmental time through control of chromatin accessibility.
PB  - bioRxiv
PY  - 2019
ST  - The temporal transcription factor E93 controls dynamic enhancer activity and chromatin accessibility during development
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/679753
ER  -


TY  - GEN
AU  - Moro, A.
AU  - Garbinato, B.
AU  - Chavez-Demoulin, V.
TI  - Analyzing privacy-aware mobility behavior using the evolution of spatio-temporal entropy
AB  - Analyzing mobility behavior of users is extremely useful to create or im- prove existing services. Several research works have been done in order to study mobility behavior of users that mainly use users' significant locations. However, these existing analysis are extremely intrusive because they require the knowledge of the frequently visited places of users, which thus makes it fairly easy to identify them. Consequently, in this paper, we present a privacy-aware methodology to analyze mobility behavior of users. We firstly propose a new metric based on the well-known Shannon entropy, called spatio-temporal entropy, to quantify the mobility level of a user during a time window. Then, we compute a sequence of spatio-temporal entropy from the location history of the user that expresses user's movements as rhythms. We secondly present how to study the effects of several groups of additional variables on the evolution of the spatio-temporal entropy of a user, such as spatio-temporal, demographic and mean of transportation variables. For this, we use Generalized Additive Models (GAMs). The main strength of GAMs is that they are not only interesting to predict a response variable, but also to understand the effects of co-variables on this response variable. The results firstly show that the spatio-temporal entropy and GAMs are an ideal com- bination to understand mobility behavior of an individual user or a group of users. We also evaluate the prediction accuracy of a global GAM compared to individual GAMs and individual AutoRegressive Integrated Moving Average (ARIMA) models. These last results highlighted that the global GAM gives more accurate predictions of spatio-temporal entropy by checking the Mean Absolute Error (MAE). In addition, this research work opens various threads, such as the prediction of demographic data of users or the creation of personalized mobility prediction models by using movement rhythm characteristics of a user.
PB  - arXiv
PY  - 2019
ST  - Analyzing privacy-aware mobility behavior using the evolution of spatio-temporal entropy
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/978-981-99-3925-1_8
ER  -


TY  - GEN
AU  - Cho, S.
AU  - Foroosh, H.
TI  - A temporal sequence learning for action recognition and prediction
AB  - In this work1, we present a method to represent a video with a sequence of words, and learn the temporal sequencing of such words as the key information for predicting and recognizing human actions. We leverage core concepts from the Natural Language Processing (NLP) literature used in sentence classification to solve the problems of action prediction and action recognition. Each frame is converted into a word that is represented as a vector using the Bag of Visual Words (BoW) encoding method. The words are then combined into a sentence to represent the video, as a sentence. The sequence of words in different actions are learned with a simple but effective Temporal Convolutional Neural Network (T-CNN) that captures the temporal sequencing of information in a video sentence. We demonstrate that a key characteristic of the proposed method is its low-latency, i.e. its ability to predict an action accurately with a partial sequence (sentence). Experiments on two datasets, UCF101 and HMDB51 show that the method on average reaches 95% of its accuracy within half the video frames. Results, also demonstrate that our method achieves compatible state-of-the-art performance in action recognition (i.e. at the completion of the sentence) in addition to action prediction.
PB  - arXiv
PY  - 2019
ST  - A temporal sequence learning for action recognition and prediction
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/wacv.2018.00045
ER  -


TY  - GEN
AU  - Hu, B.
AU  - Ahmed, U.
TI  - Characterizing the Exact Behaviors of Temporal Difference Learning Algorithms Using Markov Jump Linear System Theory
AB  - In this paper, we provide a unified analysis of temporal difference learning algorithms with linear function approximators by exploiting their connections to Markov jump linear systems (MJLS). We tailor the MJLS theory developed in the control community to characterize the exact behaviors of the first and second order moments of a large family of temporal difference learning algorithms. For both the IID and Markov noise cases, we show that the evolution of some augmented versions of the mean and covariance matrix of TD learning exactly follows the trajectory of a deterministic linear time-invariant (LTI) dynamical system. Applying the well-known LTI system theory, we obtain closed-form expressions for the mean and covariance matrix of TD learning at any time step. We provide a tight matrix spectral radius condition to guarantee the convergence of the covariance matrix of TD learning, and perform a perturbation analysis to characterize the dependence of the TD behaviors on learning rate. For the IID case, we provide an exact formula characterizing how the mean and covariance matrix of TD learning converge to the steady state values at a linear rate. For the Markov case, we use our formulas to explain how the behaviors of TD learning algorithms are affected by learning rate and various properties of the underlying Markov chain.
PB  - arXiv
PY  - 2019
ST  - Characterizing the Exact Behaviors of Temporal Difference Learning Algorithms Using Markov Jump Linear System Theory
Y2  - 2025/05/05/21:54:30
DO  - 10.1002/rnc.5648
ER  -


TY  - GEN
AU  - Cho, S.
AU  - Foroosh, H.
TI  - Spatio-Temporal Fusion Networks for Action Recognition?
AB  - The video based CNN works have focused on effective ways to fuse appearance and motion networks, but they typically lack utilizing temporal information over video frames. In this work, we present a novel spatio-temporal fusion network (STFN) that integrates temporal dynamics of appearance and motion information from entire videos. The captured temporal dynamic information is then aggregated for a better video level representation and learned via end-to-end training. The spatio-temporal fusion network consists of two set of Residual Inception blocks that extract temporal dynamics and a fusion connection for appearance and motion features. The benefits of STFN are: (a) it captures local and global temporal dynamics of complementary data to learn video-wide information; and (b) it is applicable to any network for video classification to boost performance. We explore a variety of design choices for STFN and verify how the network performance is varied with the ablation studies. We perform experiments on two challenging human activity datasets, UCF101 and HMDB51, and achieve the state-of-the-art results with the best network.
PB  - arXiv
PY  - 2019
ST  - Spatio-Temporal Fusion Networks for Action Recognition?
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/978-3-030-20887-5_22
ER  -


TY  - GEN
AU  - Wang, T.
AU  - Lei, S.
AU  - Jiang, Y.
AU  - Snoussi, H.
AU  - Shan, G.
TI  - Accelerating temporal action proposal generation via high performance computing
AB  - Temporal action recognition always depends on temporal action proposal generation to hypothesize actions and algorithms usually need to process very long video sequences and output the starting and ending times of each potential action in each video suffering from high computation cost. To address this, based on boundary sensitive network we propose a new temporal convolution network called Multipath Temporal ConvNet (MTN), which consists of two parts i.e. Multipath DenseNet and SE-ConvNet. In this work, one novel high performance ring parallel architecture based on Message Passing Interface (MPI) is further introduced into temporal action proposal generation, which is a reliable communication protocol, in order to respond to the requirements of large memory occupation and a large number of videos. Remarkably, the total data transmission is reduced by adding a connection between multiple computing load in the newly developed architecture. It is found that, compared to the traditional Parameter Server architecture, our parallel architecture has higher efficiency on temporal action detection task with multiple GPUs, which is suitable for dealing with the tasks of temporal action proposal generation, especially for large datasets of millions of videos. We conduct experiments on ActivityNet-1.3 and THUMOS14, where our method outperforms other state-of-art temporal action detection methods with high recall and high temporal precision. In addition, a time metric is further proposed here to evaluate the speed performance in the distributed training process.
PB  - arXiv
PY  - 2019
ST  - Accelerating temporal action proposal generation via high performance computing
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/s11704-021-0173-7
ER  -


TY  - GEN
AU  - Qiu, Z.
AU  - Li, D.
AU  - Li, Y.
AU  - Pan, Y.
AU  - Yao, T.
TI  - Trimmed action recognition, dense-captioning events in videos, and spatio-temporal action localization with focus on activitynet challenge 2019
AB  - This notebook paper presents an overview and comparative analysis of our systems designed for the following three tasks in ActivityNet Challenge 2019: trimmed action recognition, dense-captioning events in videos, and spatiotemporal action localization. Trimmed Action Recognition (Kinetics): We investigate and exploit multiple spatio-temporal clues for trimmed action recognition task, i.e., frame, short video clip and motion (optical flow) by leveraging 2D or 3D convolutional neural networks (CNNs). The mechanism of different quantization methods is studied as well. All activities are finally classified by late fusing the predictions from each clue. Dense-Captioning Events in Videos (ActivityNet Captions): For this task, we firstly adopt a standard "detection by classification" framework to localize temporal proposals of interest in video, and then generate the descriptions for each proposal. Specifically, a two-layer LSTM-based captioning architecture with temporal attention mechanism is leveraged to generate sentence conditioning on the input video representation and its detected attributes. Moreover, the captioning architecture is equipped with policy gradient optimization scheme to further boost video captioning. Spatio-temporal Action Localization (AVA): We present a new Long Short-Term Relation Networks (LSTR), which models both short-term and long-term human-context relation to augment features for spatio-temporal action localization. Technically, Region Proposal Network (RPN) is employed to first generate bounding box proposals on the keyframe of each video clip. LSTR then models short-term human-context interactions within each clip through spatiotemporal attention mechanism and reasons long-term temporal dynamics across video clips via Graph Convolutional Networks (GCN) in a cascaded manner. The upgraded relation-aware feature of each proposal is finally employed for classifying actions.
PB  - arXiv
PY  - 2019
ST  - Trimmed action recognition, dense-captioning events in videos, and spatio-temporal action localization with focus on activitynet challenge 2019
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/dicta52665.2021.9647106
ER  -


TY  - GEN
AU  - Maldonado, J.
AU  - Phillips, D.F.
AU  - Dumusque, X.
AU  - Udry, S.
AU  - Watson, C.
TI  - Temporal evolution and correlations of optical activity indicators measured in Sun-as-a-star observations
AB  - Context. Understanding stellar activity in solar-type stars is crucial for the physics of stellar atmospheres as well as for ongoing exoplanet programmes. Aims. We aim to test how well we understand stellar activity using our own star, the Sun, as a test case. Methods. We perform a detailed study of the main optical activity indicators (Ca ii H & K, Balmer lines, Na i D1 D2, and He i D3) measured for the Sun using the data provided by the HARPS-N solar-telescope feed at the Telescopio Nazionale Galileo. We make use of periodogram analyses to study solar rotation, and we use the pool variance technique to study the temporal evolution of active regions. The correlations between the Different activity indicators as well as the correlations between activity indexes and the derived parameters from the cross-correlation technique are analysed. We also study the temporal evolution of these correlations and their possible relationship with indicators of inhomogeneities in the solar photosphere like sunspot number or radio flux values. Results. The value of the solar rotation period is found in all the activity indicators, with the only exception being H. The derived values vary from 26.29 days (H line) to 31.23 days (He i). From an analysis of sliding periodograms we find that in most of the activity indicators the spectral power is split into several "bands" of periods around 26 and 30 days, that might be explained by the migration of active regions between the equator and a latitude of 30, spot evolution or a combination of both eFFects. In agreement with previous works a typical lifetime of active regions of ten rotation periods is inferred from the pooled variance diagrams.We find that Hβ, Hγ, H, Hϵ, and He i show a significant correlation with the S index. Significant correlations between the contrast, bisector span, and the heliocentric radial velocity with the activity indexes are also found. We show that the full width at half maximum, the bisector, and the disc-integrated magnetic field correlate with the radial velocity variations. The correlation of the S index and H changes with time, ncreasing with larger sun spot numbers and solar irradiance. A similar tendency with the S index-radial velocity correlation is also present in the data. Conclusions. Our results are consistent with a scenario in which higher activity favours the correlation between the S index and the H activity indicators and between the S index and radial velocity variations.
PB  - arXiv
PY  - 2019
ST  - Temporal evolution and correlations of optical activity indicators measured in Sun-as-a-star observations
Y2  - 2025/05/05/21:54:30
DO  - 10.1051/0004-6361/201935233
ER  -


TY  - GEN
AU  - Yurtsever, E.
AU  - Liu, Y.
AU  - Lambert, J.
AU  - Takeda, K.
AU  - Hansen, J.H.L.
TI  - Risky action recognition in lane change video clips using deep spatiotemporal networks with segmentation mask transfer
AB  - Advanced driver assistance and automated driving systems rely on risk estimation modules to predict and avoid dangerous situations. Current methods use expensive sensor setups and complex processing pipeline, limiting their availability and robustness. To address these issues, we introduce a novel deep learning based action recognition framework for classifying dangerous lane change behavior in short video clips captured by a monocular camera. We designed a deep spatiotemporal classification network that uses pre-trained state-of-the-art instance segmentation network Mask R-CNN as its spatial feature extractor for this task. The Long-Short Term Memory (LSTM) and shallower final classification layers of the proposed method were trained on a semi-naturalistic lane change dataset with annotated risk labels. A comprehensive comparison of state-of-the-art feature extractors was carried out to find the best network layout and training strategy. The best result, with a 0.937 AUC score, was obtained with the proposed network. Our code and trained models are available open-source.
PB  - arXiv
PY  - 2019
ST  - Risky action recognition in lane change video clips using deep spatiotemporal networks with segmentation mask transfer
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/itsc.2019.8917362
ER  -


TY  - GEN
AU  - Xu, H.
AU  - Das, A.
AU  - Saenko, K.
TI  - Two-stream region convolutional 3D network for temporal activity detection
AB  - We address the problem of temporal activity detection in continuous, untrimmed video streams. This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity. We introduce a new model, Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities and finally classifies selected regions into specific activities. Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines. We further improve the detection performance by efficiently integrating an optical flow based motion stream with the original RGB stream. The twostream network is jointly optimized by fusing the flow and RGB feature maps at different levels. Additionally, the training stage incorporates an online hard example mining strategy to address the extreme foreground-background imbalance typically observed in any detection pipeline. Instead of heuristically sampling the candidate segments for the final activity classification stage, we rank them according to their performance and only select the worst performers to update the model. This improves the model without heavy hyper-parameter tuning. Extensive experiments on three benchmark datasets are carried out to show superior performance over existing temporal activity detection methods. Our model achieves state-of-the-art results on the THUMOS'14 and Charades datasets. We further demonstrate that our model is a general temporal activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on the ActivityNet dataset.
PB  - arXiv
PY  - 2019
ST  - Two-stream region convolutional 3D network for temporal activity detection
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/tpami.2019.2921539
ER  -


TY  - GEN
AU  - Kuehne, H.
AU  - Richard, A.
AU  - Gall, J.
TI  - A Hybrid RNN-HMM Approach for Weakly Supervised Temporal Action Segmentation
AB  - Action recognition has become a rapidly developing research field within the last decade. But with the increasing demand for large scale data, the need of hand annotated data for the training becomes more and more impractical. One way to avoid frame-based human annotation is the use of action order information to learn the respective action classes. In this context, we propose a hierarchical approach to address the problem of weakly supervised learning of human actions from ordered action labels by structuring recognition in a coarse-to-fine manner. Given a set of videos and an ordered list of the occurring actions, the task is to infer start and end frames of the related action classes within the video and to train the respective action classifiers without any need for hand labeled frame boundaries. We address this problem by combining a framewise RNN model with a coarse probabilistic inference. This combination allows for the temporal alignment of long sequences and thus, for an iterative training of both elements. While this system alone already generates good results, we show that the performance can be further improved by approximating the number of subactions to the characteristics of the different action classes as well as by the introduction of a regularizing length prior. The proposed system is evaluated on two benchmark datasets, the Breakfast and the Hollywood extended dataset, showing a competitive performance on various weak learning tasks such as temporal action segmentation and action alignment.
PB  - arXiv
PY  - 2019
ST  - A Hybrid RNN-HMM Approach for Weakly Supervised Temporal Action Segmentation
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/tpami.2018.2884469
ER  -


TY  - GEN
AU  - Song, L.
AU  - Zhang, S.
AU  - Yu, G.
AU  - Sun, H.
TI  - TACNet: Transition-aware context network for spatio-temporal action detection
AB  - Current state-of-the-art approaches for spatio-temporal action detection have achieved impressive results but remain unsatisfactory for temporal extent detection. The main reason comes from that, there are some ambiguous states similar to the real actions which may be treated as target actions even by a well-trained network. In this paper, we define these ambiguous samples as "transitional states", and propose a Transition-Aware Context Network (TACNet) to distinguish transitional states. The proposed TACNet includes two main components, i.e., temporal context detector and transition-aware classifier. The temporal context detector can extract long-term context information with constant time complexity by constructing a recurrent network. The transition-aware classifier can further distinguish transitional states by classifying action and transitional states simultaneously. Therefore, the proposed TACNet can substantially improve the performance of spatio-temporal action detection. We extensively evaluate the proposed TACNet on UCF101-24 and J-HMDB datasets. The experimental results demonstrate that TACNet obtains competitive performance on JHMDB and significantly outperforms the state-of-the-art methods on the untrimmed UCF101-24 in terms of both frame-mAP and video-mAP. MSC Codes 68T06
PB  - arXiv
PY  - 2019
ST  - TACNet
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvpr.2019.01226
ER  -


TY  - GEN
AU  - Brodskyi, Y.
AU  - Hante, F.M.
AU  - Seidel, A.
TI  - STABILIZATION OF PARTIAL DIFFERENTIAL EQUATIONS BY SEQUENTIAL ACTION CONTROL
AB  - We present a framework of sequential action control (SAC) for stabilization of systems of partial differential equations which can be posed as abstract semilinear control problems in Hilbert spaces. We follow a late-lumping approach and show that the control action can be explicitly obtained from variational principles using adjoint information. Moreover, we analyze the closed-loop system obtained from the SAC feedback for the linear problem with quadratic stage costs. We apply this theory to a prototypical example of an unstable heat equation and provide numerical results as the verification and demonstration of the framework.
PB  - arXiv
PY  - 2019
ST  - STABILIZATION OF PARTIAL DIFFERENTIAL EQUATIONS BY SEQUENTIAL ACTION CONTROL
Y2  - 2025/05/05/21:54:30
DO  - 10.1093/imamci/dnac021
ER  -


TY  - GEN
AU  - Pi, Q.
AU  - Bian, W.
AU  - Zhou, G.
AU  - Zhu, X.
AU  - Gai, K.
TI  - Practice on Long Sequential User Behavior Modeling for Click-Through Rate Prediction
AB  - Click-through rate (CTR) prediction is critical for industrial applications such as recommender system and online advertising. Practically, it plays an important role for CTR modeling in these applications by mining user interest from rich historical behavior data. Driven by the development of deep learning, deep CTR models with ingeniously designed architecture for user interest modeling have been proposed, bringing remarkable improvement of model performance over offline metric. However, great efforts are needed to deploy these complex models to online serving system for realtime inference, facing massive traffic request. Things turn to be more difficult when it comes to long sequential user behavior data, as the system latency and storage cost increase approximately linearly with the length of user behavior sequence. In this paper, we face directly the challenge of long sequential user behavior modeling and introduce our hands-on practice with the co-design of machine learning algorithm and online serving system for CTR prediction task. (i) From serving system view, we decouple the most resource-consuming part of user interest modeling from the entire model by designing a separate module named UIC (User Interest Center). UIC maintains the latest interest state for each user, whose update depends on realtime user behavior trigger event, rather than on traffic request. Hence UIC is latency free for realtime CTR prediction. (ii) From machine learning algorithm view, we propose a novel memory-based architecture named MIMN (Multi-channel user Interest Memory Network) to capture user interests from long sequential behavior data, achieving superior performance over state-of-the-art models. MIMN is implemented in an incremental manner with UIC module. Theoretically, the co-design solution of UIC and MIMN enables us to handle the user interest modeling with unlimited length of sequential behavior data. Comparison between model performance and system efficiency proves the effectiveness of proposed solution. To our knowledge, this is one of the first industrial solutions that are capable of handling long sequential user behavior data with length scaling up to thousands. It now has been deployed in the display advertising system in Alibaba.
PB  - arXiv
PY  - 2019
ST  - Practice on Long Sequential User Behavior Modeling for Click-Through Rate Prediction
Y2  - 2025/05/05/21:54:30
DO  - 10.1145/3292500.3330666
ER  -


TY  - GEN
AU  - Pavlenko, Ya.V.
AU  - Mascareño, A.S.
AU  - Osorio, M.R.Z.
AU  - Hernández, J.I.G.
AU  - Mohorian, M.
TI  - Temporal changes of the flare activity of Proxima Cen
AB  - Context. We study temporal variations of the emission lines of Hα, Hǫ, H and K CaI, D1 and D2 NaI, He4026, and He5876 in the HARPS spectra of Proxima Centauri across an extended time of 13.2 years, from May 27, 2004, to September 30, 2017. Aims. We analyse the common behaviour and differences in the intensities and profiles of different emission lines in flare and quiet modes of Proxima activity. Methods. We compare the pseudo-equivalent widths (pEW) and profiles of the emission lines in the HARPS high-resolution (R ∼ 115,000) spectra observed at the same epochs. Results. All emission lines show variability with a timescale of at least 10 min. The strength of all lines except He4026 correlate with Hα. During strong flares the ‘red asymmetry’ appears in the Hα emission line indicating the infall of hot condensed matter into the chromosphere with velocities greater than 100 km/s disturbing chromospheric layers. As a result, the strength of the Ca II lines anti-correlates with Hα during strong flares. The HeI lines at 4026 and 5876 Å appear in the strong flares. The cores of D1 and D2 NaI lines are also seen in emission. During the minimum activity of Proxima Centauri, CaII lines and Hǫ almost disappear while the blue part of the NaI emission lines is affected by the absorption in the extending and condensing flows. Conclusions. We see different behaviour of emission lines formed in the flare regions and chromosphere. Chromosphere layers of Proxima Cen are likely heated by the flare events; these layers are cooled in the ‘non-flare’ mode. The self-absorption structures in cores of our emission lines vary with time due to the presence of a complicated system of inward and outward matter flows in the absorbing layers.
PB  - arXiv
PY  - 2019
ST  - Temporal changes of the flare activity of Proxima Cen
Y2  - 2025/05/05/21:54:30
DO  - 10.1051/0004-6361/201834258
ER  -


TY  - GEN
AU  - Degiorgio, K.
AU  - Cuzzolin, F.
TI  - Spatio-Temporal Action Localization in a Weakly Supervised Setting
AB  - Enabling computational systems with the ability to localize actions in video-based content has manifold applications. Traditionally, such a problem is approached in a fully-supervised setting where video-clips with complete frame-by-frame annotations around the actions of interest are provided for training. However, the data requirements needed to achieve adequate generalization in this setting is prohibitive. In this work, we circumvent this issue by casting the problem in a weakly supervised setting, i.e., by considering videos as labelled ‘sets’ of unlabelled video segments. Firstly, we apply unsupervised segmentation to take advantage of the elementary structure of each video. Subsequently, a convolutional neural network is used to extract RGB features from the resulting video segments. Finally, Multiple Instance Learning (MIL) is employed to predict labels at the video segment level, thus inherently performing spatio-temporal action detection. In contrast to previous work, we make use of a different MIL formulation in which the label of each video segment is continuous rather then discrete, making the resulting optimization function tractable. Additionally, we utilize a set splitting technique for regularization. Experimental results considering multiple performance indicators on the UCF-Sports data-set support the effectiveness of our approach.
PB  - arXiv
PY  - 2019
ST  - Spatio-Temporal Action Localization in a Weakly Supervised Setting
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/s00521-022-07102-x
ER  -


TY  - GEN
AU  - Mazari, A.
AU  - Sahbi, H.
TI  - Human action recognition with deep temporal pyramids
AB  - Deep convolutional neural networks (CNNs) are nowadays achieving significant leaps in different pattern recognition tasks including action recognition. Current CNNs are increasingly deeper, data-hungrier and this makes their success tributary of the abundance of labeled training data. CNNs also rely on max/average pooling which reduces dimensionality of output layers and hence attenuates their sensitivity to the availability of labeled data. However, this process may dilute the information of upstream convolutional layers and thereby affect the discrimination power of the trained representations, especially when the learned categories are fine-grained. In this paper, we introduce a novel hierarchical aggregation design, for final pooling, that controls granularity of the learned representations w.r.t the actual granularity of action categories. Our solution is based on a tree-structured temporal pyramid that aggregates outputs of CNNs at different levels. Top levels of this hierarchy are dedicated to coarse categories while deep levels are more suitable to fine-grained ones. The design of our temporal pyramid is based on solving a constrained minimization problem whose solution corresponds to the distribution of weights of different representations in the temporal pyramid. Experiments conducted using the challenging UCF101 database show the relevance of our hierarchical design w.r.t other related methods.
PB  - arXiv
PY  - 2019
ST  - Human action recognition with deep temporal pyramids
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/978-3-319-19665-7_28
ER  -


TY  - GEN
AU  - Yuan, Y.
AU  - Wang, D.
AU  - Wang, Q.
TI  - Memory-augmented temporal dynamic learning for action recognition
AB  - Human actions captured in video sequences contain two crucial factors for action recognition, i.e., visual appearance and motion dynamics. To model these two aspects, Convolutional and Recurrent Neural Networks (CNNs and RNNs) are adopted in most existing successful methods for recognizing actions. However, CNN based methods are limited in modeling long-term motion dynamics. RNNs are able to learn temporal motion dynamics but lack effective ways to tackle unsteady dynamics in long-duration motion. In this work, we propose a memory-augmented temporal dynamic learning network, which learns to write the most evident information into an external memory module and ignore irrelevant ones. In particular, we present a differential memory controller to make a discrete decision on whether the external memory module should be updated with current feature. The discrete memory controller takes in the memory history, context embedding and current feature as inputs and controls information flow into the external memory module. Additionally, we train this discrete memory controller using straight-through estimator. We evaluate this end-to-end system on benchmark datasets (UCF101 and HMDB51) of human action recognition. The experimental results show consistent improvements on both datasets over prior works and our baselines.
PB  - arXiv
PY  - 2019
ST  - Memory-augmented temporal dynamic learning for action recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.1609/aaai.v33i01.33019167
ER  -


TY  - GEN
AU  - Moinet, A.
AU  - Starnini, M.
AU  - Pastor-Satorras, R.
TI  - Random walks in non-Poissoinan activity driven temporal networks
AB  - The interest in non-Markovian dynamics within the complex systems community has recently blossomed, due to a new wealth of time-resolved data pointing out the bursty dynamics of many natural and human interactions, manifested in an inter-event time between consecutive interactions showing a heavy-tailed distribution. In particular, empirical data has shown that the bursty dynamics of temporal networks can have deep consequences on the behavior of the dynamical processes running on top of them. Here, we study the case of random walks, as a paradigm of diffusive processes, unfolding on temporal networks generated by a non-Poissonian activity driven dynamics. We derive analytic expressions for the steady state occupation probability and first passage time distribution in the infinite network size and strong aging limits, showing that the random walk dynamics on non-Markovian networks are fundamentally different from what is observed in Markovian networks. We found a particularly surprising behavior in the limit of diverging average inter-event time, in which the random walker feels the network as homogeneous, even though the activation probability of nodes is heterogeneously distributed. Our results are supported by extensive numerical simulations. We anticipate that our findings may be of interest among the researchers studying non-Markovian dynamics of time-evolving complex topologies.
PB  - arXiv
PY  - 2019
ST  - Random walks in non-Poissoinan activity driven temporal networks
Y2  - 2025/05/05/21:54:30
DO  - 10.1088/1367-2630/ab3f6e
ER  -


TY  - GEN
AU  - Wang, C.
AU  - Peng, M.
AU  - Olugbade, T.A.
AU  - De C. Williams, A.C.
AU  - Bianchi-Berthouze, N.
TI  - Learning temporal and bodily attention in protective movement behavior detection
AB  - For people with chronic pain, the assessment of protective behavior during physical functioning is essential to understand their subjective pain-related experiences (e.g., fear and anxiety toward pain and injury) and how they deal with such experiences (avoidance or reliance on specific body joints), with the ultimate goal of guiding intervention. Advances in deep learning (DL) can enable the development of such intervention. Using the EmoPain MoCap dataset, we investigate how attention-based DL architectures can be used to improve the detection of protective behavior by capturing the most informative temporal and body configurational cues characterizing specific movements and the strategies used to perform them. We propose an end-to-end deep learning architecture named BodyAttentionNet (BANet). BANet is designed to learn temporal and bodily parts that are more informative to the detection of protective behavior. The approach addresses the variety of ways people execute a movement (including healthy people) independently of the type of movement analyzed. Through extensive comparison experiments with other state-of-the-art machine learning techniques used with motion capture data, we show statistically significant improvements achieved by using these attention mechanisms. In addition, the BANet architecture requires a much lower number of parameters than the state of the art for comparable if not higher performances.
PB  - arXiv
PY  - 2019
ST  - Learning temporal and bodily attention in protective movement behavior detection
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/aciiw.2019.8925084
ER  -


TY  - GEN
AU  - Zarin, A.A.
AU  - Mark, B.
AU  - Cardona, A.
AU  - Litwin-Kumar, A.
AU  - Doe, C.Q.
TI  - A Drosophila larval premotor/motor neuron connectome generating two behaviors via distinct spatio-temporal muscle activity
AB  - Animals generate diverse motor behaviors, yet how the same motor neurons generate distinct behaviors remains an open question. Drosophila larvae have multiple behaviors – e.g. forward crawling, backward crawling, self-righting and escape – and all of the body wall motor neurons (MNs) driving these behaviors have been identified. Despite impressive progress in mapping larval motor circuits, the role of most motor neurons in locomotion remains untested, the majority of premotor neurons (PMNs) remain to be identified, and a full understanding of proprioceptor-PMN-MN connectivity is missing. Here we report a comprehensive larval proprioceptor-PMN-MN connectome; describe individual muscle/MN phase activity during both forward and backward locomotor behaviors; identify PMN-MN connectivity motifs that could generate muscle activity phase relationships, plus selected experimental validation; identify proprioceptor-PMN connectivity that provides an anatomical explanation for the role of proprioception in promoting locomotor velocity; and identify a new candidate escape motor circuit. Finally, we generate a recurrent network model that produces the observed sequence of motor activity, showing that the identified pool of premotor neurons is sufficient to generate two distinct larval behaviors. We conclude that different locomotor behaviors can be generated by a specific group of premotor neurons generating behavior-specific motor rhythms.
PB  - bioRxiv
PY  - 2019
ST  - A Drosophila larval premotor/motor neuron connectome generating two behaviors via distinct spatio-temporal muscle activity
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/617977
ER  -


TY  - GEN
AU  - Thompson, W.H.
AU  - Wright, J.
AU  - Shine, J.M.
AU  - Poldrack, R.A.
TI  - The identification of temporal communities through trajectory clustering correlates with single-trial behavioural fluctuations in neuroimaging data
AB  - Interacting sets of nodes and fluctuations in their interaction are important properties of a dynamic network system. In some cases the edges reflecting these interactions are directly quantifiable from the data collected. However, in many cases (such as functional magnetic resonance imaging (fMRI) data), the edges must be inferred from statistical relations between the nodes. Here we present a new method, Temporal Communities through Trajectory Clustering (TCTC), that derives time-varying communities directly from time-series data collected from the nodes in a network. First, we verify TCTC on resting and task fMRI data by showing that time-averaged results correspond with expected static connectivity results. We then show that the time-varying communities correlate and predict single-trial behaviour. This new perspective on temporal community detection of node-collected data identifies robust communities revealing ongoing spatiotemporal community configurations during task performance.
PB  - bioRxiv
PY  - 2019
ST  - The identification of temporal communities through trajectory clustering correlates with single-trial behavioural fluctuations in neuroimaging data
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/617027
ER  -


TY  - GEN
AU  - Yuan, Y.
AU  - Lu, Y.
AU  - Wang, Q.
TI  - Tracking as A Whole: Multi-Target Tracking by Modeling Group Behavior with Sequential Detection
AB  - Video-based vehicle detection and tracking is one of the most important components for Intelligent Transportation Systems (ITS). When it comes to road junctions, the problem becomes even more difficult due to the occlusions and complex interactions among vehicles. In order to get a precise detection and tracking result, in this work we propose a novel tracking-by-detection framework. In the detection stage, we present a sequential detection model to deal with serious occlusions. In the tracking stage, we model group behavior to treat complex interactions with overlaps and ambiguities. The main contributions of this paper are twofold: 1) Shape prior is exploited in the sequential detection model to tackle occlusions in crowded scene. 2) Traffic force is defined in the traffic scene to model group behavior, and it can assist to handle complex interactions among vehicles. We evaluate the proposed approach on real surveillance videos at road junctions and the performance has demonstrated the effectiveness of our method.
PB  - arXiv
PY  - 2019
ST  - Tracking as A Whole
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/tits.2017.2686871
ER  -


TY  - GEN
AU  - Yang, X.
AU  - Yang, X.
AU  - Liu, M.-Y.
AU  - Davis, L.
AU  - Kautz, J.
TI  - STEP: Spatio-temporal progressive learning for video action detection
AB  - In this paper, we propose Spatio-TEmporal Progressive (STEP) action detector-a progressive learning framework for spatio-temporal action detection in videos. Starting from a handful of coarse-scale proposal cuboids, our approach progressively refines the proposals towards actions over a few steps. In this way, high-quality proposals (i.e., adhere to action movements) can be gradually obtained at later steps by leveraging the regression outputs from previous steps. At each step, we adaptively extend the proposals in time to incorporate more related temporal context. Compared to the prior work that performs action detection in one run, our progressive learning framework is able to naturally handle the spatial displacement within action tubes and therefore provides a more effective way for spatio-temporal modeling. We extensively evaluate our approach on UCF101 and AVA, and demonstrate superior detection results. Remarkably, we achieve mAP of 75.0% and 18.6% on the two datasets with 3 progressive steps and using respectively only 11 and 34 initial proposals.
PB  - arXiv
PY  - 2019
ST  - STEP
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvpr.2019.00035
ER  -


TY  - GEN
AU  - Wang, F.
AU  - Song, Y.
AU  - Zhang, J.
AU  - Han, J.
AU  - Huang, D.
TI  - Temporal unet: Sample level human action recognition usingwifi
AB  - Human doing actions will result in WiFi distortion, which is widely explored for action recognition, such as the elderly fallen detection, hand sign language recognition, and keystroke estimation. As our best survey, past work recognizes human action by categorizing one complete distortion series into one action, which we term as series-level action recognition. In this paper, we introduce a much more fine-grained and challenging action recognition task into WiFi sensing domain, i.e., sample-level action recognition. In this task, every WiFi distortion sample in the whole series should be categorized into one action, which is a critical technique in precise action localization, continuous action segmentation, and real-time action recognition. To achieve WiFi-based sample-level action recognition, we fully analyze approaches in image-based semantic segmentation as well as in videobased frame-level action recognition, then propose a simple yet efficient deep convolutional neural network, i.e., Temporal Unet. Experimental results show that Temporal Unet achieves this novel task well. Codes have been made publicly available at https://github.com/geekfeiw/WiSLAR.
PB  - arXiv
PY  - 2019
ST  - Temporal unet
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icassp40776.2020.9053794
ER  -


TY  - GEN
AU  - Huang, Y.
AU  - Dai, Q.
AU  - Lu, Y.
TI  - Decoupling localization and classification in single shot temporal action detection
AB  - Video temporal action detection aims to temporally localize and recognize the action in untrimmed videos. Existing one-stage approaches mostly focus on unifying two subtasks, i.e., localization of action proposals and classification of each proposal through a fully shared backbone. However, such design of encapsulating all components of two subtasks in one single network might restrict the training by ignoring the specialized characteristic of each subtask. In this paper, we propose a novel Decoupled Single Shot temporal Action Detection (Decouple-SSAD) method to mitigate such problem by decoupling the localization and classification in a one-stage scheme. Particularly, two separate branches are designed in parallel to enable each component to own representations privately for accurate localization or classification. Each branch produces a set of action anchor layers by applying deconvolution to the feature maps of the main stream. High-level semantic information from deeper layers is thus incorporated to enhance the feature representations. We conduct extensive experiments on THUMOS14 dataset and demonstrate superior performance over state-of-the-art methods. Our code is available online1.
PB  - arXiv
PY  - 2019
ST  - Decoupling localization and classification in single shot temporal action detection
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icme.2019.00224
ER  -


TY  - GEN
AU  - Al-Faris, M.
AU  - Chiverton, J.P.
AU  - Yang, Y.
AU  - Ndzi, D.
TI  - Multi-view region adaptive multi-temporal DMM and RGB action recognition
AB  - Human action recognition remains an important yet challenging task. This work proposes a novel action recognition system. It uses a novel Multiple View Region Adaptive Multi-resolution in time Depth Motion Map (MV-RAMDMM) formulation combined with appearance information. Multiple stream 3D Convolutional Neural Networks (CNNs) are trained on the different views and time resolutions of the region adaptive Depth Motion Maps. Multiple views are synthesised to enhance the view invariance. The region adaptive weights, based on localised motion, accentuate and differentiate parts of actions possessing faster motion. Dedicated 3D CNN streams for multi-time resolution appearance information (RGB) are also included. These help to identify and differentiate between small object interactions. A pre-trained 3D-CNN is used here with fine-tuning for each stream along with multiple class Support Vector Machines (SVM)s. Average score fusion is used on the output. The developed approach is capable of recognising both human action and human-object interaction. Three public domain datasets including: MSR 3D Action, Northwestern UCLA multi-view actions and MSR 3D daily activity are used to evaluate the proposed solution. The experimental results demonstrate the robustness of this approach compared with state-of-the-art algorithms.
PB  - arXiv
PY  - 2019
ST  - Multi-view region adaptive multi-temporal DMM and RGB action recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/s10044-020-00886-5
ER  -


TY  - GEN
AU  - Kukleva, A.
AU  - Kuehne, H.
AU  - Sener, F.
AU  - Gall, J.
TI  - Unsupervised learning of action classes with continuous temporal embedding
AB  - The task of temporally detecting and segmenting actions in untrimmed videos has seen an increased attention recently. One problem in this context arises from the need to define and label action boundaries to create annotations for training which is very time and cost intensive. To address this issue, we propose an unsupervised approach for learning action classes from untrimmed video sequences. To this end, we use a continuous temporal embedding of framewise features to benefit from the sequential nature of activities. Based on the latent space created by the embedding, we identify clusters of temporal segments across all videos that correspond to semantic meaningful action classes. The approach is evaluated on three challenging datasets, namely the Breakfast dataset, YouTube Instructions, and the 50Salads dataset. While previous works assumed that the videos contain the same high level activity, we furthermore show that the proposed approach can also be applied to a more general setting where the content of the videos is unknown.
PB  - arXiv
PY  - 2019
ST  - Unsupervised learning of action classes with continuous temporal embedding
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvpr.2019.01234
ER  -


TY  - GEN
AU  - Maqbool, B.
AU  - Prakash, M.S.
AU  - Misra, R.
AU  - Bhattacharyya, S.
AU  - Singh, K.P.
TI  - A stochastic propagation model to the energy dependent rapid temporal behaviour of Cygnus X-1 as observed by AstroSat in the hard state
AB  - We report the results from analysis of six observations of Cygnus X-1 by Large Area X-ray Proportional Counters (LAXPC) and Soft X-ray Telescope (SXT) on-board AstroSat, when the source was in the hard spectral state as revealed by the broad band spectra. The spectra obtained from all the observations can be described by a single temperature Comptonizing region with disk and reflection components. The event mode data from LAXPC provides unprecedented energy dependent fractional root mean square (rms) and time-lag at different frequencies which we fit with empirical functions. We invoke a fluctuation propagation model for a simple geometry of a truncated disk with a hot inner region. Unlike other propagation models, the hard X-ray emission (> 4 keV) is assumed to be from the hot inner disk by a single temperature thermal Comptonization process. The fluctuations first cause a variation in the temperature of the truncated disk and then the temperature of the inner disk after a frequency dependent time delay. We find that the model can explain the energy dependent rms and time-lag at different frequencies.
PB  - arXiv
PY  - 2019
ST  - A stochastic propagation model to the energy dependent rapid temporal behaviour of Cygnus X-1 as observed by AstroSat in the hard state
Y2  - 2025/05/05/21:54:30
DO  - 10.1093/mnras/stz930
ER  -


TY  - GEN
AU  - Fong, B.
AU  - Speranzon, A.
AU  - Spivak, D.I.
TI  - Temporal Landscapes: A Graphical Logic of Behavior
AB  - We present an elementary introduction to a new logic for reasoning about behaviors that occur over time. This logic is based on temporal type theory. The syntax of the logic is similar to the usual first-order logic; what differs is the notion of truth value. Instead of reasoning about whether formulas are true or false, our logic reasons about temporal landscapes. A temporal landscape may be thought of as representing the set of durations over which a statement is true. To help understand the practical implications of this approach, we give a wide variety of examples where this logic is used to reason about autonomous agents.
PB  - arXiv
PY  - 2019
ST  - Temporal Landscapes
Y2  - 2025/05/05/21:54:30
DO  - 10.4204/eptcs.372.20
ER  -


TY  - GEN
AU  - Ghosh, R.
AU  - Gupta, A.
AU  - Nakagawa, A.
AU  - Soares, A.B.
AU  - Thakor, N.V.
TI  - Spatiotemporal filtering for event-based action recognition
AB  - In this paper, we address the challenging problem of action recognition, using event-based cameras. To recognise most gestural actions, often higher temporal precision is required for sampling visual information. Actions are defined by motion, and therefore, when using event-based cameras it is often unnecessary to re-sample the entire scene. Neuromorphic, event-based cameras have presented an alternative to visual information acquisition by asynchronously time-encoding pixel intensity changes, through temporally precise spikes (≈ 10 µs resolution), making them well equipped for action recognition. However, other challenges exist, which are intrinsic to event-based imagers, such as higher signal-to-noise ratio, and a spatiotemporally sparse information. One option is to convert event-data into frames, but this could result in significant temporal precision loss. In this work we introduce spatiotemporal filtering in the spike-event domain, as an alternative way of channeling spatiotemporal information through to a convolutional neural network. The filters are local spatiotemporal weight matrices, learned from the spike-event data, in an unsupervised manner. We find that appropriate spatiotemporal filtering significantly improves CNN performance beyond state-of-the-art on the event-based DVS Gesture dataset. On our newly recorded action recognition dataset, our method shows significant improvement when compared with other, standard ways of generating the spatiotemporal filters.
PB  - arXiv
PY  - 2019
ST  - Spatiotemporal filtering for event-based action recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/s12559-011-9097-0
ER  -


TY  - GEN
AU  - Liu, Y.
AU  - Howard, M.W.
TI  - Generation of scale-invariant sequential activity in linear recurrent networks
AB  - Sequential neural activity has been observed in many parts of the brain and has been proposed as a neural mechanism for memory. The natural world expresses temporal relationships at a wide range of scales. Because we cannot know the relevant scales a priori it is desirable that memory, and thus the generated sequences, are scale-invariant. Although recurrent neural network models have been proposed as a mechanism for generating sequences, the requirements for scale-invariant sequences are not known. This paper reports the constraints that enable a linear recurrent neural network model to generate scale-invariant sequential activity. A straightforward eigendecomposition analysis results in two independent conditions that are required for scale-invariance for connectivity matrices with real, distinct eigenvalues. First, the eigenvalues of the network must be geometrically spaced. Second, the eigenvectors must be related to one another via translation. These constraints are easily generalizable for matrices that have complex and distinct eigenvalues. Analogous albeit less compact constraints hold for matrices with degenerate eigenvalues. These constraints, along with considerations on initial conditions, provide a general recipe to build linear recurrent neural networks that support scale-invariant sequential activity.
PB  - bioRxiv
PY  - 2019
ST  - Generation of scale-invariant sequential activity in linear recurrent networks
Y2  - 2025/05/05/21:54:30
DO  - 10.1162/neco_a_01288
ER  -


TY  - GEN
AU  - Schlosser, .P.
AU  - Munch, .D.
AU  - Arens, .M.
TI  - Investigation on combining 3d convolution of image data and optical flow to generate temporal action proposals
AB  - In this paper, several variants of two-stream architectures for temporal action proposal generation in long, untrimmed videos are presented. Inspired by the recent advances in the field of human action recognition utilizing 3D convolutions in combination with two-stream networks and based on the Single-Stream Temporal Action Proposals (SST) architecture [3], four different two-stream architectures utilizing sequences of images on one stream and sequences of images of optical flow on the other stream are subsequently investigated. The four architectures fuse the two separate streams at different depths in the model; for each of them, a broad range of parameters is investigated systematically as well as an optimal parametrization is empirically determined. The experiments on the THUMOS 14 [11] dataset show that all four two-stream architectures are able to outperform the original single-stream SST and achieve state of the art results. Additional experiments revealed that the improvements are not restricted to a single method of calculating optical flow by exchanging the formerly used method of Brox [1] with FlowNet2 [10] and still achieving improvements.
PB  - arXiv
PY  - 2019
ST  - Investigation on combining 3d convolution of image data and optical flow to generate temporal action proposals
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvprw.2019.00300
ER  -


TY  - GEN
AU  - Farha, Y.A.
AU  - Gall, J.
TI  - MS-TCN: Multi-stage temporal convolutional network for action segmentation
AB  - Temporally locating and classifying action segments in long untrimmed videos is of particular interest to many applications like surveillance and robotics. While traditional approaches follow a two-step pipeline, by generating framewise probabilities and then feeding them to high-level temporal models, recent approaches use temporal convolutions to directly classify the video frames. In this paper, we introduce a multi-stage architecture for the temporal action segmentation task. Each stage features a set of dilated temporal convolutions to generate an initial prediction that is refined by the next one. This architecture is trained using a combination of a classification loss and a proposed smoothing loss that penalizes over-segmentation errors. Extensive evaluation shows the effectiveness of the proposed model in capturing long-range dependencies and recognizing action segments. Our model achieves state-of-the-art results on three challenging datasets: 50Salads, Georgia Tech Egocentric Activities (GTEA), and the Breakfast dataset.
PB  - arXiv
PY  - 2019
ST  - MS-TCN
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvpr.2019.00369
ER  -


TY  - GEN
AU  - Li, C.
AU  - Zhong, Q.
AU  - Di Xie
AU  - Pu, S.
TI  - Collaborative spatiotemporal feature learning for video action recognition
AB  - Spatiotemporal feature learning is of central importance for action recognition in videos. Existing deep neural network models either learn spatial and temporal features independently (C2D) or jointly with unconstrained parameters (C3D). In this paper, we propose a novel neural operation which encodes spatiotemporal features collaboratively by imposing a weight-sharing constraint on the learnable parameters. In particular, we perform 2D convolution along three orthogonal views of volumetric video data, which learns spatial appearance and temporal motion cues respectively. By sharing the convolution kernels of different views, spatial and temporal features are collaboratively learned and thus benefit from each other. The complementary features are subsequently fused by a weighted summation whose coefficients are learned end-to-end. Our approach achieves state-of-the-art performance on large-scale benchmarks and won the 1st place in the Moments in Time Challenge 2018. Moreover, based on the learned coefficients of different views, we are able to quantify the contributions of spatial and temporal features. This analysis sheds light on interpretability of the model and may also guide the future design of algorithm for video recognition.
PB  - arXiv
PY  - 2019
ST  - Collaborative spatiotemporal feature learning for video action recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvpr.2019.00806
ER  -


TY  - GEN
AU  - Wang, Y.
AU  - Long, M.
AU  - Wang, J.
AU  - Yu, P.S.
TI  - Spatiotemporal pyramid network for video action recognition
AB  - Two-stream convolutional networks have shown strong performance in video action recognition tasks. The key idea is to learn spatiotemporal features by fusing convolutional networks spatially and temporally. However, it remains unclear how to model the correlations between the spatial and temporal structures at multiple abstraction levels. First, the spatial stream tends to fail if two videos share similar backgrounds. Second, the temporal stream may be fooled if two actions resemble in short snippets, though appear to be distinct in the long term. We propose a novel spatiotemporal pyramid network to fuse the spatial and temporal features in a pyramid structure such that they can reinforce each other. From the architecture perspective, our network constitutes hierarchical fusion strategies which can be trained as a whole using a unified spatiotemporal loss. A series of ablation experiments support the importance of each fusion strategy. From the technical perspective, we introduce the spatiotemporal compact bilinear operator into video analysis tasks. This operator enables efficient training of bilinear fusion operations which can capture full interactions between the spatial and temporal features. Our final network achieves state-of-the-art results on standard video datasets.
PB  - arXiv
PY  - 2019
ST  - Spatiotemporal pyramid network for video action recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvpr.2017.226
ER  -


TY  - GEN
AU  - Hu, B.
AU  - Cai, J.
AU  - Cham, T.-J.
AU  - Yuan, J.
TI  - Progress regression RNN for online spatial-temporal action localization in unconstrained videos
AB  - Previous spatial-temporal action localization methods commonly follow the pipeline of object detection to estimate bounding boxes and labels of actions. However, the temporal relation of an action has not been fully explored. In this paper, we propose an end-to-end Progress Regression Recurrent Neural Network (PR-RNN) for online spatial-temporal action localization, which learns to infer the action by temporal progress regression. Two new action attributes, called progression and progress rate, are introduced to describe the temporal engagement and relative temporal position of an action. In our method, frame-level features are first extracted by a Fully Convolutional Network (FCN). Subsequently, detection results and action progress attributes are regressed by the Convolutional Gated Recurrent Unit (ConvGRU) based on all the observed frames instead of a single frame or a short clip. Finally, a novel online linking method is designed to connect single-frame results to spatial-temporal tubes with the help of the estimated action progress attributes. Extensive experiments demonstrate that the progress attributes improve the localization accuracy by providing more precise temporal position of an action in unconstrained videos. Our proposed PR-RNN achieves the state-of-the-art performance for most of the IoU thresholds on two benchmark datasets.
PB  - arXiv
PY  - 2019
ST  - Progress regression RNN for online spatial-temporal action localization in unconstrained videos
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/iccv.2017.237
ER  -


TY  - GEN
AU  - McNally, W.
AU  - Wong, A.
AU  - McPhee, J.
TI  - STAR-Net: Action recognition using spatio-temporal activation reprojection
AB  - While depth cameras and inertial sensors have been frequently leveraged for human action recognition, these sensing modalities are impractical in many scenarios where cost or environmental constraints prohibit their use. As such, there has been recent interest on human action recognition using low-cost, readily-available RGB cameras via deep convolutional neural networks. However, many of the deep convolutional neural networks proposed for action recognition thus far have relied heavily on learning global appearance cues directly from imaging data, resulting in highly complex network architectures that are computationally expensive and difficult to train. Motivated to reduce network complexity and achieve higher performance, we introduce the concept of spatiotemporal activation reprojection (STAR). More specifically, we reproject the spatio-temporal activations generated by human pose estimation layers in space and time using a stack of 3D convolutions. Experimental results on UTD-MHAD and J-HMDB demonstrate that an end-to-end architecture based on the proposed STAR framework (which we nickname STAR-Net) is proficient in single-environment and small-scale applications. On UTD-MHAD, STAR-Net outperforms several methods using richer data modalities such as depth and inertial sensors.
PB  - arXiv
PY  - 2019
ST  - STAR-Net
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/crv.2019.00015
ER  -


TY  - GEN
AU  - Yang, K.
AU  - Qiao, P.
AU  - Li, D.
AU  - Dou, Y.
TI  - IF-TTN: Information fused temporal transformation network for video action recognition
AB  - Effective spatiotemporal feature representation is crucial to the video-based action recognition task. Focusing on discriminate spatiotemporal feature learning, we propose Information Fused Temporal Transformation Network (IF-TTN) for action recognition on top of popular Temporal Segment Network (TSN) framework. In the network, Information Fusion Module (IFM) is designed to fuse the appearance and motion features at multiple ConvNet levels for each video snippet, forming a short-Term video descriptor. With fused features as inputs, Temporal Transformation Networks (TTN) are employed to model middle-Term temporal transformation between the neighboring snippets following a sequential order. As TSN itself depicts longterm temporal structure by segmental consensus, the proposed network comprehensively considers multiple granularity temporal features. Our IF-TTN achieves the stateof-the-Art results on two most popular action recognition datasets: UCF101 and HMDB51. Empirical investigation reveals that our architecture is robust to the input motion map quality. Replacing optical flow with the motion vectors from compressed video stream, the performance is still comparable to the flow-based methods while the testing speed is 10x faster.
PB  - arXiv
PY  - 2019
ST  - IF-TTN
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icassp40776.2020.9053394
ER  -


TY  - GEN
AU  - Belsey, P.
AU  - Nicholas, M.A.
AU  - Yttri, E.A.
TI  - Build a better mouse task – can an open-source rodent joystick enhance reaching behavior outcomes through improved monitoring of real-time spatiotemporal kinematics?
AB  - For decades, advanced behavioral tasks have only been used in human and non-human primates. However, with improved analytical and genetic techniques, there has been a growing drive to implement complex reaching, decision-making, and reaction time tasks – not in primates – but in rodents. Here, we assess the hypothesis that a mouse can learn a cued reaction time task. Moreover, we tested multiple training regimens and found that introducing elements of the reaction time task serially hindered, rather than helped task acquisition. Additionally, we include a step-by-step manual for inexpensive implementation and use of a rodent joystick for behavioral analysis. Task and analysis code for the evaluated behaviors are included such that they may be replicated and tested further. With these, we also include code for a probabilistic reward ‘two-arm bandit’ task. These various tasks, and the method to construct and implement them, will enable greatly improved study of the neural correlates of behavior in the powerful mouse model organism. In summary, we have tested and demonstrated that mice can learn sophisticated tasks with A joystick, and that targeted task design provides a significant advantage. These results of this study stand to inform the implementation of other sophisticated tasks using the mouse model.
PB  - bioRxiv
PY  - 2019
ST  - Build a better mouse task – can an open-source rodent joystick enhance reaching behavior outcomes through improved monitoring of real-time spatiotemporal kinematics?
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/560961
ER  -


TY  - GEN
AU  - Short, S.M.
AU  - Wachowiak, M.
TI  - Temporal dynamics of inhalation-linked activity across defined subpopulations of mouse olfactory bulb neurons imaged in vivo
AB  - In mammalian olfaction, inhalation drives the temporal patterning of neural activity that underlies early olfactory processing. However, it remains poorly understood how the neural circuits that process incoming olfactory information are engaged in the context of inhalation-linked dynamics. Here, we used artificial inhalation and two-photon calcium imaging to compare the dynamics of activity evoked by odorant inhalation across major cell types of the mouse olfactory bulb (OB). We expressed GCaMP6f or jRGECO1a in mitral and tufted cell subpopulations, olfactory sensory neurons and two major juxtaglomerular interneuron classes, and imaged responses to a single inhalation of odorant. Activity in all cell types was strongly linked to inhalation, and all cell types showed some variance in the latency, rise-times and durations of their inhalation-linked response. Juxtaglomerular interneuron dynamics closely matched that of sensory inputs, while mitral and tufted cells showed the highest diversity in responses, with a range of latencies and durations that could not be accounted for by heterogeneity in sensory input dynamics. Diversity was apparent even among ‘sister’ tufted cells innervating the same glomerulus. Surprisingly, inhalation-linked responses of mitral and tufted cells were highly overlapping and could not be distinguished on the basis of their inhalation-linked dynamics, with the exception of a subpopulation of superficial tufted cells expressing cholecystokinin. Overall, our results support a model in which diversity in inhalation-linked patterning of OB output arises first at the level of sensory input and is enhanced by feedforward inhibition from juxtaglomerular interneurons which differentially impact different subpopulations of OB output neurons.Inhalation drives the temporal patterning of neural activity that underlies olfactory processing and rapid odor perception, yet the dynamics of the neural circuit elements mediating this processing are poorly understood. By comparing inhalation-linked dynamics of major olfactory bulb subpopulations, we find that diversity in the timing of neural activation arises at the level of sensory input, which is then mirrored by inhibitory interneurons in the glomerular layer. Temporal diversity is higher among olfactory bulb output neurons, with different subpopulations showing distinct but nonetheless highly overlapping ranges of inhalation-linked dynamics. These results implicate feedforward inhibition by glomerular-layer interneurons in diversifying temporal responses among output neurons, which may be important for generating and shaping timing-based odor representations during natural odor sampling.
PB  - bioRxiv
PY  - 2019
ST  - Temporal dynamics of inhalation-linked activity across defined subpopulations of mouse olfactory bulb neurons imaged in vivo
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/558999
ER  -


TY  - GEN
AU  - Sicilia, A.
AU  - Pelechrinis, K.
AU  - Goldsberry, K.
TI  - DeepHoops: Evaluating micro-actions in basketball using deep feature representations of spatio-temporal data
AB  - How much is an on-ball screen worth? How much is a backdoor cut away from the ball worth? Basketball is one of a number of sports which, within the past decade, have seen an explosion in quantitative metrics and methods for evaluating players and teams. However, it is still challenging to evaluate individual off-ball events in terms of how they contribute to the success of a possession. In this study, we develop an end-to-end deep learning architecture (DeepHoops) to process a unique dataset composed of spatio-temporal tracking data from NBA games in order to generate a running stream of predictions on the expected points to be scored as a possession progresses. We frame the problem as a multi-class sequence classification problem in which our model estimates probabilities of terminal actions taken by players (e.g. take field goal, turnover, foul etc.) at each moment of a possession based on a sequence of ball and player court locations preceding the said moment. Each of these terminal actions is associated with an expected point value, which is used to estimate the expected points to be scored. One of the challenges associated with this problem is the high imbalance in the action classes. To solve this problem, we parameterize a downsampling scheme for the training phase. We demonstrate that DeepHoops is well-calibrated, estimating accurately the probabilities of each terminal action and we further showcase the model's capability to evaluate individual actions (potentially off-ball) within a possession that are not captured by boxscore statistics.
PB  - arXiv
PY  - 2019
ST  - DeepHoops
Y2  - 2025/05/05/21:54:30
DO  - 10.1145/2964284.2967247
ER  -


TY  - GEN
AU  - Yang, K.
AU  - Shen, X.
AU  - Qiao, P.
AU  - Li, D.
AU  - Dou, Y.
TI  - Exploring frame segmentation networks for temporal action localization
AB  - Temporal action localization is an important task of computer vision. Though many methods have been proposed, it still remains an open question how to predict the temporal location of action segments precisely. Most state-of-the-art works train action classifiers on video segments pre-determined by action proposal. However, recent work found that a desirable model should move beyond segment-level and make dense predictions at a fine granularity in time to determine precise temporal boundaries. In this paper, we propose a Frame Segmentation Network (FSN) that places a temporal CNN on top of the 2D spatial CNNs. Spatial CNNs are responsible for abstracting semantics in spatial dimension while temporal CNN is responsible for introducing temporal context information and performing dense predictions. The proposed FSN can make dense predictions at frame-level for a video clip using both spatial and temporal context information. FSN is trained in an end-to-end manner, so the model can be optimized in spatial and temporal domain jointly. We also adapt FSN to use it in weakly supervised scenario (WFSN), where only video level labels are provided when training. Experiment results on public dataset show that FSN achieves superior performance in both frame-level action localization and temporal action localization.
PB  - arXiv
PY  - 2019
ST  - Exploring frame segmentation networks for temporal action localization
Y2  - 2025/05/05/21:54:30
DO  - 10.1016/j.jvcir.2019.02.003
ER  -


TY  - GEN
AU  - Parsa, B.
AU  - Samani, E.U.
AU  - Hendrix, R.
AU  - Devasia, S.
AU  - Banerjee, A.G.
TI  - Toward ergonomic risk prediction via segmentation of indoor object manipulation actions using spatiotemporal convolutional networks
AB  - Automated real-time prediction of the ergonomic risks of manipulating objects is a key unsolved challenge in developing effective human-robot collaboration systems for logistics and manufacturing applications. We present a foundational paradigm to address this challenge by formulating the problem as one of action segmentation from RGB-D camera videos. Spatial features are first learned using a deep convolutional model from the video frames, which are then fed sequentially to temporal convolutional networks to semantically segment the frames into a hierarchy of actions, which are either ergonomically safe, require monitoring, or need immediate attention. For performance evaluation, in addition to an opensource kitchen dataset, we collected a new dataset comprising twenty individuals picking up and placing objects of varying weights to and from cabinet and table locations at various heights. Results show very high (87-94)% F1 overlap scores among the ground truth and predicted frame labels for videos lasting over two minutes and consisting of a large number of actions.
PB  - arXiv
PY  - 2019
ST  - Toward ergonomic risk prediction via segmentation of indoor object manipulation actions using spatiotemporal convolutional networks
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/lra.2019.2925305
ER  -


TY  - GEN
AU  - Wei, Z.
AU  - Li, B.
AU  - Guo, W.
AU  - Hu, W.
AU  - Zhao, C.
TI  - Sequential Bayesian detection of spike activities from fluorescence observations
AB  - Extracting and detecting spike activities from the fluorescence observations is an important step in understanding how neuron systems work. The main challenge lies in that the combination of the ambient noise with dynamic baseline fluctuation, often contaminates the observations, thereby deteriorating the reliability of spike detection. This may be even worse in the face of the nonlinear biological process, the coupling interactions between spikes and baseline, and the 1901.11418 critical parameters of an underlying physiological model, in which erroneous estimations of parameters will affect the detection of spikes causing further error propagation. In this paper, we propose a random finite set (RFS) based Bayesian approach. The dynamic behaviors of spike sequence, fluctuated baseline and unknown parameters are formulated as one RFS. This RFS state is capable of distinguishing the hidden active/silent states induced by spike and non-spike activities respectively, thereby negating the interaction role played by spikes and other factors. Then, premised on the RFS states, a Bayesian inference scheme is designed to simultaneously estimate the model parameters, baseline, and crucial spike activities. Our results demonstrate that the proposed scheme can gain an extra 12% detection accuracy in comparison with the state-of-the-art MLSpike method.
PB  - arXiv
PY  - 2019
ST  - Sequential Bayesian detection of spike activities from fluorescence observations
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/tmbmc.2019.2943288
ER  -


TY  - GEN
AU  - Vissani, M.
AU  - Cordella, R.
AU  - Micera, S.
AU  - Romito, L.M.
AU  - Mazzoni, A.
TI  - Spatio-temporal structure of single neuron subthalamic activity in Tourette Syndrome explored during DBS procedures
AB  - Basal ganglia dysfunctions have been suggested to play a causal role in the pathophysiology of most motor and non-motor symptoms of movement disorders as Tourette Syndrome (TS) or Parkinson’s Disease (PD). Intra/post-operative recordings from the subthalamic nucleus (STN) during Deep Brain Stimulation (DBS) procedures in PD patients have highlighted specific pathological patterns of neural activity. Spatial and temporal patterns of STN neural activity in TS are still unknown due to the lack of direct microrecordings in humans. Here, we describe for the first time specific neural activities of sensorimotor STN in TS patients, as recorded during intraoperative microrecordings. We analyzed 125 single units at 0.5 mm-spaced depths from the STN of anesthetized TS patients and we observed a large fraction of units (39/125, 31.2%) intensely bursting in the delta band (<4 Hz). In anesthetized PD patients we found similar average firing rate and spectral density of STN units, but differently to TS patients, only 4/54 (7.4%) of the units displayed bursting. Remarkably, bursting units in TS STN were not homogeneously distributed over the dorso-ventral trajectory of the recording: the highest density of bursting units was reliably found at the depth for which the clinical effect was maximal. Our results provide an unprecedented characterization of STN functional architecture and single units dynamics in TS patients, paving the way to an understanding of the role of STN subterritories in TS. Key Points Single neuron activity in Subthalamic Nucleus (STN) of patients with Tourette Syndrome (TS) was analyzed for the first time in literature.Firing rate and spectral content of single STN neurons in TS patients were found to be similar to those of anesthetized PD patients, while the analysis of arrhythmic bursting activity revealed that in TS patients the STN is characterized by a larger fraction of bursting neurons and more intense burstsBursting activity in TS was widespread across the whole STN, but with a higher density at the optimal lead location depth for DBS
PB  - bioRxiv
PY  - 2019
ST  - Spatio-temporal structure of single neuron subthalamic activity in Tourette Syndrome explored during DBS procedures
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/532200
ER  -


TY  - GEN
AU  - Bhoi, A.
TI  - Spatio-temporal Action Recognition: A Survey
AB  - The task of action recognition or action detection involves analyzing videos and determining what action or motion is being performed. The primary subject of these videos are predominantly humans performing some action. However, this requirement can be relaxed to generalize over other subjects such as animals or robots. The applications can range from anywhere between human-computer interaction to automated video editing proposals. When we consider spatio-temporal action recognition, we deal with action localization. This task not only involves determining what action is being performed, but also when and where it is being performed in said video. This paper aims to survey the plethora of approaches and algorithms attempted to solve this task, give a comprehensive comparison between them, explore various datasets available for the problem, and determine the most promising approaches.
PB  - arXiv
PY  - 2019
ST  - Spatio-temporal Action Recognition
Y2  - 2025/05/05/21:54:30
ER  -


TY  - GEN
AU  - Deb, D.
AU  - Ross, A.
AU  - Jain, A.K.
AU  - Prakah-Asante, K.
AU  - Venkatesh Prasad, K.
TI  - Actions speak louder than (pass)words: Passive authentication of smartphone∗users via deep temporal features
AB  - Prevailing user authentication schemes on smartphones rely on explicit user interaction, where a user types in a passcode or presents a biometric cue such as face, fingerprint, or iris. In addition to being cumbersome and obtrusive to the users, such authentication mechanisms pose security and privacy concerns. Passive authentication systems can tackle these challenges by frequently and unobtrusively monitoring the user's interaction with the device. In this paper, we propose a Siamese Long Short-Term Memory network architecture for passive authentication, where users can be verified without requiring any explicit authentication step. We acquired a dataset comprising of measurements from 30 smartphone sensor modalities for 37 users. We evaluate our approach on 8 dominant modalities, namely, keystroke dynamics, GPS location, accelerometer, gyroscope, magnetometer, linear accelerometer, gravity, and rotation sensors. Experimental results find that, within 3 seconds, a genuine user can be correctly verified 97.15% of the time at a false accept rate of 0.1%.
PB  - arXiv
PY  - 2019
ST  - Actions speak louder than (pass)words
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icb45273.2019.8987433
ER  -


TY  - GEN
AU  - Shah, Z.
AU  - Jithesh, V.
AU  - Sahayanathan, S.
AU  - Misra, R.
AU  - Iqbal, N.
TI  - Study on temporal and spectral behavior of 3C 279 during 2018 January flare
AB  - We present a detailed temporal and spectral study of the blazar 3C 279 using multi-wavelength observations from Swift-XRT, Swift-UVOT and Fermi-LAT during a flare in 2018 January. The temporal analysis of γ-ray light curve indicates a lag of ∼ 1 d between the 0.1-3 GeV and 3-500 GeV emission. Additionally, the γ-ray light curve shows asymmetry with slow rise-fast decay in energy band 0.1-3 GeV and fast rise-slow decay in the 3-500 GeV band. We interpret this asymmetry as a result of shift in the Compton spectral peak. This inference is further supported by the correlation studies between the flux and the parameters of the log-parabola fit to the source spectra in the energy range 0.1-500 GeV. We found that the flux correlates well with the peak spectral energy and the log-parabola fit parameters show a hard index with large curvature at high flux states. Interestingly, the hardest index with large curvature was synchronous with a very high energy flare detected by H.E.S.S. Our study of the spectral behavior of the source suggests that γ-ray emission is most likely to be associated with the Compton up-scattering of IR photons from the dusty environment. Moreover, the fit parameters indicate the increase in bulk Lorentz factor of emission region to be a dominant cause for the flux enhancement.
PB  - arXiv
PY  - 2019
ST  - Study on temporal and spectral behavior of 3C 279 during 2018 January flare
Y2  - 2025/05/05/21:54:30
DO  - 10.1093/mnras/stz151
ER  -


TY  - GEN
AU  - Varley, T.F.
AU  - Carhart-Harris, R.
AU  - Roseman, L.
AU  - Menon, D.K.
AU  - Stamatakis, E.A.
TI  - Serotonergic Psychedelics LSD & Psilocybin Increase the Fractal Dimension of Cortical Brain Activity in Spatial and Temporal Domains
AB  - Psychedelic drugs, such as psilocybin and LSD, represent unique tools for researchers in-vestigating the neural origins of consciousness. Currently, the most compelling theories of how psychedelics exert their effects is by increasing the complexity of brain activity and moving the system towards a critical point between order and disorder, creating more dynamic and complex patterns of neural activity. While the concept of criticality is of central importance to this theory, few of the published studies on psychedelics investigate it directly, testing instead related measures such as algorithmic complexity or Shannon entropy. We propose using the fractal dimension of functional activity in the brain as a measure of complexity since findings from physics suggest that as a system organizes towards criticality, it tends to take on a fractal structure. We tested two different measures of fractal dimension, one spatial and one temporal, using fMRI data from volunteers under the influence of both LSD and psilocybin. The first was the fractal dimension of cortical functional connectivity networks and the second was the fractal dimension of BOLD time-series. We were able to show that both psychedelic drugs significantly increased the fractal dimension of functional connectivity networks, and that LSD significantly increased the fractal dimension of BOLD signals, with psilocybin showing a non-significant trend in the same direction. With both LSD and psilocybin, we were able to localize changes in the fractal dimension of BOLD signals to brain areas assigned to the dorsal-attentional network. These results show that psychedelic drugs increase the fractal character of activity in the brain and we see this as an indicator that the changes in consciousness triggered by psychedelics are associated with evolution towards a critical zone. Author Summary The unique state of consciousness produced by psychedelic drugs like LSD and psilocybin (the active component in magic mushrooms) are potentially useful tools for discovering how specific changes in the brain are related to differences in perception and thought patterns. Past research into the neuroscience of psychedelics has led to the proposal of a general theory of brain function and consciousness: the Entropic Brain Hypothesis proposes that consciousness emerges when the brain is sitting near a critical tipping point between order and chaos and that the mind-expanding elements of the psychedelic experience are caused by the brain moving closer to that critical transition point. Physicists have discovered that near this critical point, many different kinds of systems, from magnets to ecosystems, take on a distinct, fractal structure. Here, we used two measures of fractal-quality of brain activity, as seen in fMRI, to test whether the activity of the brain on psychedelics is more fractal than normal. We found evidence that this is the case and interpret that as supporting the theory that, psychedelic drugs are move the brain towards a more critical state.
PB  - bioRxiv
PY  - 2019
ST  - Serotonergic Psychedelics LSD & Psilocybin Increase the Fractal Dimension of Cortical Brain Activity in Spatial and Temporal Domains
Y2  - 2025/05/05/21:54:30
DO  - 10.1016/j.neuroimage.2020.117049
ER  -


TY  - GEN
AU  - Xu, H.
AU  - Kang, B.
AU  - Sun, X.
AU  - Saenko, K.
AU  - Darrell, T.
TI  - Similarity R-C3D for few-shot temporal activity detection
AB  - Many activities of interest are rare events, with only a few labeled examples available. Therefore models for temporal activity detection which are able to learn from a few examples are desirable. In this paper, we present a conceptually simple and general yet novel framework for few-shot temporal activity detection which detects the start and end time of the few-shot input activities in an untrimmed video. Our model is end-to-end trainable and can benefit from more few-shot examples. At test time, each proposal is assigned the label of the few-shot activity class corresponding to the maximum similarity score. Our Similarity R-C3D method outperforms previous work on three large scale benchmarks for temporal activity detection (THUMOS14, ActivityNet1.2 and ActivityNet1.3 datasets) in the few-shot setting. Our code will be made available.
PB  - arXiv
PY  - 2018
ST  - Similarity R-C3D for few-shot temporal activity detection
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/tmm.2023.3234368
ER  -


TY  - GEN
AU  - Cai, Z.
AU  - Neher, H.
AU  - Vats, K.
AU  - Clausi, D.
AU  - Zelek, J.
TI  - Temporal Hockey Action Recognition via Pose and optical flows
AB  - Recognizing actions in ice hockey using computer vision poses challenges due to bulky equipment and inadequate image quality. A novel two-stream framework has been designed to improve action recognition accuracy for hockey using three main components. First, pose is estimated via the Part Affinity Fields model to extract meaningful cues from the player. Second, optical flow (using LiteFlownet) is used to extract temporal features. Third, pose and optical flow streams are fused and passed to fully-connected layers to estimate the hockey players action. A novel publicly available dataset named HARPET (Hockey Action Recognition Pose Estimation, Temporal) was created, composed of sequences of annotated actions and pose of hockey players including their hockey sticks as an extension of human body pose. Three contributions are recognized. (1) The novel two-stream architecture achieves 85% action recognition accuracy, with the inclusion of optical flows increasing accuracy by about 10%. (2) The unique localization of hand-held objects (e.g., hockey sticks) as part of pose increases accuracy by about 13%. (3) For pose estimation, a bigger and more general dataset, MSCOCO, is successfully used for transfer learning to a smaller and more specific dataset, HARPET, achieving a PCKh of 87%.
PB  - arXiv
PY  - 2018
ST  - Temporal Hockey Action Recognition via Pose and optical flows
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvprw.2019.00310
ER  -


TY  - GEN
AU  - Dai, X.
AU  - Singh, B.
AU  - Ng, J.Y.-H.
AU  - Davis, L.S.
TI  - TAN: Temporal Aggregation Network for Dense Multi-label Action Recognition
AB  - We present Temporal Aggregation Network (TAN) which decomposes 3D convolutions into spatial and temporal aggregation blocks. By stacking spatial and temporal convolutions repeatedly, TAN forms a deep hierarchical representation for capturing spatio-temporal information in videos. Since we do not apply 3D convolutions in each layer but only apply temporal aggregation blocks once after each spatial downsampling layer in the network, we significantly reduce the model complexity. The use of dilated convolutions at different resolutions of the network helps in aggregating multi-scale spatio-temporal information efficiently. Experiments show that our model is well suited for dense multi-label action recognition, which is a challenging subtopic of action recognition that requires predicting multiple action labels in each frame. We outperform state-of-the-art methods by 5% and 3% on the Charades and MultiTHUMOS dataset respectively.
PB  - arXiv
PY  - 2018
ST  - TAN
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/wacv.2019.00022
ER  -


TY  - GEN
AU  - Lin, N.
AU  - Roberts, K.R.
TI  - Explaining Unobserved Heterogeneity of Food Safety Behavioral Intention: A Sequential Mixed Method Approach
AB  - In 2015, 902 foodborne illness outbreaks were reported to the Centers for Disease Control and Prevention, resulting in 15,202 illnesses, 950 hospitalizations, and 15 deaths. Previous literature from both survey and observational studies have reported low conformity with the U.S. Food and Drug Administration (FDA) Food Code guidelines. To effectively reduce foodborne illnesses, foodservice managers and food handlers must perform proper food safety behaviors. Therefore, the purpose of this project is to identify and explain the unobserved cognitive processes within food safety behavioral intention. An explanatory sequential mixed methods design was utilized. First, a systematic review and meta-analyses of the existing literature were conducted to quantify statistical power better and summarize the effect sizes with conflicting studies. Then, an in-depth qualitative study was conducted to help explain the statistical results. Using existing observed cognitive variables grounded by the Theory of Planned Behavior, the key idea is that the qualitative inquiry was built on the quantitative results. Thus, the syntheses of both studies help explained the unobserved heterogeneity information. Study 1 included a total of 1,550 studies for screening with 46 records meeting the inclusion criteria for analyses. The overall random effect size (r) was 0.282 (p < 0.001) providing collective evidence that the TPB constructs predict food safety behavioral intention. Subjective norms were noted as the most influencial variable to food safety behavioral intention. Studies with employee motivational constructs tend to show the most positive effect on food safety intention relationships. However, the Theory of Planned Behavior model only explained a combined 22% of total true effect variance. Thus, a considerable amount of the variance (78%) within food safety behavioral intention is still unexplained. Study 2 used an online questionnaire to measure individual-level norms. Open-ended questions (14) helped create qualitative narrative texts for analyses and establishing a demographic profile of the participants. A total of 104 responses from foodservice and restaurant employees were documented for coding. Most participants were female, with a mean age of 36 with an average of about 11 years of foodservice industry experiences. The results indicated that employees are usually not influenced of other managers or coworker’s approval or disapproval of their behavior. Rather, their behavior is guided by an innate motivation for moral consideration and ethical reasoning. The data further indicated that participants experience injunctive (subjective) norms, but more from a retrospective formation, rather than a forward-looking expectance regarding food safety practices. Intrinsic motivation should be an important antecedent to form normative beliefs of food safety-related behaviors. The findings of the study results challenge the previous understanding of path directions regarding normative pressure. Limitations and future studies related to maximize food safety behavioral intentions were discussed.
PB  - SSRN
PY  - 2018
ST  - Explaining Unobserved Heterogeneity of Food Safety Behavioral Intention
Y2  - 2025/05/05/21:54:30
ER  -


TY  - GEN
AU  - Zhang, Y.
AU  - Tang, S.
AU  - Muandet, K.
AU  - Jarvers, C.
AU  - Neumann, H.
TI  - Local Temporal Bilinear Pooling for Fine-Grained Action Parsing
AB  - Fine-grained temporal action parsing is important in many applications, such as daily activity understanding, human motion analysis, surgical robotics and others requiring subtle and precise operations over a long-term period. In this paper we propose a novel bilinear pooling operation, which is used in intermediate layers of a temporal convolutional encoder-decoder net. In contrast to previous work, our proposed bilinear pooling is learnable and hence can capture more complex local statistics than the conventional counterpart. In addition, we introduce exact lower-dimension representations of our bilinear forms, so that the dimensionality is reduced without suffering from information loss nor requiring extra computation. We perform extensive experiments to quantitatively analyze our model and show the superior performances to other state-of-the-art pooling work on various datasets.
PB  - arXiv
PY  - 2018
ST  - Local Temporal Bilinear Pooling for Fine-Grained Action Parsing
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvpr.2019.01228
ER  -


TY  - GEN
AU  - Herzig, R.
AU  - Levi, E.
AU  - Xu, H.
AU  - Globerson, A.
AU  - Darrell, T.
TI  - Spatio-Temporal Action Graph Networks
AB  - Events defined by the interaction of objects in a scene are often of critical importance; yet important events may have insufficient labeled examples to train a conventional deep model to generalize to future object appearance. Activity recognition models that represent object interactions explicitly have the potential to learn in a more efficient manner than those that represent scenes with global descriptors. We propose a novel inter-object graph representation for activity recognition based on a disentangled graph embedding with direct observation of edge appearance. In contrast to prior efforts, our approach uses explicit appearance for high order relations derived from object-object interaction, formed over regions that are the union of the spatial extent of the constituent objects. We employ a novel factored embedding of the graph structure, disentangling a representation hierarchy formed over spatial dimensions from that found over temporal variation. We demonstrate the effectiveness of our model on the Charades activity recognition benchmark, as well as a new dataset of driving activities focusing on multi-object interactions with near-collision events. Our model offers significantly improved performance compared to baseline approaches without object-graph representations, or with previous graph-based models.
PB  - arXiv
PY  - 2018
ST  - Spatio-Temporal Action Graph Networks
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/iccvw.2019.00288
ER  -


TY  - GEN
AU  - Hu, Y.
AU  - Lu, M.
AU  - Lu, X.
TI  - Spatial-temporal fusion convolutional neural network for simulated driving behavior recognition
AB  - Abnormal driving behaviour is one of the leading cause of terrible traffic accidents endangering human life. Therefore, study on driving behaviour surveillance has become essential to traffic security and public management. In this paper, we conduct this promising research and employ a two stream CNN framework for video-based driving behaviour recognition, in which spatial stream CNN captures appearance information from still frames, whilst temporal stream CNN captures motion information with pre-computed optical flow displacement between a few adjacent video frames. We investigate different spatial-temporal fusion strategies to combine the intra frame static clues and inter frame dynamic clues for final behaviour recognition. So as to validate the effectiveness of the designed spatial-temporal deep learning based model, we create a simulated driving behaviour dataset, containing 1237 videos with 6 different driving behavior for recognition. Experiment result shows that our proposed method obtains noticeable performance improvements compared to the existing methods.
PB  - arXiv
PY  - 2018
ST  - Spatial-temporal fusion convolutional neural network for simulated driving behavior recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icarcv.2018.8581201
ER  -


TY  - GEN
AU  - Ye, Y.
AU  - Yang, X.
AU  - Tian, Y.
TI  - Discovering Spatio-Temporal Action Tubes
AB  - In this paper, we address the challenging problem of spatial and temporal action detection in videos. We first develop an effective approach to localize frame-level action regions through integrating static and kinematic information by the early- and late-fusion detection scheme. With the intention of exploring important temporal connections among the detected action regions, we propose a tracking-by-point-matching algorithm to stitch the discrete action regions into a continuous spatio-temporal action tube. Recurrent 3D convolutional neural network is used to predict action categories and determine temporal boundaries of the generated tubes. We then introduce an action footprint map to refine the candidate tubes based on the action-specific spatial characteristics preserved in the convolutional layers of R3DCNN. In the extensive experiments, our method achieves superior detection results on the three public benchmark datasets: UCFSports, J-HMDB and UCF101.
PB  - arXiv
PY  - 2018
ST  - Discovering Spatio-Temporal Action Tubes
Y2  - 2025/05/05/21:54:30
DO  - 10.1016/j.jvcir.2018.12.019
ER  -


TY  - GEN
AU  - Liu, Y.
AU  - Ma, L.
AU  - Zhang, Y.
AU  - Liu, W.
AU  - Chang, S.-F.
TI  - Multi-granularity Generator for Temporal Action Proposal
AB  - Temporal action proposal generation is an important task, aiming to localize the video segments containing human actions in an untrimmed video. In this paper, we propose a multi-granularity generator (MGG) to perform the temporal action proposal from different granularity perspectives, relying on the video visual features equipped with the position embedding information. First, we propose to use a bilinear matching model to exploit the rich local information within the video sequence. Afterwards, two components, namely segment proposal producer (SPP) and frame actionness producer (FAP), are combined to perform the task of temporal action proposal at two distinct granularities. SPP considers the whole video in the form of feature pyramid and generates segment proposals from one coarse perspective, while FAP carries out a finer actionness evaluation for each video frame. Our proposed MGG can be trained in an end-to-end fashion. By temporally adjusting the segment proposals with fine-grained frame actionness information, MGG achieves the superior performance over state-of-the-art methods on the public THUMOS-14 and ActivityNet-1.3 datasets. Moreover, we employ existing action classifiers to perform the classification of the proposals generated by MGG, leading to significant improvements compared against the competing methods for the video detection task.
PB  - arXiv
PY  - 2018
ST  - Multi-granularity Generator for Temporal Action Proposal
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/cvpr.2019.00372
ER  -


TY  - GEN
AU  - Ghosh, P.
AU  - Yao, Y.
AU  - Divakaran, A.
AU  - Davis, L.
TI  - Stacked Spatio-Temporal Graph Convolutional Networks for Action Segmentation
AB  - We propose novel Stacked Spatio-Temporal Graph Convolutional Networks (Stacked-STGCN) for action segmentation, i.e., predicting and localizing a sequence of actions over long videos. We extend the Spatio-Temporal Graph Convolutional Network (STGCN) originally proposed for skeleton-based action recognition to enable nodes with different characteristics (e.g., scene, actor, object, action, etc.), feature descriptors with varied lengths, and arbitrary temporal edge connections to account for large graph deformation commonly associated with complex activities. We further introduce the stacked hourglass architecture to STGCN to leverage the advantages of an encoder-decoder design for improved generalization performance and localization accuracy. We explore various descriptors such as frame-level VGG, segment-level I3D, RCNN-based object, etc. as node descriptors to enable action segmentation based on joint inference over comprehensive contextual information. We show results on CAD120 (which provides pre-computed node features and edge weights for fair performance comparison across algorithms) as well as a more complex real-world activity dataset, Charades. Our Stacked-STGCN in general achieves 4.0% performance improvement over the best reported results in F1 score on CAD120 and 1.3% in mAP on Charades using VGG features.
PB  - arXiv
PY  - 2018
ST  - Stacked Spatio-Temporal Graph Convolutional Networks for Action Segmentation
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/wacv45572.2020.9093361
ER  -


TY  - GEN
AU  - Li, Y.
AU  - Song, S.
AU  - Li, Y.
AU  - Liu, J.
TI  - Temporal bilinear networks for video action recognition
AB  - Temporal modeling in videos is a fundamental yet challenging problem in computer vision. In this paper, we propose a novel Temporal Bilinear (TB) model to capture the temporal pairwise feature interactions between adjacent frames. Compared with some existing temporal methods which are limited in linear transformations, our TB model considers explicit quadratic bilinear transformations in the temporal domain for motion evolution and sequential relation modeling. We further leverage the factorized bilinear model in linear complexity and a bottleneck network design to build our TB blocks, which also constrains the parameters and computation cost. We consider two schemes in terms of the incorporation of TB blocks and the original 2D spatial convolutions, namely wide and deep Temporal Bilinear Networks (TBN). Finally, we perform experiments on several widely adopted datasets including Kinetics, UCF101 and HMDB51. The effectiveness of our TBNs is validated by comprehensive ablation analyses and comparisons with various state-of-the-art methods.
PB  - arXiv
PY  - 2018
ST  - Temporal bilinear networks for video action recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.1609/aaai.v33i01.33018674
ER  -


TY  - GEN
AU  - Cristescu, B.-C.
AU  - Borsos, Z.
AU  - Lygeros, J.
AU  - Martínez, M.R.
AU  - Rapsomaniki, M.A.
TI  - Inference of the three-dimensional chromatin structure and its temporal behavior
AB  - Understanding the three-dimensional (3D) structure of the genome is essential for elucidating vital biological processes and their links to human disease. To determine how the genome folds within the nucleus, chromosome conformation capture methods such as HiC have recently been employed. However, computational methods that exploit the resulting high-throughput, high-resolution data are still suffering from important limitations. In this work, we explore the idea of manifold learning for the 3D chromatin structure inference and present a novel method, REcurrent Autoencoders for CHromatin 3D structure prediction (REACH-3D). Our framework employs autoencoders with recurrent neural units to reconstruct the chromatin structure. In comparison to existing methods, REACH-3D makes no transfer function assumption and permits dynamic analysis. Evaluating REACH-3D on synthetic data indicated high agreement with the ground truth. When tested on real experimental HiC data, REACH-3D recovered most faithfully the expected biological properties and obtained the highest correlation coefficient with microscopy measurements. Last, REACH-3D was applied to dynamic HiC data, where it successfully modeled chromatin conformation during the cell cycle.
PB  - arXiv
PY  - 2018
ST  - Inference of the three-dimensional chromatin structure and its temporal behavior
Y2  - 2025/05/05/21:54:30
DO  - 10.1186/s12859-016-0894-z
ER  -


TY  - GEN
AU  - Ge, R.
AU  - Gao, J.
AU  - Chen, K.
AU  - Nevatia, R.
TI  - MAC: Mining activity concepts for language-based temporal localization
AB  - We address the problem of language-based temporal localization in untrimmed videos. Compared to temporal localization with fixed categories, this problem is more challenging as the language-based queries not only have no predefined activity list but also may contain complex descriptions. Previous methods address the problem by considering features from video sliding windows and language queries and learning a subspace to encode their correlation, which ignore rich semantic cues about activities in videos and queries. We propose to mine activity concepts from both video and language modalities by applying the actionness score enhanced Activity Concepts based Localizer (ACL). Specifically, the novel ACL encodes the semantic concepts from verb-obj pairs in language queries and leverages activity classifiers’ prediction scores to encode visual concepts. Besides, ACL also has the capability to regress sliding windows as localization results. Experiments show that ACL significantly outperforms state-of-the-arts under the widely used metric, with more than 5% increase on both Charades-STA and TACoS datasets.
PB  - arXiv
PY  - 2018
ST  - MAC
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/wacv.2019.00032
ER  -


TY  - GEN
AU  - Gleason, J.
AU  - Ranjan, R.
AU  - Schwarcz, S.
AU  - Chen, J.-C.
AU  - Chellappa, R.
TI  - A proposal-based solution to spatio-temporal action detection in untrimmed videos
AB  - Existing approaches for spatio-temporal action detection in videos are limited by the spatial extent and temporal duration of the actions. In this paper, we present a modular system for spatio-temporal action detection in untrimmed security videos. We propose a two stage approach. The first stage generates dense spatio-temporal proposals using hierarchical clustering and temporal jittering techniques on frame-wise object detections. The second stage is a Temporal Refinement I3D (TRI-3D) network that performs action classification and temporal refinement on the generated proposals. The object detection-based proposal generation step helps in detecting actions occurring in a small spatial region of a video frame, while temporal jittering and refinement helps in detecting actions of variable lengths. Experimental results on the spatio-temporal action detection dataset - DIVA - show the effectiveness of our system. For comparison, the performance of our system is also evaluated on the THUMOS'14 temporal action detection dataset.
PB  - arXiv
PY  - 2018
ST  - A proposal-based solution to spatio-temporal action detection in untrimmed videos
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/wacv.2019.00021
ER  -


TY  - GEN
AU  - Xu, Y.
AU  - Zhang, C.
AU  - Cheng, Z.
AU  - Pu, S.
AU  - Wu, F.
TI  - Segregated temporal assembly recurrent networks for weakly supervised multiple action detection
AB  - This paper proposes a segregated temporal assembly recurrent (STAR) network for weakly-supervised multiple action detection. The model learns from untrimmed videos with only supervision of video-level labels and makes prediction of intervals of multiple actions. Specifically, we first assemble video clips according to class labels by an attention mechanism that learns class-variable attention weights and thus helps the noise relieving from background or other actions. Secondly, we build temporal relationship between actions by feeding the assembled features into an enhanced recurrent neural network. Finally, we transform the output of recurrent neural network into the corresponding action distribution. In order to generate more precise temporal proposals, we design a score term called segregated temporal gradient-weighted class activation mapping (ST-GradCAM) fused with attention weights. Experiments on THUMOS'14 and ActivityNet1.3 datasets show that our approach outperforms the state-of-the-art weakly-supervised method, and performs at par with the fully-supervised counterparts.
PB  - arXiv
PY  - 2018
ST  - Segregated temporal assembly recurrent networks for weakly supervised multiple action detection
Y2  - 2025/05/05/21:54:30
DO  - 10.1609/aaai.v33i01.33019070
ER  -


TY  - GEN
AU  - Xu, M.
AU  - Gao, M.
AU  - Chen, Y.-T.
AU  - Davis, L.S.
AU  - Crandall, D.J.
TI  - Temporal recurrent networks for online action detection
AB  - Most work on temporal action detection is formulated as an offline problem, in which the start and end times of actions are determined after the entire video is fully observed. However, important real-time applications including surveillance and driver assistance systems require identifying actions as soon as each video frame arrives, based only on current and historical observations. In this paper, we propose a novel framework, Temporal Recurrent Network (TRN), to model greater temporal context of a video frame by simultaneously performing online action detection and anticipation of the immediate future. At each moment in time, our approach makes use of both accumulated historical evidence and predicted future information to better recognize the action that is currently occurring, and integrates both of these into a unified end-to-end architecture. We evaluate our approach on two popular online action detection datasets, HDD and TVSeries, as well as another widely used dataset, THUMOS'14. The results show that TRN significantly outperforms the state-of-the-art.
PB  - arXiv
PY  - 2018
ST  - Temporal recurrent networks for online action detection
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/iccv.2019.00563
ER  -


TY  - GEN
AU  - Koh, R.G.L.
AU  - Nachman, A.I.
AU  - Zariffa, J.
TI  - Classification of naturally evoked compound action potentials in peripheral nerve spatiotemporal recordings
AB  - Peripheral neural signals have the potential to provide the necessary motor, sensory or autonomic information for robust control in many neuroprosthetic and neuromodulation applications. However, developing methods to recover information encoded in these signals is a significant challenge. We introduce the idea of using spatiotemporal signatures extracted from multi-contact nerve cuff electrode recordings to classify naturally evoked compound action potentials (CAP). 9 Long-Evan rats were implanted with a 56-channel nerve cuff on the sciatic nerve. Afferent activity was selectively evoked in the different fascicles of the sciatic nerve (tibial, peroneal, sural) using mechano-sensory stimuli. Spatiotemporal signatures of recorded CAPs were used to train three different classifiers. Performance was measured based on the classification accuracy, F1-score, and the ability to reconstruct original firing rates of neural pathways. The mean classification accuracies, for a 3-class problem, for the best performing classifier was 0.686 ± 0.126 and corresponding mean F1-score was 0.605 ± 0.212. The mean Pearson correlation coefficients between the original firing rates and estimated firing rates found for the best classifier was 0.728 ± 0.276. The proposed method demonstrates the possibility of classifying individual naturally evoked CAPs in peripheral neural signals recorded from extraneural electrodes, allowing for more precise control signals in neuroprosthetic applications.
PB  - bioRxiv
PY  - 2018
ST  - Classification of naturally evoked compound action potentials in peripheral nerve spatiotemporal recordings
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/469874
ER  -


TY  - GEN
AU  - Hu, G.
AU  - Cui, B.
AU  - Yu, S.
TI  - Skeleton-based action recognition with synchronous local and non-local spatio-temporal learning and frequency attention
AB  - Benefiting from its succinctness and robustness, skeleton-based action recognition has recently attracted much attention. Most existing methods utilize local networks (e.g. recurrent, convolutional, and graph convolutional networks) to extract spatio-temporal dynamics hierarchically. As a consequence, the local and non-local dependencies, which contain more details and semantics respectively, are asynchronously captured in different level of layers. Moreover, existing methods are limited to the spatio-temporal domain and ignore information in the frequency domain. To better extract synchronous detailed and semantic information from multi-domains, we propose a residual frequency attention (rFA) block to focus on discriminative patterns in the frequency domain, and a synchronous local and non-local (SLnL) block to simultaneously capture the details and semantics in the spatio-temporal domain. Besides, a soft-margin focal loss (SMFL) is proposed to optimize the learning whole process, which automatically conducts data selection and encourages intrinsic margins in classifiers. Our approach significantly outperforms other state-of-the-art methods on several large-scale datasets.
PB  - arXiv
PY  - 2018
ST  - Skeleton-based action recognition with synchronous local and non-local spatio-temporal learning and frequency attention
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icme.2019.00212
ER  -


TY  - GEN
AU  - Kong, W.
AU  - Li, N.
AU  - Liu, S.
AU  - Li, T.
AU  - Li, G.
TI  - BLP-Boundary likelihood pinpointing networks for accurate temporal action localization
AB  - Despite tremendous progress achieved in temporal action detection, state-of-the-art methods still suffer from the sharp performance deterioration when localizing the starting and ending temporal action boundaries. Although most methods apply boundary regression paradigm to tackle this problem, we argue that the direct regression lacks detailed enough information to yield accurate temporal boundaries. In this paper, we propose a novel Boundary Likelihood Pinpointing (BLP) network to alleviate this deficiency of boundary regression and improve the localization accuracy. Given a loosely localized search interval that contains an action instance, BLP casts the problem of localizing temporal boundaries as that of assigning probabilities on each equally divided unit of this interval. These generated probabilities provide useful information regarding the boundary location of the action inside this search interval. Based on these probabilities, we introduce a boundary pinpointing paradigm to pinpoint the accurate boundaries under a simple probabilistic framework. Compared with other C3D feature based detectors, extensive experiments demonstrate that BLP significantly improves the localization performance of recent state-of-the-art detectors, and achieves competitive detection mAP on both THUMOS' 14 and ActivityNet datasets, particularly when the evaluation tloU is high.
PB  - arXiv
PY  - 2018
ST  - BLP-Boundary likelihood pinpointing networks for accurate temporal action localization
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icassp.2019.8682466
ER  -


TY  - GEN
AU  - He, D.
AU  - Zhou, Z.
AU  - Gan, C.
AU  - Wang, L.
AU  - Wen, S.
TI  - Stnet: Local and global spatial-temporal modeling for action recognition
AB  - Despite the success of deep learning for static image understanding, it remains unclear what are the most effective network architectures for the spatial-temporal modeling in videos. In this paper, in contrast to the existing CNN+RNN or pure 3D convolution based approaches, we explore a novel spatial temporal network (StNet) architecture for both local and global spatial-temporal modeling in videos. Particularly, StNet stacks N successive video frames into a super-image which has 3N channels and applies 2D convolution on super-images to capture local spatial-temporal relationship. To model global spatial-temporal relationship, we apply temporal convolution on the local spatial-temporal feature maps. Specifically, a novel temporal Xception block is proposed in StNet. It employs a separate channel-wise and temporal-wise convolution over the feature sequence of video. Extensive experiments on the Kinetics dataset demonstrate that our framework outperforms several state-of-the-art approaches in action recognition and can strike a satisfying trade-off between recognition accuracy and model complexity. We further demonstrate the generalization performance of the leaned video representations on the UCF101 dataset.
PB  - arXiv
PY  - 2018
ST  - Stnet
Y2  - 2025/05/05/21:54:30
DO  - 10.1609/aaai.v33i01.33018401
ER  -


TY  - GEN
AU  - Talpur, A.
AU  - Zhang, Y.
TI  - A Study of Tourist Sequential Activity Pattern through Location Based Social Network (LBSN)
AB  - Sequential Pattern Mining (SPM) is an important component in establishing patterns and mining trends of certain activities. In the past, this technique has been used in various fields such as consumer-watch, making future predictions and analyzing and interpreting large datasets for deeply embedded rules and associations. The qualitative details of Singapore tourists' foursquare check-ins, represented in a tabular form, is an example of a sequential database. Therefore, the Pattern-Growth method which uses Prefix-Span Algorithm is used in this study to obtain the Tourist Sequential Activity Patterns. Insights into tourist movement and activity patterns is deemed beneficial for the tourism sector in many ways, such as designing better travel packages for tourists, maximizing the tourist activity participation and meeting the tourist demands. This research proposes to adopt mobile social media data for effective capturing of tourist activity information in Singapore and utilizes advanced data mining techniques for extracting valuable insights into tourist behavior. The proposed methods and findings of the study have the potential to support tourism managers and policy makers in making better decisions in tourism destination management.
PB  - arXiv
PY  - 2018
ST  - A Study of Tourist Sequential Activity Pattern through Location Based Social Network (LBSN)
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icot.2018.8705895
ER  -


TY  - GEN
AU  - Su, H.
AU  - Zhao, X.
AU  - Lin, T.
TI  - Cascaded pyramid mining network for weakly supervised temporal action localization
AB  - Weakly supervised temporal action localization, which aims at temporally locating action instances in untrimmed videos using only video-level class labels during training, is an important yet challenging problem in video analysis. Many current methods adopt the \localization by classification" framework: first do video classification, then locate temporal area contributing to the results most. However, this framework fails to locate the entire action instances and gives little consideration to the local context. In this paper, we present a novel architecture called Cas-caded Pyramid Mining Network (CPMN) to address these issues using two effective modules. First, to discover the entire temporal interval of specific action, we design a two-stage cascaded module with proposed Online Adversarial Erasing (OAE) mechanism, where new and complementary regions are mined through feeding the erased feature maps of discovered regions back to the system. Second, to exploit hierarchical contextual information in videos and reduce missing detections, we design a pyramid module which produces a scale-invariant attention map through combining the feature maps from different levels. Final, we aggregate the results of two modules to perform action localization via locating high score areas in temporal Class Activation Sequence (CAS). Extensive experiments conducted on THUMOS14 and ActivityNet-1.3 datasets demonstrate the effectiveness of our method.
PB  - arXiv
PY  - 2018
ST  - Cascaded pyramid mining network for weakly supervised temporal action localization
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/978-3-030-20890-5_36
ER  -


TY  - GEN
AU  - Lohani, S.
AU  - Martig, A.K.
AU  - Deisseroth, K.
AU  - Witten, I.B.
AU  - Moghaddam, B.
TI  - Dopamine modulation of prefrontal cortex activity is manifold and operates at multiple temporal and spatial scales
AB  - While the function of dopamine in subcortical structures is largely limited to reward and movement, dopamine neurotransmission in the prefrontal cortex (PFC) is critical to a multitude of temporally and functionally diverse processes such as attention, working memory, behavioral flexibility, action selection, and stress adaptation. How does dopamine influence PFC computation of multiple temporally diverse functions? Here we find causation between sustained and burst patterns of phasic dopamine neuron activation and contemporaneous modulation of PFC neuronal activity at multiple spatio-temporal scales. These include a multidirectional and weak impact on individual PFC neuron rate activity and a robust influence on coordinated ensemble activity, gamma oscillations, and gamma-theta coupling that persisted for minutes. In addition, PFC network responses to burst pattern of dopamine firing were selectively strengthened in behaviorally active states. Thus, dopamine modulation of PFC is spatiotemporally diverse and is dictated by the pattern of dopamine neuron activation and behavioral state. These findings provide insight on the multiplex pattern of modulation by dopamine that may influence PFC computation of temporally diverse functions.
PB  - bioRxiv
PY  - 2018
ST  - Dopamine modulation of prefrontal cortex activity is manifold and operates at multiple temporal and spatial scales
Y2  - 2025/05/05/21:54:30
DO  - 10.1016/j.celrep.2019.03.012
ER  -


TY  - GEN
AU  - Wang, W.
AU  - Wu, Y.
AU  - Liu, H.
AU  - Wang, S.
AU  - Cheng, J.
TI  - Temporal Action Detection by Joint Identification-Verification
AB  - —Temporal action detection aims at not only recognizing action category but also detecting start time and end time for each action instance in an untrimmed video. The key challenge of this task is to accurately classify the action and determine the temporal boundaries of each action instance. In temporal action detection benchmark: THUMOS 2014, large variations exist in the same action category while many similarities exist in different action categories, which always limit the performance of temporal action detection. To address this problem, we propose to use joint Identification-Verification network to reduce the intra-action variations and enlarge inter-action differences. The joint Identification-Verification network is a siamese network based on 3D ConvNets, which can simultaneously predict the action categories and the similarity scores for the input pairs of video proposal segments. Extensive experimental results on the challenging THUMOS 2014 dataset demonstrate the effectiveness of our proposed method compared to the existing state-of-art methods for temporal action detection in untrimmed videos.
PB  - arXiv
PY  - 2018
ST  - Temporal Action Detection by Joint Identification-Verification
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/icpr.2018.8545487
ER  -


TY  - GEN
AU  - Singha, J.
AU  - Gupte, N.
TI  - Chimera states in coupled map lattices: Spatiotemporally intermittent behavior and an equivalent cellular automaton
AB  - We study the existence of chimera states, i.e. mixed states, in a globally coupled sine circle map lattice, with different strengths of inter-group and intra-group coupling. We find that at specific values of the parameters of the CML, a completely random initial condition evolves to chimera states, having a phase synchronised and a phase desynchronised group, where the space time variation of the phases of the maps in the desynchronised group shows structures similar to spatiotemporally intermittent regions. Using the complex order parameter we obtain a phase diagram that identifies the region in the parameter space which supports chimera states of this type, as well as other types of phase configurations. An equivalent cellular automaton is obtained which shows space time behaviour similar to the CML. The chimera regions show coexisting deterministic and probabilistic behaviour in the subgroup probabilities which show a transition to purely probabilistic behaviour at the boundaries of the region. We also derive a mean field equation for this cellular automaton and compare its solutions with the corresponding phase configurations obtained in the parameter space, and show that its behaviour matches with the behaviour seen for the CML.
PB  - arXiv
PY  - 2018
ST  - Chimera states in coupled map lattices
Y2  - 2025/05/05/21:54:30
DO  - 10.1063/5.0016056
ER  -


TY  - GEN
AU  - Meng, L.
AU  - Zhao, B.
AU  - Chang, B.
AU  - Tung, F.
AU  - Sigal, L.
TI  - Interpretable spatio-temporal attention for video action recognition
AB  - Inspired by the observation that humans are able to process videos efficiently by only paying attention where and when it is needed, we propose an interpretable and easy plug-in spatial-temporal attention mechanism for video action recognition. For spatial attention, we learn a saliency mask to allow the model to focus on the most salient parts of the feature maps. For temporal attention, we employ a convolutional LSTM based attention mechanism to identify the most relevant frames from an input video. Further, we propose a set of regularizers to ensure that our attention mechanism attends to coherent regions in space and time. Our model not only improves video action recognition accuracy, but also localizes discriminative regions both spatially and temporally, despite being trained in a weakly-supervised manner with only classification labels (no bounding box labels or time frame temporal labels). We evaluate our approach on several public video action recognition datasets with ablation studies. Furthermore, we quantitatively and qualitatively evaluate our model's ability to localize discriminative regions spatially and critical frames temporally. Experimental results demonstrate the efficacy of our approach, showing superior or comparable accuracy with the state-of-the-art methods while increasing model interpretability.
PB  - arXiv
PY  - 2018
ST  - Interpretable spatio-temporal attention for video action recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/iccvw.2019.00189
ER  -


TY  - GEN
AU  - Oishi, K.
AU  - van den Berg, D.L.C.
AU  - Guillemot, F.
TI  - Temporal control of cortico-thalamic neuron specification by regulation of Neurogenin activity and Polycomb repressive complexes
AB  - Neural progenitor cells (NPCs) in the embryonic mammalian neocortex generate different neuronal subtypes sequentially. A long-standing hypothesis to account for this temporal fate specification process is that NPCs change their differentiation potential over time. However, the molecular mechanisms underlying these temporal changes in NPC properties are poorly understood. Here we show that Neurogenin1 and Neurogenin2 (Neurog1/2), two proneural transcription factors expressed in NPCs throughout cortical neurogenesis, specify the identity of one of the first cortical neuron subtypes generated, layer 6 cortico-thalamic neurons (CTNs). We found that Neurog1/2 specify the CTN fate through regulation of the cortical fate determinants Fezf2 and Foxp2 and that this Neurog-induced programme becomes inactive after the period of CTN production. Two independent mechanisms contribute to the arrest of CTN neuron generation at the end of layer 6 neurogenesis, including a reduction in the transcriptional activity of Neurog1/2 and the deposition of epigenetic repressive modifications mediated by Polycomb repressive complexes at the Foxp2 gene. Therefore, the duration of production of a cortical neuron subtype is controlled by multiple locking mechanisms involving both transcriptional and epigenetic processes.
PB  - bioRxiv
PY  - 2018
ST  - Temporal control of cortico-thalamic neuron specification by regulation of Neurogenin activity and Polycomb repressive complexes
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/431684
ER  -


TY  - GEN
AU  - Spreizer, S.
AU  - Aertsen, A.
AU  - Kumar, A.
TI  - From space to time: Spatial inhomogeneities lead to the emergence of spatio-temporal activity sequences in spiking neuronal networks
AB  - Spatio-temporal sequences of neuronal activity are observed in many brain regions in a variety of tasks and are thought to form the basis of any meaningful behavior. Mechanisms by which a neuronal network can generate spatio-temporal activity sequences have remained obscure. Existing models are biologically untenable because they require manual embedding of a feedforward network within a random network or supervised learning to train the connectivity of a network to generate sequences. Here, we propose a biologically plausible, generative rule to create spatio-temporal activity sequences in a network model of spiking neurons with distance dependent connectivity. We show that the emergence of spatio-temporal activity sequences requires: (1) individual neurons preferentially project a small fraction of their axons in a specific direction, and (2) the preferential projection direction of neighboring neurons is similar. Thus, an anisotropic but correlated connectivity of neuron groups suffices to generate spatio-temporal activity sequences in an otherwise random neuronal network model.
PB  - bioRxiv
PY  - 2018
ST  - From space to time
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/428649
ER  -


TY  - GEN
AU  - Ho, H.T.
AU  - Carvajal, T.M.
AU  - Bautista, J.R.
AU  - Hernandez, L.F.T.
AU  - Watanabe, K.
TI  - Using Google Trends to Examine the Spatio-Temporal Incidence and Behavioral Patterns of Dengue Disease: A Case Study in Metropolitan Manila, Philippines
AB  - Dengue is a major public health concern and an economic burden in the Philippines. Despite the country’s improved dengue surveillance, it still suffers from various setbacks and therefore needs to be complemented with alternative approaches. Previous studies have demonstrated the potential of internet-based surveillance such as Google Dengue Trends (GDT) in supplementing current epidemiological methods for predicting future dengue outbreaks and patterns. With this, our study aims to assess the temporal relationship of GDT and dengue incidence in Metropolitan Manila from previous years and examine web search behavior of the population towards the disease. The study collated and organized the population statistics and reported dengue cases in Metropolitan Manila from respective government agencies to calculate the spatial and temporal dengue incidence. The relative search volume of the term ‘dengue’ and top dengue-related search queries in Metropolitan Manila were obtained and organized from the Google trends platform. Data processing of GDT and dengue incidence was performed by conducting an ‘adjustment’ procedure and subsequently used for correlation and cross-correlation analyses. Moreover, a thematic analysis was employed on the top dengue-related search queries. Results revealed a high temporal relationship between GDT and dengue incidence when either one of the variables is adjusted. Cross-correlation showed that there is delayed effect (1-2 weeks) of GDT to dengue incidence, demonstrating its potential in predicting future dengue outbreaks and patterns in Metropolitan Manila. Thematic analysis of dengue-related search queries indicated 5 categories namely; (a) dengue, (b) sign and symptoms of dengue, (c) treatment and prevention, (d) mosquito and (e) other diseases where the majority of the search queries was ‘signs and symptoms’ which indicate the health-seeking behavior of the population towards the disease.
PB  - bioRxiv
PY  - 2018
ST  - Using Google Trends to Examine the Spatio-Temporal Incidence and Behavioral Patterns of Dengue Disease
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/424630
ER  -


TY  - GEN
AU  - Alward, B.A.
AU  - Hilliard, A.T.
AU  - York, R.A.
AU  - Fernald, R.D.
TI  - Hormonal regulation of social ascent and temporal patterns of behavior in an African cichlid
AB  - For many species, social rank determines which individuals perform certain social behaviors and when. Higher ranking or dominant (DOM) individuals maintain status through aggressive interactions and perform courtship behaviors while non-dominant (ND) individuals do not. In some species ND individuals ascend (ASC) in social rank when the opportunity arises. Many important questions related to the mechanistic basis of social ascent remain to be answered. We probed whether androgen signaling regulates social ascent in male Astatotilapia burtoni, an African cichlid whose social hierarchy can be readily controlled in the laboratory. As expected, androgen receptor (AR) antagonism abolished reproductive behavior during social ascent. However, we discovered multiple AR-dependent-and AR-independent-temporal behavioral patterns that typify social ascent and dominance. AR antagonism in ASC males reduced the speed of behavioral performance compared to DOM males. Socially ascending males, independent of AR activation, were more likely than DOM males to follow aggressive displays with another aggressive display. Further analyses revealed differences in the sequencing of aggressive and courtship behaviors, wherein DOM males were more likely than ASC males to follow male-directed aggression with courtship displays. Strikingly, this difference was driven mostly by ASC males taking longer to transition from aggression to courtship, suggesting ASC males can perform certain DOM-typical temporal behavioral patterns. Our results indicate androgen signaling drives social ascent, but hormonal signaling and social experience shape the full suite of DOM-typical behavioral patterns.
PB  - bioRxiv
PY  - 2018
ST  - Hormonal regulation of social ascent and temporal patterns of behavior in an African cichlid
Y2  - 2025/05/05/21:54:30
DO  - 10.1016/j.yhbeh.2018.12.010
ER  -


TY  - GEN
AU  - Pereira, U.
AU  - Brunel, N.
TI  - Unsupervised Learning of Persistent and Sequential Activity
AB  - Two strikingly distinct types of activity have been observed in various brain structures during delay periods of delayed response tasks: Persistent activity (PA), in which a sub-population of neurons maintains an elevated firing rate throughout an entire delay period; and Sequential activity (SA), in which sub-populations of neurons are activated sequentially in time. It has been hypothesized that both types of dynamics can be ‘learned’ by the relevant networks from the statistics of their inputs, thanks to mechanisms of synaptic plasticity. However, the necessary conditions for a synaptic plasticity rule and input statistics to learn these two types of dynamics in a stable fashion are still unclear. In particular, it is unclear whether a single learning rule is able to learn both types of activity patterns, depending on the statistics of the inputs driving the network. Here, we first characterize the complete bifurcation diagram of a firing rate model of multiple excitatory populations with an inhibitory mechanism, as a function of the parameters characterizing its connectivity. We then investigate how an unsupervised temporally asymmetric Hebbian plasticity rule shapes the dynamics of the network. Consistent with previous studies, we find that for stable learning of PA and SA, an additional stabilization mechanism, such as multiplicative homeostatic plasticity, is necessary. Using the bifurcation diagram derived for fixed connectivity, we study analytically the temporal evolution and the steady state of the learned recurrent architecture as a function of parameters characterizing the external inputs. Slow changing stimuli lead to PA, while fast changing stimuli lead to SA. Our network model shows how a network with plastic synapses can stably and flexibly learn PA and SA in an unsupervised manner.
PB  - bioRxiv
PY  - 2018
ST  - Unsupervised Learning of Persistent and Sequential Activity
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/414813
ER  -


TY  - GEN
AU  - Song, X.
AU  - Lan, C.
AU  - Zeng, W.
AU  - Yang, J.
AU  - Sun, X.
TI  - Temporal-spatial mapping for action recognition
AB  - Deep learning models have enjoyed great success for image related computer vision tasks like image classification and object detection. For video related tasks like human action recognition, however, the advancements are not as significant yet. The main challenge is the lack of effective and efficient models in modeling the rich temporal spatial information in a video. We introduce a simple yet effective operation, termed Temporal-Spatial Mapping (TSM), for capturing the temporal evolution of the frames by jointly analyzing all the frames of a video. We propose a video level 2D feature representation by transforming the convolutional features of all frames to a 2D feature map, referred to as VideoMap. With each row being the vectorized feature representation of a frame, the temporalspatial features are compactly represented, while the temporal dynamic evolution is also well embedded. Based on the VideoMap representation, we further propose a temporal attention model within a shallow convolutional neural network to efficiently exploit the temporal-spatial dynamics. The experiment results show that the proposed scheme achieves the state-of-the-art performance, with 4.2% accuracy gain over Temporal Segment Network (TSN), a competing baseline method, on the challenging human action benchmark dataset HMDB51.
PB  - arXiv
PY  - 2018
ST  - Temporal-spatial mapping for action recognition
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/tcsvt.2019.2896029
ER  -


TY  - GEN
AU  - Moreau, Q.
AU  - Candidi, M.
AU  - Era, V.
AU  - Tieri, G.
AU  - Aglioti, S.M.
TI  - Frontal and occipito-temporal Theta activity as marker of error monitoring in Human-Avatar joint performance
AB  - Discrepancies between sensory predictions and action outcome are at the base of error coding. However, these phenomena have mainly been studied focusing on individual performance. Here, we explored prediction errors during a human-avatar motor interaction and focused on both the classical frontal error-related brain responses and the activity of the action observation network. Our motor interaction paradigm required healthy individuals to synchronize their reach-to-grasp movements with those of a virtual partner in conditions that did (Interactive) or did not require (Cued) movement prediction and adaptation to the partner’s actions. Crucially, in 30% of the trials the virtual partner suddenly and unpredictably changed its movement trajectory thereby violating the human participant’s expectation. These changes elicited error-related neuromarkers (ERN/Pe - Theta/Alpha modulations) over fronto-central electrodes mainly during the Interactive condition. Source localization and connectivity analyses showed that the frontal Theta activity induced by violations of the expected interactive movements was in phase with occipito-temporal Theta activity. These results expand current knowledge about the neural correlates of on-line motor interactions linking the frontal error-monitoring system to visual, body motion-related, responses.
PB  - bioRxiv
PY  - 2018
ST  - Frontal and occipito-temporal Theta activity as marker of error monitoring in Human-Avatar joint performance
Y2  - 2025/05/05/21:54:30
DO  - 10.1101/402149
ER  -


TY  - GEN
AU  - Saeed, A.
AU  - Ozcelebi, T.
AU  - Trajanovski, S.
AU  - Lukkien, J.
TI  - Learning behavioral context recognition with multi-stream temporal convolutional networks
AB  - Smart devices of everyday use (such as smartphones and wearables) are increasingly integrated with sensors that provide immense amounts of information about a person's daily life such as behavior and context. The automatic and unobtrusive sensing of behavioral context can help develop solutions for assisted living, fitness tracking, sleep monitoring, and several other fields. Towards addressing this issue, we raise the question: can a machine learn to recognize a diverse set of contexts and activities in a real-life through joint learning from raw multi-modal signals (e.g. accelerometer, gyroscope and audio etc.)? In this paper, we propose a multi-stream temporal convolutional network to address the problem of multi-label behavioral context recognition. A four-stream network architecture handles learning from each modality with a contextualization module which incorporates extracted representations to infer a user's context. Our empirical evaluation suggests that a deep convolutional network trained end-to-end achieves an optimal recognition rate. Furthermore, the presented architecture can be extended to include similar sensors for performance improvements and handles missing modalities through multi-task learning without any manual feature engineering on highly imbalanced and sparsely labeled dataset.
PB  - arXiv
PY  - 2018
ST  - Learning behavioral context recognition with multi-stream temporal convolutional networks
Y2  - 2025/05/05/21:54:30
DO  - 10.1007/978-3-030-29888-3_16
ER  -


TY  - GEN
AU  - Ahsan, U.
AU  - Madhok, R.
AU  - Essa, I.
TI  - Video jigsaw: Unsupervised learning of spatiotemporal context for video action recognition
AB  - We propose a self-supervised learning method to jointly reason about spatial and temporal context for video recognition. Recent self-supervised approaches have used spatial context [9, 34] as well as temporal coherency [32] but a combination of the two requires extensive preprocessing such as tracking objects through millions of video frames [59] or computing optical flow to determine frame regions with high motion [30]. We propose to combine spatial and temporal context in one self-supervised framework without any heavy preprocessing. We divide multiple video frames into grids of patches and train a network to solve jigsaw puzzles on these patches from multiple frames. So the network is trained to correctly identify the position of a patch within a video frame as well as the position of a patch over time. We also propose a novel permutation strategy that outperforms random permutations while significantly reducing computational and memory constraints. We use our trained network for transfer learning tasks such as video activity recognition and demonstrate the strength of our approach on two benchmark video action recognition datasets without using a single frame from these datasets for unsupervised pretraining of our proposed video jigsaw network.
PB  - arXiv
PY  - 2018
ST  - Video jigsaw
Y2  - 2025/05/05/21:54:30
DO  - 10.1109/wacv.2019.00025
ER  -


TY  - GEN
AU  - Ding, G.
AU  - Yao, A.
TI  - Temporal Action Segmentation with High-level Complex Activity Labels
AB  - The temporal action segmentation task segments videos temporally and predicts action labels for all frames. Fully supervising such a segmentation model requires dense frame-wise action annotations, which are expensive and tedious to collect. This work is the first to propose a Constituent Action Discovery (CAD) framework that only requires the video-wise high-level complex activity label as supervision for temporal action segmentation. The proposed approach automatically discovers constituent video actions using an activity classification task. Specifically, we define a finite number of latent action prototypes to construct video-level dual representations with which these prototypes are learned collectively through the activity classification training. This setting endows our approach with the capability to discover potentially shared actions across multiple complex activities. Due to the lack of action-level supervision, we adopt the Hungarian matching algorithm to relate latent action prototypes to ground truth semantic classes for evaluation. We show that with the high-level supervision, the Hungarian matching can be extended from the existing video and activity levels to the global level. The global-level matching allows for action sharing across activities, which has never been considered in the literature before. Extensive experiments demonstrate that our discovered actions can help perform temporal action segmentation and activity recognition tasks.
PB  - arXiv
PY  - 2021
ST  - Temporal Action Segmentation with High-level Complex Activity Labels
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/tmm.2022.3231099
ER  -


TY  - GEN
AU  - Gong, S.
AU  - Mo, X.
AU  - Cao, R.
AU  - Tu, W.
AU  - Bai, R.
TI  - Spatio-temporal Parking behaviour forecasting and analysis before and during COVID-19
AB  - Parking demand forecasting and behaviour analysis have received increasing attention in recent years because of their critical role in mitigating traffic congestion and understanding travel behaviours. However, previous studies usually only consider temporal dependence but ignore the spatial correlations among parking lots for parking prediction. This is mainly due to the lack of direct physical connections or observable interactions between them. Thus, how to quantify the spatial correlation remains a significant challenge. To bridge the gap, in this study, we propose a spatial-aware parking prediction framework, which includes two steps, i.e. spatial connection graph construction and spatio-temporal forecasting. A case study in Ningbo, China is conducted using parking data of over one million records before and during COVID-19. The results show that the approach is superior on parking occupancy forecasting than baseline methods, especially for the cases with high temporal irregularity such as during COVID-19. Our work has revealed the impact of the pandemic on parking behaviour and also accentuated the importance of modelling spatial dependence in parking behaviour forecasting, which can benefit future studies on epidemiology and human travel behaviours.
PB  - arXiv
PY  - 2021
ST  - Spatio-temporal Parking behaviour forecasting and analysis before and during COVID-19
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/icassp49357.2023.10095012
ER  -


TY  - GEN
AU  - Huang, L.
AU  - Wang, L.
AU  - Li, H.
TI  - Foreground-action consistency network forweakly supervised temporal action localization
AB  - As a challenging task of high-level video understanding, weakly supervised temporal action localization has been attracting increasing attention. With only video annotations, most existing methods seek to handle this task with a localization-by-classification framework, which generally adopts a selector to select snippets of high probabilities of actions or namely the foreground. Nevertheless, the existing foreground selection strategies have a major limitation of only considering the unilateral relation from foreground to actions, which cannot guarantee the foreground-action consistency. In this paper, we present a framework named FAC-Net based on the I3D backbone, on which three branches are appended, named class-wise foreground classification branch, class-agnostic attention branch and multiple instance learning branch. First, our class-wise foreground classification branch regularizes the relation between actions and foreground to maximize the foreground-background separation. Besides, the class-agnostic attention branch and multiple instance learning branch are adopted to regularize the foregroundaction consistency and help to learn a meaningful foreground classifier. Within each branch, we introduce a hybrid attention mechanism, which calculates multiple attention scores for each snippet, to focus on both discriminative and less-discriminative snippets to capture the full action boundaries. Experimental results on THUMOS14 and ActivityNet1.3 demonstrate the state-of-the-art performance of our method. Our code is available at https://github.com/LeonHLJ/FAC-Net.
PB  - arXiv
PY  - 2021
ST  - Foreground-action consistency network forweakly supervised temporal action localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/iccv48922.2021.00790
ER  -


TY  - GEN
AU  - Xu, X.
AU  - Wang, Y.
AU  - Wang, L.
AU  - Yu, B.
AU  - Jia, J.
TI  - Conditional temporal variational autoencoder for action video prediction
AB  - To synthesize a realistic action sequence based on a single human image, it is crucial to model both motion patterns and diversity in the action video. This paper proposes an Action Conditional Temporal Variational AutoEncoder (ACT-VAE) to improve motion prediction accuracy and capture movement diversity. ACT-VAE predicts pose sequences for an action clips from a single input image. It is implemented as a deep generative model that maintains temporal coherence according to the action category with a novel temporal modeling on latent space. Further, ACT-VAE is a general action sequence prediction framework. When connected with a plug-and-play Pose-to-Image (P2I) network, ACT-VAE can synthesize image sequences. Extensive experiments bear out our approach can predict accurate pose and synthesize realistic image sequences, surpassing state-of-the-art approaches. Compared to existing methods, ACT-VAE improves model accuracy and preserves diversity.
PB  - arXiv
PY  - 2021
ST  - Conditional temporal variational autoencoder for action video prediction
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/s11263-023-01832-8
ER  -


TY  - GEN
AU  - Yang, Y.
TI  - Spatio-temporal human action recognition modelwith flexible-interval sampling and normalization
AB  - Human action recognition [?] is a well-known computer vision and pattern recognition task of identifying which action a man is actually doing. Extracting the keypoint information of a single human with both spatial and temporal features of action sequences plays an essential role to accomplish the task [?], [?], [?], [?], [?], [?], [?]. In this paper, we propose a human action system for Red-Green-Blue(RGB) input video with our own designed module. Based on the efficient Gated Recurrent Unit(GRU) [?], [?], [?] for spatio-temporal feature extraction, we add another sampling module and normalization module to improve the performance of the model in order to recognize the human actions. Furthermore, we build a novel dataset with a similar background and discriminative actions for both human keypoint prediction and behavior recognition. To get a better result, we retrain the pose model with our new dataset to get better performance. Experimental results demonstrate the effectiveness of the proposed model on our own human behavior recognition dataset and some public datasets.
PB  - arXiv
PY  - 2021
ST  - Spatio-temporal human action recognition modelwith flexible-interval sampling and normalization
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-319-03731-8_47
ER  -


TY  - GEN
AU  - Cao, M.
AU  - Zhang, C.
AU  - Chen, L.
AU  - Shou, M.Z.
AU  - Zou, Y.
TI  - Deep Motion Prior for Weakly-Supervised Temporal Action Localization
AB  - Weakly-Supervised Temporal Action Localization (WSTAL) aims to localize actions in untrimmed videos with only video-level labels. Currently, most state-of-the-art WSTAL methods follow a Multi-Instance Learning (MIL) pipeline: producing snippet-level predictions first and then aggregating to the video-level prediction. However, we argue that existing methods have overlooked two important drawbacks: 1) inadequate use of motion information and 2) the incompatibility of prevailing cross-entropy training loss. In this paper, we analyze that the motion cues behind the optical flow features are complementary informative. Inspired by this, we propose to build a context-dependent motion prior, termed as motionness. Specifically, a motion graph is introduced to model motionness based on the local motion carrier (e.g., optical flow). In addition, to highlight more informative video snippets, a motion-guided loss is proposed to modulate the network training conditioned on motionness scores. Extensive ablation studies confirm that motionness efficaciously models action-of-interest, and the motion-guided loss leads to more accurate results. Besides, our motion-guided loss is a plug-and-play loss function and is applicable with existing WSTAL methods. Without loss of generality, based on the standard MIL pipeline, our method achieves new state-of-the-art performance on three challenging benchmarks, including THUMOS’14, ActivityNet v1.2 and v1.3.
PB  - arXiv
PY  - 2021
ST  - Deep Motion Prior for Weakly-Supervised Temporal Action Localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/tip.2022.3193752
ER  -


TY  - GEN
AU  - Lee, P.
AU  - Byun, H.
TI  - Learning action completeness from points for weakly-supervised temporal action localization
AB  - We tackle the problem of localizing temporal intervals of actions with only a single frame label for each action instance for training. Owing to label sparsity, existing work fails to learn action completeness, resulting in fragmentary action predictions. In this paper, we propose a novel framework, where dense pseudo-labels are generated to provide completeness guidance for the model. Concretely, we first select pseudo background points to supplement point-level action labels. Then, by taking the points as seeds, we search for the optimal sequence that is likely to contain complete action instances while agreeing with the seeds. To learn completeness from the obtained sequence, we introduce two novel losses that contrast action instances with background ones in terms of action score and feature similarity, respectively. Experimental results demonstrate that our completeness guidance indeed helps the model to locate complete action instances, leading to large performance gains especially under high IoU thresholds. Moreover, we demonstrate the superiority of our method over existing state-ofthe-art methods on four benchmarks: THUMOS'14, GTEA, BEOID, and ActivityNet. Notably, our method even performs comparably to recent fully-supervised methods, at the 6× cheaper annotation cost. Our code is available at https://github.com/Pilhyeon.
PB  - arXiv
PY  - 2021
ST  - Learning action completeness from points for weakly-supervised temporal action localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/iccv48922.2021.01339
ER  -


TY  - GEN
AU  - Chen, T.
AU  - Zhou, D.
AU  - Wang, J.
AU  - He, X.
AU  - Ding, E.
TI  - Learning multi-granular spatio-temporal graph network for skeleton-based action recognition
AB  - The task of skeleton-based action recognition remains a core challenge in human-centred scene understanding due to the multiple granularities and large variation in human motion. Existing approaches typically employ a single neural representation for different motion patterns, which has difficulty in capturing fine-grained action classes given limited training data. To address the aforementioned problems, we propose a novel multi-granular spatiotemporal graph network for skeleton-based action classification that jointly models the coarse- and fine-grained skeleton motion patterns. To this end, we develop a dual-head graph network consisting of two interleaved branches, which enables us to extract features at two spatio-temporal resolutions in an effective and efficient manner. Moreover, our network utilises a cross-head communication strategy to mutually enhance the representations of both heads. We conducted extensive experiments on three large-scale datasets, namely NTU RGB+D 60, NTU RGB+D 120, and Kinetics- Skeleton, and achieves the state-of-the-art performance on all the benchmarks, which validates the effectiveness of our method.
PB  - arXiv
PY  - 2021
ST  - Learning multi-granular spatio-temporal graph network for skeleton-based action recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1145/3474085.3475574
ER  -


TY  - GEN
AU  - Chen, Z.
AU  - Wu, Y.
AU  - Zhang, N.
AU  - Wang, M.
AU  - Hu, Z.
TI  - High temporal resolution total-body dynamic PET imaging based on pixel-level time-activity curve correction
AB  - Dynamic positron emission tomography (dPET) is currently a widely used medical imaging technique for the clinical diagnosis, staging and therapy guidance of all kinds of human cancers. Higher temporal imaging resolution for the early stage of radiotracer metabolism is desired; however, in this case, the reconstructed images with short frame durations always suffer from a limited image signal-to-noise ratio (SNR) and unsatisfactory image spatial resolution. The appearance of uEXPLORER (United Imaging Healthcare, Inc.) with higher PET imaging sensitivity and resolution may help solving this problem. In this work, based on dynamic PET data acquired by uEXPLORER, we proposed a dPET processing method that denoises images with short frame durations via pixel-level time-activity curve (TAC) correction based on third-order Hermite interpolation (Pitch-In). The proposed method was validated and compared to several state-of-the-art methods to demonstrate its superior performance in terms of high temporal resolution dPET image noise reduction and imaging contrast. Higher stability and feasibility of the proposed Pitch-In method for future clinical application with high temporal resolution (HTR) dPET imaging can be expected.
PB  - arXiv
PY  - 2021
ST  - High temporal resolution total-body dynamic PET imaging based on pixel-level time-activity curve correction
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/tbme.2022.3176097
ER  -


TY  - GEN
AU  - Keshvarikhojasteh, H.
AU  - Mohammadzade, H.
AU  - Behroozi, H.
TI  - Temporal Action Localization Using Gated Recurrent Units
AB  - Temporal Action Localization (TAL) task which is to predict the start and end of each action in a video along with the class label of the action has numerous applications in the real world. But due to the complexity of this task, acceptable accuracy rates have not been achieved yet, whereas this is not the case regarding the action recognition task. In this paper, we propose a new network based on Gated Recurrent Unit (GRU) and two novel post-processing methods for TAL task. Specifically, we propose a new design for the output layer of the conventionally GRU resulting in the so-called GRU-Split network. Moreover, linear interpolation is used to generate the action proposals with precise start and end times. Finally, to rank the generated proposals appropriately, we use a Learn to Rank (LTR) approach. We evaluated the performance of the proposed method on Thumos14 and ActivityNet-1.3 datasets. Results show the superiority of the performance of the proposed method compared to state-of-the-art. Specifically in the mean Average Precision (mAP) metric at Intersection over Union (IoU) of 0.7 on Thumos14, we get 27.52% accuracy which is 5.12% better than that of state-of-the-art methods.
PB  - arXiv
PY  - 2021
ST  - Temporal Action Localization Using Gated Recurrent Units
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/s00371-022-02495-1
ER  -


TY  - GEN
AU  - Mojtahedi, A.
AU  - Dadashzadeh, M.
AU  - Azizkhani, M.
AU  - Mohammadian, A.
AU  - Almasi, R.
TI  - Assessing climate and human activity effects on lake characteristics using spatio-temporal satellite data and an emotional neural network
AB  - Different sensing methods provide valuable information for comprehensive monitoring strategies, which are crucial for the ecological management of lakes and watersheds. Subsequently, the resulting spatio-temporal information can be considered the fundamental knowledge for the water resources management of watersheds. Lake Urmia is deemed one of the most important aquatic habitats in Iran. It has been experiencing significant changes during recent years due to climate change, anthropogenic activities, and a lack of coherent management approaches. Hence, awareness of the hydro-ecological factors during the last few decades is critical for identifying the problems. In this research, the impacts of changes in key parameters such as precipitation, evapotranspiration, water surface temperatures, suspended sediment concentration, saline features, and vegetation are explored using satellite imagery. The primary purpose of this study is to evaluate the Lake Urmia crisis concerning human-involved and climate factors such as the agriculture sector and construction of the causeway. In this regard, a limbic based Emotional Artificial Neural Network (EANN) is developed as a non-linear universal mapping and implemented for the first time to demonstrate the interactions between the considered hydro-ecological factors and the sensitivity of the two indicators the lake health. Providing a comprehensive spatio-temporal analysis is another objective of this study in order to detect the onset of deterioration in the parameters. The values of the efficiency criteria were measured to evaluate the sensitivity of the EANN models to the related inputs. The quantitative results confirm that the combination of both climate and anthropogenic factors, including the agricultural sector's overdraft, leads to the most efficient EANN model and, consequently, is considered the leading cause of the crisis.
PB  - Research Square
PY  - 2021
ST  - Assessing climate and human activity effects on lake characteristics using spatio-temporal satellite data and an emotional neural network
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/s12665-022-10185-3
ER  -


TY  - GEN
AU  - Nakabayashi, M.
AU  - Kanamori, T.
AU  - Matsukawa, A.
AU  - Matsuda, I.
AU  - Hanya, G.
TI  - Temporal Activity Patterns of Sympatric Bornean Carnivore Species: Implications for Niche Partitioning and Conservation
AB  - To propose proper conservation measures and to elucidate coexistence mechanisms of sympatric carnivore species, we assessed their temporal activity patterns using 37,379 photos collected for more than three years at three study sites in Borneo. We categorized activity patterns of nine carnivore species (one bear, three civets, two felids, one skunk, one mustelid, one linsang) by calculating the photo-capturing proportions at each period (day, night, twilight). We then evaluated temporal activity overlaps by calculating the overlap coefficients. We identified six nocturnal (three civets, one felid, one skunk, one linsang), two diurnal (one felids, one mustelid), and one cathemeral (bear) species. Temporal activity overlaps were high among the nocturnal species. The two felid species possessing morphological and ecological similarities exhibited clear temporal niche segregation, but the three civet species did not. Broad dietary breadth may compensate for the high temporal niche overlaps among the nocturnal species. Despite the high species richness of Bornean carnivores, almost half are threatened with extinction. By comparing individual radio-tracking and our data, we propose that a long-term study of at least three years is necessary to understand animals’ temporal activity patterns by camera-trapping and to avoid diverting conservationists away from effective protection measures.
PB  - Research Square
PY  - 2021
ST  - Temporal Activity Patterns of Sympatric Bornean Carnivore Species
Y2  - 2025/05/05/21:54:29
DO  - 10.21203/rs.3.rs-753526/v1
ER  -


TY  - GEN
AU  - Yan, J.
AU  - Wang, J.
AU  - Li, Q.
AU  - Wang, C.
AU  - Pu, S.
TI  - Self-supervised regional and temporal auxiliary tasks for facial action unit recognition
AB  - Automatic facial action unit (AU) recognition is a challenging task due to the scarcity of manual annotations. To alleviate this problem, a large amount of efforts has been dedicated to exploiting various methods which leverage numerous unlabeled data. However, many aspects with regard to some unique properties of AUs, such as the regional and relational characteristics, are not sufficiently explored in previous works. Motivated by this, we take the AU properties into consideration and propose two auxiliary AU related tasks to bridge the gap between limited annotations and the model performance in a self-supervised manner via the unlabeled data. Specifically, to enhance the discrimination of regional features with AU relation embedding, we design a task of RoI inpainting to recover the randomly cropped AU patches. Meanwhile, a single image based optical flow estimation task is proposed to leverage the dynamic change of facial muscles and encode the motion information into the global feature representation. Based on these two self-supervised auxiliary tasks, local features, mutual relation and motion cues of AUs are better captured in the backbone network with the proposed regional and temporal based auxiliary task learning (RTATL) framework. Extensive experiments on BP4D and DISFA demonstrate the superiority of our method and new state-of-the-art performances are achieved.
PB  - arXiv
PY  - 2021
ST  - Self-supervised regional and temporal auxiliary tasks for facial action unit recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1145/3474085.3475674
ER  -


TY  - GEN
AU  - Hong, F.-T.
AU  - Feng, J.-C.
AU  - Xu, D.
AU  - Shan, Y.
AU  - Zheng, W.-S.
TI  - Cross-modal consensus network for weakly supervised temporal action localization
AB  - Weakly supervised temporal action localization (WS-TAL) is a challenging task that aims to localize action instances in the given video with video-level categorical supervision. Previous works use the appearance and motion features extracted from pre-trained feature encoder directly, e.g., feature concatenation or score-level fusion. In this work, we argue that the features extracted from the pre-trained extractors, e.g., I3D, which are trained for trimmed video action classification, but not specific for WS-TAL task, leading to inevitable redundancy and sub-optimization. Therefore, the feature re-calibration is needed for reducing the task-irrelevant information redundancy. Here, we propose a cross-modal consensus network (CO2-Net) to tackle this problem. In CO2-Net, we mainly introduce two identical proposed cross-modal consensus modules (CCM) that design a cross-modal attention mechanism to filter out the task-irrelevant information redundancy using the global information from the main modality and the cross-modal local information from the auxiliary modality. Moreover, we further explore inter-modality consistency, where we treat the attention weights derived from each CCM as the pseudo targets of the attention weights derived from another CCM to maintain the consistency between the predictions derived from two CCMs, forming a mutual learning manner. Finally, we conduct extensive experiments on two commonly used temporal action localization datasets, THUMOS14 and ActivityNet1.2, to verify our method, which we achieve the state-of-the-art results. The experimental results show that our proposed cross-modal consensus module can produce more representative features for temporal action localization.
PB  - arXiv
PY  - 2021
ST  - Cross-modal consensus network for weakly supervised temporal action localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1145/3474085.3475298
ER  -


TY  - GEN
AU  - Zhu, Z.
AU  - Tang, W.
AU  - Wang, L.
AU  - Zheng, N.
AU  - Hua, G.
TI  - Enriching local and global contexts for temporal action localization
AB  - Effectively tackling the problem of temporal action localization (TAL) necessitates a visual representation that jointly pursues two confounding goals, i.e., fine-grained discrimination for temporal localization and sufficient visual invariance for action classification. We address this challenge by enriching both the local and global contexts in the popular two-stage temporal localization framework, where action proposals are first generated followed by action classification and temporal boundary regression. Our proposed model, dubbed ContextLoc, can be divided into three sub-networks: L-Net, G-Net and P-Net. L-Net enriches the local context via fine-grained modeling of snippet-level features, which is formulated as a query-and-retrieval process. G-Net enriches the global context via higher-level modeling of the video-level representation. In addition, we introduce a novel context adaptation module to adapt the global context to different proposals. P-Net further models the context-aware inter-proposal relations. We explore two existing models to be the P-Net in our experiments. The efficacy of our proposed method is validated by experimental results on the THUMOS14 (54.3% at tIoU@0.5) and ActivityNet v1.3 (56.01% at tIoU@0.5) datasets, which outperforms recent states of the art. Code is available at https://github.com/buxiangzhiren/ContextLoc.
PB  - arXiv
PY  - 2021
ST  - Enriching local and global contexts for temporal action localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/iccv48922.2021.01326
ER  -


TY  - GEN
AU  - Su, H.
AU  - Zhuang, P.
AU  - Li, Y.
AU  - Wu, W.
AU  - Qiao, Y.
TI  - Transferable knowledge-based multi-granularity aggregation network for temporal action localization: Submission to activitynet challenge 2021
AB  - This technical report presents an overview of our solution used in the submission to 2021 HACS Temporal Action Localization Challenge on both Supervised Learning Track and Weakly-Supervised Learning Track. Temporal Action Localization (TAL) requires to not only precisely locate the temporal boundaries of action instances, but also accurately classify the untrimmed videos into specific categories. However, Weakly-Supervised TAL indicates locating the action instances using only video-level class labels. In this paper, to train a supervised temporal action localizer, we adopt Temporal Context Aggregation Network (TCANet) to generate high-quality action proposals through “local and global” temporal context aggregation and complementary as well as progressive boundary refinement. As for the WSTAL, a novel framework is proposed to handle the poor quality of CAS generated by simple classification network, which can only focus on local discriminative parts, rather than locate the entire interval of target actions. Specifically, we propose to utilize convolutional kernels with varied dilation rates to enlarge the receptive fields, which is found to be capable of transferring the discriminative information to surrounding non-discriminative regions. Then we design a cascaded module with proposed Online Adversarial Erasing (OAE) mechanism to further mine more relevant regions of target actions through feeding the erased feature maps of discovered regions back to the system. Besides, inspired by the transfer learning method, we also adopt an additional module to transfer the knowledge from trimmed videos (HACS Clips dataset) to untrimmed videos (HACS Segments dataset), aiming at promoting the classification performance on untrimmed videos. Finally, we employ a boundary regression module embedded with Outer-Inner-Contrastive (OIC) loss to automatically predict the boundaries based on the enhanced CAS. Our proposed scheme achieves 39.91 and 29.78 average mAP on the challenge testing set of supervised and weakly-supervised temporal action localization track respectively.
PB  - arXiv
PY  - 2021
ST  - Transferable knowledge-based multi-granularity aggregation network for temporal action localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/tmm.2020.2999184
ER  -


TY  - GEN
AU  - Tran, Q.
AU  - Tran, L.
AU  - Hai, L.C.
AU  - Van, L.N.
AU  - Than, K.
TI  - From implicit to explicit feedback: A deep neural network for modeling sequential behaviours and long-short term preferences of online users
AB  - In this work, we examine the advantages of using multiple types of behaviour in recommendation systems. Intuitively, each user has to do some implicit actions (e.g., click) before making an explicit decision (e.g., purchase). Previous studies showed that implicit and explicit feedback have different roles for a useful recommendation. However, these studies either exploit implicit and explicit behaviour separately or ignore the semantic of sequential interactions between users and items. In addition, we go from the hypothesis that a user's preference at a time is a combination of long-term and short-term interests. In this paper, we propose some Deep Learning architectures. The first one is Implicit to Explicit (ITE), to exploit users' interests through the sequence of their actions. And two versions of ITE with Bidirectional Encoder Representations from Transformers based (BERT-based) architecture called BERT-ITE and BERT-ITE-Si, which combine users' long- and short-term preferences without and with side information to enhance user representation. The experimental results show that our models outperform previous state-of-the-art ones and also demonstrate our views on the effectiveness of exploiting the implicit to explicit order as well as combining long- and short-term preferences in two large scale datasets.
PB  - arXiv
PY  - 2021
ST  - From implicit to explicit feedback
Y2  - 2025/05/05/21:54:29
DO  - 10.1016/j.neucom.2022.01.023
ER  -


TY  - GEN
AU  - Huang, J.
AU  - Liu, Y.
AU  - Gong, S.
AU  - Jin, H.
TI  - Cross-Sentence temporal and semantic relations in video activity localisation
AB  - Video activity localisation has recently attained increasing attention due to its practical values in automatically localising the most salient visual segments corresponding to their language descriptions (sentences) from untrimmed and unstructured videos. For supervised model training, a temporal annotation of both the start and end time index of each video segment for a sentence (a video moment) must be given. This is not only very expensive but also sensitive to ambiguity and subjective annotation bias, a much harder task than image labelling. In this work, we develop a more accurate weakly-supervised solution by introducing Cross-Sentence Relations Mining (CRM) in video moment proposal generation and matching when only a paragraph description of activities without per-sentence temporal annotation is available. Specifically, we explore two cross-sentence relational constraints: (1) Temporal ordering and (2) semantic consistency among sentences in a paragraph description of video activities. Existing weakly-supervised techniques only consider within-sentence video segment correlations in training without considering cross-sentence paragraph context. This can mislead due to ambiguous expressions of individual sentences with visually indiscriminate video moment proposals in isolation. Experiments on two publicly available activity localisation datasets show the advantages of our approach over the state-of-the-art weakly supervised methods, especially so when the video activity descriptions become more complex.
PB  - arXiv
PY  - 2021
ST  - Cross-Sentence temporal and semantic relations in video activity localisation
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/iccv48922.2021.00711
ER  -


TY  - GEN
AU  - Giordano, L.
AU  - Martelli, A.
AU  - Dupré, D.T.
TI  - Reasoning about actions with EL ontologies with temporal answer sets
AB  - We propose an approach based on Answer Set Programming for reasoning about actions with domain descriptions including ontological knowledge, expressed in the lightweight description logic EL⊥. We consider a temporal action theory, which allows for non-deterministic actions and causal rules to deal with ramifications, and whose extensions are defined by temporal answer sets. We provide conditions under which action consistency can be guaranteed with respect to an ontology, by a polynomial encoding of an action theory extended with an EL⊥ knowledge base (in normal form) into a temporal action theory.
PB  - arXiv
PY  - 2021
ST  - Reasoning about actions with EL ontologies with temporal answer sets
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-031-15707-3_18
ER  -


TY  - GEN
AU  - Zatsarynna, O.
AU  - Farha, Y.A.
AU  - Gall, J.
TI  - Multi-modal temporal convolutional network for anticipating actions in egocentric videos
AB  - Anticipating human actions is an important task that needs to be addressed for the development of reliable intelligent agents, such as self-driving cars or robot assistants. While the ability to make future predictions with high accuracy is crucial for designing the anticipation approaches, the speed at which the inference is performed is not less important. Methods that are accurate but not sufficiently fast would introduce a high latency into the decision process. Thus, this will increase the reaction time of the underlying system. This poses a problem for domains such as autonomous driving, where the reaction time is crucial. In this work, we propose a simple and effective multi-modal architecture based on temporal convolutions. Our approach stacks a hierarchy of temporal convolutional layers and does not rely on recurrent layers to ensure a fast prediction. We further introduce a multi-modal fusion mechanism that captures the pairwise interactions between RGB, flow, and object modalities. Results on two large-scale datasets of egocentric videos, EPIC-Kitchens-55 and EPIC-Kitchens-100, show that our approach achieves comparable performance to the state-of-the-art approaches while being significantly faster.
PB  - arXiv
PY  - 2021
ST  - Multi-modal temporal convolutional network for anticipating actions in egocentric videos
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvprw53098.2021.00254
ER  -


TY  - GEN
AU  - Vo-Ho, V.-K.
AU  - Le, N.
AU  - Yamazaki, K.
AU  - Sugimoto, A.
AU  - Tran, M.-T.
TI  - AGENT-ENVIRONMENT NETWORK FOR TEMPORAL ACTION PROPOSAL GENERATION
AB  - Temporal action proposal generation is an essential and challenging task that aims at localizing temporal intervals containing human actions in untrimmed videos. Most of existing approaches are unable to follow the human cognitive process of understanding the video context due to lack of attention mechanism to express the concept of an action or an agent who performs the action or the interaction between the agent and the environment. Based on the action definition that a human, known as an agent, interacts with the environment and performs an action that affects the environment, we propose a contextual Agent-Environment Network. Our proposed contextual AEN involves (i) agent pathway, operating at a local level to tell about which humans/agents are acting and (ii) environment pathway operating at a global level to tell about how the agents interact with the environment. Comprehensive evaluations on 20-action THUMOS-14 and 200- action ActivityNet-1.3 datasets with different backbone networks, i.e C3D and SlowFast, show that our method robustly exhibits outperformance against state-of-the-art methods regardless of the employed backbone network.
PB  - arXiv
PY  - 2021
ST  - AGENT-ENVIRONMENT NETWORK FOR TEMPORAL ACTION PROPOSAL GENERATION
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/icassp39728.2021.9415101
ER  -


TY  - GEN
AU  - Hernández, V.
AU  - León, A.
AU  - Guzmán, I.
AU  - Linares, C.A.H.
AU  - Luna, I.
TI  - An Ecological Approach to the Effects of Water-Source Locations and Time-Based Schedules on Entropy and Spatio-Temporal Behavioral Features
AB  - In behavior analysis, the modulation of the effect of time-based schedules by the spatial characteristics of the environment has been scarcely studied. Furthermore, the spatial organization of behavior, despite its ubiquity and ecological relevance, has not been widely addressed. The purpose of the present work was to analyze the effect of water delivery location (peripheral vs. central) on the spatial organization of water-feeding behavior under time-based schedules. One group of rats was exposed to a Fixed Time 30 s-water-delivery schedule and a second group to a Variable Time 30 s schedule. For both groups, in the first phase, the water dispenser was located in the perimetral zone. In the second condition, the water dispenser was located in the central zone. Each location was presented for 20 sessions. Rat’s trajectories, distance to the dispenser, accumulated time in regions, and entropy measures were analyzed. A differential effect of the location of water delivery in interaction with the time-based schedule was observed on all the analyzed spatial qualities of behavior. The findings are discussed in relation to the ecological proposal of Timberlake's behavioral systems.
PB  - bioRxiv
PY  - 2021
ST  - An Ecological Approach to the Effects of Water-Source Locations and Time-Based Schedules on Entropy and Spatio-Temporal Behavioral Features
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2021.07.15.452514
ER  -


TY  - GEN
AU  - Kiffer, F.C.
AU  - Luitel, K.
AU  - Tran, F.H.
AU  - Yun, S.
AU  - Eisch, A.J.
TI  - Effects of a 33-ion sequential beam galactic cosmic ray analog on male mouse behavior and evaluation of CDDO-EA as a radiation countermeasure
AB  - In long-term spaceflight, astronauts will face unique cognitive loads and social challenges which will be complicated by communication delays with Earth. It is important to understand the central nervous system (CNS) effects of deep spaceflight and the associated unavoidable exposure to galactic cosmic radiation (GCR). Rodent studies show single- or simple-particle combination exposure alters CNS endpoints, including hippocampal-dependent behavior. An even better Earth-based simulation of GCR is now available, consisting of a 33-beam (33-GCR) exposure. However, the effect of whole-body 33-GCR exposure on rodent behavior is unknown, and no 33-GCR CNS countermeasures have been tested. Here astronaut-age-equivalent (6mo-old) C57BL/6J male mice were exposed to 33-GCR (75cGy, a Mars mission dose). Pre-/during/post-Sham or 33-GCR exposure, mice received a diet containing a 'vehicle' formulation alone or with the antioxidant/anti-inflammatory compound CDDO‐EA as a potential countermeasure. Behavioral testing beginning 4mo post-irradiation suggested radiation and diet did not affect measures of exploration/anxiety-like behaviors (open field, elevated plus maze) or recognition of a novel object. However, in 3-Chamber Social Interaction (3-CSI), CDDO-EA/33-GCR mice failed to spend more time exploring a holder containing a novel mouse vs. a novel object (empty holder), suggesting sociability deficits. Also, Vehicle/33-GCR and CDDO-EA/Sham mice failed to discriminate between a novel stranger vs. familiarized stranger mouse, suggesting blunted preference for social novelty. CDDO-EA given pre-/during/post-irradiation did not attenuate the 33-GCR-induced blunting of preference for social novelty. Future elucidation of the mechanisms underlying 33-GCR-induced blunting of preference for social novelty will improve risk analysis for astronauts which may in-turn improve countermeasures.
PB  - bioRxiv
PY  - 2021
ST  - Effects of a 33-ion sequential beam galactic cosmic ray analog on male mouse behavior and evaluation of CDDO-EA as a radiation countermeasure
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2021.07.15.451917
ER  -


TY  - GEN
AU  - Wang, C.
AU  - Cai, H.
AU  - Zou, Y.
AU  - Xiong, Y.
TI  - RGB stream is enough for temporal action detection
AB  - State-of-the-art temporal action detectors to date are based on two-stream input including RGB frames and optical flow. Although combining RGB frames and optical flow boosts performance significantly, optical flow is a handdesigned representation which not only requires heavy computation, but also makes it methodologically unsatisfactory that two-stream methods are often not learned end-to-end jointly with the flow. In this paper, we argue that optical flow is dispensable in high-accuracy temporal action detection and image level data augmentation (ILDA) is the key solution to avoid performance degradation when optical flow is removed. To evaluate the effectiveness of ILDA, we design a simple yet efficient one-stage temporal action detector based on single RGB stream named DaoTAD. Our results show that when trained with ILDA, DaoTAD has comparable accuracy with all existing state-of-the-art two-stream detectors while surpassing the inference speed of previous methods by a large margin and the inference speed is astounding 6668 fps on GeForce GTX 1080 Ti. Code is available at https://github.com/Media-Smart/vedatad.
PB  - arXiv
PY  - 2021
ST  - RGB stream is enough for temporal action detection
Y2  - 2025/05/05/21:54:29
DO  - 10.1117/12.2589340
ER  -


TY  - GEN
AU  - Ning, R.
AU  - Zhang, C.
AU  - Zou, Y.
TI  - SRF-NET: Selective receptive field network for anchor-free temporal action detection
AB  - Temporal action detection (TAD) is a challenging task which aims to temporally localize and recognize the human action in untrimmed videos. Current mainstream one-stage TAD approaches localize and classify action proposals relying on pre-defined anchors, where the location and scale for action instances are set by designers. Obviously, such an anchor-based TAD method limits its generalization capability and will lead to performance degradation when videos contain rich action variation. In this study, we explore to remove the requirement of pre-defined anchors for TAD methods. A novel TAD model termed as Selective Receptive Field Network (SRF-Net) is developed, in which the location offsets and classification scores at each temporal location can be directly estimated in the feature map and SRF-Net is trained in an end-to-end manner. Innovatively, a building block called Selective Receptive Field Convolution (SRFC) is dedicatedly designed which is able to adaptively adjust its receptive field size according to multiple scales of input information at each temporal location in the feature map. Extensive experiments are conducted on the THUMOS14 dataset, and superior results are reported comparing to state-of-the-art TAD approaches.
PB  - arXiv
PY  - 2021
ST  - SRF-NET
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/icassp39728.2021.9414253
ER  -


TY  - GEN
AU  - Wu, L.
AU  - Zou, Y.
AU  - Zhang, C.
TI  - Long-short temporal modeling for efficient action recognition
AB  - Efficient long-short temporal modeling is key for enhancing the performance of action recognition task. In this paper, we propose a new two-stream action recognition network, termed as MENet, consisting of a Motion Enhancement (ME) module and a Video-level Aggregation (VLA) module to achieve long-short temporal modeling. Specifically, motion representations have been proved effective in capturing short-term and high-frequency action. However, current motion representations are calculated from adjacent frames, which may have poor interpretation and bring useless information (noisy or blank). Thus, for short-term motions, we design an efficient ME module to enhance the short-term motions by mingling the motion saliency among neighboring segments. As for long-term aggregations, VLA is adopted at the top of the appearance branch to integrate the long-term dependencies across all segments. The two components of MENet are complementary in temporal modeling. Extensive experiments are conducted on UCF101 and HMDB51 benchmarks, which verify the effectiveness and efficiency of our proposed MENet.
PB  - arXiv
PY  - 2021
ST  - Long-short temporal modeling for efficient action recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/icassp39728.2021.9413807
ER  -


TY  - GEN
AU  - Calderó, M.S.
AU  - Varas, D.
AU  - Bou-Balust, E.
TI  - Spatio-temporal context for action detection
AB  - Research in action detection has grown in the recent years, as it plays a key role in video understanding. Modelling the interactions (either spatial or temporal) between actors and their context has proven to be essential for this task. While recent works use spatial features with aggregated temporal information, this work proposes to use non-aggregated temporal information. This is done by adding an attention based method that leverages spatio-temporal interactions between elements in the scene along the clip. The main contribution of this work is the introduction of two cross attention blocks to effectively model the spatial relations and capture short range temporal interactions. Experiments on the AVA dataset show the advantages of the proposed approach that models spatio-temporal relations between relevant elements in the scene, outperforming other methods that model actor interactions with their context by +0.31 mAP.
PB  - arXiv
PY  - 2021
ST  - Spatio-temporal context for action detection
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/wacv61041.2025.00880
ER  -


TY  - GEN
AU  - Vasileiou, V.I.
AU  - Kardaris, N.
AU  - Maragos, P.
TI  - Exploring temporal context and human movement dynamics for online action detection in videos
AB  - Nowadays, the interaction between humans and robots is constantly expanding, requiring more and more human motion recognition applications to operate in real time. However, most works on temporal action detection and recognition perform these tasks in offline manner, i.e. temporally segmented videos are classified as a whole. In this paper, based on the recently proposed framework of Temporal Recurrent Networks, we explore how temporal context and human movement dynamics can be effectively employed for online action detection. Our approach uses various state-of-the-art architectures and appropriately combines the extracted features in order to improve action detection. We evaluate our method on a challenging but widely used dataset for temporal action localization, THUMOS’14. Our experiments show significant improvement over the baseline method, achieving state-of-the art results on THUMOS’14.
PB  - arXiv
PY  - 2021
ST  - Exploring temporal context and human movement dynamics for online action detection in videos
Y2  - 2025/05/05/21:54:29
DO  - 10.23919/eusipco54536.2021.9616092
ER  -


TY  - GEN
AU  - Bagchi, A.
AU  - Mahmood, J.
AU  - Fernandes, D.
AU  - Sarvadevabhatla, R.K.
TI  - Hear me out: Fusional approaches for audio augmented temporal action localization
AB  - State of the art architectures for untrimmed video Temporal Action Localization (TAL) have only considered RGB and Flow modalities, leaving the information-rich audio modality totally unexploited. Audio fusion has been explored for the related but arguably easier problem of trimmed (clip-level) action recognition. However, TAL poses a unique set of challenges. In this paper, we propose simple but effective fusion-based approaches for TAL. To the best of our knowledge, our work is the first to jointly consider audio and video modalities for supervised TAL. We experimentally show that our schemes consistently improve performance for state of the art video-only TAL approaches. Specifically, they help achieve new state of the art performance on large-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14 (57.18 mAP@0.5). Our experiments include ablations involving multiple fusion schemes, modality combinations and TAL architectures. Our code, models and associated data are available at https://github.com/skelemoa/tal-hmo.
PB  - arXiv
PY  - 2021
ST  - Hear me out
Y2  - 2025/05/05/21:54:29
DO  - 10.5220/0010832700003124
ER  -


TY  - GEN
AU  - Qing, Z.
AU  - Wang, X.
AU  - Huang, Z.
AU  - Gao, C.
AU  - Sang, N.
TI  - Exploring Stronger Feature for Temporal Action Localization
AB  - Temporal action localization aims to localize starting and ending time with action category. Limited by GPU memory, mainstream methods pre-extract features for each video. Therefore, feature quality determines the upper bound of detection performance. In this technical report, we explored classic convolution-based backbones and the recent surge of transformer-based backbones. We found that the transformer-based methods can achieve better classification performance than convolution-based, but they cannot generate accuracy action proposals. In addition, extracting features with larger frame resolution to reduce the loss of spatial information can also effectively improve the performance of temporal action localization. Finally, we achieve 42.42% in terms of mAP on validation set with a single SlowFast [9] feature by a simple combination: BMN [16]+TCANet [19], which is 1.87% higher than the result of 2020 [20]’s multi-model ensemble. Finally, we achieve Rank 1st on the CVPR2021 HACS supervised Temporal Action Localization Challenge.
PB  - arXiv
PY  - 2021
ST  - Exploring Stronger Feature for Temporal Action Localization
Y2  - 2025/05/05/21:54:29
ER  -


TY  - GEN
AU  - Wang, X.
AU  - Qing, Z.
AU  - Huang, Z.
AU  - Gao, C.
AU  - Sang, N.
TI  - Proposal relation network for temporal action detection
AB  - This technical report presents our solution for temporal action detection task in AcitivityNet Challenge 2021. The purpose of this task is to locate and identify actions of interest in long untrimmed videos. The crucial challenge of the task comes from that the temporal duration of action varies dramatically, and the target actions are typically embedded in a background of irrelevant activities. Our solution builds on BMN [10], and mainly contains three steps: 1) action classification and feature encoding by Slowfast [6], CSN [13] and ViViT [1]; 2) proposal generation. We improve BMN by embedding the proposed Proposal Relation Network (PRN), by which we can generate proposals of high quality; 3) action detection. We calculate the detection results by assigning the proposals with corresponding classification results. Finally, we ensemble the results under different settings and achieve 44.7% on the test set, which improves the champion result in ActivityNet 2020 [17] by 1.9% in terms of average mAP.
PB  - arXiv
PY  - 2021
ST  - Proposal relation network for temporal action detection
Y2  - 2025/05/05/21:54:29
DO  - 10.1016/j.patcog.2023.110245
ER  -


TY  - GEN
AU  - Wang, X.
AU  - Qing, Z.
AU  - Huang, Z.
AU  - Shao, Y.
AU  - Sang, N.
TI  - Weakly-supervised temporal action localization through local-global background modeling
AB  - Weakly-Supervised Temporal Action Localization (WSTAL) task aims to recognize and localize temporal starts and ends of action instances in an untrimmed video with only video-level label supervision. Due to lack of negative samples of background category, it is difficult for the network to separate foreground and background, resulting in poor detection performance. In this report, we present our 2021 HACS Challenge-Weakly-supervised Learning Track [24] solution that based on BaSNet [10] to address above problem. Specifically, we first adopt pre-trained CSN [17], Slowfast [5], TDN [19], and ViViT [1] as feature extractors to get feature sequences. Then our proposed Local-Global Background Modeling Network (LGBM-Net) is trained to localize instances by using only video-level labels based on Multi-Instance Learning (MIL). Finally, we ensemble multiple models to get the final detection results and reach 22.45% mAP on the test set.
PB  - arXiv
PY  - 2021
ST  - Weakly-supervised temporal action localization through local-global background modeling
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/iccv.2019.00560
ER  -


TY  - GEN
AU  - Liu, X.
AU  - Wang, Q.
AU  - Hu, Y.
AU  - Bai, S.
AU  - Bai, X.
TI  - End-to-end Temporal Action Detection with Transformer
AB  - Temporal action detection (TAD) aims to determine the semantic label and the temporal interval of every action instance in an untrimmed video. It is a fundamental and challenging task in video understanding. Previous methods tackle this task with complicated pipelines. They often need to train multiple networks and involve hand-designed operations, such as non-maximal suppression and anchor generation, which limit the flexibility and prevent end-to-end learning. In this paper, we propose an end-to-end Transformer-based method for TAD, termed TadTR. Given a small set of learnable embeddings called action queries, TadTR adaptively extracts temporal context information from the video for each query and directly predicts action instances with the context. To adapt Transformer to TAD, we propose three improvements to enhance its locality awareness. The core is a temporal deformable attention module that selectively attends to a sparse set of key snippets in a video. A segment refinement mechanism and an actionness regression head are designed to refine the boundaries and confidence of the predicted instances, respectively. With such a simple pipeline, TadTR requires lower computation cost than previous detectors, while preserving remarkable performance. As a self-contained detector, it achieves state-of-the-art performance on THUMOS14 (56.7% mAP) and HACS Segments (32.09% mAP). Combined with an extra action classifier, it obtains 36.75% mAP on ActivityNet-1.3. Code is available at https://github.com/xlliu7/TadTR.
PB  - arXiv
PY  - 2021
ST  - End-to-end Temporal Action Detection with Transformer
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/tip.2022.3195321
ER  -


TY  - GEN
AU  - Ehsanpour, M.
AU  - Saleh, F.
AU  - Savarese, S.
AU  - Reid, I.
AU  - Rezatofighi, H.
TI  - JRDB-Act: A large-scale dataset for spatio-temporal action, social group and activity detection
AB  - The availability of large-scale video action understanding datasets has facilitated advances in the interpretation of visual scenes containing people. However, learning to recognise human actions and their social interactions in an unconstrained real-world environment comprising numerous people, with potentially highly unbalanced and long-tailed distributed action labels from a stream of sensory data captured from a mobile robot platform remains a significant challenge, not least owing to the lack of a reflective large-scale dataset. In this paper, we introduce JRDB-Act, as an extension of the existing JRDB, which is captured by a social mobile manipulator and reflects a real distribution of human daily-life actions in a university campus environment. JRDB-Act has been densely annotated with atomic actions, comprises over 2.8M action labels, constituting a large-scale spatio-temporal action detection dataset. Each human bounding box is labeled with one pose-based action label and multiple (optional) interaction-based action labels. Moreover JRDB-Act provides social group annotation, conducive to the task of grouping individuals based on their interactions in the scene to infer their social activities (common activities in each social group). Each annotated label in JRDB-Act is tagged with the annotators’ confidence level which contributes to the development of reliable evaluation strategies. In order to demonstrate how one can effectively utilise such annotations, we develop an end-to-end trainable pipeline to learn and infer these tasks, i.e. individual action and social group detection. The data and the evaluation code is publicly available at https://jrdb.erc.monash.edu/.
PB  - arXiv
PY  - 2021
ST  - JRDB-Act
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr52688.2022.02031
ER  -


TY  - GEN
AU  - Preston, D.
AU  - Johnson, S.G.
TI  - Effects Of Acclimation, Population, And Sex On Behavioral Thermoregulation, CTMax, Symptoms Of Heat Stress, And Gene Expression of Melanoplus Differentialis, A Generalist Grasshopper - Does Temporal Thermal Heterogeneity Prepare Populations For A Warming World?
AB  - Insects thermoregulate using both canalized and plastic mechanisms. Populations of insects utilize these mechanisms to different extents, and while it is posited that the degree of thermal fluctuation a population experiences can determine the optimal combination of mechanisms to utilize, this is still being elucidated. We used three populations of the generalist grasshopper, Melanoplus differentialis (Thomas, 1856), from sites experiencing different degrees of thermal heterogeneity to test for correlations between thermal heterogeneity and 1) behavioral thermoregulation, 2) upper temperature tolerance, 3) the ability to thermally acclimate, and 4) gene expression. We found that 1) behavioral thermoregulation did not differ among sites, 2) CTMax of males, but not females, was higher at more thermally heterogeneous sites, 3) there was acclimation in some of the tested traits, but thermally heterogeneous sites did not always have the most plastic individuals, and 4) there were differences in gene expression among sites, but these differences were not between the most and least thermally heterogeneous sites. We concluded that thermal heterogeneity may play a selective role in some, but not all, of the measured thermoregulatory traits and their plasticity.
PB  - Research Square
PY  - 2021
ST  - Effects Of Acclimation, Population, And Sex On Behavioral Thermoregulation, CTMax, Symptoms Of Heat Stress, And Gene Expression of Melanoplus Differentialis, A Generalist Grasshopper - Does Temporal Thermal Heterogeneity Prepare Populations For A Warming World?
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/s10905-022-09805-4
ER  -


TY  - GEN
AU  - Yuan, F.
AU  - Li, Q.
AU  - Yang, Y.
AU  - Mostafavi, A.
TI  - Unraveling the temporal importance of community-scale human activity features for rapid assessment of flood impacts
AB  - The objective of this research is to explore the temporal importance of community-scale human activity features for rapid assessment of flood impacts. Ultimate flood impact data, such as flood inundation maps and insurance claims, becomes available only weeks and months after the floods have receded. Crisis response managers, however, need near-real-time data to prioritize emergency response. This time lag creates a need for rapid flood impact assessment. Accordingly, community-scale big data (such as satellite imagery) has been utilized for the early estimation of end flood impacts. Some recent studies have shown promising results for using human activity fluctuations as indicators of flood impacts. Existing studies, however, used mainly a single community-scale activity feature for the estimation of flood impacts and have not investigated their temporal importance for indicating flood impacts. Hence, in this study, we examined the importance of heterogeneous human activity features (such as human mobility, visits to points-of-interest, and social media posts) in different flood event stages. Using four community-scale big data categories we derived ten features related to the variations in human activity (e.g., travel, credit card transactions, and online communications) and evaluated their temporal importance for rapid assessment of flood impacts. Using multiple random forest models, we examined the temporal importance of each feature in indicating the extent of flood impacts (measured by the flood insurance claims and flood inundations) in the context of the 2017 Hurricane Harvey in Harris County, Texas. Our findings reveal that 1) fluctuations in human activity index and percentage of congested roads are the most important indicators for rapid flood impact assessment during response and recovery stages; 2) variations in credit card transactions assumed a middle ranking in both response and recovery stages; and 3) patterns of geolocated social media posts (Twitter) were of low importance across flood stages. Insights derived from data analysis reveal the potential for harnessing community-scale data characterizing human activity fluctuations for rapid assessment of flood impacts. The results of this research could rapidly forge a multi-tool enabling crisis managers to identify hotspots with severe flood impacts at various stages then to plan and prioritize effective response strategies.
PB  - arXiv
PY  - 2021
ST  - Unraveling the temporal importance of community-scale human activity features for rapid assessment of flood impacts
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/access.2021.3137651
ER  -


TY  - GEN
AU  - Feng, Y.
AU  - Jiang, J.
AU  - Huang, Z.
AU  - Tang, M.
AU  - Gao, Y.
TI  - Relation modeling in spatio-temporal action localization
AB  - This paper presents our solution to the AVA-Kinetics Crossover Challenge of ActivityNet workshop at CVPR 2021. Our solution utilizes multiple types of relation modeling methods for spatio-temporal action detection and adopts a training strategy to integrate multiple relation modeling in end-to-end training over the two large-scale video datasets. Learning with memory bank and finetuning for long-tailed distribution are also investigated to further improve the performance. In this paper, we detail the implementations of our solution and provide experiments results and corresponding discussions. We finally achieve 40.67 mAP on the test set of AVA-Kinetics.
PB  - arXiv
PY  - 2021
ST  - Relation modeling in spatio-temporal action localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr46437.2021.00053
ER  -


TY  - GEN
AU  - Stevenson, R.F.
AU  - Janacek, J.T.
AU  - Larson, M.S.
AU  - Lin, J.J.
AU  - Yassa, M.A.
TI  - Medial temporal lobe high-frequency activity at feedback signals error and predicts subsequent memory during spatial learning
AB  - The ability to incorporate information about feedback is critical for associative learning. The medial temporal lobe (MTL) and prefrontal cortex (PFC) are thought to be involved in processing feedback as new associations are learned. However, the relative contributions of these regions to feedback processing and subsequent memory performance in humans are poorly understood. To address this question, we tested pre-surgical epilepsy patients with depth electrodes implanted in the MTL and PFC using a spatial memory task in which subjects learned object-location associations over time. During encoding, subjects were shown objects at random locations along the circumference of an invisible circle. For each training block, the same objects were shown at the top of the circle and subjects used a mouse wheel to rotate the object to where it appeared during encoding. After subjects finished placing each object, the object was shown in the correct location for one second as feedback. We found increased high-frequency activity (HFA; 40-100 Hz), thought to reflect local excitatory activity, in the MTL and dorsolateral PFC (dlPFC) at feedback for high error trials. In the MTL, this HFA error signal predicted greater trial-by-trial decreases in error from one training block to the next indicating that these signals are involved in updating memory representations or modifying incorrect associations during learning. The opposite pattern of activity was observed during retrieval, with greater MTL and dlPFC HFA predicting lower error, replicating previous results from our group. Overall, these data suggest putative mechanisms for the learning of object-location associations.
PB  - Research Square
PY  - 2021
ST  - Medial temporal lobe high-frequency activity at feedback signals error and predicts subsequent memory during spatial learning
Y2  - 2025/05/05/21:54:29
DO  - 10.21203/rs.3.rs-194736/v1
ER  -


TY  - GEN
AU  - Shinohara, S.
AU  - Okamoto, H.
AU  - Manome, N.
AU  - Moriyama, T.
AU  - Chung, U.-I.
TI  - Simulation of foraging behavior using a decision-making agent with Bayesian and inverse Bayesian inference: Temporal correlations and power laws in displacement patterns
AB  - It has been stated that in human migratory behavior, the step length series may have temporal correlation and that there is some relationship between this time dependency and the fact that the frequency distribution of step length follows a power-law distribution. Furthermore, the frequency of occurrence of the step length in some large marine organisms has been found to switch between power-law and exponential distributions, depending on the difficulty of prey acquisition. However, to date it has not been clarified how the aforementioned three phenomena arise: the positive correlation created in the step length series, the relation between the positive correlation of the step length series and the form of an individual's step length distribution, and the switching between power-law and exponential distributions depending on the abundance of prey. This study simulated foraging behavior using the Bayesian decision-making agent simultaneously performing both knowledge learning and knowledge-based inference to analyze how the aforementioned three phenomena arise. In the agent with learning and inference, past experiences were stored as hypotheses (knowledge) and they were used in current foraging behavior; at the same time, the hypothesis continued to be updated based on new experiences. The simulation results show that the agent with both learning and inference has a mechanism that simultaneously causes all the phenomena.
PB  - bioRxiv
PY  - 2021
ST  - Simulation of foraging behavior using a decision-making agent with Bayesian and inverse Bayesian inference
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2021.06.08.447450
ER  -


TY  - GEN
AU  - Schaap, K.J.
AU  - Fuchslueger, L.
AU  - Hoosbeek, M.R.
AU  - Lugli, L.F.
AU  - Quesada, C.A.
TI  - Litter inputs and phosphatase activity control the temporal variability of organic phosphorus in a tropical forest soil in the Central Amazon
AB  - Purpose. The tropical phosphorus-cycle and its impacts on phosphorus (P) availability are a major uncertainty in projections of forest productivity. In highly weathered soils with low P concentrations, plant and microbial communities depend on biological and physical processes to acquire P. We explored the seasonality and relative importance of drivers controlling the fluctuation of common P pools via processes such as litter production and decomposition, and soil phosphatase activity. Methods. We analyzed variation of standard tropical soil phosphorus pools over one year. In addition, we measured litterfall, its decomposition rates and soil extracellular phosphatase enzyme activity and tested their relation to the fluctuations in P-fractions. Results. Our results show clear patterns of seasonal variability of soil P fractions during the year. We found that modeled P released during litter decomposition is positively related to change in organic P fractions, while net change in organic P fractions was negatively related to phosphatase activities in the top 5 cm. Conclusion. We conclude that input of organic P forms by litter decomposition and organically produced phosphatases are the two main factors controlling seasonal soil P fluctuation, and therefore the P economy in P impoverished soils. Organic soil P follows a clear seasonal pattern, indicating tight cycling of the nutrient, while reinforcing the importance of studying soil P as an integrated dynamic system in a tropical forest context.
PB  - Research Square
PY  - 2021
ST  - Litter inputs and phosphatase activity control the temporal variability of organic phosphorus in a tropical forest soil in the Central Amazon
Y2  - 2025/05/05/21:54:29
DO  - 10.21203/rs.3.rs-548371/v1
ER  -


TY  - GEN
AU  - Krohn, S.
AU  - von Schwanenflug, N.
AU  - Waschke, L.
AU  - Garrett, D.D.
AU  - Finke, C.
TI  - A spatiotemporal complexity architecture of human brain activity
AB  - The human brain operates in large-scale functional networks, collectively subsumed as the functional connectome1-13. Recent work has begun to unravel the organization of the connectome, including the temporal dynamics of brain states14-20, the trade-off between segregation and integration9,15,21-23, and a functional hierarchy from lower-order unimodal to higher-order transmodal processing systems24-27. However, it remains unknown how these network properties are embedded in the brain and if they emerge from a common neural foundation. Here we apply time-resolved estimation of brain signal complexity to uncover a unifying principle of brain organization, linking the connectome to neural variability6,28-31. Using functional magnetic resonance imaging (fMRI), we show that neural activity is marked by spontaneous “complexity drops” that reflect episodes of increased pattern regularity in the brain, and that functional connections among brain regions are an expression of their simultaneous engagement in such episodes. Moreover, these complexity drops ubiquitously propagate along cortical hierarchies, suggesting that the brain intrinsically reiterates its own functional architecture. Globally, neural activity clusters into temporal complexity states that dynamically shape the coupling strength and configuration of the connectome, implementing a continuous re-negotiation between cost-efficient segregation and communication-enhancing integration9,15,21,23. Furthermore, complexity states resolve the recently discovered association between anatomical and functional network hierarchies comprehensively25-27,32. Finally, brain signal complexity is highly sensitive to age and reflects inter-individual differences in cognition and motor function. In sum, we identify a spatiotemporal complexity architecture of neural activity - a functional “complexome” that gives rise to the network organization of the human brain.
PB  - bioRxiv
PY  - 2021
ST  - A spatiotemporal complexity architecture of human brain activity
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2021.06.04.446948
ER  -


TY  - GEN
AU  - Su, H.
AU  - Li, K.
AU  - Feng, J.
AU  - Wu, W.
AU  - Qiao, Y.
TI  - TSI: Temporal saliency integration for video action recognition
AB  - Efficient spatiotemporal modeling is an important yet challenging problem for video action recognition. Existing state-of-the-art methods exploit neighboring feature differences to obtain motion clues for short-term temporal modeling with a simple convolution. However, only one local convolution is incapable of handling various kinds of actions because of the limited receptive field. Besides, action-irrelated noises brought by camera movement will also harm the quality of extracted motion features. In this paper, we propose a Temporal Saliency Integration (TSI) block, which mainly contains a Salient Motion Excitation (SME) module and a Cross-perception Temporal Integration (CTI) module. Specifically, SME aims to highlight the motion-sensitive area through spatial-level localglobal motion modeling, where the saliency alignment and pyramidal motion modeling are conducted successively between adjacent frames to capture motion dynamics with fewer noises caused by misaligned background. CTI is designed to perform multi-perception temporal modeling through a group of separate 1D convolutions respectively. Meanwhile, temporal interactions across different perceptions are integrated with the attention mechanism. Through these two modules, long short-term temporal relationships can be encoded efficiently by introducing limited additional parameters. Extensive experiments are conducted on several popular benchmarks (i.e., Something-Something V1 & V2, Kinetics-400, UCF-101, and HMDB-51), which demonstrate the effectiveness of our proposed method.
PB  - arXiv
PY  - 2021
ST  - TSI
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-031-19830-4_42
ER  -


TY  - GEN
AU  - El-Assal, M.
AU  - Tirilly, P.
AU  - Bilasco, I.M.
TI  - A study on the effects of pre-processing on spatio-temporal action recognition using spiking neural networks trained with STDP
AB  - There has been an increasing interest in spiking neural networks in recent years. SNNs are seen as hypothetical solutions for the bottlenecks of ANNs in pattern recognition, such as energy efficiency [1]. But current methods such as ANN-to-SNN conversion and back-propagation do not take full advantage of these networks, and unsupervised methods have not yet reached a success comparable to advanced artificial neural networks. It is important to study the behavior of SNNs trained with unsupervised learning methods such as spiketiming dependent plasticity (STDP) on video classification tasks, including mechanisms to model motion information using spikes, as this information is critical for video understanding. This paper presents multiple methods of transposing temporal information into a static format, and then transforming the visual information into spikes using latency coding. These methods are paired with two types of temporal fusion known as early and late fusion, and are used to help the spiking neural network in capturing the spatio-temporal features from videos. In this paper, we rely on the network architecture of a convolutional spiking neural network trained with STDP, and we test the performance of this network when challenged with action recognition tasks. Understanding how a spiking neural network responds to different methods of movement extraction and representation can help reduce the performance gap between SNNs and ANNs. In this paper we show the effect of the similarity in the shape and speed of certain actions on action recognition with spiking neural networks, we also highlight the effectiveness of some methods compared to others.
PB  - arXiv
PY  - 2021
ST  - A study on the effects of pre-processing on spatio-temporal action recognition using spiking neural networks trained with STDP
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cbmi50038.2021.9461922
ER  -


TY  - GEN
AU  - Wang, L.
AU  - Yang, H.
AU  - Wu, W.
AU  - Yao, H.
AU  - Huang, H.
TI  - Temporal action proposal generation with transformers
AB  - Transformer networks are effective at modeling long-range contextual information and have recently demonstrated exemplary performance in the natural language processing domain. Conventionally, the temporal action proposal generation (TAPG) task is divided into two main sub-tasks: boundary prediction and proposal confidence prediction, which rely on the frame-level dependencies and proposal-level relationships separately. To capture the dependencies at different levels of granularity, this paper intuitively presents an unified temporal action proposal generation framework with original Transformers, called TAPG Transformer, which consists of a Boundary Transformer and a Proposal Transformer. Specifically, the Boundary Transformer captures long-term temporal dependencies to predict precise boundary information and the Proposal Transformer learns the rich inter-proposal relationships for reliable confidence evaluation. Extensive experiments are conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, and the results demonstrate that TAPG Transformer outperforms state-of-the-art methods. Equipped with the existing action classifier, our method achieves remarkable performance on the temporal action localization task. Codes and models will be available.
PB  - arXiv
PY  - 2021
ST  - Temporal action proposal generation with transformers
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/s00138-024-01521-7
ER  -


TY  - GEN
AU  - Liu, Y.
AU  - Wang, L.
AU  - Wang, Y.
AU  - Ma, X.
AU  - Qiao, Y.
TI  - FineAction: A Fine-Grained Video Dataset for Temporal Action Localization
AB  - Temporal action localization (TAL) is an important and challenging problem in video understanding. However, most existing TAL benchmarks are built upon the coarse granularity of action classes, which exhibits two major limitations in this task. First, coarse-level actions can make the localization models overfit in high-level context information, and ignore the atomic action details in the video. Second, the coarse action classes often lead to the ambiguous annotations of temporal boundaries, which are inappropriate for temporal action localization. To tackle these problems, we develop a novel large-scale and fine-grained video dataset, coined as FineAction, for temporal action localization. In total, FineAction contains 103K temporal instances of 106 action categories, annotated in 17K untrimmed videos. Compared to the existing TAL datasets, our FineAction takes distinct characteristics of fine action classes with rich diversity, dense annotations of multiple instances, and co-occurring actions of different classes, which introduces new opportunities and challenges for temporal action localization. To benchmark FineAction, we systematically investigate the performance of several popular temporal localization methods on it, and deeply analyze the influence of fine-grained instances in temporal action localization. As a minor contribution, we present a simple baseline approach for handling the fine-grained action detection, which achieves an mAP of 13.17% on our FineAction. We believe that FineAction can advance research of temporal action localization and beyond. The dataset is available at https://deeperaction.github.io/datasets/fineaction.
PB  - arXiv
PY  - 2021
ST  - FineAction
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/tip.2022.3217368
ER  -


TY  - GEN
AU  - Xiao, J.
AU  - Shang, X.
AU  - Yao, A.
AU  - Chua, T.-S.
TI  - NExT-QA: Next phase of question-answering to explaining temporal actions
AB  - We introduce NExT-QA, a rigorously designed video question answering (VideoQA) benchmark to advance video understanding from describing to explaining the temporal actions. Based on the dataset, we set up multi-choice and open-ended QA tasks targeting causal action reasoning, temporal action reasoning, and common scene comprehension. Through extensive analysis of baselines and established VideoQA techniques, we find that top-performing methods excel at shallow scene descriptions but are weak in causal and temporal action reasoning. Furthermore, the models that are effective on multi-choice QA, when adapted to open-ended QA, still struggle in generalizing the answers. This raises doubt on the ability of these models to reason and highlights possibilities for improvement. With detailed results for different question types and heuristic observations for future works, we hope NExT-QA will guide the next generation of VQA research to go beyond superficial description towards a deeper understanding of videos.
PB  - arXiv
PY  - 2021
ST  - NExT-QA
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr46437.2021.00965
ER  -


TY  - GEN
AU  - Zola, F.
AU  - Segurola, L.
AU  - Bruse, J.L.
AU  - Idoate, M.G.
TI  - Temporal graph-based approach for behavioural entity classification
AB  - Graph-based analyses have gained a lot of relevance in the past years due to their high potential in describing complex systems by detailing the actors involved, their relations and their behaviours. Nevertheless, in scenarios where these aspects are evolving over time, it is not easy to extract valuable information or to characterize correctly all the actors. In this study, a two phased approach for exploiting the potential of graph structures in the cybersecurity domain is presented. The main idea is to convert a network classification problem into a graph-based behavioural one. We extract these graph structures that can represent the evolution of both normal and attack entities and apply a temporal dissection approach in order to highlight their micro-dynamics. Further, three clustering techniques are applied to the normal entities in order to aggregate similar behaviours, mitigate the imbalance problem and reduce noisy data. Our approach suggests the implementation of two promising deep learning paradigms for entity classification based on Graph Convolutional Networks.
PB  - arXiv
PY  - 2021
ST  - Temporal graph-based approach for behavioural entity classification
Y2  - 2025/05/05/21:54:29
DO  - 10.18239/jornadas_2021.34.12
ER  -


TY  - GEN
AU  - Zhang, X.-Y.
AU  - Shi, H.
AU  - Li, C.
AU  - Shi, X.
TI  - Action shuffling for weakly supervised temporal localization
AB  - Weakly supervised action localization is a challenging task with extensive applications, which aims to identify actions and the corresponding temporal intervals with only video-level annotations available. This paper analyzes the order-sensitive and location-insensitive properties of actions, and embodies them into a self-augmented learning framework to improve the weakly supervised action localization performance. To be specific, we propose a novel two-branch network architecture with intra/inter-action shuffling, referred to as ActShufNet. The intra-action shuffling branch lays out a self-supervised order prediction task to augment the video representation with inner-video relevance, whereas the inter-action shuffling branch imposes a reorganizing strategy on the existing action contents to augment the training set without resorting to any external resources. Furthermore, the global-local adversarial training is presented to enhance the model's robustness to irrelevant noises. Extensive experiments are conducted on three benchmark datasets, and the results clearly demonstrate the efficacy of the proposed method.
PB  - arXiv
PY  - 2021
ST  - Action shuffling for weakly supervised temporal localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/tip.2022.3185485
ER  -


TY  - GEN
AU  - Crombie, D.
AU  - Spacek, M.A.
AU  - Leibold, C.
AU  - Busse, L.
TI  - Spiking activity in the visual thalamus is coupled to pupil dynamics across temporal scales
AB  - The processing of sensory information, even at early processing stages, is influenced by the internal state of the animal. Internal states, such as arousal, are often characterized by relating neural activity to a single “level” of arousal, defined by a behavioral indicator such as pupil size. In this study, we expand the understanding of arousal-related modulations in sensory systems by uncovering multiple timescales of pupil dynamics and their relationship to neural activity. Specifically, we observed coupling between spiking activity in the mouse dorsal lateral geniculate nucleus (dLGN) of the thalamus and pupil dynamics across timescales spanning three orders of magnitude, from seconds to minutes. Throughout all of these timescales, two distinct spiking patterns - tonic spikes and bursts - preferred opposing phases of pupil dynamics. This multi-scale coupling captures modulations distinct from those captured by pupil size per se, transitions between locomotion and quiescence, or saccadic eye movements. Furthermore, coupling persisted even during viewing of a naturalistic movie, where it contributed to differences in how visual information was encoded. We conclude that dLGN spiking activity is influenced by arousal processes associated with pupil dynamics occurring simultaneously across a broad range of timescales, with implications for the transfer of sensory information to the cortex.
PB  - bioRxiv
PY  - 2021
ST  - Spiking activity in the visual thalamus is coupled to pupil dynamics across temporal scales
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2021.04.30.442134
ER  -


TY  - GEN
AU  - Li, Z.-Y.
AU  - Si, L.-H.
AU  - Shen, B.
AU  - Ling, X.
AU  - Yang, X.
TI  - Altered Regional Brain Functional Activity Predominantly Involving the Right Superior Temporal Gyrus in Patients With Vestibular Migraine Diagnosed According to the Diagnostic Criteria of the Bárány Society and the International Headache Society
AB  - Background: Vestibular migraine (VM) is considered one of the most common cause of episodic central vestibular disorders, the mechanism of VM is currently still unclear. It is worth investigating whether VM belongs to the migraine subtype or is a separate disorder. The development of functional nuclear magnetic resonance (fMRI) in recent years offers the possibility to explore the pathogenesis of VM in depth. The study aimed to investigate resting-state functional brain activity alterations in patients with VM diagnosed based on the diagnostic criteria of the Bárány Society and the International Headache Society. Methods: Seventeen patients with VM who received treatment in our hospital from December 2018 to December 2020 were enrolled. Clinical data of all patients were collected. Eight patients with migraine and 17 health controls (HCs) were also included. All subjects underwent fMRI examination. The amplitude of low frequency fluctuation (ALFF), fractional amplitude of low frequency fluctuation (fALFF) and regional homogeneity (ReHo) were calculated to observe the changes in spontaneous brain activity in patients with VM. Then brain regions with altered spontaneous brain activity were selected for seeded-based functional connectivity (FC) analysis to explore the changes in FC in patients with VM. Results: Among 17 patients with VM, there were 7 males and 10 females with an average age of 39.47±9.78 years old. All patients with VM had a history of migraine. Twelve (70.6%) patients with VM had recurrent spontaneous vertigo, 2 (11.7%) patients had visually-induced vertigo, and 3 (17.6%) patients had head motion-induced vertigo. All 17 patients with VM reported worsening of dizziness vertigo during visual stimulation. The migraine-like symptoms were photophobia or phonophobia (n=15, 88.2%), migraine-like headache (n=8, 47.1%), visual aura during VM onset (n= 7, 41.2%). 5 (29.4%) patients with VM had hyperactive response during the caloric test, and 12 (70.6%) patients had caloric test intolerance. Eleven (64.7%) patients had a history of motion sickness. VM patients showed exhibited significantly increased ALFF and fALFF values in the right temporal lobe (STG and MTG), and significantly increased ReHo values in the right STG, MTG and ITG in comparison with HCs. Compared with patients with migraine, patients with VM showed significantly decreased ALFF values in the right median cingulate and paracingulate gyri, significantly increased fALFF values in the right parietal lobe (postcentral gyrus and superior parietal gyrus), and the right frontal lobe (supplementary motor areas and dorsolateral superior frontal gyrus), as well as significantly increased ReHo values in the right thalamus. Compared with HCs, patients with migraine showed significantly increased ALFF values in the right limbic lobe (right parahippocampal gyrus and right fusiform gyrus), left ITG and the right frontal lobe (supplementary motor areas, right median cingulate and paracingulate gyri, and right right inferior frontal gyrus), significantly decreased ALFF values in the pons and brainstem, significantly decreased ReHo values in the frontal cortex (including left and right supplementary motor areas, left dorsolateral superior frontal gyrus, left median cingulate and paracingulate gyri, right paracentral lobule, right dorsolateral superior frontal gyrus, left and right middle frontal gyrus). Conclusions: Ventral stream of visual processing and allocentric spatial cognition in patients with VM may be impaired. Vertigo attacks in patients with VM may be related to increased spontaneous activity in the right parietal lobe-frontal lobe-thalamus; patients with VM and migraine both had altered brain function, but the underlying mechanism seems to be different.
PB  - Research Square
PY  - 2021
ST  - Altered Regional Brain Functional Activity Predominantly Involving the Right Superior Temporal Gyrus in Patients With Vestibular Migraine Diagnosed According to the Diagnostic Criteria of the Bárány Society and the International Headache Society
Y2  - 2025/05/05/21:54:29
DO  - 10.21203/rs.3.rs-467377/v1
ER  -


TY  - GEN
AU  - Shushruth, S.
AU  - Zylberberg, A.
AU  - Shadlen, M.N.
TI  - Sequential sampling from memory underlies action selection during abstract decision making
AB  - The study of perceptual decision making in monkeys has provided insights into the process by which sensory evidence is integrated towards a decision. When monkeys make decisions with the knowledge of the motor actions the decisions bear upon, the process of evidence integration is instantiated by neurons involved in the selection of said actions. It is less clear how monkeys make decisions when unaware of the actions required to communicate their choice-what we refer to as 'abstract' decisions. We investigated this by training monkeys to associate the direction of motion of a noisy random-dot display with the color of two targets. Crucially, the targets were displayed at unpredictable locations after the motion stimulus was extinguished. We found that monkeys postponed decision formation until the targets were revealed. Neurons in the parietal association area LIP represented the integration of evidence leading to a choice, but as the stimulus was no longer visible, the samples of evidence must have been retrieved from short-term memory. Our results imply that when decisions are temporally unyoked from the motor actions they bear upon, decision formation is protracted until they can be framed in terms of motor actions.
PB  - bioRxiv
PY  - 2021
ST  - Sequential sampling from memory underlies action selection during abstract decision making
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2021.04.30.442176
ER  -


TY  - GEN
AU  - Luo, W.
AU  - Zhang, T.
AU  - Yang, W.
AU  - Wu, F.
AU  - Zhang, Y.
TI  - Action unit memory network for weakly supervised temporal action localization
AB  - Weakly supervised temporal action localization aims to detect and localize actions in untrimmed videos with only video-level labels during training. However, without frame-level annotations, it is challenging to achieve localization completeness and relieve background interference. In this paper, we present an Action Unit Memory Network (AUMN) for weakly supervised temporal action localization, which can mitigate the above two challenges by learning an action unit memory bank. In the proposed AUMN, two attention modules are designed to update the memory bank adaptively and learn action units specific classifiers. Furthermore, three effective mechanisms (diversity, homogeneity and sparsity) are designed to guide the updating of the memory network. To the best of our knowledge, this is the first work to explicitly model the action units with a memory network. Extensive experimental results on two standard benchmarks (THUMOS14 and ActivityNet) demonstrate that our AUMN performs favorably against state-of-the-art methods. Specifically, the average mAP of IoU thresholds from 0.1 to 0.5 on the THUMOS14 dataset is significantly improved from 47.0% to 52.1%.
PB  - arXiv
PY  - 2021
ST  - Action unit memory network for weakly supervised temporal action localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr46437.2021.00984
ER  -


TY  - GEN
AU  - Ghamsarian, N.
AU  - Taschwer, M.
AU  - Putzgruber-Adamitsch, D.
AU  - Sarny, S.
AU  - Schoeffmann, K.
TI  - Relevance detection in cataract surgery videos by spatio-temporal action localization
AB  - In cataract surgery, the operation is performed with the help of a microscope. Since the microscope enables watching real-time surgery by up to two people only, a major part of surgical training is conducted using the recorded videos. To optimize the training procedure with the video content, the surgeons require an automatic relevance detection approach. In addition to relevance-based retrieval, these results can be further used for skill assessment and irregularity detection in cataract surgery videos. In this paper, a three-module framework is proposed to detect and classify the relevant phase segments in cataract videos. Taking advantage of an idle frame recognition network, the video is divided into idle and action segments. To boost the performance in relevance detection, the cornea where the relevant surgical actions are conducted is detected in all frames using Mask R-CNN. The spatiotemporally localized segments containing higher-resolution information about the pupil texture and actions, and complementary temporal information from the same phase are fed into the relevance detection module. This module consists of four parallel recurrent CNNs being responsible to detect four relevant phases that have been defined with medical experts. The results will then be integrated to classify the action phases as irrelevant or one of four relevant phases. Experimental results reveal that the proposed approach outperforms static CNNs and different configurations of feature-based and end-to-end recurrent networks.
PB  - arXiv
PY  - 2021
ST  - Relevance detection in cataract surgery videos by spatio-temporal action localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/icpr48806.2021.9412525
ER  -


TY  - GEN
AU  - Ling, L.
AU  - Qian, X.
AU  - Ukkusuri, S.V.
AU  - Guo, S.
TI  - Spatiotemporal impacts of human activities and socio-demographics during the COVID-19 outbreak in the U.S.
AB  - Understanding inuencing factors is essential for the surveillance and prevention of infectious diseases, and the factors are likely to vary spatially and temporally as the disease progresses. Taking daily cases and deaths data during the coronavirus disease 2019 (COVID-19) outbreak in the U.S. as a case study, we develop a mobility-augmented geographically and temporally weighted regression (M-GTWR) model to quantify the spatiotemporal impacts of social-demographic factors and human activities on the COVID-19 dynamics. Different from the base GTWR model, we incorporate a mobility-adjusted distance weight matrix where travel mobility is used in addition to the spatial adjacency to capture the correlations among local observations. The model residuals suggest that the proposed model achieves a substantial improvement over other benchmark methods in addressing the spatiotemporal nonstationarity. Our results reveal that the impacts of social-demographic and human activity variables present significant spatiotemporal heterogeneity. In particular, a 1% increase in population density may lead to 0.63% and 0.71% more daily cases and deaths, and a 1% increase in the mean commuting time may result in 0.22% and 0.95% increases in daily cases and deaths. Although increased human activities will, in general, intensify the disease outbreak, we report that the effects of grocery and pharmacy-related activities are insignificant in areas with high population density. And activities at the workplace and public transit are found to either increase or decrease the number of cases and deaths, depending on particular locations. The results of our study establish a quantitative framework for identifying influencing factors during a disease outbreak, and the obtained insights may have significant implications in guiding the policy-making against infectious diseases.
PB  - arXiv
PY  - 2021
ST  - Spatiotemporal impacts of human activities and socio-demographics during the COVID-19 outbreak in the U.S.
Y2  - 2025/05/05/21:54:29
DO  - 10.1186/s12889-022-13793-7
ER  -


TY  - GEN
AU  - Ding, X.
AU  - Wang, K.
AU  - Wang, C.
AU  - Lan, T.
AU  - Liu, L.
TI  - Sequential convolutional network for behavioral pattern extraction in gait recognition
AB  - As a unique and promising biometric, video-based gait recognition has broad applications. The key step of this methodology is to learn the walking pattern of individuals, which, however, often suffers challenges to extract the behavioral feature from a sequence directly. Most existing methods just focus on either the appearance or the motion pattern. To overcome these limitations, we propose a sequential convolutional network (SCN) from a novel perspective, where spatiotemporal features can be learned by a basic convolutional backbone. In SCN, behavioral information extractors (BIE) are constructed to comprehend intermediate feature maps in time series through motion templates where the relationship between frames can be analyzed, thereby distilling the information of the walking pattern. Furthermore, a multi-frame aggregator in SCN performs feature integration on a sequence whose length is uncertain, via a mobile 3D convolutional layer. To demonstrate the effectiveness, experiments have been conducted on two popular public benchmarks, CASIA-B and OU-MVLP, and our approach is demonstrated superior performance, comparing with the state-of-art methods.
PB  - arXiv
PY  - 2021
ST  - Sequential convolutional network for behavioral pattern extraction in gait recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1016/j.neucom.2021.08.054
ER  -


TY  - GEN
AU  - Jin, C.
AU  - Chen, Y.
AU  - Li, G.
AU  - Zhang, T.
AU  - Li, T.
TI  - Low pass filter for anti-aliasing in temporal action localization
AB  - In temporal action localization methods, temporal downsampling operations are widely used to extract proposal features, but they often lead to the aliasing problem, due to lacking consideration of sampling rates. This paper aims to verify the existence of aliasing in TAL methods and investigate utilizing low pass filters to solve this problem by inhibiting the high-frequency band. However, the highfrequency band usually contains large amounts of specific information, which is important for model inference. Therefore, it is necessary to make a tradeoff between anti-aliasing and reserving high-frequency information. To acquire optimal performance, this paper learns different cutoff frequencies for different instances dynamically. This design can be plugged into most existing temporal modeling programs requiring only one additional cutoff frequency parameter. Integrating low pass filters to the downsampling operations significantly improves the detection performance and achieves comparable results on THUMOS'14, ActivityNet 1.3, and Charades datasets. Experiments demonstrate that anti-aliasing with low pass filters in TAL is advantageous and efficient.
PB  - arXiv
PY  - 2021
ST  - Low pass filter for anti-aliasing in temporal action localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/jssc.1982.1051856
ER  -


TY  - GEN
AU  - Khan, S.
AU  - Cuzzolin, F.
TI  - Spatiotemporal Deformable Scene Graphs for Complex Activity Detection
AB  - Long-term complex activity recognition and localisation can be crucial for decision making in autonomous systems such as smart cars and surgical robots. Here we address the problem via a novel deformable, spatiotemporal scene graph approach, consisting of three main building blocks: (i) action tube detection, (ii) the modelling of the deformable geometry of parts, and (iii) a graph convolutional network. Firstly, action tubes are detected in a series of snippets. Next, a new 3D deformable RoI pooling layer is designed for learning the flexible, deformable geometry of the constituent action tubes. Finally, a scene graph is constructed by considering all parts as nodes and connecting them based on different semantics such as order of appearance, sharing the same action label and feature similarity. We also contribute fresh temporal complex activity annotation for the recently released ROAD autonomous driving and SARAS-ESAD surgical action datasets and show the adaptability of our framework to different domains. Our method is shown to significantly outperform graph-based competitors on both augmented datasets.
PB  - arXiv
PY  - 2021
ST  - Spatiotemporal Deformable Scene Graphs for Complex Activity Detection
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/globecom46510.2021.9685640
ER  -


TY  - GEN
AU  - Chen, M.-H.
AU  - Li, B.
AU  - Bao, Y.
AU  - AlRegib, G.
TI  - Action segmentation with mixed temporal domain adaptation
AB  - The main progress for action segmentation comes from densely-annotated data for fully-supervised learning. Since manual annotation for frame-level actions is time-consuming and challenging, we propose to exploit auxiliary unlabeled videos, which are much easier to obtain, by shaping this problem as a domain adaptation (DA) problem. Although various DA techniques have been proposed in recent years, most of them have been developed only for the spatial direction. Therefore, we propose Mixed Temporal Domain Adaptation (MTDA) to jointly align frame- and video-level embedded feature spaces across domains, and further integrate with the domain attention mechanism to focus on aligning the frame-level features with higher domain discrepancy, leading to more effective domain adaptation. Finally, we evaluate our proposed methods on three challenging datasets (GTEA, 50Salads, and Breakfast), and validate that MTDA outperforms the current state-of-the-art methods on all three datasets by large margins (e.g. 6.4% gain on F1@50 and 6.8% gain on the edit score for GTEA).
PB  - arXiv
PY  - 2021
ST  - Action segmentation with mixed temporal domain adaptation
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/wacv45572.2020.9093535
ER  -


TY  - GEN
AU  - Qu, S.
AU  - Chen, G.
AU  - Li, Z.
AU  - Lu, F.
AU  - Knoll, A.
TI  - ACM-Net: Action context modeling network for weakly-supervised temporal action localization
AB  - Weakly-supervised temporal action localization aims to localize action instances temporal boundary and identify the corresponding action category with only video-level labels. Traditional methods mainly focus on foreground and background frames separation with only a single attention branch and class activation sequence. However, we argue that apart from the distinctive foreground and background frames there are plenty of semantically ambiguous action context frames. It does not make sense to group those context frames to the same background class since they are semantically related to a specific action category. Consequently, it is challenging to suppress action context frames with only a single class activation sequence. To address this issue, in this paper, we propose an action-context modeling network termed ACM-Net, which integrates a three-branch attention module to measure the likelihood of each temporal point being action instance, context, or non-action background, simultaneously. Then based on the obtained three-branch attention values, we construct three-branch class activation sequences to represent the action instances, contexts, and non-action backgrounds, individually. To evaluate the effectiveness of our ACM-Net, we conduct extensive experiments on two benchmark datasets, THUMOS-14 and ActivityNet-1.3. The experiments show that our method can outperform current state-of-the-art methods, and even achieve comparable performance with fully-supervised methods. Code can be found at https://github.com/ispc-lab/ACM-Net.
PB  - arXiv
PY  - 2021
ST  - ACM-Net
Y2  - 2025/05/05/21:54:29
DO  - 10.1609/aaai.v35i3.16322
ER  -


TY  - GEN
AU  - Wang, X.
AU  - Zhang, S.
AU  - Qing, Z.
AU  - Gao, C.
AU  - Sang, N.
TI  - Self-supervised learning for semi-supervised temporal action proposal
AB  - Self-supervised learning presents a remarkable performance to utilize unlabeled data for various video tasks. In this paper, we focus on applying the power of self-supervised methods to improve semi-supervised action proposal generation. Particularly, we design an effective Self-supervised Semi-supervised Temporal Action Proposal (SSTAP) framework. The SSTAP contains two crucial branches, i.e., temporal-aware semi-supervised branch and relation-aware self-supervised branch. The semi-supervised branch improves the proposal model by introducing two temporal perturbations, i.e., temporal feature shift and temporal feature flip, in the mean teacher framework. The self-supervised branch defines two pretext tasks, including masked feature reconstruction and clip-order prediction, to learn the relation of temporal clues. By this means, SSTAP can better explore unlabeled videos, and improve the discriminative abilities of learned action features. We extensively evaluate the proposed SSTAP on THUMOS14 and ActivityNet v1.3 datasets. The experimental results demonstrate that SSTAP significantly outperforms state-of-the-art semi-supervised methods and even matches fully-supervised methods. Code is available at https://github.com/wangxiang1230/SSTAP.
PB  - arXiv
PY  - 2021
ST  - Self-supervised learning for semi-supervised temporal action proposal
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr46437.2021.00194
ER  -


TY  - GEN
AU  - Ju, C.
AU  - Zhao, P.
AU  - Chen, S.
AU  - Zhang, X.
AU  - Tian, Q.
TI  - Adaptive mutual supervision for weakly-supervised temporal action localization
AB  - Weakly-supervised temporal action localization aims to localize actions in untrimmed videos with only video-level action category labels. Most of previous methods ignore the incompleteness issue of Class Activation Sequences (CAS), suffering from trivial localization results. To solve this issue, we introduce an adaptive mutual supervision framework (AMS) with two branches, where the base branch adopts CAS to localize the most discriminative action regions, while the supplementary branch localizes the less discriminative action regions through a novel adaptive sampler. The adaptive sampler dynamically updates the input of the supplementary branch with a sampling weight sequence negatively correlated with the CAS from the base branch, thereby prompting the supplementary branch to localize the action regions underestimated by the base branch. To promote mutual enhancement between these two branches, we construct mutual location supervision. Each branch leverages location pseudo-labels generated from the other branch as localization supervision. By alternately optimizing the two branches in multiple iterations, we progressively complete action regions. Extensive experiments on THUMOS14 and ActivityNet1.2 demonstrate that the proposed AMS method significantly outperforms the state-of-the-art methods.
PB  - arXiv
PY  - 2021
ST  - Adaptive mutual supervision for weakly-supervised temporal action localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/tmm.2022.3213478
ER  -


TY  - GEN
AU  - Pin˜eiro, M.
AU  - Mena, W.
AU  - Ewer, J.
AU  - Orio, P.
TI  - Extracting temporal relationships between weakly coupled peptidergic and motoneuronal signaling: Application to Drosophila ecdysis behavior
AB  - Neuromodulators, such as neuropeptides, can regulate and reconfigure neural circuits to alter their output, affecting in this way animal physiology and behavior. The interplay between the activity of neuronal circuits, their modulation by neuropeptides, and the resulting behavior, is still poorly understood. Here, we present a quantitative framework to study the relationships between the temporal pattern of activity of peptidergic neurons and of motoneurons during Drosophila ecdysis behavior, a highly stereotyped motor sequence that is critical for insect growth. We analyzed, in the time and frequency domains, simultaneous intracellular calcium recordings of peptidergic CCAP (crustacean cardioactive peptide) neurons and motoneurons obtained from isolated central nervous systems throughout fictive ecdysis behavior induced ex vivo by Ecdysis triggering hormone. We found that the activity of both neuronal populations is tightly coupled in a cross-frequency manner, suggesting that CCAP neurons modulate the faster oscillation of motoneurons. To explore this idea further, we used a probabilistic logistic model to show that calcium dynamics in CCAP neurons can predict the oscillation of motoneurons, both in a simple model and in a conductance-base model capable of simulating many of the observed neural dynamics features. Finally, we developed an algorithm to quantify the motor behavior observed in videos of pupal ecdysis, and compared their features to the patterns of neuronal calcium activity recorded ex vivo. We found that the motor activity of the intact animal is more regular than the motoneuronal activity recorded from the ex vivo preparations during fictive ecdysis behavior; the analysis of movement patterns also allowed us to identify a new post-ecdysis phase.
PB  - bioRxiv
PY  - 2021
ST  - Extracting temporal relationships between weakly coupled peptidergic and motoneuronal signaling
Y2  - 2025/05/05/21:54:29
DO  - 10.1371/journal.pcbi.1008933
ER  -


TY  - GEN
AU  - Lee, S.
AU  - Deshpande, S.S.
AU  - Merricks, E.M.
AU  - Schevon, C.A.
AU  - Van Drongelen, W.
TI  - Spatiotemporal correlations of mesoscale neocortical activities in patients with focal epilepsy
AB  - Brain function is reflected in both the action potentials of individual neurons and interactions through e.g. synaptic currents reflected in widespread, slow fluctuations of the local field potential (LFP). We analyzed microelectrode array data to determine state-dependent correlations between action potential and LFP during seizure events as well as interictally in patients with focal epilepsy. We also examined activity in two different cortical network territories: the seizure core and surrounding penumbra (Schevon et al., 2012). The cross-correlation of spiking activity in the core showed an association of the ictal action potentials with the global oscillatory aspect of the seizure activity and indicated local failure of inhibitory restraint surrounding the ictal spike. These patterns were not observed in the penumbra. Our analyses from clinical recordings and a model of a single ictal spike in the core revealed that both the temporal and spatial components of the networkfs cross-correlation can be approximated by a sine cardinal (sinc) function. The biological interpretation of these findings is that important functional differences across the neocortical network exist, with a critical role of the millimeter-range excitatory connections within the grey matter. Therefore, localized intervention that prevents escape of hyperactivity from the seizure core may be considered as a therapeutic strategy.
PB  - medRxiv
PY  - 2021
ST  - Spatiotemporal correlations of mesoscale neocortical activities in patients with focal epilepsy
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2021.03.31.21254715
ER  -


TY  - GEN
AU  - Faraji, F.
AU  - Lotfi, F.
AU  - Khorramdel, J.
AU  - Najafi, A.
AU  - Ghaffari, A.
TI  - Drowsiness detection based on driver temporal behavior using a new developed dataset
AB  - Driver drowsiness detection has been the subject of many researches in the past few decades and various methods have been developed to detect it. In this study, as an image based approach with adequate accuracy, along with expedite process, we applied YOLOv3 (You Look Only Once-version3) CNN (Convolutional Neural Network) for extracting facial features automatically. Then, LSTM (Long-Short Term Memory) neural network is employed to learn driver temporal behaviors including yawning and blinking time period as well as sequence classification. To train YOLOv3, we utilized our collected dataset alongside transfer learning method. Moreover, the dataset for LSTM training process is produced by the mentioned CNN and is formatted as a two-dimensional sequence comprised of eye blinking and yawning time durations. The developed dataset considers both disturbances such as illumination and drivers head posture. To have real-time experiments a multi thread framework is developed to run both CNN and LSTM in parallel. Finally, results indicate the hybrid of CNN and LSTM ability in drowsiness detection and the effectiveness of the proposed method.
PB  - arXiv
PY  - 2021
ST  - Drowsiness detection based on driver temporal behavior using a new developed dataset
Y2  - 2025/05/05/21:54:29
DO  - 10.2139/ssrn.4031268
ER  -


TY  - GEN
AU  - Zhang, C.
AU  - Cao, M.
AU  - Yang, D.
AU  - Chen, J.
AU  - Zou, Y.
TI  - CoLA: Weakly-supervised temporal action localization with snippet contrastive learning
AB  - Weakly-supervised temporal action localization (WSTAL) aims to localize actions in untrimmed videos with only video-level labels. Most existing models follow the "localization by classification" procedure: locate temporal regions contributing most to the video-level classification. Generally, they process each snippet (or frame) individually and thus overlook the fruitful temporal context relation. Here arises the single snippet cheating issue: "hard" snippets are too vague to be classified. In this paper, we argue that learning by comparing helps identify these hard snippets and we propose to utilize snippet Contrastive learning to Localize Actions, CoLA for short. Specifically, we propose a Snippet Contrast (SniCo) Loss to refine the hard snippet representation in feature space, which guides the network to perceive precise temporal boundaries and avoid the temporal interval interruption. Besides, since it is infeasible to access frame-level annotations, we introduce a Hard Snippet Mining algorithm to locate the potential hard snippets. Substantial analyses verify that this mining strategy efficaciously captures the hard snippets and SniCo Loss leads to more informative feature representation. Extensive experiments show that CoLA achieves state-of-the-art results on THUMOS'14 and ActivityNet v1.2 datasets.
PB  - arXiv
PY  - 2021
ST  - CoLA
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr46437.2021.01575
ER  -


TY  - GEN
AU  - Liu, Z.
AU  - Wang, L.
AU  - Tang, W.
AU  - Zheng, N.
AU  - Hua, G.
TI  - Weakly supervised temporal action localization through learning explicit subspaces for action and context
AB  - Weakly-supervised Temporal Action Localization (WS-TAL) methods learn to localize temporal starts and ends of action instances in a video under only video-level supervision. Existing WS-TAL methods rely on deep features learned for action recognition. However, due to the mismatch between classification and localization, these features cannot distinguish the frequently co-occurring contextual background, i.e., the context, and the actual action instances. We term this challenge action-context confusion, and it will adversely affect the action localization accuracy. To address this challenge, we introduce a framework that learns two feature subspaces respectively for actions and their context. By explicitly accounting for action visual elements, the action instances can be localized more precisely without the distraction from the context. To facilitate the learning of these two feature subspaces with only video-level categorical labels, we leverage the predictions from both spatial and temporal streams for snippets grouping. In addition, an unsupervised learning task is introduced to make the proposed module focus on mining temporal information. The proposed approach outperforms state-of-the-art WS-TAL methods on three benchmarks, i.e., THUMOS14, ActivityNet v1.2 and v1.3 datasets.
PB  - arXiv
PY  - 2021
ST  - Weakly supervised temporal action localization through learning explicit subspaces for action and context
Y2  - 2025/05/05/21:54:29
DO  - 10.1609/aaai.v35i3.16323
ER  -


TY  - GEN
AU  - Chang, S.
AU  - Wang, P.
AU  - Wang, F.
AU  - Li, H.
AU  - Feng, J.
TI  - Augmented transformer with adaptive graph for temporal action proposal generation
AB  - Temporal action proposal generation (TAPG) is a fundamental and challenging task in video understanding, especially in temporal action detection. Most previous works focus on capturing the local temporal context and can well locate simple action instances with clean frames and clear boundaries. However, they generally fail in complicated scenarios where interested actions involve irrelevant frames and background clutters, and the local temporal context becomes less effective. To deal with these problems, we present an augmented transformer with adaptive graph network (ATAG) to exploit both long-range and local temporal contexts for TAPG. Specifically, we enhance the vanilla transformer by equipping a snippet actionness loss and a front block, dubbed augmented transformer, and it improves the abilities of capturing long-range dependencies and learning robust feature for noisy action instances. Moreover, an adaptive graph convolutional network (GCN) is proposed to build local temporal context by mining the position information and difference between adjacent features. The features from the two modules carry rich semantic information of the video, and are fused for effective sequential proposal generation. Extensive experiments are conducted on two challenging datasets, THUMOS14 and ActivityNet1.3, and the results demonstrate that our method outperforms state-of-the-art TAPG methods. Our code will be released soon.
PB  - arXiv
PY  - 2021
ST  - Augmented transformer with adaptive graph for temporal action proposal generation
Y2  - 2025/05/05/21:54:29
DO  - 10.1145/3552458.3556443
ER  -


TY  - GEN
AU  - Xu, M.
AU  - Pérez-Rúa, J.-M.
AU  - Zhu, X.
AU  - Ghanem, B.
AU  - Martinez, B.
TI  - Low-fidelity end-to-end video encoder pre-training for temporal action localization
AB  - Most existing temporal action localization (TAL) methods rely on a transfer learning pipeline, first optimizing a video encoder on a large action classification dataset (i.e., source domain), followed by freezing the encoder and training a TAL head on the action localization dataset (i.e., target domain). This results in a task discrepancy problem for the video encoder – trained for action classification, but used for TAL. Intuitively, joint optimization with both the video encoder and TAL head is an obvious solution to this discrepancy. However, this is not operable for TAL subject to the GPU memory constraints, due to the prohibitive computational cost in processing long untrimmed videos. In this paper, we resolve this challenge by introducing a novel low-fidelity (LoFi) video encoder optimization method. Instead of always using the full training configurations in TAL learning, we propose to reduce the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution so that jointly optimizing the video encoder and TAL head becomes operable under the same memory conditions of a mid-range hardware budget. Crucially, this enables the gradients to flow backwards through the video encoder conditioned on a TAL supervision loss, favourably solving the task discrepancy problem and providing more effective feature representations. Extensive experiments show that the proposed LoFi optimization approach can significantly enhance the performance of existing TAL methods. Encouragingly, even with a lightweight ResNet18 based video encoder in a single RGB stream, our method surpasses two-stream (RGB + optical flow) ResNet50 based alternatives, often by a good margin.
PB  - arXiv
PY  - 2021
ST  - Low-fidelity end-to-end video encoder pre-training for temporal action localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr52733.2024.01739
ER  -


TY  - GEN
AU  - Liu, Z.
AU  - Wang, L.
AU  - Zhang, Q.
AU  - Zheng, N.
AU  - Hua, G.
TI  - ACSNet: Action-Context Separation Network for weakly supervised temporal action localization
AB  - The object of Weakly-supervised Temporal Action Localization (WS-TAL) is to localize all action instances in an untrimmed video with only video-level supervision. Due to the lack of frame-level annotations during training, current WS-TAL methods rely on attention mechanisms to localize the foreground snippets or frames that contribute to the video-level classification task. This strategy frequently confuse context with the actual action, in the localization result. Separating action and context is a core problem for precise WS-TAL, but it is very challenging and has been largely ignored in the literature. In this paper, we introduce an Action-Context Separation Network (ACSNet) that explicitly takes into account context for accurate action localization. It consists of two branches (i.e., the Foreground-Background branch and the Action-Context branch). The Foreground-Background branch first distinguishes foreground from background within the entire video while the Action-Context branch further separates the foreground as action and context. We associate video snippets with two latent components (i.e., a positive component and a negative component), and their different combinations can effectively characterize foreground, action and context. Furthermore, we introduce extended labels with auxiliary context categories to facilitate the learning of action-context separation. Experiments on THUMOS14 and ActivityNet v1.2/v1.3 datasets demonstrate the ACSNet outperforms existing state-of-the-art WS-TAL methods by a large margin.
PB  - arXiv
PY  - 2021
ST  - ACSNet
Y2  - 2025/05/05/21:54:29
DO  - 10.1609/aaai.v35i3.16322
ER  -


TY  - GEN
AU  - Qing, Z.
AU  - Su, H.
AU  - Gan, W.
AU  - Gao, C.
AU  - Sang, N.
TI  - Temporal context aggregation network for temporal action proposal refinement
AB  - Temporal action proposal generation aims to estimate temporal intervals of actions in untrimmed videos, which is a challenging yet important task in the video understanding field. The proposals generated by current methods still suffer from inaccurate temporal boundaries and inferior confidence used for retrieval owing to the lack of efficient temporal modeling and effective boundary context utilization. In this paper, we propose Temporal Context Aggregation Network (TCANet) to generate high-quality action proposals through “local and global” temporal context aggregation and complementary as well as progressive boundary refinement. Specifically, we first design a Local-Global Temporal Encoder (LGTE), which adopts the channel grouping strategy to efficiently encode both “local and global” temporal inter-dependencies. Furthermore, both the boundary and internal context of proposals are adopted for frame-level and segment-level boundary regressions, respectively. Temporal Boundary Regressor (TBR) is designed to combine these two regression granularities in an end-to-end fashion, which achieves the precise boundaries and reliable confidence of proposals through progressive refinement. Extensive experiments are conducted on three challenging datasets: HACS, ActivityNet-v1.3, and THUMOS-14, where TCANet can generate proposals with high precision and recall. By combining with the existing action classifier, TCANet can obtain remarkable temporal action detection performance compared with other methods. Not surprisingly, the proposed TCANet won the 1st place in the CVPR 2020 - HACS challenge leaderboard on temporal action localization task.
PB  - arXiv
PY  - 2021
ST  - Temporal context aggregation network for temporal action proposal refinement
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr46437.2021.00055
ER  -


TY  - GEN
AU  - Lin, C.
AU  - Xu, C.
AU  - Luo, D.
AU  - Huang, F.
AU  - Fu, Y.
TI  - Learning salient boundary feature for anchor-free temporal action localization
AB  - Temporal action localization is an important yet challenging task in video understanding. Typically, such a task aims at inferring both the action category and localization of the start and end frame for each action instance in a long, untrimmed video. While most current models achieve good results by using pre-defined anchors and numerous actionness, such methods could be bothered with both large number of outputs and heavy tuning of locations and sizes corresponding to different anchors. Instead, anchor-free methods is lighter, getting rid of redundant hyper-parameters, but gains few attention. In this paper, we propose the first purely anchor-free temporal localization method, which is both efficient and effective. Our model includes (i) an end-to-end trainable basic predictor, (ii) a saliency-based refinement module to gather more valuable boundary features for each proposal with a novel boundary pooling, and (iii) several consistency constraints to make sure our model can find the accurate boundary given arbitrary proposals. Extensive experiments show that our method beats all anchor-based and actionness-guided methods with a remarkable margin on THUMOS14, achieving state-of-the-art results, and comparable ones on ActivityNet v1.3. Code is available at https://github.com/ TencentYoutuResearch / ActionDetection - AFSD.
PB  - arXiv
PY  - 2021
ST  - Learning salient boundary feature for anchor-free temporal action localization
Y2  - 2025/05/05/21:54:29
ER  -


TY  - GEN
AU  - Bo, Y.
AU  - Lu, Y.
AU  - He, W.
TI  - CLTA: Contents and length-based temporal attention for few-shot action recognition
AB  - Few-shot action recognition has attracted increasing attention due to the difficulty in acquiring the properly labelled training samples. Current works have shown that preserving spatial information and comparing video descriptors are crucial for few-shot action recognition. However, the importance of preserving temporal information is not well discussed. In this paper, we propose a Contents and Length-based Temporal Attention (CLTA) model, which learns customized temporal attention for the individual video to tackle the few-shot action recognition problem. CLTA utilizes the Gaussian likelihood function as the template to generate temporal attention and trains the learning matrices to study the mean and standard deviation based on both frame contents and length. We show that even a not fine-tuned backbone with an ordinary softmax classifier can still achieve similar or better results compared to the state-of-the-art few-shot action recognition with precisely captured temporal attention.
PB  - arXiv
PY  - 2021
ST  - CLTA
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/s10489-024-05294-4
ER  -


TY  - GEN
AU  - Ren, P.
AU  - Xiao, G.
AU  - Chang, X.
AU  - Li, Z.
AU  - Chen, X.
TI  - NAS-TC: Neural architecture search on temporal convolutions for complex action recognition
AB  - In the field of complex action recognition in videos, the quality of the designed model plays a crucial role in the final performance. However, artificially designed network structures often rely heavily on the researchers' knowledge and experience. Accordingly, because of the automated design of its network structure, Neural architecture search (NAS) has achieved great success in the image processing field and attracted substantial research attention in recent years. Although some NAS methods have reduced the number of GPU search days required to single digits in the image field, directly using 3D convolution to extend NAS to the video field is still likely to produce a surge in computing volume. To address this challenge, we propose a new processing framework called Neural Architecture Search-Temporal Convolutional (NAS-TC). Our proposed framework is divided into two phases. In the first phase, the classical CNN network is used as the backbone network to complete the computationally intensive feature extraction task. In the second stage, a simple stitching search to the cell is used to complete the relatively lightweight long-range temporal-dependent information extraction. This ensures our method will have more reasonable parameter assignments and can handle minute-level videos. Finally, we conduct sufficient experiments on multiple benchmark datasets and obtain competitive recognition accuracy.
PB  - arXiv
PY  - 2021
ST  - NAS-TC
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/tpami.2017.2712608
ER  -


TY  - GEN
AU  - Li, Z.
AU  - Farha, Y.A.
AU  - Gall, J.
TI  - Temporal action segmentation from timestamp supervision
AB  - Temporal action segmentation approaches have been very successful recently. However, annotating videos with frame-wise labels to train such models is very expensive and time consuming. While weakly supervised methods trained using only ordered action lists require less annotation effort, the performance is still worse than fully supervised approaches. In this paper, we propose to use timestamp supervision for the temporal action segmentation task. Timestamps require a comparable annotation effort to weakly supervised approaches, and yet provide a more supervisory signal. To demonstrate the effectiveness of timestamp supervision, we propose an approach to train a segmentation model using only timestamps annotations. Our approach uses the model output and the annotated timestamps to generate frame-wise labels by detecting the action changes. We further introduce a confidence loss that forces the predicted probabilities to monotonically decrease as the distance to the timestamps increases. This ensures that all and not only the most distinctive frames of an action are learned during training. The evaluation on four datasets shows that models trained with timestamps annotations achieve comparable performance to the fully supervised approaches.
PB  - arXiv
PY  - 2021
ST  - Temporal action segmentation from timestamp supervision
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr46437.2021.00826
ER  -


TY  - GEN
AU  - Qin, X.
AU  - Zhao, H.
AU  - Lin, G.
AU  - Xu, S.
AU  - Li, X.
TI  - PcmNet: Position-sensitive context modeling network for temporal action localization
AB  - Temporal action localization is an important and challenging task that aims to locate temporal regions in real-world untrimmed videos where actions occur and recognize their classes. It is widely acknowledged that video context is a critical cue for video understanding, and exploiting the context has become an important strategy to boost localization performance. However, previous state-of-the-art methods focus more on exploring semantic context which captures the feature similarity among frames or proposals, and neglect positional context which is vital for temporal localization. In this paper, we propose a temporal-position-sensitive context modeling approach to incorporate both positional and semantic information for more precise action localization. Specifically, we first augment feature representations with directed temporal positional encoding, and then conduct attention-based information propagation, in both frame-level and proposal-level. Consequently, the generated feature representations are significantly empowered with the discriminative capability of encoding the position-aware context information, and thus benefit boundary detection and proposal evaluation. We achieve state-of-the-art performance on both two challenging datasets, THUMOS-14 and ActivityNet-1.3, demonstrating the effectiveness and generalization ability of our method.
PB  - arXiv
PY  - 2021
ST  - PcmNet
Y2  - 2025/05/05/21:54:29
DO  - 10.1016/j.neucom.2022.08.040
ER  -


TY  - GEN
AU  - Tirupattur, P.
AU  - Rawat, Y.
AU  - Duarte, K.
AU  - Shah, M.
TI  - Modeling Multi-Label Action Dependencies for Temporal Action Localization
AB  - Real-world videos contain many complex actions with inherent relationships between action classes. In this work, we propose an attention-based architecture that models these action relationships for the task of temporal action localization in untrimmed videos. As opposed to previous works that leverage video-level co-occurrence of actions, we distinguish the relationships between actions that occur at the same time-step and actions that occur at different time-steps (i.e. those which precede or follow each other). We define these distinct relationships as action dependencies. We propose to improve action localization performance by modeling these action dependencies in a novel attention-based Multi-Label Action Dependency (MLAD) layer. The MLAD layer consists of two branches: a Co-occurrence Dependency Branch and a Temporal Dependency Branch to model co-occurrence action dependencies and temporal action dependencies, respectively. We observe that existing metrics used for multi-label classification do not explicitly measure how well action dependencies are modeled, therefore, we propose novel metrics that consider both co-occurrence and temporal dependencies between action classes. Through empirical evaluation and extensive analysis, we show improved performance over state-of-the-art methods on multi-label action localization benchmarks (MultiTHUMOS and Charades) in terms of f-mAP and our proposed metric.
PB  - arXiv
PY  - 2021
ST  - Modeling Multi-Label Action Dependencies for Temporal Action Localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr46437.2021.00151
ER  -


TY  - GEN
AU  - Kahatapitiya, K.
AU  - Ryoo, M.S.
TI  - Coarse-fine networks for temporal activity detection in videos
AB  - In this paper, we introduce Coarse-Fine Networks, a twostream architecture which benefits from different abstractions of temporal resolution to learn better video representations for long-term motion. Traditional Video models process inputs at one (or few) fixed temporal resolution without any dynamic frame selection. However, we argue that, processing multiple temporal resolutions of the input and doing so dynamically by learning to estimate the importance of each frame can largely improve video representations, specially in the domain of temporal activity localization. To this end, we propose (1) 'Grid Pool', a learned temporal downsampling layer to extract coarse features, and, (2) 'Multi-stage Fusion', a spatio-temporal attention mechanism to fuse a finegrained context with the coarse features. We show that our method outperforms the state-of-the-arts for action detection in public datasets including Charades with a significantly reduced compute and memory footprint. The code is available at https://github.com/kkahatapitiya/Coarse-Fine-Networks.
PB  - arXiv
PY  - 2021
ST  - Coarse-fine networks for temporal activity detection in videos
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr46437.2021.00828
ER  -


TY  - GEN
AU  - Senande-Rivera, M.
AU  - Insua-Costa, D.
AU  - Miguez-Macho, G.
TI  - Spatial and temporal expansion of global fire activity in response to climate change
AB  - Global warming is expected to alter wildfire potential and fire season severity, but the magnitude and location of change is still unclear. Here, we show that climate largely determines present fire-prone regions and their fire season. We categorize these regions according to the climatic characteristics of their fire season into four classes, within general Boreal, Temperate, Tropical and Arid climate zones. Based on climate model projections, we assess the modification of the fire-prone regions in extent and fire season length at the end of the 21st century. We find that due to global warming, the global fire-prone area will increase by 27%, mostly in Boreal (+95%) and Temperate (+17%) zones, where there will also be significant lengthenings of the potential fire season. Our estimates of the global expansion of fire-prone areas highlight the large but uneven impact of a warming climate on Earth’s environment.
PB  - Research Square
PY  - 2021
ST  - Spatial and temporal expansion of global fire activity in response to climate change
Y2  - 2025/05/05/21:54:29
DO  - 10.21203/rs.3.rs-143619/v1
ER  -


TY  - GEN
AU  - Straka, M.
AU  - Piatriková, L.
AU  - van Bokhoven, P.
AU  - Buzna, L.
TI  - A matrix approach to detect temporal behavioral patterns at electric vehicle charging stations
AB  - Based on the electric vehicle (EV) arrival times and the duration of EV connection to the charging station, we identify charging patterns and derive groups of charging stations with similar charging patterns applying two approaches. The ruled based approach derives the charging patterns by specifying a set of time intervals and a threshold value. In the second approach, we combine the modified l-p norm (as a matrix dissimilarity measure) with hierarchical clustering and apply them to automatically identify charging patterns and groups of charging stations associated with such patterns. A dataset collected in a large network of public charging stations is used to test both approaches. Using both methods, we derived charging patterns. The first, rule-based approach, performed well at deriving predefined patterns and the latter, hierarchical clustering, showed the capability of delivering unexpected charging patterns.
PB  - arXiv
PY  - 2021
ST  - A matrix approach to detect temporal behavioral patterns at electric vehicle charging stations
Y2  - 2025/05/05/21:54:29
DO  - 10.1016/j.trpro.2021.07.186
ER  -


TY  - GEN
AU  - Lubinus, C.
AU  - Einhäuser, W.
AU  - Schiller, F.
AU  - Straube, B.
AU  - van Kemenade, B.M.
TI  - Action-based predictions affect visual perception, neural processing, and pupil size, regardless of temporal predictability
AB  - Sensory consequences of one’s own action are often perceived as less intense, and lead to reduced neural responses, compared to externally generated stimuli. Presumably, such sensory attenuation is due to predictive mechanisms based on the motor command (efference copy). However, sensory attenuation has also been observed outside the context of voluntary action, namely when stimuli are temporally predictable. Here, we aimed at disentangling the effects of motor and temporal predictability-based mechanisms on the attenuation of sensory action consequences. During fMRI data acquisition, participants (N = 25) judged which of two visual stimuli was brighter. In predictable blocks, the stimuli appeared temporally aligned with their button press (active) or aligned with an automatically generated cue (passive). In unpredictable blocks, stimuli were presented with a variable delay after button press/cue, respectively. Eye tracking was performed to investigate pupil-size changes and to ensure proper fixation. Self-generated stimuli were perceived as darker and led to less neural activation in visual areas than their passive counterparts, indicating sensory attenuation for self-generated stimuli independent of temporal predictability. Pupil size was larger during self-generated stimuli, which correlated negatively with blood oxygenation level dependent (BOLD) response: the larger the pupil, the smaller the BOLD amplitude in visual areas. Our results suggest that sensory attenuation in visual cortex is driven by action-based predictive mechanisms rather than by temporal predictability. This effect may be related to changes in pupil diameter. Altogether, these results emphasize the role of the efference copy in the processing of sensory action consequences.
PB  - bioRxiv
PY  - 2021
ST  - Action-based predictions affect visual perception, neural processing, and pupil size, regardless of temporal predictability
Y2  - 2025/05/05/21:54:29
DO  - 10.1167/19.10.290b
ER  -


TY  - GEN
AU  - Meng, Y.
AU  - Panda, R.
AU  - Lin, C.-C.
AU  - Oliva, A.
AU  - Feris, R.
TI  - AdaFuse: Adaptive temporal fusion network for efficient action recognition
AB  - Temporal modelling is the key for efficient video action recognition. While understanding temporal information can improve recognition accuracy for dynamic actions, removing temporal redundancy and reusing past features can significantly save computation leading to efficient action recognition. In this paper, we introduce an adaptive temporal fusion network, called AdaFuse, that dynamically fuses channels from current and past feature maps for strong temporal modelling. Specifically, the necessary information from the historical convolution feature maps is fused with current pruned feature maps with the goal of improving both recognition accuracy and efficiency. In addition, we use a skipping operation to further reduce the computation cost of action recognition. Extensive experiments on Something V1&V2, Jester and Mini-Kinetics show that our approach can achieve about 40% computation savings with comparable accuracy to state-of-the-art methods. The project page can be found at https://mengyuest.github.io/AdaFuse/
PB  - arXiv
PY  - 2021
ST  - AdaFuse
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/s11554-024-01541-6
ER  -


TY  - GEN
AU  - Nebisoy, A.
AU  - Malekzadeh, S.
TI  - Video action recognition using spatio-temporal optical flow video frames
AB  - Recognizing human actions based on videos has became one of the most popular areas of research in computer vision in recent years. This area has many applications such as surveillance, robotics, health care, video search and human-computer interaction. There are many problems associated with recognizing human actions in videos such as cluttered backgrounds, obstructions, viewpoints variation, execution speed and camera movement. A large number of methods have been proposed to solve the problems. This paper focus on spatial and temporal pattern recognition for the classification of videos using Deep Neural Networks. This model takes RGB images and Optical Flow as input data and outputs an action class number. The final recognition accuracy was about 94%.
PB  - arXiv
PY  - 2021
ST  - Video action recognition using spatio-temporal optical flow video frames
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/iccv51070.2023.01267
ER  -


TY  - GEN
AU  - Singh, A.
AU  - Chakraborty, O.
AU  - Varshney, A.
AU  - Saenko, K.
AU  - Das, A.
TI  - Semi-supervised action recognition with temporal contrastive learning
AB  - Learning to recognize actions from only a handful of labeled videos is a challenging problem due to the scarcity of tediously collected activity labels. We approach this problem by learning a two-pathway temporal contrastive model using unlabeled videos at two different speeds leveraging the fact that changing video speed does not change an action. Specifically, we propose to maximize the similarity between encoded representations of the same video at two different speeds as well as minimize the similarity between different videos played at different speeds. This way we use the rich supervisory information in terms of 'time' that is present in otherwise unsupervised pool of videos. With this simple yet effective strategy of manipulating video playback rates, we considerably outperform video extensions of sophisticated state-of-the-art semi-supervised image recognition methods across multiple diverse benchmark datasets and network architectures. Interestingly, our proposed approach benefits from out-of-domain unlabeled videos showing generalization and robustness. We also perform rigorous ablations and analysis to validate our approach. Project page: https://cvir.github.io/TCL/.
PB  - arXiv
PY  - 2021
ST  - Semi-supervised action recognition with temporal contrastive learning
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr46437.2021.01025
ER  -


TY  - GEN
AU  - Roumazeilles, L.
AU  - Schurz, M.
AU  - Lojkiewiez, M.
AU  - Mars, R.B.
AU  - Sallet, J.
TI  - Social prediction modulates activity of macaque superior temporal cortex
AB  - The ability to attribute thoughts to others, also called theory of mind (TOM), has been extensively studied. Computationally, the basis of TOM in humans has been interpreted within the predictive coding framework and associated with activity in the temporo-parietal junction (TPJ). However, the evolutionary origins of these human mindreading abilities have been challenged since the concept was coined. Here we identify a brain region in the Rhesus macaque that shares computational properties with the human TPJ. We revealed, using a non-linguistic task and functional magnetic resonance imaging, that activity in a region of the macaque middle superior temporal cortex was specifically modulated by the predictability of social interactions. As in human TPJ, this region could be distinguished from other temporal regions involved in face processing. Our result suggests the existence of a precursor for the theory of mind ability in the last common ancestor of human and old-world monkeys.
PB  - bioRxiv
PY  - 2021
ST  - Social prediction modulates activity of macaque superior temporal cortex
Y2  - 2025/05/05/21:54:29
DO  - 10.1126/sciadv.abh2392
ER  -


TY  - GEN
AU  - Nawhal, M.
AU  - Mori, G.
TI  - Activity graph transformer for temporal action localization
AB  - We introduce Activity Graph Transformer, an end-to-end learnable model for temporal action localization, that receives a video as input and directly predicts a set of action instances that appear in the video. Detecting and localizing action instances in untrimmed videos requires reasoning over multiple action instances in a video. The dominant paradigms in the literature process videos temporally to either propose action regions or directly produce frame-level detections. However, sequential processing of videos is problematic when the action instances have non-sequential dependencies and/or non-linear temporal ordering, such as overlapping action instances or re-occurrence of action instances over the course of the video. In this work, we capture this non-linear temporal structure by reasoning over the videos as non-sequential entities in the form of graphs. We evaluate our model on challenging datasets: THUMOS14, Charades, and EPIC-Kitchens-100. Our results show that our proposed model outperforms the state-of-the-art by a considerable margin.
PB  - arXiv
PY  - 2021
ST  - Activity graph transformer for temporal action localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-031-44223-0_45
ER  -


TY  - GEN
AU  - Suarez-Castellanos, I.M.
AU  - Dossi, E.
AU  - Vion-Bailly, J.
AU  - Huberfeld, G.
AU  - N’Djin, W.A.
TI  - Spatio-temporal characterization of causal electrophysiological activity stimulated by single pulse focused ultrasound: An ex vivo study on hippocampal brain slices
AB  - Objective: The brain operates via generation, transmission and integration of neuronal signals and most neurological disorders are related to perturbation of these processes. Neurostimulation by Focused Ultrasound (FUS) is a promising technology with potential to rival other clinically-used techniques for the investigation of brain function and treatment of numerous neurological diseases. The purpose of this study was to characterize spatial and temporal aspects of causal electrophysiological signals directly stimulated by short, single pulses of focused ultrasound (FUS) on ex vivo mouse hippocampal brain slices. Approach: MicroElectrode Arrays (MEA) are used to study the spatio-temporal dynamics of extracellular neuronal activities both at the single neuron and neural net works scales. Hence, MEAs provide an excellent platform for characterization of electrical activity generated, modulated and transmitted in response to FUS exposure. In this study, a novel mixed FUS/MEA platform was designed for the spatio-temporal description of the causal responses generated by single 1.78 MHz FUS pulses in ex vivo mouse hippocampal brain slices. Main results: Our results show that FUS pulses can generate local field potentials (LFPs), sustained by synchronized neuronal post-synaptic potentials, and reproducing network activities. LFPs induced by FUS stimulation were found to be repeatable to consecutive FUS pulses though exhibiting a wide range of amplitudes (50 – 600 µV), durations (20 - 200 ms), and response delays (10 – 60 ms). Moreover, LFPs were spread across the hippocampal slice following single FUS pulses thus demonstrating that FUS may be capable of stimulating different neural structures within the hippocampus. Significance: Current knowledge on neurostimulation by ultrasound describes neuronal activity generated by trains of repetitive ultrasound pulses. This novel study details the causal neural responses produced by single-pulse FUS neurostimulation while illustrating the distribution and propagation properties of this neural activity along major neural pathways of the hippocampus.
PB  - arXiv
PY  - 2021
ST  - Spatio-temporal characterization of causal electrophysiological activity stimulated by single pulse focused ultrasound
Y2  - 2025/05/05/21:54:29
DO  - 10.1088/1741-2552/abdfb1
ER  -


TY  - GEN
AU  - Wharton, Z.
AU  - Behera, A.
AU  - Liu, Y.
AU  - Bessis, N.
TI  - Coarse temporal attention network (CTA-Net) for driver's activity recognition
AB  - There is significant progress in recognizing traditional human activities from videos focusing on highly distinctive actions involving discriminative body movements, body-object and/or human-human interactions. Driver’s activities are different since they are executed by the same subject with similar body parts movements, resulting in subtle changes. To address this, we propose a novel framework by exploiting the spatiotemporal attention to model the subtle changes. Our model is named Coarse Temporal Attention Network (CTA-Net), in which coarse temporal branches are introduced in a trainable glimpse network. The goal is to allow the glimpse to capture high-level temporal relationships, such as ‘during’, ‘before’ and ‘after’ by focusing on a specific part of a video. These branches also respect the topology of the temporal dynamics in the video, ensuring that different branches learn meaningful spatial and temporal changes. The model then uses an innovative attention mechanism to generate high-level action specific contextual information for activity recognition by exploring the hidden states of an LSTM. The attention mechanism helps in learning to decide the importance of each hidden state for the recognition task by weighing them when constructing the representation of the video. Our approach is evaluated on four publicly accessible datasets and significantly outperforms the state-of-the-art by a considerable margin with only RGB video as input.
PB  - arXiv
PY  - 2021
ST  - Coarse temporal attention network (CTA-Net) for driver's activity recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/wacv48630.2021.00132
ER  -


TY  - GEN
AU  - Perrett, T.
AU  - Masullo, A.
AU  - Burghardt, T.
AU  - Mirmehdi, M.
AU  - Damen, D.
TI  - Temporal-relational CrossTransformers for few-shot action recognition
AB  - We propose a novel approach to few-shot action recognition, finding temporally-corresponding frame tuples between the query and videos in the support set. Distinct from previous few-shot works, we construct class prototypes using the CrossTransformer attention mechanism to observe relevant sub-sequences of all support videos, rather than using class averages or single best matches. Video representations are formed from ordered tuples of varying numbers of frames, which allows sub-sequences of actions at different speeds and temporal offsets to be compared.1 Our proposed Temporal-Relational CrossTransformers (TRX) achieve state-of-the-art results on few-shot splits of Kinetics, Something-Something V2 (SSv2), HMDB51 and UCF101. Importantly, our method outperforms prior work on SSv2 by a wide margin (12%) due to the its ability to model temporal relations. A detailed ablation showcases the importance of matching to multiple support set videos and learning higher-order relational CrossTransformers.
PB  - arXiv
PY  - 2021
ST  - Temporal-relational CrossTransformers for few-shot action recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr46437.2021.00054
ER  -


TY  - GEN
AU  - Abdalkader, R.
AU  - Chaleckis, R.
AU  - Wheelock, C.E.
AU  - Kamei, K.-I.
TI  - Spatiotemporal determination of metabolite activities in multiple corneal epithelium barriers on a chip
AB  - The corneal epithelial barrier maintains the metabolic activities of the ocular surface by regulating membrane transporters and metabolic enzymes responsible for the homeostasis of the eye as well as the pharmacokinetic behavior of drugs. Despite its importance, no established biomimetic in vitro methods are available to perform the spatiotemporal investigation of corneal metabolism and determine the transportation of endogenous and exogenous molecules. This study introduces multiple corneal epithelium barriers on a chip, namely, Cornea-Chip, which enables the spatiotemporal collection as well as analysis of micro-scaled extracellular metabolites from both the apical and basolateral sides of the barriers. Longitudinal samples collected during 48 h period were analyzed using untargeted liquid chromatography–mass spectrometry metabolomics method, and 104 metabolites were annotated. The shifts in extracellular metabolites and quantitative analysis of the mRNA associated with membrane transporters could allow the investigation of the correlation between the expression of and the secretion and transportation of metabolites across the polarized corneal epithelial barrier. The Cornea-Chip might provide a non-invasive, simple, and effectively informative method to determine pharmacokinetics and pharmacodynamics as well as to discover novel biomarkers for drug toxicological and safety tests as an alternative to animal experiments.
PB  - bioRxiv
PY  - 2021
ST  - Spatiotemporal determination of metabolite activities in multiple corneal epithelium barriers on a chip
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2021.01.08.425838
ER  -


TY  - GEN
AU  - Vissat, L.L.
AU  - Blackburn, J.K.
AU  - Getz, W.M.
TI  - A relative-motion method for parsing spatio-temporal behaviour of dyads using GPS relocation data
AB  - 1. In this paper, we introduce a novel method for classifying and computing the frequencies of movement modes of intra and interspecific dyads, focusing in particular on distance-mediated approach, retreat, following and side by side movement modes. 2. Besides distance, the method includes factors such as sex, age, time of day, or season that cause frequencies of movement modes to deviate from random. 3. We demonstrate and validate our method using both simulated and empirical data. Our simulated data were obtained from a relative-motion, biased random-walk (RM-BRW) model with attraction and repulsion circumferences. Our empirical data were GPS relocation time series collected from African elephants in Etosha National Park, Namibia. The simulated data were primarily used to validate our method while the empirical data analysis were used to illustrate the types of behavioral assessment that our methodology reveals. 4. Our methodology facilitates automated, observer-bias-free analysis of the locomotive interactions of dyads using GPS relocation data, which is becoming increasingly ubiquitous as telemetry and related technologies improve. Our method should open up a whole new vista of behavioral-interaction type analyses to movement and behavioral ecologists.
PB  - bioRxiv
PY  - 2021
ST  - A relative-motion method for parsing spatio-temporal behaviour of dyads using GPS relocation data
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2021.01.04.425238
ER  -


TY  - GEN
AU  - Liu, Y.
AU  - Wang, K.
AU  - Lan, H.
AU  - Lin, L.
TI  - Temporal Contrastive Graph Learning for video action recognition and retrieval
AB  - Attempt to fully discover the temporal diversity and chronological characteristics for self-supervised video representation learning, this work takes advantage of the temporal dependencies within videos and further proposes a novel self-supervised method named Temporal Contrastive Graph Learning (TCGL). In contrast to the existing methods that ignore modeling elaborate temporal dependencies, our TCGL roots in a hybrid graph contrastive learning strategy to jointly regard the inter-snippet and intra-snippet temporal dependencies as self-supervision signals for temporal representation learning. To model multi-scale temporal dependencies, our TCGL integrates the prior knowledge about the frame and snippet orders into graph structures, i.e., the intra-/inter- snippet temporal contrastive graphs. By randomly removing edges and masking nodes of the intra-snippet graphs or inter-snippet graphs, our TCGL can generate different correlated graph views. Then, specific contrastive learning modules are designed to maximize the agreement between nodes in different views. To adaptively learn the global context representation and recalibrate the channel-wise features, we introduce an adaptive video snippet order prediction module, which leverages the relational knowledge among video snippets to predict the actual snippet orders. Experimental results demonstrate the superiority of our TCGL over the state-of-the-art methods on large-scale action recognition and video retrieval benchmarks.
PB  - arXiv
PY  - 2021
ST  - Temporal Contrastive Graph Learning for video action recognition and retrieval
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/wacv45572.2020.9093278
ER  -


TY  - GEN
AU  - Islam, A.
AU  - Long, C.
AU  - Radke, R.
TI  - A hybrid attention mechanism for weakly-supervised temporal action localization
AB  - Weakly supervised temporal action localization is a challenging vision task due to the absence of ground-truth temporal locations of actions in the training videos. With only video-level supervision during training, most existing methods rely on a Multiple Instance Learning (MIL) framework to predict the start and end frame of each action category in a video. However, the existing MIL-based approach has a major limitation of only capturing the most discriminative frames of an action, ignoring the full extent of an activity. Moreover, these methods cannot model background activity effectively, which plays an important role in localizing foreground activities. In this paper, we present a novel framework named HAM-Net with a hybrid attention mechanism which includes temporal soft, semi-soft and hard attentions to address these issues. Our temporal soft attention module, guided by an auxiliary background class in the classification module, models the background activity by introducing an “action-ness” score for each video snippet. Moreover, our temporal semi-soft and hard attention modules, calculating two attention scores for each video snippet, help to focus on the less discriminative frames of an action to capture the full action boundary. Our proposed approach outperforms recent state-of-the-art methods by at least 2.2% mAP at IoU threshold 0.5 on the THUMOS14 dataset, and by at least 1.3% mAP at IoU threshold 0.75 on the ActivityNet1.2 dataset. Code can be found at: https://github.com/asrafulashiq/hamnet.
PB  - arXiv
PY  - 2021
ST  - A hybrid attention mechanism for weakly-supervised temporal action localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1609/aaai.v35i2.16256
ER  -


TY  - GEN
AU  - Batal, I.
AU  - Soni, A.
TI  - Multi-channel sequential behavior networks for user modeling in online advertising
AB  - Multiple content providers rely on native advertisement for revenue by placing ads within the organic content of their pages. We refer to this setting as “queryless” to differentiate from search advertisement where a user submits a search query and gets back related ads. Understanding user intent is critical because relevant ads improve user experience and increase the likelihood of delivering clicks that have value to our advertisers. This paper presents Multi-Channel Sequential Behavior Network (MC-SBN), a deep learning approach for embedding users and ads in a semantic space in which relevance can be evaluated. Our proposed user encoder architecture summarizes user activities from multiple input channels–such as previous search queries, visited pages, or clicked ads–into a user vector. It uses multiple RNNs to encode sequences of event sessions from the different channels and then applies an attention mechanism to create the user representation. A key property of our approach is that user vectors can be maintained and updated incrementally, which makes it feasible to be deployed for large-scale serving. We conduct extensive experiments on real-world datasets. The results demonstrate that MC-SBN can improve the ranking of relevant ads and boost the performance of both click prediction and conversion prediction in the queryless native advertising setting. MSC Codes 68T07, 68T50
PB  - arXiv
PY  - 2020
ST  - Multi-channel sequential behavior networks for user modeling in online advertising
Y2  - 2025/05/05/21:54:29
DO  - 10.1049/cje.2016.03.025
ER  -


TY  - GEN
AU  - Wang, L.
AU  - Tong, Z.
AU  - Ji, B.
AU  - Wu, G.
TI  - TDN: Temporal difference networks for efficient action recognition
AB  - Temporal modeling still remains challenging for action recognition in videos. To mitigate this issue, this paper presents a new video architecture, termed as Temporal Difference Network (TDN), with a focus on capturing multi-scale temporal information for efficient action recognition. The core of our TDN is to devise an efficient temporal module (TDM) by explicitly leveraging a temporal difference operator, and systematically assess its effect on short-term and long-term motion modeling. To fully capture temporal information over the entire video, our TDN is established with a two-level difference modeling paradigm. Specifically, for local motion modeling, temporal difference over consecutive frames is used to supply 2D CNNs with finer motion pattern, while for global motion modeling, temporal difference across segments is incorporated to capture long-range structure for motion feature excitation. TDN provides a simple and principled temporal modeling framework and could be instantiated with the existing CNNs at a small extra computational cost. Our TDN presents a new state of the art on the Something-Something V1 & V2 datasets and is on par with the best performance on the Kinetics-400 dataset. In addition, we conduct in-depth ablation studies and plot the visualization results of our TDN, hopefully providing insightful analysis on temporal difference modeling. We release the code at https://github.com/MCG-NJU/TDN.
PB  - arXiv
PY  - 2020
ST  - TDN
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr46437.2021.00193
ER  -


TY  - GEN
AU  - Li, J.
AU  - Xie, X.
AU  - Zhao, Z.
AU  - Pan, Q.
AU  - Shi, G.
TI  - Temporal graph modeling for skeleton-based action recognition
AB  - Graph Convolutional Networks (GCNs), which model skeleton data as graphs, have obtained remarkable performance for skeleton-based action recognition. Particularly, the temporal dynamic of skeleton sequence conveys significant information in the recognition task. For temporal dynamic modeling, GCN-based methods only stack multi-layer 1D local convolutions to extract temporal relations between adjacent time steps. With the repeat of a lot of local convolutions, the key temporal information with non-adjacent temporal distance may be ignored due to the information dilution. Therefore, these methods still remain unclear how to fully explore temporal dynamic of skeleton sequence. In this paper, we propose a Temporal Enhanced Graph Convolutional Network (TE-GCN) to tackle this limitation. The proposed TE-GCN constructs temporal relation graph to capture complex temporal dynamic. Specifically, the constructed temporal relation graph explicitly builds connections between semantically related temporal features to model temporal relations between both adjacent and non-adjacent time steps. Meanwhile, to further explore the sufficient temporal dynamic, multi-head mechanism is designed to investigate multi-kinds of temporal relations. Extensive experiments are performed on two widely used large-scale datasets, NTU-60 RGB+D and NTU-120 RGB+D. And experimental results show that the proposed model achieves the state-of-the-art performance by making contribution to temporal modeling for action recognition.
PB  - arXiv
PY  - 2020
ST  - Temporal graph modeling for skeleton-based action recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/s11042-021-11136-z
ER  -


TY  - GEN
AU  - Mo, S.
AU  - Tan, X.
AU  - Xia, J.
AU  - Ren, P.
TI  - Towards improving spatiotemporal action recognition in videos
AB  - Spatiotemporal action recognition deals with locating and classifying actions in videos. Motivated by the latest state-of-the-art real-time object detector You Only Watch Once (YOWO), we aim to modify its structure to increase action detection precision and reduce computational time. Specifically, we propose four novel approaches in attempts to improve YOWO and address the imbalanced class issue in videos by modifying the loss function. We consider two moderate-sized datasets to apply our modification of YOWO - the popular Joint-annotated Human Motion Data Base (J-HMDB-21) and a private dataset of restaurant video footage provided by a Carnegie Mellon University-based startup, Agot.AI. The latter involves fast-moving actions with small objects as well as unbalanced data classes, making the task of action localization more challenging. We implement our proposed methods in the GitHub repository1.
PB  - arXiv
PY  - 2020
ST  - Towards improving spatiotemporal action recognition in videos
Y2  - 2025/05/05/21:54:29
DO  - 10.1142/s0218126623502031
ER  -


TY  - GEN
AU  - He, B.
AU  - Yang, X.
AU  - Wu, Z.
AU  - Lim, S.-N.
AU  - Shrivastava, A.
TI  - GTA: Global temporal attention for video action understanding
AB  - Self-attention learns pairwise interactions to model long-range dependencies, yielding great improvements for video action recognition. In this paper, we seek a deeper understanding of self-attention for temporal modeling in videos. We first demonstrate that the entangled modeling of spatio-temporal information by flattening all pixels is sub-optimal, failing to capture temporal relationships among frames explicitly. To this end, we introduce Global Temporal Attention (GTA), which performs global temporal attention on top of spatial attention in a decoupled manner. We apply GTA on both pixels and semantically similar regions to capture temporal relationships at different levels of spatial granularity. Unlike conventional self-attention that computes an instance-specific attention matrix, GTA directly learns a global attention matrix that is intended to encode temporal structures that generalize across different samples. We further augment GTA with a cross-channel multi-head fashion to exploit channel interactions for better temporal modeling. Extensive experiments on 2D and 3D networks demonstrate that our approach consistently enhances temporal modeling and provides state-of-the-art performance on three video action recognition datasets.
PB  - arXiv
PY  - 2020
ST  - GTA
Y2  - 2025/05/05/21:54:29
DO  - 10.1016/j.patcog.2022.109135
ER  -


TY  - GEN
AU  - Ju, C.
AU  - Zhao, P.
AU  - Zhang, Y.
AU  - Wang, Y.
AU  - Tian, Q.
TI  - Point-level temporal action localization: Bridging fully-supervised proposals to weakly-supervised losses
AB  - Point-Level temporal action localization (PTAL) aims to localize actions in untrimmed videos with only one timestamp annotation for each action instance. Existing methods adopt the frame-level prediction paradigm to learn from the sparse single-frame labels. However, such a framework inevitably suffers from a large solution space. This paper attempts to explore the proposal-based prediction paradigm for point-level annotations, which has the advantage of more constrained solution space and consistent predictions among neighboring frames. The point-level annotations are first used as the keypoint supervision to train a keypoint detector. At the location prediction stage, a simple but effective mapper module, which enables back-propagation of training errors, is then introduced to bridge the fully-supervised framework with weak supervision. To our best of knowledge, this is the first work to leverage the fully-supervised paradigm for the point-level setting. Experiments on THUMOS14, BEOID, and GTEA verify the effectiveness of our proposed method both quantitatively and qualitatively, and demonstrate that our method outperforms state-of-the-art methods.
PB  - arXiv
PY  - 2020
ST  - Point-level temporal action localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-030-88004-0_4
ER  -


TY  - GEN
AU  - Wang, D.
AU  - Hu, D.
AU  - Li, X.
AU  - Dou, D.
TI  - Temporal relational modeling with self-supervision for action segmentation
AB  - Temporal relational modeling in video is essential for human action understanding, such as action recognition and action segmentation. Although Graph Convolution Networks (GCNs) have shown promising advantages in relation reasoning on many tasks, it is still a challenge to apply graph convolution networks on long video sequences effectively. The main reason is that large number of nodes (i.e., video frames) makes GCNs hard to capture and model temporal relations in videos. To tackle this problem, in this paper, we introduce an effective GCN module, Dilated Temporal Graph Reasoning Module (DTGRM), designed to model temporal relations and dependencies between video frames at various time spans. In particular, we capture and model temporal relations via constructing multi-level dilated temporal graphs where the nodes represent frames from different moments in video. Moreover, to enhance temporal reasoning ability of the proposed model, an auxiliary self-supervised task is proposed to encourage the dilated temporal graph reasoning module to find and correct wrong temporal relations in videos. Our DTGRM model outperforms state-of-the-art action segmentation models on three challenging datasets: 50Salads, Georgia Tech Egocentric Activities (GTEA), and the Breakfast dataset. The code is available at https://github.com/redwang/DTGRM.
PB  - arXiv
PY  - 2020
ST  - Temporal relational modeling with self-supervision for action segmentation
Y2  - 2025/05/05/21:54:29
DO  - 10.1609/aaai.v35i4.16377
ER  -


TY  - GEN
AU  - Li, X.
AU  - Liu, C.
AU  - Shuai, B.
AU  - Chen, H.
AU  - Tighe, J.
TI  - NUTA: Non-uniform temporal aggregation for action recognition
AB  - In the world of action recognition research, one primary focus has been on how to construct and train networks to model the spatial-temporal volume of an input video. These methods typically uniformly sample a segment of an input clip (along the temporal dimension). However, not all parts of a video are equally important to determine the action in the clip. In this work, we focus instead on learning where to extract features, so as to focus on the most informative parts of the video. We propose a method called the nonuniform temporal aggregation (NUTA), which aggregates features only from informative temporal segments. We also introduce a synchronization method that allows our NUTA features to be temporally aligned with traditional uniformly sampled video features, so that both local and clip-level features can be combined. Our model has achieved state-of-the-art performance on four widely used large-scale action-recognition datasets (Kinetics400, Kinetics700, Something-something V2 and Charades). In addition, we have created a visualization to illustrate how the proposed NUTA method selects only the most relevant parts of a video clip.
PB  - arXiv
PY  - 2020
ST  - NUTA
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/wacv51458.2022.00090
ER  -


TY  - GEN
AU  - Mokashe, S.
AU  - Nadkarni, S.
TI  - Role of slow temporal dynamics in reliable activity of stochastically driven neurons
AB  - Neuronal networks maintain robust patterns of activity despite a backdrop of noise from various sources. Mutually inhibiting neurons is a standard network motif implicated in rhythm generation. In an elementary network motif of two neurons capable of swapping from an active state to a quiescent state, we ask how different sources of stochasticity alter firing patterns. In this system, the alternating activity occurs via combined action of a calcium-dependent potassium current, sAHP (slow afterhyperpolarization), and a fast GABAergic synapse. We show that simulating extrinsic noise arising from background activity extends the dynamical range of neuronal firing. Extrinsic noise also has the effect of increasing the switching frequency via a faster build-up of sAHP current. We show that switching frequency as a function of input current has a non-monotonic behavior. Interestingly the noise tolerance of this system varies with the input current. It shows maximum robustness to noise at an input current that corresponds to the minimum switching frequency between the neurons. The slow decay time scale of sAHP conductance allows neurons to act as a low-pass filter, attenuate noise, and integrate over ion channel fluctuations. Additionally, we show that the slow inactivation time of the sAHP channel allows the neuron to act as an action potential counter. We propose that this intrinsic property of the current allows the network to maintain rhythmic activity critical for various functions, despite the noise, and operate as a temporal integrator.
PB  - bioRxiv
PY  - 2020
ST  - Role of slow temporal dynamics in reliable activity of stochastically driven neurons
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2020.12.11.422204
ER  -


TY  - GEN
AU  - Plizzari, C.
AU  - Cannici, M.
AU  - Matteucci, M.
TI  - Spatial temporal transformer network for skeleton-based action recognition
AB  - Skeleton-based human action recognition has achieved a great interest in recent years, as skeleton data has been demonstrated to be robust to illumination changes, body scales, dynamic camera views, and complex background. Nevertheless, an effective encoding of the latent information underlying the 3D skeleton is still an open problem. In this work, we propose a novel Spatial-Temporal Transformer network (ST-TR) which models dependencies between joints using the Transformer self-attention operator. In our ST-TR model, a Spatial Self-Attention module (SSA) is used to understand intra-frame interactions between different body parts, and a Temporal Self-Attention module (TSA) to model inter-frame correlations. The two are combined in a two-stream network which outperforms state-of-the-art models using the same input data on both NTU-RGB+D 60 and NTU-RGB+D 120.
PB  - arXiv
PY  - 2020
ST  - Spatial temporal transformer network for skeleton-based action recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-030-68796-0_50
ER  -


TY  - GEN
AU  - Lohani, S.
AU  - Moberly, A.H.
AU  - Benisty, H.
AU  - Higley, M.J.
AU  - Cardin, J.A.
TI  - Dual color mesoscopic imaging reveals spatiotemporally heterogeneous coordination of cholinergic and neocortical activity
AB  - Variation in an animal's behavioral state is linked to fluctuations in brain activity and cognitive ability. In the neocortex, state-dependent control of circuit dynamics may reflect neuromodulatory influences including acetylcholine (ACh). While early literature suggested ACh exerts broad, homogeneous control over cortical function, recent evidence indicates potential anatomical and functional segregation of cholinergic signaling. Additionally, it is unclear whether states as defined by different behavioral markers reflect heterogeneous cholinergic and cortical network activity. We performed simultaneous, dual-color mesoscopic imaging of both ACh and calcium across the neocortex of awake mice to investigate their relationships with behavioral variables. We find that increasing arousal, categorized by different motor behaviors, is associated with spatiotemporally dynamic patterns of cholinergic release and enhanced large-scale network correlations. Overall, our findings demonstrate that ACh provides a highly dynamic and spatially heterogeneous signal that links fluctuations in behavior to functional reorganization of cortical networks.
PB  - bioRxiv
PY  - 2020
ST  - Dual color mesoscopic imaging reveals spatiotemporally heterogeneous coordination of cholinergic and neocortical activity
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2020.12.09.418632
ER  -


TY  - GEN
AU  - Li, T.
AU  - Zhang, R.
AU  - Li, Q.
TI  - Multi scale temporal graph networks for skeleton-based action recognition
AB  - Graph convolutional networks (GCNs) can effectively capture the features of related nodes and improve the performance of model. More attention is paid to employing GCN in Skeleton-Based action recognition. But existing methods based on GCNs have two problems. First, the consistency of temporal and spatial features is ignored for extracting features node by node and frame by frame. To obtain spatiotemporal features simultaneously, we design a generic representation of skeleton sequences for action recognition and propose a novel model called Temporal Graph Networks (TGN). Secondly, the adjacency matrix of graph describing the relation of joints are mostly depended on the physical connection between joints. To appropriate describe the relations between joints in skeleton graph, we propose a multi-scale graph strategy, adopting a full-scale graph, part-scale graph and core-scale graph to capture the local features of each joint and the contour features of important joints. Experiments were carried out on two large datasets and results show that TGN with our graph strategy outperforms state-of-the-art methods.
PB  - arXiv
PY  - 2020
ST  - Multi scale temporal graph networks for skeleton-based action recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.5121/csit.2020.101605
ER  -


TY  - GEN
AU  - Liang, J.
AU  - Cao, L.
AU  - Xiong, X.
AU  - Yu, T.
AU  - Hauptmann, A.
TI  - Spatial-temporal alignment network for action recognition and detection
AB  - This paper studies how to introduce viewpoint-invariant feature representations that can help action recognition and detection. Although we have witnessed great progress of action recognition in the past decade, it remains challenging yet interesting how to efficiently model the geometric variations in large scale datasets. This paper proposes a novel Spatial-Temporal Alignment Network (STAN) that aims to learn geometric invariant representations for action recognition and action detection. The STAN model is very light-weighted and generic, which could be plugged into existing action recognition models like ResNet3D and the SlowFast with a very low extra computational cost. We test our STAN model extensively on AVA, Kinetics-400, AVA-Kinetics, Charades, and Charades-Ego datasets. The experimental results show that the STAN model can consistently improve the state of the arts in both action detection and action recognition tasks. We will release our data, models and code.
PB  - arXiv
PY  - 2020
ST  - Spatial-temporal alignment network for action recognition and detection
Y2  - 2025/05/05/21:54:29
DO  - 10.1117/12.2644209
ER  -


TY  - GEN
AU  - Yau, T.
AU  - Malekmohammadi, S.
AU  - Rasouli, A.
AU  - Rohani, M.
AU  - Luo, J.
TI  - Graph-SIM: A graph-based spatiotemporal interaction modelling for pedestrian action prediction
AB  - One of the most crucial yet challenging tasks for autonomous vehicles in urban environments is predicting the future behaviour of nearby pedestrians, especially at points of crossing. Predicting behaviour depends on many social and environmental factors, particularly interactions between road users. Capturing such interactions requires a global view of the scene and dynamics of the road users in three-dimensional space. This information, however, is missing from the current pedestrian behaviour benchmark datasets. Motivated by these challenges, we propose 1) a novel graph-based model for predicting pedestrian crossing action. Our method models pedestrians' interactions with nearby road users through clustering and relative importance weighting of interactions using features obtained from the bird's-eye-view. 2) We introduce a new dataset that provides 3D bounding box and pedestrian behavioural annotations for the existing nuScenes dataset. On the new data, our approach achieves state-of-the-art performance by improving on various metrics by more than 15% in comparison to existing methods. The dataset is available at https:// github.com/huawei-noah/datasets/PePScenes.
PB  - arXiv
PY  - 2020
ST  - Graph-SIM
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/icra48506.2021.9561107
ER  -


TY  - GEN
AU  - Comba, A.
AU  - Faisal, S.M.
AU  - Dunn, P.J.
AU  - Castro, M.G.
AU  - Lowenstein, P.R.
TI  - Spatiotemporal analysis of glioma heterogeneity reveals Col1A1 as an actionable target to disrupt tumor mesenchymal differentiation, invasion and malignancy
AB  - Intra-tumoral heterogeneity and diffuse infiltration are hallmarks of glioblastoma that challenge treatment efficacy. However, the mechanisms that set up both tumor heterogeneity and invasion remain poorly understood. Herein, we present a comprehensive spatiotemporal study that aligns distinctive intra-tumoral histopathological structures, oncostreams, with dynamic properties and a unique, actionable, spatial transcriptomic signature. Oncostreams are dynamic multicellular fascicles of spindle-like and aligned cells with mesenchymal properties, detected using ex vivo explants and in vivo intravital imaging. Their density correlates with tumor aggressiveness in genetically engineered mouse glioma models, and high-grade human gliomas. Oncostreams facilitate the intra-tumoral distribution of tumoral and non-tumoral cells, and the invasion of the normal brain. These fascicles are defined by a specific molecular signature that regulates their organization and function. Oncostreams structure and function depend on overexpression of COL1A1. COL1A1 is a central gene in the dynamic organization of glioma mesenchymal transformation, and a powerful regulator of glioma malignant behavior. Inhibition of COL1A1 eliminated oncostreams, reprogramed the malignant histopathological phenotype, reduced expression of the mesenchymal associated genes, induced changes in the tumor microenvironment and prolonged animal survival. Oncostreams represent a novel pathological marker of potential value for diagnosis, prognosis, and treatment.
PB  - bioRxiv
PY  - 2020
ST  - Spatiotemporal analysis of glioma heterogeneity reveals Col1A1 as an actionable target to disrupt tumor mesenchymal differentiation, invasion and malignancy
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2020.12.01.404970
ER  -


TY  - GEN
AU  - Zhao, C.
AU  - Thabet, A.
AU  - Ghanem, B.
TI  - Video Self-Stitching Graph Network for Temporal Action Localization
AB  - Temporal action localization (TAL) in videos is a challenging task, especially due to the large variation in action temporal scales. Short actions usually occupy a major proportion in the datasets, but tend to have the lowest performance. In this paper, we confront the challenge of short actions and propose a multi-level cross-scale solution dubbed as video self-stitching graph network (VSGN). We have two key components in VSGN: video self-stitching (VSS) and cross-scale graph pyramid network (xGPN). In VSS, we focus on a short period of a video and magnify it along the temporal dimension to obtain a larger scale. We stitch the original clip and its magnified counterpart in one input sequence to take advantage of the complementary properties of both scales. The xGPN component further exploits the cross-scale correlations by a pyramid of cross-scale graph networks, each containing a hybrid module to aggregate features from across scales as well as within the same scale. Our VSGN not only enhances the feature representations, but also generates more positive anchors for short actions and more short training samples. Experiments demonstrate that VSGN obviously improves the localization performance of short actions as well as achieving the state-of-the-art overall performance on THUMOS-14 and ActivityNet-v1.3. VSGN code is available at https://github.com/coolbay/VSGN.
PB  - arXiv
PY  - 2020
ST  - Video Self-Stitching Graph Network for Temporal Action Localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/iccv48922.2021.01340
ER  -


TY  - GEN
AU  - Huang, Z.
AU  - Shen, X.
AU  - Tian, X.
AU  - Huang, J.
AU  - Hua, X.-S.
TI  - Spatio-temporal inception graph convolutional networks for skeleton-based action recognition
AB  - Skeleton-based human action recognition has attracted much attention with the prevalence of accessible depth sensors. Recently, graph convolutional networks (GCNs) have been widely used for this task due to their powerful capability to model graph data. The topology of the adjacency graph is a key factor for modeling the correlations of the input skeletons. Thus, previous methods mainly focus on the design/learning of the graph topology. But once the topology is learned, only a single-scale feature and one transformation exist in each layer of the networks. Many insights, such as multi-scale information and multiple sets of transformations, that have been proven to be very effective in convolutional neural networks (CNNs), have not been investigated in GCNs. The reason is that, due to the gap between graph-structured skeleton data and conventional image/video data, it is very challenging to embed these insights into GCNs. To overcome this gap, we reinvent the split-transform-merge strategy in GCNs for skeleton sequence processing. Specifically, we design a simple and highly modularized graph convolutional network architecture for skeleton-based action recognition. Our network is constructed by repeating a building block that aggregates multi-granularity information from both the spatial and temporal paths. Extensive experiments demonstrate that our network outperforms state-of-the-art methods by a significant margin with only 1/5 of the parameters and 1/10 of the FLOPs. Code is available at https://github.com/yellowtownhz/STIGCN.
PB  - arXiv
PY  - 2020
ST  - Spatio-temporal inception graph convolutional networks for skeleton-based action recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1145/3394171.3413666
ER  -


TY  - GEN
AU  - Segu, M.
AU  - Pirovano, F.
AU  - Fumagalli, G.
AU  - Fabris, A.
TI  - Depth-aware action recognition: Pose-motion encoding through temporal heatmaps
AB  - Most state-of-the-art methods for action recognition rely only on 2D spatial features encoding appearance, motion or pose. However, 2D data lacks the depth information, which is crucial for recognizing fine-grained actions. In this paper, we propose a depth-aware volumetric descriptor that encodes pose and motion information in a unified representation for action classification in-the-wild. Our framework is robust to many challenges inherent to action recognition, e.g. variation in viewpoint, scene, clothing and body shape. The key component of our method is the Depth-Aware Pose Motion representation (DA-PoTion), a new video descriptor that encodes the 3D movement of semantic keypoints of the human body. Given a video, we produce human joint heatmaps for each frame using a state-of-the-art 3D human pose regressor and we give each of them a unique color code according to the relative time in the clip. Then, we aggregate such 3D time-encoded heatmaps for all human joints to obtain a fixed-size descriptor (DA-PoTion), which is suitable for classifying actions using a shallow 3D convolutional neural network (CNN). The DA-PoTion alone defines a new state-of-the-art on the Penn Action Dataset. Moreover, we leverage the intrinsic complementarity of our pose motion descriptor with appearance based approaches by combining it with Inflated 3D ConvNet (I3D) to define a new state-of-the-art on the JHMDB Dataset.
PB  - arXiv
PY  - 2020
ST  - Depth-aware action recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr.2014.333
ER  -


TY  - GEN
AU  - Lewis, S.M.
AU  - Xu, L.
AU  - Rigolli, N.
AU  - Seminara, A.
AU  - Gire, D.H.
TI  - Plume dynamics structure the spatiotemporal activity of glomerular networks in the mouse olfactory bulb
AB  - Although mice locate resources using turbulent airborne odor plumes, the stochasticity and intermittency of fluctuating plumes create challenges for interpreting odor cues in natural environments. Population activity within the olfactory bulb (OB), is thought to process this complex spatial and temporal information, but how plume dynamics impact odor representation in this early stage of the mouse olfactory system is not known. Limitations in odor detection technology have made it impossible to measure plume fluctuations while simultaneously recording from the mouse’s brain. Thus, previous studies have measured OB activity following controlled odor pulses of varying profiles or frequencies, but this approach only captures a subset of features found within olfactory plumes. Adequately sampling this feature space is difficult given a lack of knowledge regarding which features the brain extracts during exposure to natural olfactory scenes. Here we measured OB responses to naturally fluctuating odor plumes using a miniature, adapted odor sensor combined with wide-field GCaMP6f signaling from the dendrites of mitral and tufted (MT) cells imaged in olfactory glomeruli of head-fixed mice. We precisely tracked plume dynamics and imaged glomerular responses to this fluctuating input, while varying flow conditions across a range of ethologically-relevant values. We found that a consistent portion of MT activity in glomeruli follows odor concentration dynamics, and the strongest responding glomeruli are the best at following fluctuations within odor plumes. Further, the reliability and average response magnitude of glomerular populations of MT cells are affected by the flow condition in which the animal samples the plume, with the fidelity of plume following by MT cells increasing in conditions of higher flow velocity where odor dynamics result in intermittent whiffs of stronger concentration. Thus, the flow environment in which an animal encounters an odor has a large-scale impact on the temporal representation of an odor plume in the OB. Additionally, across flow conditions odor dynamics are a major driver of activity in many glomerular networks. Taken together, these data demonstrate that plume dynamics structure olfactory representations in the first stage of odor processing in the mouse olfactory system.
PB  - bioRxiv
PY  - 2020
ST  - Plume dynamics structure the spatiotemporal activity of glomerular networks in the mouse olfactory bulb
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2020.11.25.399089
ER  -


TY  - GEN
AU  - Raimo, D.
AU  - Sarracino, A.
AU  - de Arcangelis, L.
TI  - Role of inhibitory neurons in temporal correlations of critical and supercritical spontaneous activity
AB  - Experimental and numerical results suggest that the brain can be viewed as a system acting close to a critical point, as confirmed by scale-free distributions of relevant quantities in a variety of different systems and models. Less attention has received the investigation of the temporal correlation functions in brain activity in different, healthy and pathological, conditions. Here we perform this analysis by means of a model with short and long-term plasticity which implements the novel feature of different recovery rates for excitatory and inhibitory neurons, found experimentally. We evidence the important role played by inhibitory neurons in the supercritical state: We detect an unexpected oscillatory behaviour of the correlation decay, whose frequency depends on the fraction of inhibitory neurons and their connectivity degree. This behaviour can be rationalized by the observation that bursts in activity become more frequent and with a smaller amplitude as inhibition becomes more relevant.
PB  - arXiv
PY  - 2020
ST  - Role of inhibitory neurons in temporal correlations of critical and supercritical spontaneous activity
Y2  - 2025/05/05/21:54:29
DO  - 10.1016/j.physa.2020.125555
ER  -


TY  - GEN
AU  - Shi, B.
AU  - Dai, Q.
AU  - Hoffman, J.
AU  - Darrell, T.
AU  - Xu, H.
TI  - Temporal action detection with multi-level supervision
AB  - Training temporal action detection in videos requires large amounts of labeled data, yet such annotation is expensive to collect. Incorporating unlabeled or weakly-labeled data to train action detection model could help reduce annotation cost. In this work, we first introduce the Semi-supervised Action Detection (SSAD) task with a mixture of labeled and unlabeled data and analyze different types of errors in the proposed SSAD baselines which are directly adapted from the semi-supervised classification task. To alleviate the main error of action incompleteness (i.e., missing parts of actions) in SSAD baselines, we further design an unsupervised foreground attention (UFA) module utilizing the “independence” between foreground and background motion. Then we incorporate weakly-labeled data into SSAD and propose Omni-supervised Action Detection (OSAD) with three levels of supervision. An information bottleneck (IB) suppressing the scene information in non-action frames while preserving the action information is designed to help overcome the accompanying action-context confusion problem in OSAD baselines. We extensively benchmark against the baselines for SSAD and OSAD on our created data splits in THUMOS14 and ActivityNet1.2, and demonstrate the effectiveness of the proposed UFA and IB methods. Lastly, the benefit of our full OSAD-IB model under limited annotation budgets is shown by exploring the optimal annotation strategy for labeled, unlabeled and weakly-labeled data.
PB  - arXiv
PY  - 2020
ST  - Temporal action detection with multi-level supervision
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/iccv48922.2021.00792
ER  -


TY  - GEN
AU  - Churamani, N.
AU  - Kalkan, S.
AU  - Gunes, H.
TI  - Spatio-temporal analysis of facial actions using lifecycle-aware capsule networks
AB  - Most state-of-the-art approaches for Facial Action Unit (AU) detection rely upon evaluating facial expressions from static frames, encoding a snapshot of heightened facial activity. In real-world interactions, however, facial expressions are usually more subtle and evolve in a temporal manner requiring AU detection models to learn spatial as well as temporal information. In this paper, we focus on both spatial and spatio-temporal features encoding the temporal evolution of facial AU activation. For this purpose, we propose the Action Unit Lifecycle-Aware Capsule Network (AULA-Caps) that performs AU detection using both frame and sequence-level features. While at the frame-level the capsule layers of AULA-Caps learn spatial feature primitives to determine AU activations, at the sequence-level, it learns temporal dependencies between contiguous frames by focusing on relevant spatio-temporal segments in the sequence. The learnt feature capsules are routed together such that the model learns to selectively focus more on spatial or spatio-temporal information depending upon the AU lifecycle. The proposed model is evaluated on the commonly used BP4D and GFT benchmark datasets obtaining state-of-the-art results on both the datasets.
PB  - arXiv
PY  - 2020
ST  - Spatio-temporal analysis of facial actions using lifecycle-aware capsule networks
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/fg52635.2021.9666978
ER  -


TY  - GEN
AU  - Faerberg, D.F.
AU  - Gurarie, V.
AU  - Ruvinsky, I.
TI  - Inferring temporal organization of postembryonic development from high-content behavioral tracking
AB  - Understanding temporal regulation of development remains an important challenge. Whereas average, species-typical timing of many developmental processes has been established, less is known about inter-individual variability and correlations in timing of specific events. We addressed these questions in the context of postembryonic development in Caenorhabditis elegans. Based on patterns of locomotor activity of freely moving animals, we inferred durations of four larval stages (L1-L4) in over 100 individuals. Analysis of these data supports several notable conclusions. Individuals have consistently faster or slower rates of development because durations of L1 through L3 stages are positively correlated. The last larval stage, the L4, is less variable than earlier stages and its duration is largely independent of the rate of early larval development, implying existence of two distinct larval epochs. We argue that characteristic patterns of variation and correlation arise because duration of each stage tends to scale relative to total developmental time. This scaling relationship suggests that each larval stage is not limited by an absolute duration, but is instead terminated when a subset of events that must occur prior to adulthood have been completed. The approach described here offers a scalable platform that will facilitate the study of temporal regulation of postembryonic development.
PB  - bioRxiv
PY  - 2020
ST  - Inferring temporal organization of postembryonic development from high-content behavioral tracking
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2020.11.11.378166
ER  -


TY  - GEN
AU  - Heidari, N.
AU  - Iosifidis, A.
TI  - Progressive Spatio-Temporal Graph Convolutional Network for Skeleton-Based Human Action Recognition
AB  - Graph convolutional networks have been very successful in skeleton-based human action recognition where the sequence of skeletons is modeled as a graph. However, most of the graph convolutional network-based methods in this area train a deep feed-forward network with a fixed topology that leads to high computational complexity and restricts their application in low computation scenarios. In this paper, we propose a method to automatically find a compact and problem-specific topology for spatio-temporal graph convolutional networks in a progressive manner. Experimental results on two widely used datasets for skeleton-based human action recognition indicate that the proposed method has competitive or even better classification performance compared to the state-of-the-art methods while it has much lower computational complexity.
PB  - arXiv
PY  - 2020
ST  - Progressive Spatio-Temporal Graph Convolutional Network for Skeleton-Based Human Action Recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/icassp39728.2021.9413860
ER  -


TY  - GEN
AU  - Yang, D.
AU  - Dai, R.
AU  - Wang, Y.
AU  - Francesca, G.
AU  - Brémond, F.
TI  - Selective Spatio-Temporal Aggregation Based Pose Refinement System: Towards Understanding Human Activities in Real-World Videos
AB  - Taking advantage of human pose data for understanding human activities has attracted much attention these days. However, state-of-the-art pose estimators struggle in obtaining high-quality 2D or 3D pose data due to occlusion, truncation and low-resolution in real-world unannotated videos. Hence, in this work, we propose 1) a Selective Spatio-Temporal Aggregation mechanism, named SST-A, that refines and smooths the keypoint locations extracted by multiple expert pose estimators, 2) an effective weakly-supervised self-training framework which leverages the aggregated poses as pseudo ground-truth instead of handcrafted annotations for real-world pose estimation. Extensive experiments are conducted for evaluating not only the upstream pose refinement but also the downstream action recognition performance on four datasets, Toyota Smarthome, NTU-RGB+D, Charades, and Kinetics-50. We demonstrate that the skeleton data refined by our Pose-Refinement system (SSTA-PRS) is effective at boosting various existing action recognition models, which achieves competitive or state-of-the-art performance.
PB  - arXiv
PY  - 2020
ST  - Selective Spatio-Temporal Aggregation Based Pose Refinement System
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/wacv48630.2021.00241
ER  -


TY  - GEN
AU  - Stergiou, A.
AU  - Poppe, R.
TI  - Multi-temporal convolutions for human action recognition in videos
AB  - Effective extraction of temporal patterns is crucial for the recognition of temporally varying actions in video. We argue that the fixed-sized spatio-temporal convolution kernels used in convolutional neural networks (CNNs) can be improved to extract informative motions that are executed at different time scales. To address this challenge, we present a novel spatio-temporal convolution block that is capable of extracting spatio-temporal patterns at multiple temporal resolutions. Our proposed multi-temporal convolution (MTConv) blocks utilize two branches that focus on brief and prolonged spatio-temporal patterns, respectively. The extracted time-varying features are aligned in a third branch, with respect to global motion patterns through recurrent cells. The proposed blocks are lightweight and can be integrated into any 3D-CNN architecture. This introduces a substantial reduction in computational costs. Extensive experiments on Kinetics, Moments in Time and HACS action recognition benchmark datasets demonstrate competitive performance of MTConvs compared to the state-of-the-art with a significantly lower computational footprint1
PB  - arXiv
PY  - 2020
ST  - Multi-temporal convolutions for human action recognition in videos
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/ijcnn52387.2021.9533515
ER  -


TY  - GEN
AU  - Heidari, N.
AU  - Iosifidis, A.
TI  - On the spatial attention in spatio-temporal graph convolutional networks for skeleton-based human action recognition
AB  - Graph convolutional networks (GCNs) achieved promising performance in skeleton-based human action recognition by modeling a sequence of skeletons as a spatio-temporal graph. Most of the recently proposed GCN-based methods improve the performance by learning the graph structure at each layer of the network using spatial attention applied on a predefined graph Adjacency matrix that is optimized jointly with model’s parameters in an end-to-end manner. In this paper, we analyze the spatial attention used in spatio-temporal GCN layers and propose a symmetric spatial attention for better reflecting the symmetric property of the relative positions of the human body joints when executing actions. We also highlight the connection of spatio-temporal GCN layers employing additive spatial attention to bilinear layers, and we propose the spatio-temporal bilinear network (ST-BLN) which does not require the use of predefined Adjacency matrices and allows for more flexible design of the model. Experimental results show that the three models lead to effectively the same performance. Moreover, by exploiting the flexibility provided by the proposed ST-BLN, one can increase the efficiency of the model.
PB  - arXiv
PY  - 2020
ST  - On the spatial attention in spatio-temporal graph convolutional networks for skeleton-based human action recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/ijcnn52387.2021.9534440
ER  -


TY  - GEN
AU  - Miningou Zobon, N.T.
AU  - Jędrzejewska-Szmek, J.
AU  - Blackwell, K.T.
TI  - Temporal pattern and synergy influence activity of ERK signaling pathways during L-LTP induction
AB  - Long lasting long-term potentiation (L-LTP) is a cellular mechanism of learning and memory storage. Studies have demonstrated a requirement for the extracellular signal-regulated kinase (ERK) activation in L-LTP produced by a diversity of temporal stimulation patterns. Multiple signaling pathways converge to activate ERK, with different pathways being required for different stimulation patterns. We addressed the critical questions of whether maximal activation of ERK requires multiple pathways, and whether different temporal patterns select different signaling pathways for ERK activation. We developed a computational model of five signaling pathways (including two novel pathways) leading to ERK activation during L-LTP. Simulations show that calcium and cAMP work synergistically to activate ERK, and that stimuli given with large inter-trial intervals activate more ERK than shorter intervals, a temporal sensitivity similar to PKA but contrary to CaMKII. These results suggest that signaling pathways with different temporal sensitivity facilitate ERK activation to diversity of temporal patterns.
PB  - bioRxiv
PY  - 2020
ST  - Temporal pattern and synergy influence activity of ERK signaling pathways during L-LTP induction
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2020.11.04.368571
ER  -


TY  - GEN
AU  - Lv, F.
AU  - Li, M.
AU  - Guo, T.
AU  - Jin, T.
AU  - Ng, W.
TI  - XDM: Improving Sequential Deep Matching with Unclicked User Behaviors for Recommender System
AB  - Deep learning-based sequential recommender systems have recently attracted increasing attention from both academia and industry. Most of industrial Embedding-Based Retrieval (EBR) systems for recommendation share the similar ideas with sequential recommenders. Among them, how to comprehensively capture sequential user interest is a fundamental problem. However, most existing sequential recommendation models take as input clicked or purchased behavior sequences from user-item interactions. This leads to incomprehensive user representation and sub-optimal model performance, since they ignore the complete user behavior exposure data, i.e., items impressed yet unclicked by users. In this work, we attempt to incorporate and model those unclicked item sequences using a new learning approach in order to explore better sequential recommendation technique. An efficient triplet metric learning algorithm is proposed to appropriately learn the representation of unclicked items. Our method can be simply integrated with existing sequential recommendation models by a confidence fusion network and further gain better user representation. The offline experimental results based on real-world E-commerce data demonstrate the effectiveness and verify the importance of unclicked items in sequential recommendation. Moreover we deploy our new model (named XDM) into EBR of recommender system at Taobao, outperforming the previous deployed generation SDM.
PB  - arXiv
PY  - 2020
ST  - XDM
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-031-00129-1_31
ER  -


TY  - GEN
AU  - Heidari, N.
AU  - Iosifidis, A.
TI  - Temporal attention-augmented graph convolutional network for efficient skeleton-based human action recognition
AB  - Graph convolutional networks (GCNs) have been very successful in modeling non-Euclidean data structures, like sequences of body skeletons forming actions modeled as spatio-temporal graphs. Most GCN-based action recognition methods use deep feed-forward networks with high computational complexity to process all skeletons in an action. This leads to a high number of floating point operations (ranging from 16G to 100G FLOPs) to process a single sample, making their adoption in restricted computation application scenarios infeasible. In this paper, we propose a temporal attention module (TAM) for increasing the efficiency in skeleton-based action recognition by selecting the most informative skeletons of an action at the early layers of the network. We incorporate the TAM in a lightweight GCN topology to further reduce the overall number of computations. Experimental results on two benchmark datasets show that the proposed method outperforms with a large margin the baseline GCN-based method while having ×2.9 less number of computations. Moreover, it performs on par with the state-of-the-art with up to ×9.6 less number of computations.
PB  - arXiv
PY  - 2020
ST  - Temporal attention-augmented graph convolutional network for efficient skeleton-based human action recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/icpr48806.2021.9412091
ER  -


TY  - GEN
AU  - Zhai, Y.
AU  - Wang, L.
AU  - Tang, W.
AU  - Yuan, J.
AU  - Hua, G.
TI  - Two-stream consensus network for weakly-supervised temporal action localization
AB  - Weakly-supervised Temporal Action Localization (W-TAL) aims to classify and localize all action instances in an untrimmed video under only video-level supervision. However, without frame-level annotations, it is challenging for W-TAL methods to identify false positive action proposals and generate action proposals with precise temporal boundaries. In this paper, we present a Two-Stream Consensus Network (TSCN) to simultaneously address these challenges. The proposed TSCN features an iterative refinement training method, where a frame-level pseudo ground truth is iteratively updated, and used to provide frame-level supervision for improved model training and false positive action proposal elimination. Furthermore, we propose a new attention normalization loss to encourage the predicted attention to act like a binary selection, and promote the precise localization of action instance boundaries. Experiments conducted on the THUMOS14 and ActivityNet datasets show that the proposed TSCN outperforms current state-of-the-art methods, and even achieves comparable results with some recent fully-supervised methods.
PB  - arXiv
PY  - 2020
ST  - Two-stream consensus network for weakly-supervised temporal action localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-030-58539-6_3
ER  -


TY  - GEN
AU  - Chen, C.-F.
AU  - Panda, R.
AU  - Ramakrishnan, K.
AU  - Oliva, A.
AU  - Fan, Q.
TI  - Deep analysis of CNN-based Spatio-temporal representations for action recognition
AB  - In recent years, a number of approaches based on 2D or 3D convolutional neural networks (CNN) have emerged for video action recognition, achieving state-of-the-art results on several large-scale benchmark datasets. In this paper, we carry out in-depth comparative analysis to better understand the differences between these approaches and the progress made by them. To this end, we develop an unified framework for both 2D-CNN and 3D-CNN action models, which enables us to remove bells and whistles and provides a common ground for fair comparison. We then conduct an effort towards a large-scale analysis involving over 300 action recognition models. Our comprehensive analysis reveals that a) a significant leap is made in efficiency for action recognition, but not in accuracy; b) 2D-CNN and 3D-CNN models behave similarly in terms of spatio-temporal representation abilities and transferability. Our codes are available at https://github.com/IBM/action-recognition-pytorch.
PB  - arXiv
PY  - 2020
ST  - Deep analysis of CNN-based Spatio-temporal representations for action recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr46437.2021.00610
ER  -


TY  - GEN
AU  - Mohammed, H.
AU  - Li, Y.
AU  - Di Grazia, P.
AU  - Agger, S.
AU  - Hollis, E.
TI  - Temporal regulation of motor behavior on a modified forelimb dexterity test in mice
AB  - Hand and arm manual dexterity is a hallmark of humans and non-human primates. While rodents are less dexterous than primates, they provide powerful models for testing neural circuit function in behavioral output, including dexterous behaviors. In rodents, the single pellet reach task has been used extensively to study both dexterous forelimb motor learning as well as recovery from injury; however, mice exhibit high variability in task acquisition in comparison to rats and a significant percentage fail to learn the task. We have created a recessed version of the task that requires greater dexterity. This subtle modification increases both task difficulty as well as the proportion of mice that show an improvement with training. Furthermore, motor cortex inactivation shows a greater effect on the execution of the recessed forelimb reach task, with distinct effects on reach targeting vs grasping components depending on the timing of inhibitory activation. Kinematic analysis revealed differences in reach targeting upon transient cortical inhibition prior to reach onset. In summary, the recessed single pellet reach task provides a robust assessment of forelimb dexterity in mice and a tool for studying skilled motor acquisition and execution.
PB  - bioRxiv
PY  - 2020
ST  - Temporal regulation of motor behavior on a modified forelimb dexterity test in mice
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2020.10.18.344507
ER  -


TY  - GEN
AU  - Koshkin, V.
AU  - de Oliveira, M.B.
AU  - Kochmann, S.
AU  - Peng, C.
AU  - Krylov, S.N.
TI  - Sequential measurements of catalytic activities of multi-drug-resistance transporters and cytochrome P450 enzymes by cytometry of reaction rate constant
AB  - Cytometry of reaction rate constant (CRRC) is an accurate and robust approach to characterize cell-population heterogeneity using rate constants of cellular processes for which kinetic mechanisms are known. We work on a CRRC-based method to develop predictors of tumor chemoresistance driven by two processes: drug extrusion by multi-drug-resistance (MDR) transporters and drug inactivation by cytochrome-P450 enzymes (CYP). Each of the two possess is studied with its specific substrate and the process activity is characterized by a corresponding unimolecular rate constant. Due to the incompatibility of MDR and CYP assays, MDR and CYP activities may be difficult to measure simultaneously suggesting that they may need to be measured sequentially. The sequential measurements may also impose a problem: the results of the second assay may be affected by artifacts exerted by the first assay. The goal of this work was to understand whether the cells have a memory of the first assay that significantly affects the results of the second assay. To achieve this goal, we compared CRRC results for two orders of sequential measurements: the MDR→CYP order in which MDR activity is measured before CYP activity and the CYP→MDR order in which CYP activity is measured before MDR activity. It was found that the results of the CYP assay were similar in both orders; on the contrary, the results of the MDR assay were significantly different. Our findings suggest that MDR and CYP activity can be studied sequentially provided that MDR activity is measured first and CYP activity second.
PB  - bioRxiv
PY  - 2020
ST  - Sequential measurements of catalytic activities of multi-drug-resistance transporters and cytochrome P450 enzymes by cytometry of reaction rate constant
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2020.10.15.340976
ER  -


TY  - GEN
AU  - Vegh, J.
AU  - Berki, Á.J.
TI  - On the Spatiotemporal Behavior in Biology-Mimicking Computing Systems
AB  - Both the growing demand to cope with”big data” (based on, or assisted by, artificial intelligence) and the interest in understanding the operation of our brain more completely, stimulated the efforts to build biology-mimicking computing systems from inexpensive conventional components and build different (”neuromorphic”) computing systems. On one side, those systems require an unusually large number of processors, which introduces performance limitations and nonlinear scaling. On the other side, the neuronal operation drastically differs from the conventional workloads. The conduction time (transfer time) is ignored in both in conventional computing and”spatiotemporal” computational models of neural networks, although von Neumann warned:”In the human nervous system the conduction times along the lines (axons) can be longer than the synaptic delays, hence our above procedure of neglecting them aside of τ [the processing time] would be unsound” [1], section 6.3. This difference alone makes imitating biological behavior in technical implementation hard. Besides, the recent issues in computing called the attention to that the temporal behavior is a general feature of computing systems, too. Some of their effects in both biological and technical systems were already noticed. Instead of introducing some”looks like” models, the correct handling of the transfer time is suggested here. Introducing the temporal logic, based on the Minkowski transform, gives quantitative insight into the operation of both kinds of computing systems, furthermore provides a natural explanation of decades-old empirical phenomena. Without considering their temporal behavior correctly, neither effective implementation nor a true imitation of biological neural systems are possible.
PB  - Research Square
PY  - 2020
ST  - On the Spatiotemporal Behavior in Biology-Mimicking Computing Systems
Y2  - 2025/05/05/21:54:29
DO  - 10.21203/rs.3.rs-88297/v2
ER  -


TY  - GEN
AU  - Srivastava, A.
AU  - Dutta, O.
AU  - Ap, P.
AU  - Agarwal, S.
AU  - Gupta, J.
TI  - A variational information bottleneck based method to compress sequential networks for human action recognition
AB  - In the last few years, compression of deep neural networks has become an important strand of machine learning and computer vision research. Deep models require sizeable computational complexity and storage, when used for instance for Human Action Recognition (HAR) from videos, making them unsuitable to be deployed on edge devices. In this paper, we address this issue and propose a method to effectively compress Recurrent Neural Networks (RNNs) such as Gated Recurrent Units (GRUs) and Long-Short-Term-Memory Units (LSTMs) that are used for HAR. We use a Variational Information Bottleneck (VIB) theory-based pruning approach to limit the information flow through the sequential cells of RNNs to a small subset. Further, we combine our pruning method with a specific group-lasso regularization technique that significantly improves compression. The proposed techniques reduce model parameters and memory footprint from latent representations, with little or no reduction in the validation accuracy while increasing the inference speed several-fold. We perform experiments on the three widely used Action Recognition datasets, viz. UCF11, HMDB51, and UCF101, to validate our approach. It is shown that our method achieves over 70 times greater compression than the nearest competitor with comparable accuracy for the task of action recognition on UCF11.
PB  - arXiv
PY  - 2020
ST  - A variational information bottleneck based method to compress sequential networks for human action recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/wacv48630.2021.00279
ER  -


TY  - GEN
AU  - Li, R.
AU  - Jiang, S.
AU  - Tan, S.
AU  - Qian, H.
AU  - Ge, R.
TI  - Impacted spike frequency adaptation associated with reduction of KCNQ2/3 promotes seizure activity in temporal lobe epilepsy
AB  - Although numerous epilepsy-related genes have been identified by unbiased genome-wide screening based on samples from both animal models and patients, the druggable targets for temporal lobe epilepsy (TLE) are still limited. Meanwhile, a large number of candidate genes that might promote or inhibit seizure activities are waiting for further validation. In this study, we first analyzed two public databases and determined the significant down-regulations of two M-type potassium channel genes (KCNQ2/3) expressions in hippocampus samples from TLE patients. Then we reproduced the similar pathological changes in the pilocarpine mouse model of TLE and further detected the decrease of spike frequency adaptation driven by impacted M-currents on dentate gyrus granule neurons. Finally, we employed a small-scale simulation of dentate gyrus network to investigate potential functional consequences of disrupted neuronal excitability. We demonstrated that the impacted spike frequency adaptation of granule cells facilitated the epileptiform activity among the entire network, including prolonged seizure duration and reduced interictal intervals. Our results identify a new mechanism contributing to ictogenesis in TLE and suggest a novel target for the anti-epileptic drug discovery.
PB  - bioRxiv
PY  - 2020
ST  - Impacted spike frequency adaptation associated with reduction of KCNQ2/3 promotes seizure activity in temporal lobe epilepsy
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2020.09.25.313254
ER  -


TY  - GEN
AU  - Filipovic, M.
AU  - Mitrevski, B.
AU  - Antognini, D.
AU  - Faltings, B.
AU  - Musat, C.
TI  - Modeling online behavior in recommender systems: The importance of temporal context
AB  - Recommender systems research tends to evaluate model performance offline and on randomly sampled targets, yet the same systems are later used to predict user behavior sequentially from a fixed point in time. Simulating online recommender system performance is notoriously difficult and the discrepancy between online and offline behaviors is typically not accounted for in offline evaluations. This disparity permits weaknesses to go unnoticed until the model is deployed in a production setting. In this paper, we first demonstrate how omitting temporal context when evaluating recommender system performance leads to false confidence. To overcome this, we postulate that offline evaluation protocols can only model real-life use-cases if they account for temporal context. Next, we propose a training procedure to further embed the temporal context in existing models. We use a multi-objective approach to introduce temporal context into traditionally time-unaware recommender systems and confirm its advantage via the proposed evaluation protocol. Finally, we validate that the Pareto Fronts obtained with the added objective dominate those produced by state-of-the-art models that are only optimized for accuracy on three real-world publicly available datasets. The results show that including our temporal objective can improve recall@20 by up to 20%.
PB  - arXiv
PY  - 2020
ST  - Modeling online behavior in recommender systems
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-319-60438-1_16
ER  -


TY  - GEN
AU  - Végh, J.
AU  - Berki, Á.J.
TI  - On the spatiotemporal behavior in biology-mimicking computing systems
AB  - The payload performance of conventional computing systems, from single processors to supercomputers, reached its limits the nature enables. Both the growing demand to cope with”big data” (based on, or assisted by, artificial intelligence) and the interest in understanding the operation of our brain more completely, stimulated the efforts to build biology-mimicking computing systems from inexpensive conventional components and build different (”neuromorphic”) computing systems. On one side, those systems require an unusually large number of processors, which introduces performance limitations and nonlinear scaling. On the other side, the neuronal operation drastically differs from the conventional workloads. The conventional computing (including both its mathematical background and physical implementation) is based on assuming instant interaction, while the biological neuronal systems have a”spatiotemporal” behavior, although conduction time is typically ignored in computational models of neural network function. This difference alone makes imitating biological behavior in technical implementation hard. Besides, the recent issues in computing called the attention to that the temporal behavior is a general feature of computing systems, too. Some of their effects in both biological and technical systems were already noticed. Nevertheless, handling of those issues is incomplete/improper. Introducing temporal logic, based on the Minkowski transform, gives quantitative insight into the operation of both kinds of computing systems, furthermore provides a natural explanation of decades-old empirical phenomena. Without considering their temporal behavior correctly, neither effective implementation nor a true imitation of biological neural systems are possible.
PB  - arXiv
PY  - 2020
ST  - On the spatiotemporal behavior in biology-mimicking computing systems
Y2  - 2025/05/05/21:54:29
DO  - 10.21203/rs.3.rs-88297/v2
ER  -


TY  - GEN
AU  - Su, H.
AU  - Gan, W.
AU  - Wu, W.
AU  - Qiao, Y.
AU  - Yan, J.
TI  - BSN++: Complementary boundary regressor with scale-balanced relation modeling for temporal action proposal generation
AB  - Generating human action proposals in untrimmed videos is an important yet challenging task with wide applications. Current methods often suffer from the noisy boundary locations and the inferior quality of confidence scores used for proposal retrieving. In this paper, we present BSN++, a new framework which exploits complementary boundary regressor and relation modeling for temporal proposal generation. First, we propose a novel boundary regressor based on the complementary characteristics of both starting and ending boundary classifiers. Specifically, we utilize the U-shaped architecture with nested skip connections to capture rich contexts and introduce bi-directional boundary matching mechanism to improve boundary precision. Second, to account for the proposal-proposal relations ignored in previous methods, we devise a proposal relation block to which includes two self-attention modules from the aspects of position and channel. Furthermore, we find that there inevitably exists data imbalanced problems in the positive/negative proposals and temporal durations, which harm the model performance on tail distributions. To relieve this issue, we introduce the scale-balanced re-sampling strategy. Extensive experiments are conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, which demonstrate that BSN++ achieves the state-of-the-art performance. Not surprisingly, the proposed BSN++ ranked 1st place in the CVPR19 - ActivityNet challenge leaderboard on temporal action localization task.
PB  - arXiv
PY  - 2020
ST  - BSN++
Y2  - 2025/05/05/21:54:29
DO  - 10.1609/aaai.v35i3.16363
ER  -


TY  - GEN
AU  - Zhu, S.
AU  - Bukharin, A.
AU  - Xie, L.
AU  - Yang, S.
AU  - Xie, Y.
TI  - High-resolution spatio-temporal model for county-level COVID-19 activity in the U.S.
AB  - We present an interpretable high-resolution spatio-temporal model to estimate COVID-19 deaths together with confirmed cases one-week ahead of the current time, at the county-level and weekly aggregated, in the United States. A notable feature of our spatio-temporal model is that it considers the (a) temporal auto- and pairwise correlation of the two local time series (confirmed cases and death of the COVID-19), (b) dynamics between locations (propagation between counties), and (c) covariates such as local within-community mobility and social demographic factors. The within-community mobility and demographic factors, such as total population and the proportion of the elderly, are included as important predictors since they are hypothesized to be important in determining the dynamics of COVID-19. To reduce the model’s high-dimensionality, we impose sparsity structures as constraints and emphasize the impact of the top ten metropolitan areas in the nation, which we refer (and treat within our models) as hubs in spreading the disease. Our retrospective out-of-sample county-level predictions were able to forecast the subsequently observed COVID-19 activity accurately. The proposed multi-variate predictive models were designed to be highly interpretable, with clear identification and quantification of the most important factors that determine the dynamics of COVID-19. Ongoing work involves incorporating more covariates, such as education and income, to improve prediction accuracy and model interpretability.
PB  - arXiv
PY  - 2020
ST  - High-resolution spatio-temporal model for county-level COVID-19 activity in the U.S.
Y2  - 2025/05/05/21:54:29
DO  - 10.1145/3468876
ER  -


TY  - GEN
AU  - Johnson-Freyd, P.
AU  - Aytac, J.
AU  - Hulette, G.
TI  - Topos semantics for a higher-order temporal logic of actions
AB  - TLA is a popular temporal logic for writing stuttering-invariant specifications of digital systems. However, TLA lacks higher-order features useful for specifying modern software written in higher-order programming languages. We use categorical techniques to recast a real-time semantics for TLA in terms of the actions of a group of time dilations, or “stutters,” and an extension by a monoid incorporating delays, or “falters.” Via the geometric morphism of the associated presheaf topoi induced by the inclusion of stutters into falters, we construct the first model of a higher-order TLA.
PB  - arXiv
PY  - 2020
ST  - Topos semantics for a higher-order temporal logic of actions
Y2  - 2025/05/05/21:54:29
DO  - 10.4204/eptcs.323.11
ER  -


TY  - GEN
AU  - Bhatia, S.
AU  - Kleinjan, D.J.
AU  - Uttley, K.
AU  - Dellepiane, N.
AU  - Bickmore, W.A.
TI  - Q-STARZ: Quantitative spatial and temporal assessment of regulatory element activity in zebrafish
AB  - Noncoding regions of the genome harbouring cis-regulatory elements (CREs) or enhancers drive spatial and temporal gene expression. Mutations or single nucleotide polymorphisms (SNPs) in enhancers have been widely implicated in human diseases and disease-predispositions. However, our ability to assay the regulatory potential of genetic variants in enhancers is currently very limited, in part because of the need to assay these elements in an appropriate biological context. Here, we describe a method for simultaneous quantitative assessment of the spatial and temporal activity of wild-type (Wt) and disease-associated, mutant (Mut) human CRE alleles using live imaging in zebrafish embryonic development. We generated transgenic lines harbouring a dual-CRE dual-reporter cassette in a pre-defined neutral docking site in the zebrafish genome. Using this single transgenic cassette, the functional activity of each CRE allele is reported via expression of a specific fluorescent reporter, allowing the simultaneous visualisation of the activity of both alleles. This can reveal where and when in embryonic development the wild-type allele is active and how this activity is altered by the disease-associated mutation.
PB  - bioRxiv
PY  - 2020
ST  - Q-STARZ
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2020.09.13.290460
ER  -


TY  - GEN
AU  - Huang, K.
AU  - Han, Y.
AU  - Chen, K.
AU  - Wei, P.
AU  - Wang, L.
TI  - Mapping mouse behavior with an unsupervised spatio-temporal sequence decomposition framework
AB  - Objective quantification of animal behavior is crucial to understanding the relationship between brain activity and behavior. For rodents, this has remained a challenge due to the high-dimensionality and large temporal variability of their behavioral features. Inspired by the natural structure of animal behavior, the present study uses a parallel, and multi-stage approach to decompose motion features and generate an objective metric for mapping rodent behavior into the animal feature space. Incorporating a three-dimensional (3D) motion-capture system and unsupervised clustering into this approach, we developed a novel framework that can automatically identify animal behavioral phenotypes from experimental monitoring. We demonstrate the efficacy of our framework by generating an “autistic-like behavior space” that can robustly characterize a transgenic mouse disease model based on motor activity without human supervision. The results suggest that our framework features a broad range of applications, including animal disease model phenotyping and the modeling of relationships between neural circuits and behavior.
PB  - bioRxiv
PY  - 2020
ST  - Mapping mouse behavior with an unsupervised spatio-temporal sequence decomposition framework
Y2  - 2025/05/05/21:54:29
DO  - 10.15669/pnst.2.603
ER  -


TY  - GEN
AU  - Nishimura, H.
AU  - Ivanovic, B.
AU  - Gaidon, A.
AU  - Pavone, M.
AU  - Schwager, M.
TI  - Risk-sensitive sequential action control with multi-modal human trajectory forecasting for safe crowd-robot interaction
AB  - This paper presents a novel online framework for safe crowd-robot interaction based on risk-sensitive stochastic optimal control, wherein the risk is modeled by the entropic risk measure. The sampling-based model predictive control relies on mode insertion gradient optimization for this risk measure as well as Trajectron++, a state-of-the-art generative model that produces multimodal probabilistic trajectory forecasts for multiple interacting agents. Our modular approach decouples the crowd-robot interaction into learning-based prediction and model-based control, which is advantageous compared to end-to-end policy learning methods in that it allows the robot’s desired behavior to be specified at run time. In particular, we show that the robot exhibits diverse interaction behavior by varying the risk sensitivity parameter. A simulation study and a real-world experiment show that the proposed online framework can accomplish safe and efficient navigation while avoiding collisions with more than 50 humans in the scene.
PB  - arXiv
PY  - 2020
ST  - Risk-sensitive sequential action control with multi-modal human trajectory forecasting for safe crowd-robot interaction
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/iros45743.2020.9341469
ER  -


TY  - GEN
AU  - Homdee, N.
AU  - Goins, H.
AU  - Bankole, A.
AU  - Anderson, M.S.
AU  - Lach, J.
TI  - Actionable interpretation of machine learning models for sequential data: Dementia-related agitation use case
AB  - Machine learning has shown successes for complex learning problems in which data/parameters can be multidimensional and too complex for a first-principles based analysis. Some applications that utilize machine learning require human interpretability, not just to understand a particular result (classification, detection, etc.) but also for humans to take action based on that result. Black-box machine learning model interpretation has been studied, but recent work has focused on validation and improving model performance. In this work, actionable interpretation of black-box machine learning models is presented. The proposed technique focuses on the extraction of actionable measures to help users make a decision or take an action. Actionable interpretation can be implemented in most traditional black-box machine learning models. It uses the already trained model, used training data, and data processing techniques to extract actionable items from the model outcome and its time-series inputs. An implementation of the actionable interpretation is shown with a use case: dementia-related agitation prediction and the ambient environment. It is shown that actionable items can be extracted, such as the decreasing of in-home light level, which is triggering an agitation episode. This use case of actionable interpretation can help dementia caregivers take action to intervene and prevent agitation.
PB  - arXiv
PY  - 2020
ST  - Actionable interpretation of machine learning models for sequential data
Y2  - 2025/05/05/21:54:29
DO  - 10.1177/089719000001300408
ER  -


TY  - GEN
AU  - Shahnazian, D.
AU  - Senoussi, M.
AU  - Krebs, R.M.
AU  - Verguts, T.
AU  - Holroyd, C.B.
TI  - Neural representations of task context and temporal order during action sequence execution
AB  - Since routine action sequences can share a great deal of similarity in terms of their stimulus response mappings, their correct execution relies crucially on the ability to preserve contextual and temporal information (Lashley, 1951). However, there are few empirical studies on the neural mechanism and the brain areas maintaining such information. To address this gap in the literature, we recently recorded the blood-oxygen level dependent (BOLD) response in a newly developed coffee-tea making task (Holroyd et al., 2018). The task involves the execution of 4 action sequences that each feature 6 decision states. Here we report a reanalysis of this dataset using a data-driven approach, namely multivariate pattern analysis (MVPA), that examines context-dependent neural activity across several predefined regions of interest. Results highlight involvement of the inferior-temporal gyrus and lateral prefrontal cortex in maintaining temporal and contextual information for the execution of hierarchically-organized action sequences. Furthermore, temporal information seems to be more strongly encoded in areas over the left hemisphere.
PB  - bioRxiv
PY  - 2020
ST  - Neural representations of task context and temporal order during action sequence execution
Y2  - 2025/05/05/21:54:29
DO  - 10.1111/tops.12533
ER  -


TY  - GEN
AU  - Sarracino, A.
AU  - Arviv, O.
AU  - Shriki, O.
AU  - de Arcangelis, L.
TI  - Predicting brain evoked response to external stimuli from temporal correlations of spontaneous activity
AB  - The relation between spontaneous and stimulated global brain activity is a fundamental problem in the understanding of brain functions. This question is investigated both theoretically and experimentally within the context of nonequilibrium fluctuation-dissipation relations. We consider the stochastic coarse-grained Wilson-Cowan model in the linear noise approximation and compare analytical results to experimental data from magnetoencephalography (MEG) of human brain. The short time behavior of the autocorrelation function for spontaneous activity is characterized by a double-exponential decay, with two characteristic times, differing by two orders of magnitude. Conversely, the response function exhibits a single exponential decay in agreement with experimental data for evoked activity under visual stimulation. Results suggest that the brain response to weak external stimuli can be predicted from the observation of spontaneous activity and pave the way to controlled experiments on the brain response under different external perturbations.
PB  - arXiv
PY  - 2020
ST  - Predicting brain evoked response to external stimuli from temporal correlations of spontaneous activity
Y2  - 2025/05/05/21:54:29
DO  - 10.1103/physrevresearch.2.033355
ER  -


TY  - GEN
AU  - Gevorgyan, M.
TI  - Modification method for single-stage object detectors that allows to exploit the temporal Behaviour of a scene to improve detection accuracy
AB  - A simple modification method for single-stage generic object detection neural networks, such as YOLO and SSD, is proposed, which allows for improving the detection accuracy on video data by exploiting the temporal behavior of the scene in the detection pipeline. It is shown that, using this method, the detection accuracy of the base network can be considerably improved, especially for occluded and hidden objects. It is shown that a modified network is more prone to detect hidden objects with more confidence than an unmodified one. A weakly supervised training method is proposed, which allows for training a modified network without requiring any additional annotated data. MSC Codes 68T45, 68T07
PB  - arXiv
PY  - 2020
ST  - Modification method for single-stage object detectors that allows to exploit the temporal Behaviour of a scene to improve detection accuracy
Y2  - 2025/05/05/21:54:29
DO  - 10.1142/s0218001422500343
ER  -


TY  - GEN
AU  - Singh, G.
TI  - Online spatiotemporal action detection and prediction via causal representations
AB  - In this thesis, we focus on video action understanding problems from an online and real-time processing point of view. We start with the conversion of the traditional offline spatiotemporal action detection pipeline into an online spatiotemporal action tube detection system. An action tube is a set of bounding connected over time, which bounds an action instance in space and time. Next, we explore the future prediction capabilities of such detection methods by extending the an existing action tube into the future by regression. Later, we seek to establish that online/causal representations can achieve similar performance to that of offline three dimensional (3D) convolutional neural networks (CNNs) on various tasks, including action recognition, temporal action segmentation and early prediction. To this end, we propose various action tube detection approaches from either single or multiple frames. We start by introducing supervised action proposals for frame-level action detection and solving two energy optimisation formulations to detect the spatial and temporal boundaries of action tubes. Further, we propose an incremental tube construction algorithm to handle the online action detection problem. There, the real-time capabilities are made possible by introducing real-time frame-level action detection and real-time optical flow in the action detection pipeline for efficiency. Next, we extend our frame-level approach to multiple frames with the help of a novel proposal to for predicting flexible action’micro-tubes’ from a pair of frames. We extend the micro-tube prediction network in order to regress the future of each micro-tube, which is then fed to our proposed future action tube prediction framework. We convert 3D CNNs to causal 3D CNNs by replacing every 3D convolution with recurrent convolution, and by making use of sophisticated initialisation to handle the problems of recurrent modules. We show that our action tube detectors perform better than previous state-of-the-art methods, while exhibiting online and real-time capabilities. We evaluate each action tube detector and predictor on publicly available benchmarks to show the comparison with other state-of-the-art approaches. We also show that our flexible micro-tube proposals not only improve action detection performance but can also handle sparse annotations. Finally, we demonstrate the causal capabilities of our causal 3D CNN.
PB  - arXiv
PY  - 2020
ST  - Online spatiotemporal action detection and prediction via causal representations
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/iccv.2017.393
ER  -


TY  - GEN
AU  - Derner, M.
AU  - Chaieb, L.
AU  - Dehnen, G.
AU  - Mormann, F.
AU  - Fell, J.
TI  - Auditory beat stimulation modulates memory-related single-neuron activity in the human medial temporal lobe
AB  - Auditory beats are composed of two sine waves using nearby frequencies, which can either be applied as a superposed signal to both ears or to each ear separately. In the first case, the beat sensation results from hearing an amplitude-modulated signal (monaural beat). In the second case, it is generated by phase-sensitive neurons in the brain stem (binaural beat). We investigated the effects of monaural and binaural 5 Hz beat stimulation on neural activity and memory performance in neurosurgical patients performing an associative recognition task. Previously, we had reported that these beat stimulation conditions modulated memory performance in opposite directions. Here, we analyzed data from a patient subgroup, in which microwires were implanted in the amygdala, hippocampus, entorhinal cortex and parahippocampal cortex. We identified neurons responding with firing rate changes to binaural versus monaural 5 Hz beat stimulation. In these neurons, we correlated the differences in firing rates for binaural versus monaural beats to the memory-related differences for remembered versus forgotten items and associations. In the left hemisphere for these neurons, we detected statistically significant negative correlations between firing rate differences for binaural versus monaural beats and remembered versus forgotten items/associations. Importantly, such negative correlations were also observed between beat stimulation-related firing rate differences in the baseline window and memory-related firing rate differences in the poststimulus windows. In line with concepts of homeostatic plasticity, we interpret our findings as indicating that beat stimulation is linked to memory performance via shifting baseline firing levels.
PB  - bioRxiv
PY  - 2020
ST  - Auditory beat stimulation modulates memory-related single-neuron activity in the human medial temporal lobe
Y2  - 2025/05/05/21:54:29
DO  - 10.3390/brainsci11030364
ER  -


TY  - GEN
AU  - López-Cifuentes, A.
AU  - Escudero-Viñolo, M.
AU  - Bescós, J.
TI  - A prospective study on Sequence-Driven temporal sampling and Ego-Motion compensation for action recognition in the EPIC-Kitchens dataset
AB  - Action recognition is currently one of the top-challenging research fields in computer vision. Convolutional Neural Networks (CNNs) have significantly boosted its performance but rely on fixed-size spatio-temporal windows of analysis, reducing CNNs temporal receptive fields. Among action recognition datasets, egocentric recorded sequences have become of important relevance while entailing an additional challenge: ego-motion is unavoidably transferred to these sequences. The proposed method aims to cope with it by estimating this ego-motion or camera motion. The estimation is used to temporally partition video sequences into motion-compensated temporal chunks showing the action under stable backgrounds and allowing for a content-driven temporal sampling. A CNN trained in an end-to-end fashion is used to extract temporal features from each chunk, which are late fused. This process leads to the extraction of features from the whole temporal range of an action, increasing the temporal receptive field of the network. This document is best viewed offline where some figures play as animation.1
PB  - arXiv
PY  - 2020
ST  - A prospective study on Sequence-Driven temporal sampling and Ego-Motion compensation for action recognition in the EPIC-Kitchens dataset
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/tpami.2020.2991965
ER  -


TY  - GEN
AU  - Xie, T.
AU  - Tzelepis, C.
AU  - Patras, I.
TI  - Boundary uncertainty in a single-stage temporal action localization network
AB  - In this paper, we address the problem of temporal action localization with a single stage neural network. In the proposed architecture we model the boundary predictions as uni-variate Gaussian distributions in order to model their uncertainties, which is the first in this area to the best of our knowledge. We use two uncertainty-aware boundary regression losses: first, the Kullback-Leibler divergence between the ground truth location of the boundary and the Gaussian modeling the prediction of the boundary and second, the expectation of the `1 loss under the same Gaussian. We show that with both uncertainty modeling approaches improve the detection performance by more than 1.5% in mAP@tIoU=0.5 and that the proposed simple one-stage network performs closely to more complex one and two stage networks.
PB  - arXiv
PY  - 2020
ST  - Boundary uncertainty in a single-stage temporal action localization network
Y2  - 2025/05/05/21:54:29
DO  - 10.3390/electronics13061099
ER  -


TY  - GEN
AU  - Gupta, A.
AU  - Desai, M.
AU  - Liang, W.
AU  - Kannan, M.
TI  - Spatiotemporal action recognition in restaurant videos
AB  - Spatiotemporal action recognition is the task of locating and classifying actions in videos. Our project applies this task to analyzing video footage of restaurant workers preparing food, for which potential applications include automated checkout and inventory management. Such videos are quite different from the standardized datasets that researchers are used to, as they involve small objects, rapid actions, and notoriously unbalanced data classes. We explore two approaches – one involving the familiar object detector “You Only Look Once” (YOLO), and another applying a recently proposed analogue for action recognition, “You Only Watch Once” (YOWO). In the first, we design and implement a novel, recurrent modification of YOLO using convolutional LSTMs and explore the various subtleties in the training of such a network. In the second, we study the ability of YOWO’s three-dimensional convolutions to capture the spatiotemporal features of our unique dataset, which was generously lent by CMU-based startup Agot.
PB  - arXiv
PY  - 2020
ST  - Spatiotemporal action recognition in restaurant videos
Y2  - 2025/05/05/21:54:29
DO  - 10.1142/s0218126623502031
ER  -


TY  - GEN
AU  - Xie, T.-T.
AU  - Tzelepis, C.
AU  - Patras, I.
TI  - Temporal action localization with variance-aware networks
AB  - This work addresses the problem of temporal action localization with Variance-Aware Networks (VAN), i.e., DNNs that use second-order statistics in the input and/or the output of regression tasks. We first propose a network (VANp) that when presented with the second-order statistics of the input, i.e., each sample has a mean and a variance, it propagates the mean and the variance throughout the network to deliver outputs with second order statistics. In this framework, both the input and the output could be interpreted as Gaussians. To do so, we derive differentiable analytic solutions, or reasonable approximations, to propagate across commonly used NN layers. To train the network, we define a differentiable loss based on the KL-divergence between the predicted Gaussian and a Gaussian around the ground truth action borders, and use standard back-propagation. Importantly, the variances propagation in VANp does not require any additional parameters, and during testing, does not require any additional computations either. In action localization, the means and the variances of the input are computed at pooling operations, that are typically used to bring arbitrarily long videos to a vector with fixed dimensions. Second, we propose two alternative formulations that augment the first (respectively, the last) layer of a regression network with additional parameters so as to take in the input (respectively, predict in the output) both means and variances. Results in the action localization problem show that the incorporation of second order statistics improves over the baseline network, and that VANp surpasses the accuracy of virtually all other two-stage networks without involving any additional parameters.1
PB  - arXiv
PY  - 2020
ST  - Temporal action localization with variance-aware networks
Y2  - 2025/05/05/21:54:29
DO  - 10.1145/3394171.3413687
ER  -


TY  - GEN
AU  - Yang, L.
AU  - Peng, H.
AU  - Zhang, D.
AU  - Fu, J.
AU  - Han, J.
TI  - Revisiting anchor mechanisms for temporal action localization
AB  - Most of the current action localization methods follow an anchor-based pipeline: depicting action instances by pre-defined anchors, learning to select the anchors closest to the ground truth, and predicting the confidence of anchors with refinements. Pre-defined anchors set prior about the location and duration for action instances, which facilitates the localization for common action instances but limits the flexibility for tackling action instances with drastic varieties, especially for extremely short or extremely long ones. To address this problem, this paper proposes a novel anchor-free action localization module that assists action localization by temporal points. Specifically, this module represents an action instance as a point with its distances to the starting boundary and ending boundary, alleviating the pre-defined anchor restrictions in terms of action localization and duration. The proposed anchor-free module is capable of predicting the action instances whose duration is either extremely short or extremely long. By combining the proposed anchor-free module with a conventional anchor-based module, we propose a novel action localization framework, called A2Net. The cooperation between anchor-free and anchor-based modules achieves superior performance to the state-of-the-art on THUMOS14 (45.5% vs. 42.8%). Furthermore, comprehensive experiments demonstrate the complementarity between the anchor-free and the anchor-based module, making A2Net simple but effective.
PB  - arXiv
PY  - 2020
ST  - Revisiting anchor mechanisms for temporal action localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/tip.2020.3016486
ER  -


TY  - GEN
AU  - Li, Y.
AU  - Lin, W.
AU  - See, J.
AU  - Yan, K.
AU  - Yang, C.
TI  - CFAD: Coarse-to-fine action detector for spatiotemporal action localization
AB  - Most current pipelines for spatio-temporal action localization connect frame-wise or clip-wise detection results to generate action proposals, where only local information is exploited and the efficiency is hindered by dense per-frame localization. In this paper, we propose Coarse-to-Fine Action Detector (CFAD), an original end-to-end trainable framework for efficient spatio-temporal action localization. The CFAD introduces a new paradigm that first estimates coarse spatio-temporal action tubes from video streams, and then refines the tubes’ location based on key timestamps. This concept is implemented by two key components, the Coarse and Refine Modules in our framework. The parameterized modeling of long temporal information in the Coarse Module helps obtain accurate initial tube estimation, while the Refine Module selectively adjusts the tube location under the guidance of key timestamps. Against other methods, the proposed CFAD achieves competitive results on action detection benchmarks of UCF101-24, UCFSports and JHMDB-21 with inference speed that is 3.3× faster than the nearest competitor.
PB  - arXiv
PY  - 2020
ST  - CFAD
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-030-58517-4_30
ER  -


TY  - GEN
AU  - Plizzari, C.
AU  - Cannici, M.
AU  - Matteucci, M.
TI  - Skeleton-based action recognition via spatial and temporal transformer networks
AB  - Skeleton-based Human Activity Recognition has achieved great interest in recent years as skeleton data has demonstrated being robust to illumination changes, body scales, dynamic camera views, and complex background. In particular, Spatial-Temporal Graph Convolutional Networks (ST-GCN) demonstrated to be effective in learning both spatial and temporal dependencies on non-Euclidean data such as skeleton graphs. Nevertheless, an effective encoding of the latent information underlying the 3D skeleton is still an open problem, especially when it comes to extracting effective information from joint motion patterns and their correlations. In this work, we propose a novel Spatial-Temporal Transformer network (ST-TR) which models dependencies between joints using the Transformer self-attention operator. In our ST-TR model, a Spatial Self-Attention module (SSA) is used to understand intra-frame interactions between different body parts, and a Temporal Self-Attention module (TSA) to model inter-frame correlations. The two are combined in a two-stream network, whose performance is evaluated on three large-scale datasets, NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics Skeleton 400, consistently improving backbone results. Compared with methods that use the same input data, the proposed ST-TR achieves state-of-the-art performance on all datasets when using joints' coordinates as input, and results on-par with state-of-the-art when adding bones information.
PB  - arXiv
PY  - 2020
ST  - Skeleton-based action recognition via spatial and temporal transformer networks
Y2  - 2025/05/05/21:54:29
DO  - 10.1016/j.cviu.2021.103219
ER  -


TY  - GEN
AU  - Yang, L.
AU  - Zhang, D.
AU  - Zhao, T.
AU  - Han, J.
TI  - Equivalent classification mapping for weakly supervised temporal action localization
AB  - Weakly supervised temporal action localization is a newly emerging yet widely studied topic in recent years. The existing methods can be categorized into two localization-by-classification pipelines, i.e., the pre-classification pipeline and the post-classification pipeline. The pre-classification pipeline first performs classification on each video snippet and then aggregate the snippet-level classification scores to obtain the video-level classification score, while the post-classification pipeline aggregates the snippet-level features first and then predicts the video-level classification score based on the aggregated feature. Although the classifiers in these two pipelines are used in different ways, the role they play is exactly the same—to classify the given features to identify the corresponding action categories. To this end, an ideal classifier can make both pipelines work. This inspires us to simultaneously learn these two pipelines in a unified framework to obtain an effective classifier. Specifically, in the proposed learning framework, we implement two parallel network streams to model the two localization-by-classification pipelines simultaneously and make the two network streams share the same classifier, thus achieving the novel Equivalent Classification Mapping (ECM) mechanism. Considering that an ideal classifier would make the classification results of the two network streams be identical and make the frame-level classification scores obtained from the pre-classification pipeline and the feature aggregation weights in the post-classification pipeline be consistent, we further introduce an equivalent classification loss and an equivalent weight transition module to endow the proposed learning framework with such properties. Comprehensive experiments are carried on three benchmarks and the proposed ECM achieves superior performance over other state-of-the-art methods.
PB  - arXiv
PY  - 2020
ST  - Equivalent classification mapping for weakly supervised temporal action localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/tpami.2022.3178957
ER  -


TY  - GEN
AU  - Okuda, K.S.
AU  - Keyser, M.
AU  - Gurevich, D.B.
AU  - Distel, M.
AU  - Hogan, B.M.
TI  - Live-imaging of endothelial Erk activity reveals dynamic and sequential signalling events during regenerative angiogenesis
AB  - The formation of new blood vessel networks occurs via angiogenesis during development, tissue repair and disease. Angiogenesis is regulated by intracellular endothelial signalling pathways, induced downstream of Vascular endothelial growth factors (VEGFs) and their receptors (VEGFRs). A major challenge in understanding angiogenesis is interpreting how signalling events occur dynamically within endothelial cell populations during sprouting, proliferation and migration. Erk is a central downstream effector of Vegf-signalling and reports the signalling that drives angiogenesis. We generated a vascular Erk biosensor transgenic line in zebrafish using a kinase translocation reporter that allows live-imaging of Erk-signalling dynamics. We demonstrate the utility of this line to live-image Erk activity during physiologically relevant angiogenic events. Further, we reveal dynamic and sequential endothelial cell Erk-signalling events following blood vessel wounding. Initial signalling is dependent upon Ca2+ in the earliest responding endothelial cells, but is independent of Vegfr-signalling and local inflammation. The sustained regenerative response however, involves a Vegfr-dependent mechanism that initiates concomitant with the wound inflammatory response. This work thus reveals a highly dynamic sequence in regenerative angiogenesis that was not previously appreciated. Altogether, this study demonstrates the utility of a unique biosensor strain for analysing dynamic endothelial Erk-signalling events and validates a new resource for the study of vascular signalling in real-time.
PB  - bioRxiv
PY  - 2020
ST  - Live-imaging of endothelial Erk activity reveals dynamic and sequential signalling events during regenerative angiogenesis
Y2  - 2025/05/05/21:54:29
DO  - 10.7554/elife.62196
ER  -


TY  - GEN
AU  - Doroniewicz, I.
AU  - Ledwoń, D.
AU  - Bugdol, M.
AU  - Mitas, A.W.
AU  - Myśliwiec, A.
TI  - Assessment of the validity of parameters describing temporal and spatial characteristics of spontaneous infant activity in the period of fidgety movements
AB  - Background: The assessment of spontaneous activity of infants is of fundamental importance to the diagnosis and prediction of abnormal psychomotor development in children. Comprehensive and early diagnosis allows for quick and effective treatment and therapy. Subjective methods are based on the knowledge and experience of the diagnostician. The lack of objective methods to assess the motor development of infants makes it necessary to search for solutions for reliable, credible, and reproducible assessment expressed in numerical or pictorial terms. This study discusses the possibilities of pictorial standardization and optimization of measurable infant behavior based on video recordings. Methods: The authors attempt to perform computer analysis of spontaneous movements depending on the left, right, and front head position. The study was based on data of 26 healthy infants aged 7 to 15 weeks, with three infants included in an in-depth analysis. The selected films represented the input data for the parameters used as the author's temporal and spatial characteristics describing the global movements of the upper and lower limbs. The obtained videos were used as the input data for the algorithm of automatic detection of characteristic points using the OpenPose library. Results: The following movement characteristics were analysed: Factor of Movement's Area (FMA) ("amount of movement in the movement"), Factor of Movement's Shape (FMS) ("circularity” or "ellipticity" of the movement), Center of Movement's Area (CMA) ("inward and outward" and "up and down" movements). Preliminary analysis of the videos showed that the activity of the limbs, especially the upper limbs, may depend on the position of the head. Conclusions: The movement behavior of the infants varies in terms of the range and quality of movement, depending on age and head position.
PB  - Research Square
PY  - 2020
ST  - Assessment of the validity of parameters describing temporal and spatial characteristics of spontaneous infant activity in the period of fidgety movements
Y2  - 2025/05/05/21:54:29
DO  - 10.21203/rs.3.rs-47505/v1
ER  -


TY  - GEN
AU  - Wang, X.
AU  - Gao, C.
AU  - Zhang, S.
AU  - Sang, N.
TI  - Multi-Level temporal pyramid network for action detection
AB  - Currently, one-stage frameworks have been widely applied for temporal action detection, but they still suffer from the challenge that the action instances span a wide range of time. The reason is that these one-stage detectors, e.g., Single Shot Multi-Box Detector (SSD), extract temporal features only applying a single-level layer for each head, which is not discriminative enough to perform classification and regression. In this paper, we propose a Multi-Level Temporal Pyramid Network (MLTPN) to improve the discrimination of the features. Specially, we first fuse the features from multiple layers with different temporal resolutions, to encode multi-layer temporal information. We then apply a multi-level feature pyramid architecture on the features to enhance their discriminative abilities. Finally, we design a simple yet effective feature fusion module to fuse the multi-level multi-scale features. By this means, the proposed MLTPN can learn rich and discriminative features for different action instances with different durations. We evaluate MLTPN on two challenging datasets: THUMOS14 and Activitynet v1.3, and the experimental results show that MLTPN obtains competitive performance on Activitynet v1.3 and outperforms the state-of-the-art approaches on THUMOS14 significantly.
PB  - arXiv
PY  - 2020
ST  - Multi-Level temporal pyramid network for action detection
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-030-60639-8_4
ER  -


TY  - GEN
AU  - Wang, J.
AU  - Lin, Y.
AU  - Ma, A.J.
TI  - Self-supervised learning using consistency regularization of spatio-temporal data augmentation for action recognition
AB  - Self-supervised learning has shown great potentials in improving the deep learning model in an unsupervised manner by constructing surrogate supervision signals directly from the unlabeled data. Different from existing works, we present a novel way to obtain the surrogate supervision signal based on high-level feature maps under consistency regularization. In this paper, we propose a Spatio-Temporal Consistency Regularization between different output features generated from a siamese network including a clean path fed with original video and a noise path fed with the corresponding augmented video. Based on the Spatio-Temporal characteristics of video, we develop two video-based data augmentation methods, i.e., Spatio-Temporal Transformation and Intra-Video Mixup. Consistency of the former one is proposed to model transformation consistency of features, while the latter one aims at retaining spatial invariance to extract action-related features. Extensive experiments demonstrate that our method achieves substantial improvements compared with state-of-the-art self-supervised learning methods for action recognition. When using our method as an additional regularization term and combine with current surrogate supervision signals, we achieve 22% relative improvement over the previous state-of-the-art on HMDB51 and 7% on UCF101.
PB  - arXiv
PY  - 2020
ST  - Self-supervised learning using consistency regularization of spatio-temporal data augmentation for action recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.21437/interspeech.2022-10462
ER  -


TY  - GEN
AU  - Chen, J.
AU  - Kong, Y.
AU  - Bao, W.
TI  - Group activity prediction with sequential relational anticipation model
AB  - In this paper, we propose a novel approach to predict group activities given the beginning frames with incomplete activity executions. Existing action1 prediction approaches learn to enhance the representation power of the partial observation2. However, for group activity prediction, the relation evolution of people’s activity and their positions over time is an important cue for predicting group activity. To this end, we propose a sequential relational anticipation model (SRAM) that summarizes the relational dynamics in the partial observation and progressively anticipates the group representations with rich discriminative information. Our model explicitly anticipates both activity features and positions by two graph auto-encoders, aiming to learn a discriminative group representation for group activity prediction. Experimental results on two popularly used datasets demonstrate that our approach significantly outperforms the state-of-the-art activity prediction methods.
PB  - arXiv
PY  - 2020
ST  - Group activity prediction with sequential relational anticipation model
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-030-58589-1_35
ER  -


TY  - GEN
AU  - Bai, Y.
AU  - Wang, Y.
AU  - Tong, Y.
AU  - Liu, Q.
AU  - Liu, J.
TI  - Boundary Content Graph Neural Network for temporal action proposal generation
AB  - Temporal action proposal generation plays an important role in video action understanding, which requires localizing high-quality action content precisely. However, generating temporal proposals with both precise boundaries and high-quality action content is extremely challenging. To address this issue, we propose a novel Boundary Content Graph Neural Network (BC-GNN) to model the insightful relations between the boundary and action content of temporal proposals by the graph neural networks. In BC-GNN, the boundaries and content of temporal proposals are taken as the nodes and edges of the graph neural network, respectively, where they are spontaneously linked. Then a novel graph computation operation is proposed to update features of edges and nodes. After that, one updated edge and two nodes it connects are used to predict boundary probabilities and content confidence score, which will be combined to generate a final high-quality proposal. Experiments are conducted on two mainstream datasets: ActivityNet-1.3 and THUMOS14. Without the bells and whistles, BC-GNN outperforms previous state-of-the-art methods in both temporal action proposal and temporal action detection tasks.
PB  - arXiv
PY  - 2020
ST  - Boundary Content Graph Neural Network for temporal action proposal generation
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-030-58604-1_8
ER  -


TY  - GEN
AU  - Kalfaoglu, M.E.
AU  - Kalkan, S.
AU  - Alatan, A.A.
TI  - Late Temporal Modeling in 3D CNN Architectures with BERT for Action Recognition
AB  - In this work, we combine 3D convolution with late temporal modeling for action recognition. For this aim, we replace the conventional Temporal Global Average Pooling (TGAP) layer at the end of 3D convolutional architecture with the Bidirectional Encoder Representations from Transformers (BERT) layer in order to better utilize the temporal information with BERT’s attention mechanism. We show that this replacement improves the performances of many popular 3D convolution architectures for action recognition, including ResNeXt, I3D, SlowFast and R(2+1)D. Moreover, we provide the-state-of-the-art results on both HMDB51 and UCF101 datasets with 85.10% and 98.69% top-1 accuracy, respectively. The code is publicly available4
PB  - arXiv
PY  - 2020
ST  - Late Temporal Modeling in 3D CNN Architectures with BERT for Action Recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-030-68238-5_48
ER  -


TY  - GEN
AU  - Yang, Z.
AU  - Yin, K.
TI  - Improving skeleton-based action recognitionwith robust spatial and temporal features
AB  - Recently skeleton-based action recognition has made significant progresses in the computer vision community. Most state-of-the-art algorithms are based on Graph Convolutional Networks (GCN), and target at improving the network structure of the backbone GCN layers. In this paper, we propose a novel mechanism to learn more robust discriminative features in space and time. More specifically, we add a Discriminative Feature Learning (DFL) branch to the last layers of the network to extract discriminative spatial and temporal features to help regularize the learning. We also formally advocate the use of Direction-Invariant Features (DIF) as input to the neural networks. We show that action recognition accuracy can be improved when these robust features are learned and used. We compare our results with those of ST-GCN and related methods on four datasets: NTU-RGBD60, NTU-RGBD120, SYSU 3DHOI and Skeleton-Kinetics.
PB  - arXiv
PY  - 2020
ST  - Improving skeleton-based action recognitionwith robust spatial and temporal features
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/s10489-022-03589-y
ER  -


TY  - GEN
AU  - Kivivirta, K.I.
AU  - Herbert, D.
AU  - Roessner, C.
AU  - Marsch-Martinez, N.
AU  - Becker, A.
TI  - High-resolution temporal transcript profiling during Arabidopsis thaliana gynoecium morphogenesis uncovers the chronology of gene regulatory network activity and reveals novel developmental regulators
AB  - The gynoecium is the most complex organ formed by the flowering plants. It encloses the ovules, provides a surface for pollen contact and self-incompatibility reactions, allows pollen tube growth and, post fertilization, and develops into the fruit. Consequently, the regulation of gynoecium morphogenesis is complex and appropriate timing of this process in part determines reproductive success. However, little is known about the global control of gynoecium development, even though many regulatory genes have been characterized. Here, we characterized dynamic gene expression changes using laser-microdissected gynoecium tissue from four developmental stages in Arabidopsis. We provide a high-resolution map of global expression dynamics during gynoecium morphogenesis and link these to the gynoecium interactome. We reveal groups of genes acting together early and others acting late in morphogenesis. Clustering of co-expressed genes enables comparisons between the leaf, shoot apex, and gynoecium transcriptomes allowing the dissection of common and distinct regulators. Furthermore, our results lead to the discovery of the LESSER FERTILITY1-4 (LEF1-4) genes, which, when mutated, lead to impaired gynoecium expansion, illustrating that global transcriptome analyses reveal yet unknown developmental regulators. Our data show that highly interacting proteins, such as SEPALLATA3, AGAMOUS, and TOPLESS are expressed more evenly during development, but switch interactors in time, whereas stage-specific proteins have only few interactors. Our analysis connects specific transcriptional regulator activities, protein interactions, and underlying metabolic processes towards the development of a dynamic network model for gynoecium development.
PB  - bioRxiv
PY  - 2020
ST  - High-resolution temporal transcript profiling during Arabidopsis thaliana gynoecium morphogenesis uncovers the chronology of gene regulatory network activity and reveals novel developmental regulators
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2020.07.29.227314
ER  -


TY  - GEN
AU  - Kumawat, S.
AU  - Verma, M.
AU  - Nakashima, Y.
AU  - Raman, S.
TI  - Depthwise spatio-temporal STFT convolutional neural networks for human action recognition
AB  - Conventional 3D convolutional neural networks (CNNs) are computationally expensive, memory intensive, prone to overfitting, and most importantly, there is a need to improve their feature learning capabilities. To address these issues, we propose spatio-temporal short term Fourier transform (STFT) blocks, a new class of convolutional blocks that can serve as an alternative to the 3D convolutional layer and its variants in 3D CNNs. An STFT block consists of non-trainable convolution layers that capture spatially and/or temporally local Fourier information using a STFT kernel at multiple low frequency points, followed by a set of trainable linear weights for learning channel correlations. The STFT blocks significantly reduce the space-time complexity in 3D CNNs. In general, they use 3.5 to 4.5 times less parameters and 1.5 to 1.8 times less computational costs when compared to the state-of-the-art methods. Furthermore, their feature learning capabilities are significantly better than the conventional 3D convolutional layer and its variants. Our extensive evaluation on seven action recognition datasets, including Something2 v1 and v2, Jester, Diving-48, Kinetics-400, UCF 101, and HMDB 51, demonstrate that STFT blocks based 3D CNNs achieve on par or even better performance compared to the state-of-the-art methods.
PB  - arXiv
PY  - 2020
ST  - Depthwise spatio-temporal STFT convolutional neural networks for human action recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/tpami.2021.3076522
ER  -


TY  - GEN
AU  - Li, X.
AU  - Shuai, B.
AU  - Tighe, J.
TI  - Directional temporal modeling for action recognition
AB  - Many current activity recognition models use 3D convolutional neural networks (e.g. I3D, I3D-NL) to generate local spatial-temporal features. However, such features do not encode clip-level ordered temporal information. In this paper, we introduce a channel independent directional convolution (CIDC) operation, which learns to model the temporal evolution among local features. By applying multiple CIDC units we construct a light-weight network that models the clip-level temporal evolution across multiple spatial scales. Our CIDC network can be attached to any activity recognition backbone network. We evaluate our method on four popular activity recognition datasets and consistently improve upon state-of-the-art techniques. We further visualize the activation map of our CIDC network and show that it is able to focus on more meaningful, action related parts of the frame.
PB  - arXiv
PY  - 2020
ST  - Directional temporal modeling for action recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-030-58539-6_17
ER  -


TY  - GEN
AU  - Decramer, T.
AU  - Premereur, E.
AU  - Caprara, I.
AU  - Theys, T.
AU  - Janssen, P.
TI  - Temporal dynamics of neural activity in macaque frontal cortex assessed with large-scale recordings
AB  - We make eye movements to objects before grasping these objects, and the gaze direction generally indicates where the object will be grasped. Hence, the brain has to coordinate eye-, arm- and hand movements. We performed large-scale recordings (more than 2000 responsive sites) in frontal cortex of monkeys during a saccade-reach-grasp task. When an object appeared in peripheral vision, the first burst of activity emerged in prearcuate areas (the FEF and area 45B), followed by dorsal and ventral premotor cortex, and a buildup of activity in primary motor cortex. After the saccade, prearcuate activity remained elevated while primary motor and premotor activity rose in anticipation of the upcoming arm and hand movement. Some premotor and prearcuate sites were equally active when the object appeared in peripheral vision and at the fovea, suggesting a role in eye-hand coordination. Thus, a large part of lateral frontal cortex is active during a saccade-reach-grasp task.
PB  - bioRxiv
PY  - 2020
ST  - Temporal dynamics of neural activity in macaque frontal cortex assessed with large-scale recordings
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2020.07.20.211730
ER  -


TY  - GEN
AU  - Su, H.
AU  - Shao, H.
AU  - Feng, J.
AU  - Li, H.
AU  - Yan, J.
TI  - Complementary Boundary Generator with Scale-Invariant Relation Modeling for Temporal Action Localization: Submission to ActivityNet Challenge 2020
AB  - This technical report presents an overview of our solution used in the submission to ActivityNet Challenge 2020 Task 1 (temporal action localization/detection). Temporal action localization requires to not only precisely locate the temporal boundaries of action instances, but also accurately classify the untrimmed videos into specific categories. In this paper, we decouple the temporal action localization task into two stages (i.e. proposal generation and classification) and enrich the proposal diversity through exhaustively exploring the influences of multiple components from different but complementary perspectives. Specifically, in order to generate high-quality proposals, we consider several factors including the video feature encoder, the proposal generator, the proposal-proposal relations, the scale imbalance, and ensemble strategy. Finally, in order to obtain accurate detections, we need to further train an optimal video classifier to recognize the generated proposals. Our proposed scheme achieves the state-of-the-art performance on the temporal action localization task with 42.26 average mAP on the challenge testing set.
PB  - arXiv
PY  - 2020
ST  - Complementary Boundary Generator with Scale-Invariant Relation Modeling for Temporal Action Localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1609/aaai.v35i3.16363
ER  -


TY  - GEN
AU  - Weng, J.
AU  - Luo, D.
AU  - Wang, Y.
AU  - Jiang, X.
AU  - Yuan, J.
TI  - Temporal distinct representation learning for action recognition
AB  - Motivated by the previous success of Two-Dimensional Convolutional Neural Network (2D CNN) on image recognition, researchers endeavor to leverage it to characterize videos. However, one limitation of applying 2D CNN to analyze videos is that different frames of a video share the same 2D CNN kernels, which may result in repeated and redundant information utilization, especially in the spatial semantics extraction process, hence neglecting the critical variations among frames. In this paper, we attempt to tackle this issue through two ways. 1) Design a sequential channel filtering mechanism, i.e., Progressive Enhancement Module (PEM), to excite the discriminative channels of features from different frames step by step, and thus avoid repeated information extraction. 2) Create a Temporal Diversity Loss (TD Loss) to force the kernels to concentrate on and capture the variations among frames rather than the image regions with similar appearance. Our method is evaluated on benchmark temporal reasoning datasets Something-Something V1 and V2, and it achieves visible improvements over the best competitor by 2.4% and 1.3%, respectively. Besides, performance improvements over the 2D-CNN-based state-of-the-arts on the large-scale dataset Kinetics are also witnessed.
PB  - arXiv
PY  - 2020
ST  - Temporal distinct representation learning for action recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-030-58571-6_22
ER  -


TY  - GEN
AU  - Min, K.
AU  - Corso, J.J.
TI  - Adversarial Background-Aware Loss for Weakly-supervised Temporal Activity Localization
AB  - Temporally localizing activities within untrimmed videos has been extensively studied in recent years. Despite recent advances, existing methods for weakly-supervised temporal activity localization struggle to recognize when an activity is not occurring. To address this issue, we propose a novel method named A2CL-PT. Two triplets of the feature space are considered in our approach: one triplet is used to learn discriminative features for each activity class, and the other one is used to distinguish the features where no activity occurs (i.e. background features) from activity-related features for each video. To further improve the performance, we build our network using two parallel branches which operate in an adversarial way: the first branch localizes the most salient activities of a video and the second one finds other supplementary activities from non-localized parts of the video. Extensive experiments performed on THUMOS14 and ActivityNet datasets demonstrate that our proposed method is effective. Specifically, the average mAP of IoU thresholds from 0.1 to 0.9 on the THUMOS14 dataset is significantly improved from 27.9% to 30.0%.
PB  - arXiv
PY  - 2020
ST  - Adversarial Background-Aware Loss for Weakly-supervised Temporal Activity Localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-030-58568-6_17
ER  -


TY  - GEN
AU  - Choi, Y.-P.
AU  - Kang, K.
AU  - Kim, H.K.
AU  - Kim, J.-M.
TI  - Temporal decays and asymptotic behaviors for a vlasov equation with a flocking term coupled to incompressible fluid flow
AB  - We are concerned with large-time behaviors of solutions for Vlasov–Navier–Stokes equations in two dimensions and Vlasov-Stokes system in three dimensions including the effect of velocity alignment/misalignment. We first revisit the large-time behavior estimate for our main system and refine assumptions on the dimensions and a communication weight function. In particular, this allows us to take into account the effect of the misalignment interactions between particles. We then use a sharp heat kernel estimate to obtain the exponential time decay of fluid velocity to its average in L∞-norm. For the kinetic part, by employing a certain type of Sobolev norm weighted by modulations of averaged particle velocity, we prove the exponential time decay of the particle distribution, provided that local particle distribution function is uniformly bounded. Moreover, we show that the support of particle distribution function in velocity shrinks to a point, which is the mean of averaged initial particle and fluid velocities, exponentially fast as time goes to infinity. This also provides that for any p ∈ [1, ∞], the p-Wasserstein distance between the particle distribution function and the tensor product of the local particle distributions and Dirac measure at that point in velocity converges exponentially fast to zero as time goes to infinity.
PB  - arXiv
PY  - 2020
ST  - Temporal decays and asymptotic behaviors for a vlasov equation with a flocking term coupled to incompressible fluid flow
Y2  - 2025/05/05/21:54:29
DO  - 10.1016/j.nonrwa.2021.103410
ER  -


TY  - GEN
AU  - Kreider, J.J.
AU  - Nehrkorn, A.
AU  - Bänsch, S.
AU  - Kirsch, C.
AU  - Westphal, C.
TI  - Honeybees optimize their foraging behaviour in relation to spatio-temporal changes in nectar and pollen availability
AB  - Intensified agriculture increasingly threatens wild and managed bees by promoting landscape uniformity and reducing floral resource availability whereas urban areas can provide continuous floral resources within green spaces and private gardens. Mass-flowering events of crops and trees, such as lime trees (Tilia spp.), can provide ample floral resources but only for short time periods. Using waggle dance decoding, pollen analysis and bee abundance recordings, we investigated the temporal shift in honeybee foraging behaviour in response to lime tree mass-flowering. Honeybees in urban areas extended their foraging range during lime tree flowering. Foraging behaviour of honeybees in rural areas did not change to such an extent and honeybees foraged in sown flower strips. Our results suggest that honeybees optimize their foraging behaviour to exploit highly rewarding resources instead of extending foraging ranges in times of floral resource scarcity.
PB  - bioRxiv
PY  - 2020
ST  - Honeybees optimize their foraging behaviour in relation to spatio-temporal changes in nectar and pollen availability
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2020.07.08.193268
ER  -


TY  - GEN
AU  - Shi, L.
AU  - Zhang, Y.
AU  - Cheng, J.
AU  - Lu, H.
TI  - Decoupled Spatial-Temporal Attention Network for Skeleton-Based Action Recognition
AB  - Dynamic skeletal data, represented as the 2D/3D coordinates of human joints, has been widely studied for human action recognition due to its high-level semantic information and environmental robustness. However, previous methods heavily rely on designing handcrafted traversal rules or graph topologies to draw dependencies between the joints, which are limited in performance and generalizability. In this work, we present a novel decoupled spatial-temporal attention network (DSTA-Net) for skeleton-based action recognition. It involves solely the attention blocks, allowing for modeling spatial-temporal dependencies between joints without the requirement of knowing their positions or mutual connections. Specifically, to meet the specific requirements of the skeletal data, three techniques are proposed for building attention blocks, namely, spatial-temporal attention decoupling, decoupled position encoding and spatial global regularization. Besides, from the data aspect, we introduce a skeletal data decoupling technique to emphasize the specific characteristics of space/time and different motion scales, resulting in a more comprehensive understanding of the human actions. To test the effectiveness of the proposed method, extensive experiments are conducted on four challenging datasets for skeleton-based gesture and action recognition, namely, SHREC, DHG, NTU-60 and NTU-120, where DSTA-Net achieves state-of-the-art performance on all of them.
PB  - arXiv
PY  - 2020
ST  - Decoupled Spatial-Temporal Attention Network for Skeleton-Based Action Recognition
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-030-69541-5_3
ER  -


TY  - GEN
AU  - Perez-Rua, J.-M.
AU  - Toisoul, A.
AU  - Martinez, B.
AU  - Zhu, X.
AU  - Xiang, T.
TI  - Egocentric Action Recognition by Video Attention and Temporal Context
AB  - We present the submission of Samsung AI Centre Cambridge to the CVPR2020 EPIC-Kitchens Action Recognition Challenge. In this challenge, action recognition is posed as the problem of simultaneously predicting a single ‘verb’ and ‘noun’ class label given an input trimmed video clip. That is, a ‘verb’ and a ‘noun’ together define a compositional ‘action’ class. The challenging aspects of this real-life action recognition task include small fast moving objects, complex hand-object interactions, and occlusions. At the core of our submission is a recently-proposed spatial-temporal video attention model, called ‘W3’ (‘What-Where-When’) attention [6]. We further introduce a simple yet effective contextual learning mechanism to model ‘action’ class scores directly from long-term temporal behaviour based on the ‘verb’ and ‘noun’ prediction scores. Our solution achieves strong performance on the challenge metrics without using object-specific reasoning nor extra training data. In particular, our best solution with multimodal ensemble achieves the 2nd best position for ‘verb’, and 3rd best for ‘noun’ and ‘action’ on the Seen Kitchens test set.
PB  - arXiv
PY  - 2020
ST  - Egocentric Action Recognition by Video Attention and Temporal Context
Y2  - 2025/05/05/21:54:29
DO  - 10.1016/b978-0-44-321495-0.00016-4
ER  -


TY  - GEN
AU  - Ding, X.
AU  - Wang, N.
AU  - Gao, X.
AU  - Wang, X.
AU  - Liu, T.
TI  - Weakly supervised temporal action localization with segment-level labels
AB  - Temporal action localization presents a trade-off between test performance and annotation-time cost. Fully supervised methods achieve good performance with time-consuming boundary annotations. Weakly supervised methods with cheaper video-level category label annotations result in worse performance. In this paper, we introduce a new segment-level supervision setting: segments are labeled when annotators observe actions happening here. We incorporate this segment-level supervision along with a novel localization module in the training. Specifically, we devise a partial segment loss regarded as a loss sampling to learn integral action parts from labeled segments. Since the labeled segments are only parts of actions, the model tends to overfit along with the training process. To tackle this problem, we first obtain a similarity matrix from discriminative features guided by a sphere loss. Then, a propagation loss is devised based on the matrix to act as a regularization term, allowing implicit unlabeled segments propagation during training. Experiments validate that our method can outperform the video-level supervision methods with almost same the annotation time.
PB  - arXiv
PY  - 2020
ST  - Weakly supervised temporal action localization with segment-level labels
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-030-88004-0_4
ER  -


TY  - GEN
AU  - Liu, Z.
AU  - Liu, Y.
AU  - Tao, Z.
AU  - Wang, L.
AU  - Feng, Y.
TI  - Human activity recognition based on dynamic Spatio-Temporal relations
AB  - Human activity, which usually consists of several actions (sub-activities), generally covers interactions among persons and/or objects. In particular, human actions involve certain spatial and temporal relationships, are the components of more complicated activity, and evolve dynamically over time. Therefore, the description of a single human action and the modeling of the evolution of successive human actions are two major issues in human activity recognition. In this paper, we develop a method for human activity recognition that tackles these two issues. In the proposed method, an activity is divided into several successive actions represented by spatiotemporal patterns, and the evolution of these actions are captured by a sequential model. A refined comprehensive spatiotemporal graph is utilized to represent a single action, which is a qualitative representation of a human action incorporating both the spatial and temporal relations of the participant objects. Next, a discrete hidden Markov model is applied to model the evolution of action sequences. Moreover, a fully automatic partition method is proposed to divide a long-term human activity video into several human actions based on variational objects and qualitative spatial relations. Finally, a hierarchical decomposition of the human body is introduced to obtain a discriminative representation for a single action. Experimental results on the Cornell Activity Dataset demonstrate the efficiency and effectiveness of the proposed approach, which will enable long videos of human activity to be better recognized.
PB  - arXiv
PY  - 2020
ST  - Human activity recognition based on dynamic Spatio-Temporal relations
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/access.2020.3009136
ER  -


TY  - GEN
AU  - Kim, S.
AU  - Leong, A.
AU  - Nayar, C.
AU  - Kim, M.
AU  - Yang, H.W.
TI  - CDK2 activity determines the timing of cell-cycle commitment and sequential DNA replication
AB  - To enter the cell cycle, mammalian cells must cross a point of no return (the commitment point), after which they proceed through the cell cycle regardless of changes in external signaling. This process is tightly regulated by the cyclin-dependent kinases (CDKs) and downstream molecules such as retinoblastoma (Rb). Here we show that CDK2 activity coordinates the timing of cell-cycle commitment and DNA replication. CDK4/6 activation initiates Rb phosphorylation and E2F activity, causing a gradual increase in CDK2 activity. Once CDK2 activity reaches a threshold level, CDK2 triggers the commitment point by maintaining Rb phosphorylation and subsequently initiates DNA replication. While the timing of the commitment point is tightly coupled with DNA replication, our experiments, which acutely increased CDK2 activity, suggest that the timing of the commitment point is before DNA replication. These findings highlight how cells utilize a safety mechanism to maintain genome stability by protecting against incomplete DNA replication.
PB  - bioRxiv
PY  - 2020
ST  - CDK2 activity determines the timing of cell-cycle commitment and sequential DNA replication
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2020.06.24.169409
ER  -


TY  - GEN
AU  - Paoletti, G.
AU  - Cavazza, J.
AU  - Beyan, C.
AU  - Del Bue, A.
TI  - Subspace clustering for action recognition with covariance representations and temporal pruning
AB  - This paper tackles the problem of human action recognition, defined as classifying which action is displayed in a trimmed sequence, from skeletal data. Albeit state-of-the-art approaches designed for this application are all supervised, in this paper we pursue a more challenging direction: Solving the problem with unsupervised learning. To this end, we propose a novel subspace clustering method, which exploits covariance matrix to enhance the action’s discriminability and a timestamp pruning approach that allow us to better handle the temporal dimension of the data. Through a broad experimental validation, we show that our computational pipeline surpasses existing unsupervised approaches but also can result in favorable performances as compared to supervised methods.
PB  - arXiv
PY  - 2020
ST  - Subspace clustering for action recognition with covariance representations and temporal pruning
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/icpr48806.2021.9412060
ER  -


TY  - GEN
AU  - Clarke, R.
AU  - Terry, A.R.
AU  - Pennington, H.
AU  - Regan, M.
AU  - Merrill, B.J.
TI  - Sequential activation of guide RNAs for algorithmic multiplexing of Cas9 activities
AB  - Genetic manipulation of mammalian cells is instrumental to modern biomedical research but is currently limited by poor capabilities of sequentially controlling multiple manipulations in cells. Currently, either highly multiplexed manipulations can be delivered to populations of cells all at one time, or gene regulatory sequences can be engineered to conditionally activate a few manipulations within individual cells. Here, we provide proof-of-principle for a new system enabling multiple genetic manipulations to be executed as a preprogrammed cascade of events. The system leverages the programmability of the S. pyogenes Cas9 RNA-guided nuclease and is based on flexible arrangements of individual modules of activity. The basic module consists of an inactive single guide RNA (sgRNA) -like component that is converted to an active state through the effects of another sgRNA. Modules can be arranged to bring about an algorithmic program of genetic manipulations without the need for engineering cell type specific promoters or gene regulatory sequences. With the expanding diversity of available tools that utilize spCas9 to edit, repress or activate genes, this sgRNA-based system provides multiple levels for interfacing with host cell biology. In addition, ability of the system to progress through multiple modules from episomal plasmid DNA makes it suitable for applications sensitive to the presence of heterologous genomic DNA sequences and broadly applicable to biomedical research and mammalian cell engineering.
PB  - bioRxiv
PY  - 2020
ST  - Sequential activation of guide RNAs for algorithmic multiplexing of Cas9 activities
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2020.06.20.162982
ER  -


TY  - GEN
AU  - Ahmad, S.
AU  - Teng, Z.
AU  - Bryant, A.
AU  - Kleinman, E.
AU  - El-Nasr, M.S.
TI  - Modeling individual and team behavior through spatio-temporal analysis
AB  - Modeling players’ behaviors in games has gained increased momentum in the past few years. This area of research has wide applications, including modeling learners and understanding player strategies, to mention a few. In this paper, we present a new methodology, called Interactive Behavior Analytics (IBA), comprised of two visualization systems, a labeling mechanism, and abstraction algorithms that use Dynamic Time Warping and clustering algorithms. The methodology is packaged in a seamless interface to facilitate knowledge discovery from game data. We demonstrate the use of this methodology with data from two multiplayer team-based games: BoomTown, a game developed by Gallup, and DotA 2. The results of this work show the effectiveness of this method in modeling, and developing human-interpretable models of team and individual behavior.
PB  - arXiv
PY  - 2020
ST  - Modeling individual and team behavior through spatio-temporal analysis
Y2  - 2025/05/05/21:54:29
DO  - 10.1145/3311350.3347188
ER  -


TY  - GEN
AU  - Liu, K.
AU  - Liu, W.
AU  - Ma, H.
AU  - Tan, M.
AU  - Gan, C.
TI  - A Real-time Action Representation with Temporal Encoding and Deep Compression
AB  - Deep neural networks have achieved remarkable success for video-based action recognition. However, most of existing approaches cannot be deployed in practice due to the high computational cost. To address this challenge, we propose a new real-time convolutional architecture, called Temporal Convolutional 3D Network (T-C3D), for action representation. T-C3D learns video action representations in a hierarchical multi-granularity manner while obtaining a high process speed. Specifically, we propose a residual 3D Convolutional Neural Network (CNN) to capture complementary information on the appearance of a single frame and the motion between consecutive frames. Based on this CNN, we develop a new temporal encoding method to explore the temporal dynamics of the whole video. Furthermore, we integrate deep compression techniques with T-C3D to further accelerate the deployment of models via reducing the size of the model. By these means, heavy calculations can be avoided when doing the inference, which enables the method to deal with videos beyond real-time speed while keeping promising performance. Our method achieves clear improvements on UCF101 action recognition benchmark against state-of-the-art real-time methods by 5.4% in terms of accuracy and 2 times faster in terms of inference speed with a less than 5MB storage model. We validate our approach by studying its action representation performance on four different benchmarks over three different tasks. Extensive experiments demonstrate comparable recognition performance to the state-of-the-art methods. The source code and the pre-trained models are publicly available at https://github.com/tc3d.
PB  - arXiv
PY  - 2020
ST  - A Real-time Action Representation with Temporal Encoding and Deep Compression
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/tcsvt.2020.2984569
ER  -


TY  - GEN
AU  - Li, S.
AU  - Farha, Y.A.
AU  - Liu, Y.
AU  - Cheng, M.-M.
AU  - Gall, J.
TI  - MS-TCN++: Multi-Stage Temporal Convolutional Network for Action Segmentation
AB  - —With the success of deep learning in classifying short trimmed videos, more attention has been focused on temporally segmenting and classifying activities in long untrimmed videos. State-of-the-art approaches for action segmentation utilize several layers of temporal convolution and temporal pooling. Despite the capabilities of these approaches in capturing temporal dependencies, their predictions suffer from over-segmentation errors. In this paper, we propose a multi-stage architecture for the temporal action segmentation task that overcomes the limitations of the previous approaches. The first stage generates an initial prediction that is refined by the next ones. In each stage we stack several layers of dilated temporal convolutions covering a large receptive field with few parameters. While this architecture already performs well, lower layers still suffer from a small receptive field. To address this limitation, we propose a dual dilated layer that combines both large and small receptive fields. We further decouple the design of the first stage from the refining stages to address the different requirements of these stages. Extensive evaluation shows the effectiveness of the proposed model in capturing long-range dependencies and recognizing action segments. Our models achieve state-of-the-art results on three datasets: 50Salads, Georgia Tech Egocentric Activities (GTEA), and the Breakfast dataset.
PB  - arXiv
PY  - 2020
ST  - MS-TCN++
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr.2019.00369
ER  -


TY  - GEN
AU  - Chen, S.
AU  - Pan, J.
AU  - Song, G.
AU  - Li, H.
AU  - Liu, Y.
TI  - Actor-Context-Actor Relation Network for Spatio-Temporal Action Localization 1st place solution for AVA-Kinetics Crossover in AcitivityNet Challenge 2020
AB  - This technical report introduces our winning solution to the spatio-temporal action localization track, AVA-Kinetics Crossover, in ActivityNet Challenge 2020. Our entry is mainly based on Actor-Context-Actor Relation Network [14]. We describe technical details for the new AVA-Kinetics dataset, together with some experimental results. Without any bells and whistles, we achieved 39.62 mAP on the test set of AVA-Kinetics, which outperforms other entries by a large margin. Code will be available at: https://github.com/Siyu-C/ACAR-Net.
PB  - arXiv
PY  - 2020
ST  - Actor-Context-Actor Relation Network for Spatio-Temporal Action Localization 1st place solution for AVA-Kinetics Crossover in AcitivityNet Challenge 2020
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr46437.2021.00053
ER  -


TY  - GEN
AU  - Song, Y.
AU  - Chen, S.
AU  - Zhao, Y.
AU  - Jin, Q.
TI  - Team RUC AI·M3Technical Report at Activitynet 2020 Task 2: Exploring Sequential Events Detection for Dense Video Captioning
AB  - Detecting meaningful events in an untrimmed video is essential for dense video captioning. In this work, we propose a novel and simple model for event sequence generation and explore temporal relationships of the event sequence in the video. The proposed model omits inefficient two-stage proposal generation and directly generates event boundaries conditioned on bi-directional temporal dependency in one pass. Experimental results show that the proposed event sequence generation model can generate more accurate and diverse events within a small number of proposals. For the event captioning, we follow our previous work [3] to employ the intra-event captioning models into our pipeline system. The overall system achieves state-of-the-art performance on the dense-captioning events in video task with 9.894 METEOR score on the challenge testing set.
PB  - arXiv
PY  - 2020
ST  - Team RUC AI·M3Technical Report at Activitynet 2020 Task 2
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr.2018.00782
ER  -


TY  - GEN
AU  - Pan, J.
AU  - Chen, S.
AU  - Shou, M.Z.
AU  - Shao, J.
AU  - Li, H.
TI  - Actor-context-actor relation network for spatio-temporal action localization
AB  - Localizing persons and recognizing their actions from videos is a challenging task towards high-level video understanding. Recent advances have been achieved by modeling direct pairwise relations between entities. In this paper, we take one step further, not only model direct relations between pairs but also take into account indirect higher-order relations established upon multiple elements. We propose to explicitly model the Actor-Context-Actor Relation, which is the relation between two actors based on their interactions with the context. To this end, we design an Actor-Context-Actor Relation Network (ACAR-Net) which builds upon a novel High-order Relation Reasoning Operator and an Actor-Context Feature Bank to enable indirect relation reasoning for spatio-temporal action localization. Experiments on AVA and UCF101-24 datasets show the advantages of modeling actor-context-actor relations, and visualization of attention maps further verifies that our model is capable of finding relevant higher-order relations to support action detection. Notably, our method ranks first in the AVA-Kinetics action localization task of ActivityNet Challenge 2020, outperforming other entries by a significant margin (+6.71 mAP). The code is available online.
PB  - arXiv
PY  - 2020
ST  - Actor-context-actor relation network for spatio-temporal action localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr46437.2021.00053
ER  -


TY  - GEN
AU  - Qing, Z.
AU  - Wang, X.
AU  - Sang, Y.
AU  - Zhang, S.
AU  - Sang, N.
TI  - Temporal Fusion Network for Temporal Action Localization: Submission to ActivityNet Challenge 2020 (Task E)
AB  - This technical report analyzes a temporal action localization method we used in the HACS competition which is hosted in Activitynet Challenge 2020. The goal of our task is to locate the start time and end time of the action in the untrimmed video, and predict action category. Firstly, we utilize the video-level feature information to train multiple video-level action classification models. In this way, we can get the category of action in the video. Secondly, we focus on generating high quality temporal proposals. For this purpose, we apply BMN to generate a large number of proposals to obtain high recall rates. We then refine these proposals by employing a cascade structure network called Refine Network, which can predict position offset and new IOU under the supervision of ground truth. To make the proposals more accurate, we use bidirectional LSTM, Nonlocal and Transformer to capture temporal relationships between local features of each proposal and global features of the video data. Finally, by fusing the results of multiple models, our method obtains 40.55% on the validation set and 40.53% on the test set in terms of mAP, and achieves Rank 1 in this challenge.
PB  - arXiv
PY  - 2020
ST  - Temporal Fusion Network for Temporal Action Localization
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/iccst50977.2020.00118
ER  -


TY  - GEN
AU  - Wang, D.
AU  - Jiang, M.
AU  - Syed, M.
AU  - Subramanian, S.
AU  - Chawla, N.V.
TI  - Calendar graph neural networks for modeling time structures in spatiotemporal user behaviors
AB  - User behavior modeling is important for industrial applications such as demographic attribute prediction, content recommendation, and target advertising. Existing methods represent behavior log as a sequence of adopted items and find sequential patterns; however, concrete location and time information in the behavior log, reflecting dynamic and periodic patterns, joint with the spatial dimension, can be useful for modeling users and predicting their characteristics. In this work, we propose a novel model based on graph neural networks for learning user representations from spatiotemporal behavior data. A behavior log comprises a sequence of sessions; and a session has a location, start time, end time, and a sequence of adopted items. Our model’s architecture incorporates two networked structures. One is a tripartite network of items, sessions, and locations. The other is a hierarchical calendar network of hour, week, and weekday nodes. It first aggregates embeddings of location and items into session embeddings via the tripartite network, and then generates user embeddings from the session embeddings via the calendar structure. The user embeddings preserve spatial patterns and temporal patterns of a variety of periodicity (e.g., hourly, weekly, and weekday patterns). It adopts the attention mechanism to model complex interactions among the multiple patterns in user behaviors. Experiments on real datasets (i.e., clicks on news articles in a mobile app) show our approach outperforms strong baselines for predicting missing demographic attributes.
PB  - arXiv
PY  - 2020
ST  - Calendar graph neural networks for modeling time structures in spatiotemporal user behaviors
Y2  - 2025/05/05/21:54:29
DO  - 10.1145/3394486.3403308
ER  -


TY  - GEN
AU  - Qi, P.
AU  - Zhu, X.
AU  - Zhou, G.
AU  - Fan, Y.
AU  - Gai, K.
TI  - Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction
AB  - MSC Codes Machine Learning (stat.ML), Information Retrieval (cs.IR), Machine Learning (cs.LG) Rich user behavior data has been proven to be of great value for click-through rate prediction tasks, especially in industrial applications such as recommender systems and online advertising. Both industry and academy have paid much attention to this topic and propose different approaches to modeling with long sequential user behavior data. Among them, memory network based model MIMN[8] proposed by Alibaba, achieves SOTA with the co-design of both learning algorithm and serving system. MIMN is the first industrial solution that can model sequential user behavior data with length scaling up to 1000. However, MIMN fails to precisely capture user interests given a specific candidate item when the length of user behavior sequence increases further, say, by 10 times or more. This challenge exists widely in previously proposed approaches. In this paper, we tackle this problem by designing a new modeling paradigm, which we name as Search-based Interest Model (SIM). SIM extracts user interests with two cascaded search units: (i) General Search Unit (GSU) acts as a general search from the raw and arbitrary long sequential behavior data, with query information from candidate item, and gets a Sub user Behavior Sequence (SBS) which is relevant to candidate item; (ii) Exact Search Unit (ESU) models the precise relationship between candidate item and SBS. This cascaded search paradigm enables SIM with a better ability to model lifelong sequential behavior data in both scalability and accuracy. Apart from the learning algorithm, we also introduce our hands-on experience on how to implement SIM in large scale industrial systems. Since 2019, SIM has been deployed in the display advertising system in Alibaba, bringing 7.1% CTR and 4.4% RPM lift, which is significant to the business. Serving the main traffic in our real system now, SIM models sequential user behavior data with maximum length reaching up to 54000, pushing SOTA to 54x.
PB  - arXiv
PY  - 2020
ST  - Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction
Y2  - 2025/05/05/21:54:29
DO  - 10.1145/3340531.3412744
ER  -


TY  - GEN
AU  - Tang, X.
AU  - Liu, Y.
AU  - Shah, N.
AU  - Mitra, P.
AU  - Wang, S.
TI  - Knowing your FATE: Friendship, Action and Temporal Explanations for User Engagement Prediction on Social Apps
AB  - With the rapid growth and prevalence of social network applications (Apps) in recent years, understanding user engagement has become increasingly important, to provide useful insights for future App design and development. While several promising neural modeling approaches were recently pioneered for accurate user engagement prediction, their black-box designs are unfortunately limited in model explainability. In this paper, we study a novel problem of explainable user engagement prediction for social network Apps. First, we propose a flexible definition of user engagement for various business scenarios, based on future metric expectations. Next, we design an end-to-end neural framework, FATE, which incorporates three key factors that we identify to influence user engagement, namely friendships, user actions, and temporal dynamics to achieve explainable engagement predictions. FATE is based on a tensor-based graph neural network (GNN), LSTM and a mixture attention mechanism, which allows for (a) predictive explanations based on learned weights across different feature categories, (b) reduced network complexity, and (c) improved performance in both prediction accuracy and training/inference time. We conduct extensive experiments on two large-scale datasets from Snapchat, where FATE outperforms state-of-the-art approaches by ≈10% error and ≈20% runtime reduction. We also evaluate explanations from FATE, showing strong quantitative and qualitative performance.
PB  - arXiv
PY  - 2020
ST  - Knowing your FATE
Y2  - 2025/05/05/21:54:29
DO  - 10.1145/3394486.3403276
ER  -


TY  - GEN
AU  - Végh, J.
TI  - Why do we need to introduce temporal behavior in both modern science and modern computing: Introducing temporal logic into computing science
AB  - Classic science seemed to be completed more than a century ago, facing only a few (but growing number of!) unexplained issues. Introducing time-dependence to classic science explained those issues, and its consequent use led to a series of modern sciences, including relativis-tic and quantum physics. Classic computing science today seems to be completed, facing only a few (but growing number of!) unexplained issues. Introducing time-dependence to classic computing science explains those issues. Can this change also lead to a revolution, and resulting in a modern computing science, in the same way as it resulted in the birth of the modern science?
PB  - arXiv
PY  - 2020
ST  - Why do we need to introduce temporal behavior in both modern science and modern computing
Y2  - 2025/05/05/21:54:29
DO  - 10.34257/gjcstavol20is1pg13
ER  -


TY  - GEN
AU  - Chagnaud, B.P.
AU  - Perelmuter, J.
AU  - Forlano, P.
AU  - Bass, A.H.
TI  - Gap junction mediated feed-forward inhibition ensures ultra precise temporal patterning in vocal behavior
AB  - Precise neuronal firing is especially important for behaviors highly dependent on the correct sequencing and timing of muscle activity patterns, such as acoustic signalling. We show that extreme temporal precision in motoneuronal firing within a hindbrain network that directly determines call duration, pulse repetition rate and fundamental frequency in a teleost fish, the Gulf toadfish, depends on gap junction-mediated, feed-forward glycinergic inhibition that generates a period of reduced probability of motoneuron activation. Super-resolution microscopy confirms glycinergic release sites contacting motoneuron somata and dendrites. Synchronous motoneuron activity can also induce action potential firing in pre-motoneurons, a feature that could figure prominently into motor timing. Gap junction-mediated, feed-forward glycinergic inhibition provides a novel means for achieving temporal precision in the millisecond range for rapid modulation of an acoustic signal and perhaps other motor behaviors.
PB  - bioRxiv
PY  - 2020
ST  - Gap junction mediated feed-forward inhibition ensures ultra precise temporal patterning in vocal behavior
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2020.05.27.119339
ER  -


TY  - GEN
AU  - Fang, X.-S.
AU  - Bidin, C.M.
AU  - Zhao, G.
AU  - Zhang, L.-Y.
AU  - Kumar, Y.B.
TI  - Stellar activity with LAMOST. III. Temporal variability pattern in Pleiades, Praesepe, and Hyades
AB  - We present the results from a systematic study of temporal variation of stellar activity in young late-type stars. We used multi-epoch LAMOST low-resolution spectra of over 300 member candidates in three young open clusters: Pleiades, Praesepe, and Hyades. The spectral measurements of TiO band strength near 7050 Å (TiO2) and equivalent width of Hα line (EWHα) are used as the tracers of cool-spot coverage and chromospheric emission strength, respectively. The analysis of time-variation patterns of these two tracers suggested there exist detectable variabilities in TiO2 and EWHα, and their time-scales are in the wide range from days to years. Results showed that more active stars, younger and fast rotators, tend to have larger activity variations. There is a tendency of anti-correlation between temporal variations in TiO2 and EWHα. Also, appreciable anti-correlation in the rotational phase between Hα emission and K2 brightness is detected in some M dwarfs, indicating spatial co-location of the plages with cool starspots, however, cool stars do not always show such co-location features. Furthermore, spot coverage and Hα emission were evident at all rotational phases of several M dwarfs, indicating a basal level of activity, perhaps due to many small and randomly located active regions in the atmosphere.
PB  - arXiv
PY  - 2020
ST  - Stellar activity with LAMOST. III. Temporal variability pattern in Pleiades, Praesepe, and Hyades
Y2  - 2025/05/05/21:54:29
DO  - 10.1093/mnras/staa1392
ER  -


TY  - GEN
AU  - Afrashteh, N.
AU  - Inayat, S.
AU  - Contreras, E.B.
AU  - McNaughton, B.L.
AU  - Mohajerani, M.H.
TI  - Spatiotemporal structure of sensory-evoked and spontaneous activity revealed by mesoscale imaging in anesthetized and awake mice
AB  - Brain activity propagates across the cortex in diverse spatiotemporal patterns, both as a response to sensory stimulation and during spontaneous activity. Despite been extensively studied, the relationship between the characteristics of such patterns during spontaneous and evoked activity is not completely understood. To investigate this relationship, we compared visual, auditory, and tactile evoked activity patterns elicited with different stimulus strengths and spontaneous activity motifs in lightly anesthetized and awake mice using mesoscale wide-field voltage-sensitive dye and glutamate imaging respectively. The characteristics of cortical activity that we compared include amplitude, speed, direction, and complexity of propagation trajectories in spontaneous and evoked activity patterns. We found that the complexity of the propagation trajectories of spontaneous activity, quantified as their fractal dimension, is higher than the one from sensory evoked responses. Moreover, the speed and direction of propagation, are modulated by the amplitude during both, spontaneous and evoked activity. Finally, we found that spontaneous activity had similar amplitude and speed when compared to evoked activity elicited with low stimulus strengths. However, this similarity gradually decreased when the strength of stimuli eliciting evoked responses increased. Altogether, these findings are consistent with the fact that even primary sensory areas receive widespread inputs from other cortical regions, and that, during rest, the cortex tends to reactivate traces of complex, multi-sensory experiences that may have occurred in a range of different behavioural contexts.
PB  - bioRxiv
PY  - 2020
ST  - Spatiotemporal structure of sensory-evoked and spontaneous activity revealed by mesoscale imaging in anesthetized and awake mice
Y2  - 2025/05/05/21:54:29
DO  - 10.1101/2020.05.22.111021
ER  -


TY  - GEN
AU  - Shao, D.
AU  - Zhao, Y.
AU  - Dai, B.
AU  - Lin, D.
TI  - Intra- and inter-action understanding via temporal action parsing
AB  - Current methods for action recognition primarily rely on deep convolutional networks to derive feature embeddings of visual and motion features. While these methods have demonstrated remarkable performance on standard benchmarks, we are still in need of a better understanding as to how the videos, in particular their internal structures, relate to high-level semantics, which may lead to benefits in multiple aspects, e.g. interpretable predictions and even new methods that can take the recognition performances to a next level. Towards this goal, we construct TAPOS, a new dataset developed on sport videos with manual annotations of sub-actions, and conduct a study on temporal action parsing on top 1. Our study shows that a sport activity usually consists of multiple sub-actions and that the awareness of such temporal structures is beneficial to action recognition. We also investigate a number of temporal parsing methods, and thereon devise an improved method that is capable of mining sub-actions from training data without knowing the labels of them. On the constructed TAPOS, the proposed method is shown to reveal intra-action information, i.e. how action instances are made of sub-actions, and inter-action information, i.e. one specific sub-action may commonly appear in various actions.
PB  - arXiv
PY  - 2020
ST  - Intra- and inter-action understanding via temporal action parsing
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvpr42600.2020.00081
ER  -


TY  - GEN
AU  - Kasai, S.
AU  - Ishikawa, Y.
AU  - Hayashi, M.
AU  - Hara, K.
AU  - Kataoka, H.
TI  - Retrieving and highlighting action with spatiotemporal reference
AB  - In this paper, we present a framework that jointly retrieves and spatiotemporally highlights actions in videos by enhancing current deep cross-modal retrieval methods. Our work takes on the novel task of action highlighting, which visualizes where and when actions occur in an untrimmed video setting. Action highlighting is a fine-grained task, compared to conventional action recognition tasks which focus on classification or window-based localization. Leveraging weak supervision from annotated captions, our framework acquires spatiotemporal relevance maps and generates local embeddings which relate to the nouns and verbs in captions. Through experiments, we show that our model generates various maps conditioned on different actions, in which conventional visual reasoning methods only go as far as to show a single deterministic saliency map. Also, our model improves retrieval recall over our baseline without alignment by 2-3% on the MSR-VTT dataset.
PB  - arXiv
PY  - 2020
ST  - Retrieving and highlighting action with spatiotemporal reference
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/icip40778.2020.9190820
ER  -


TY  - GEN
AU  - Wilson, S.R.
AU  - Magdy, W.
AU  - McGillivray, B.
AU  - Tyson, G.
TI  - Analyzing temporal relationships between trending terms on twitter and urban dictionary activity
AB  - As an online, crowd-sourced, open English-language slang dictionary, the Urban Dictionary platform contains a wealth of opinions, jokes, and definitions of terms, phrases, acronyms, and more. However, it is unclear exactly how activity on this platform relates to larger conversations happening elsewhere on the web, such as discussions on larger, more popular social media platforms. In this research, we study the temporal activity trends on Urban Dictionary and provide the first analysis of how this activity relates to content being discussed on a major social network: Twitter. By collecting the whole of Urban Dictionary, as well as a large sample of tweets over seven years, we explore the connections between the words and phrases that are defined and searched for on Urban Dictionary and the content that is talked about on Twitter. Through a series of cross-correlation calculations, we identify cases in which Urban Dictionary activity closely reflects the larger conversation happening on Twitter. Then, we analyze the types of terms that have a stronger connection to discussions on Twitter, finding that Urban Dictionary activity that is positively correlated with Twitter is centered around terms related to memes, popular public figures, and offline events. Finally, We explore the relationship between periods of time when terms are trending on Twitter and the corresponding activity on Urban Dictionary, revealing that new definitions are more likely to be added to Urban Dictionary for terms that are currently trending on Twitter.
PB  - arXiv
PY  - 2020
ST  - Analyzing temporal relationships between trending terms on twitter and urban dictionary activity
Y2  - 2025/05/05/21:54:29
DO  - 10.1145/3394231.3397905
ER  -


TY  - GEN
AU  - Stahl, D.O.
TI  - A Dynamic Behavioral Model of Sequential Decisions
AB  - Most important economic decision problems are sequential, and thus naturally represented as Markov Decision Problems (MDP). After reviewing the theory of MDPs, the applicability of MDPs to real-life sequential decisions appears impractical. The central question addressed in this essay is how ordinary humans behave in the real-life sequential decision problems they face. A formal behavioral approach is presented, and it also appears impractical for all but toy MDPs. After engaging in introspection, a key insight is the enormous extent to which information and reinforcement provided by parents, teachers and others shapes our behavior in real-life. With these insights, an integrated framework of dynamic behavior emerges in which genetic evolution, short-term reinforcement learning and long-term acquisition of knowledge via institutions are seen as important aspects.
PB  - SSRN
PY  - 2020
ST  - A Dynamic Behavioral Model of Sequential Decisions
Y2  - 2025/05/05/21:54:29
DO  - 10.2139/ssrn.3579795
ER  -


TY  - GEN
AU  - Liu, Q.
AU  - Sha, D.
AU  - Liu, W.
AU  - Hu, T.
AU  - Yang, C.
TI  - Spatiotemporal Patterns of COVID-19 Impact on Human Activities and Environment in China Using Nighttime Light and Air Quality Data
AB  - In order to analyze the impact of COVID-19 on people’s lives, activities and the natural environment, this paper investigates the spatial and temporal characteristics of Night Time Light (NTL) radiance and Air Quality Index (AQI) before and during the pandemic in mainland China. Our results show that the monthly average NTL brightness is much lower during the quarantine period than before. This study categorizes NTL into three classes: residential area, transportation and public facilities and commercial centers, with NTL radiance ranges of 5~20, 20~40 and greater than 40 (nW･ccmm−2･ssrr−1) [1], respectively. We found that the Number Of Pixels (NOP) with NTL detection increased in the residential area and decreased in the commercial centers for most of the provinces after the shutdown, while transportation and public facilities generally stayed the same. More specifically, we examined these factors in Wuhan, where the first confirmed cases were reported, and where the earliest quarantine measures were taken. Observations and analysis of pixels associated with commercial centers were observed to have lower NTL radiance values, indicating a dimming behavior, while residential area pixels recorded increased levels of brightness, after the beginning of the lockdown. The study also discovered a significant decreasing trend in the daily average AQI for the whole country, with cleaner air in most provinces during February and March, compared to January 2020. In conclusion, the outbreak and spread of COVID-19 has had a crucial impact on people’s daily lives and activity ranges through the increased implementation of lockdown and quarantine policies. On the other hand, the air quality of China has improved with the reduction of non-essential industries and motor vehicle usage.
PB  - arXiv
PY  - 2020
ST  - Spatiotemporal Patterns of COVID-19 Impact on Human Activities and Environment in China Using Nighttime Light and Air Quality Data
Y2  - 2025/05/05/21:54:29
DO  - 10.3390/rs12101576
ER  -


TY  - GEN
AU  - McBroom, J.
AU  - Yacef, K.
AU  - Koprinska, I.
TI  - DETECT: A hierarchical clustering algorithm for behavioural trends in temporal educational data
AB  - Techniques for clustering student behaviour offer many opportunities to improve educational outcomes by providing insight into student learning. However, one important aspect of student behaviour, namely its evolution over time, can often be challenging to identify using existing methods. This is because the objective functions used by these methods do not explicitly aim to find cluster trends in time, so these trends may not be clearly represented in the results. This paper presents ‘DETECT’ (Detection of Educational Trends Elicited by Clustering Time-series data), a novel divisive hierarchical clustering algorithm that incorporates temporal information into its objective function to prioritise the detection of behavioural trends. The resulting clusters are similar in structure to a decision tree, with a hierarchy of clusters defined by decision rules on features. DETECT is easy to apply, highly customisable, applicable to a wide range of educational datasets and yields easily interpretable results. Through a case study of two online programming courses (N > 600), this paper demonstrates two example applications of DETECT: 1) to identify how cohort behaviour develops over time and 2) to identify student behaviours that characterise exercises where many students give up.
PB  - arXiv
PY  - 2020
ST  - DETECT
Y2  - 2025/05/05/21:54:29
DO  - 10.1007/978-3-030-52237-7_30
ER  -


TY  - GEN
AU  - Toger, M.
AU  - Shuttleworth, I.
AU  - Östh, J.
TI  - How average is average? Temporal patterns in human behaviour as measured by mobile phone data – or why chose Thursdays.
AB  - Mobile phone data – with file sizes scaling into terabytes – easily overwhelm the computational capacity available to some researchers. Moreover, for ethical reasons, data access is often granted only to particular subsets, restricting analyses to cover single days, weeks, or geographical areas. Consequently, it is frequently impossible to set a particular analysis or event in its context and know how typical it is, compared to other days, weeks or months. This is important for academic referees questioning research on mobile phone data and for the analysts in deciding how to sample, how much data to process, and which events are anomalous. All these issues require an understanding of variability in Big Data to answer the question of how average is average? This paper provides a method, using a large mobile phone dataset, to answer these basic but necessary questions. We show that file size is a robust proxy for the activity level of phone users by profiling the temporal variability of the data at an hourly, daily and monthly level. We then apply time-series analysis to isolate temporal periodicity. Finally, we discuss confidence limits to anomalous events in the data. We recommend an analytical approach to mobile phone data selection which suggests that ideally data should be sampled across days, across working weeks, and across the year, to obtain a representative average. However, where this is impossible, the temporal variability is such that specific weekdays’ data can provide a fair picture of other days in their general structure.
PB  - arXiv
PY  - 2020
ST  - How average is average? Temporal patterns in human behaviour as measured by mobile phone data – or why chose Thursdays.
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/itsc.2011.6082974
ER  -


TY  - GEN
AU  - Cruz, R.S.
AU  - Cherian, A.
AU  - Fernando, B.
AU  - Campbell, D.
AU  - Gould, S.
TI  - Inferring temporal compositions of actions using probabilistic automata
AB  - This paper presents a framework to recognize temporal compositions of atomic actions in videos. Specifically, we propose to express temporal compositions of actions as semantic regular expressions and derive an inference framework using probabilistic automata to recognize complex actions as satisfying these expressions on the input video features. Our approach is different from existing works that either predict long-range complex activities as unordered sets of atomic actions, or retrieve videos using natural language sentences. Instead, the proposed approach allows recognizing complex fine-grained activities using only pretrained action classifiers, without requiring any additional data, annotations or neural network training. To evaluate the potential of our approach, we provide experiments on synthetic datasets and challenging real action recognition datasets, such as MultiTHUMOS and Charades. We conclude that the proposed approach can extend state-of-Theart primitive action classifiers to vastly more complex activities without large performance degradation.
PB  - arXiv
PY  - 2020
ST  - Inferring temporal compositions of actions using probabilistic automata
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvprw50498.2020.00192
ER  -


TY  - GEN
AU  - Chai, X.
AU  - Guo, X.
AU  - Xiao, J.
AU  - Jiang, J.
TI  - Analysis of spatial-temporal behavior pattern of the share bike usage during COVID-19 pandemic in Beijing
AB  - During the epidemics of COVID-19, the whole world is experiencing a serious crisis on public health and economy. Understanding the human mobility during the pandemic helps one to design intervention strategies and resilience measures. The widely used Bike Sharing System (BSS) can characterize the activities of urban dwellers over time & space in big cities but is rarely reported in epidemiological research. In this paper, we present a human mobility analyzing framework based on BSS data, which examines the spatiotemporal characteristics of share bike users, detects the key time nodes of different pandemic stages, and demonstrats the evolution of human mobility due to the onset of the COVID-19 threat and administrative restrictions. We assessed the net impact of the pandemic by using the result of co-location analysis between share bike usage and POIs (Point Of Interest). Our results show the pandemic reduced the overall bike usage by 64.8%, then an average increase (15.9%) in share bike usage appeared afterwards, suggesting that productive and residential activities have partially recovered but far from the ordinary days. These findings could be a reference for epidemiological researches and inform policymaking in the context of the current COVID-19 outbreak and other epidemic events at city-scale.
PB  - arXiv
PY  - 2020
ST  - Analysis of spatial-temporal behavior pattern of the share bike usage during COVID-19 pandemic in Beijing
Y2  - 2025/05/05/21:54:29
DO  - 10.1111/tgis.12784
ER  -


TY  - GEN
AU  - Ben-Ari, R.
AU  - Nacson, M.S.
AU  - Azulai, O.
AU  - Barzelay, U.
AU  - Rotman, D.
TI  - TAEN: Temporal aware embedding network for few-shot action recognition
AB  - Classification of new class entities requires collecting and annotating hundreds or thousands of samples that is often prohibitively costly. Few-shot learning suggests learning to classify new classes using just a few examples. Only a small number of studies address the challenge of few-shot learning on spatio-temporal patterns such as videos. In this paper, we present the Temporal Aware Embedding Network (TAEN) for few-shot action recognition, that learns to represent actions, in a metric space as a trajectory, conveying both short term semantics and longer term connectivity between action parts. We demonstrate the effectiveness of TAEN on two few shot tasks, video classification and temporal action detection and evaluate our method on the Kinetics-400 and on ActivityNet 1.2 few-shot benchmarks. With training of just a few fully connected layers we reach comparable results to prior art on both few shot video classification and temporal detection tasks, while reaching state-of-the-art in certain scenarios.
PB  - arXiv
PY  - 2020
ST  - TAEN
Y2  - 2025/05/05/21:54:29
DO  - 10.1109/cvprw53098.2021.00313
ER  -


TY  - GEN
AU  - Kushner, T.
AU  - Sharma, A.
TI  - Bursts of activity: Temporal patterns of help-seeking and support in online mental health forums
AB  - Recent years have seen a rise in social media platforms that provide peer-to-peer support to individuals suffering from mental distress. Studies on the impact of these platforms have focused on either short-term scales of single-post threads, or long-term changes over arbitrary period of time (months or years). While important, such arbitrary periods do not necessarily follow users’ progressions through acute periods of distress. Using data from Talklife, a mental health platform, we find that user activity follows a distinct pattern of high activity periods with interleaving periods of no activity, and propose a method for identifying such bursts & breaks in activity. We then show how studying activity during bursts can provide a personalized, medium-term analysis for a key question in online mental health communities: What characteristics of user activity lead some users to find support and help, while others fall short? Using two independent outcome metrics, moments of cognitive change and self-reported changes in mood during a burst of activity, we identify two actionable features that can improve outcomes for users: persistence within bursts, and giving complex emotional support to others. Our results demonstrate the value of considering bursts as a natural unit of analysis for psychosocial change in online mental health communities.
PB  - arXiv
PY  - 2020
ST  - Bursts of activity
Y2  - 2025/05/05/21:54:29
DO  - 10.1145/3366423.3380056
ER  -


TY  - GEN
AU  - Wu, W.
AU  - Hua, Y.
AU  - Zheng, C.
AU  - Chen, C.
AU  - Lu, A.
TI  - SkeletonMAE: Spatial-Temporal Masked Autoencoders for Self-supervised Skeleton Action Recognition
AB  - Fully supervised skeleton-based action recognition has achieved great progress with the blooming of deep learning techniques. However, these methods require sufficient labeled data which is not easy to obtain. In contrast, self-supervised skeleton-based action recognition has attracted more attention. With utilizing the unlabeled data, more generalizable features can be learned to alleviate the overfitting problem and reduce the demand of massive labeled training data. Inspired by the MAE [15], we propose a spatial-temporal masked autoencoder framework for self-supervised 3D skeleton-based action recognition (SkeletonMAE). Following MAE’s masking and reconstruction pipeline, we utilize a skeleton based encoder-decoder transformer architecture to reconstruct the masked skeleton sequences. A novel masking strategy, named Spatial-Temporal Masking, is introduced in terms of both joint-level and frame-level for the skeleton sequence. This pre-training strategy makes the encoder output generalizable skeleton features with spatial and temporal dependencies. Given the unmasked skeleton sequence, the encoder is fine-tuned for the action recognition task. Extensive experiments show that our SkeletonMAE achieves remarkable performance and outperforms the state-of-the-art methods on both NTU RGB+D and NTU RGB+D 120 datasets.
PB  - arXiv
PY  - 2022
ST  - SkeletonMAE
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icmew59549.2023.00045
ER  -


TY  - GEN
AU  - Qiu, W.
TI  - GLOBAL BEHAVIOR OF TEMPORAL DISCRETIZATIONS FOR VOLTERRA INTEGRODIFFERENTIAL EQUATIONS WITH CERTAIN NONSMOOTH KERNELS
AB  - In this work, the z-transform is presented to analyze time-discrete solutions for Volterra integrodifferential equations (VIDEs) with nonsmooth multi-term kernels in the Hilbert space, and this class of continuous problem was first considered and analyzed by Hannsgen and Wheeler (SIAM J Math Anal 15 (1984) 579-594). This work discusses three cases of kernels βq(t) included in the integrals for the multi-term VIDEs, from which we use corresponding numerical techniques to approximate the solution of multi-term VIDEs in different cases. Firstly, for the case of β1(t), β2(t) ∈ L1(ℝ+), the Crank-Nicolson (CN) method and interpolation quadrature (IQ) rule are applied to time-discrete solutions of the multi-term VIDEs; secondly, for the case of β1(t) ∈ L1(ℝ+) and β2(t) ∈ L1,loc(ℝ+), second-order backward differentiation formula (BDF2) and second-order convolution quadrature (CQ) are employed to discretize the multi-term problem in the time direction; thirdly, for the case of β1(t), β2(t) ∈ L1,loc(ℝ+), we utilize the CN method and trapezoidal CQ (TCQ) rule to approximate temporally the multi-term problem. Then for the discrete solution of three cases, the long-time global stability and convergence are proved based on the z-transform and certain appropriate assumptions. Furthermore, the long-time estimate of the third case is confirmed by the numerical tests. MSC Codes 65M12, 65M22, 45K05, 45E10
PB  - arXiv
PY  - 2022
ST  - GLOBAL BEHAVIOR OF TEMPORAL DISCRETIZATIONS FOR VOLTERRA INTEGRODIFFERENTIAL EQUATIONS WITH CERTAIN NONSMOOTH KERNELS
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/bf02020486
ER  -


TY  - GEN
AU  - Behrmann, N.
AU  - Golestaneh, S.A.
AU  - Kolter, Z.
AU  - Gall, J.
AU  - Noroozi, M.
TI  - Unified Fully and Timestamp Supervised Temporal Action Segmentation via Sequence to Sequence Translation
AB  - This paper introduces a unified framework for video action segmentation via sequence to sequence (seq2seq) translation in a fully and timestamp supervised setup. In contrast to current state-of-the-art frame-level prediction methods, we view action segmentation as a seq2seq translation task, i.e., mapping a sequence of video frames to a sequence of action segments. Our proposed method involves a series of modifications and auxiliary loss functions on the standard Transformer seq2seq translation model to cope with long input sequences opposed to short output sequences and relatively few videos. We incorporate an auxiliary supervision signal for the encoder via a frame-wise loss and propose a separate alignment decoder for an implicit duration prediction. Finally, we extend our framework to the timestamp supervised setting via our proposed constrained k-medoids algorithm to generate pseudo-segmentations. Our proposed framework performs consistently on both fully and timestamp supervised settings, outperforming or competing state-of-the-art on several datasets. Our code is publicly available at https://github.com/boschresearch/UVAST.
PB  - arXiv
PY  - 2022
ST  - Unified Fully and Timestamp Supervised Temporal Action Segmentation via Sequence to Sequence Translation
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-19833-5_4
ER  -


TY  - GEN
AU  - Li, D.
AU  - Yang, X.
AU  - Tang, Y.
AU  - Zhang, C.
AU  - Zhang, W.
TI  - Active Learning with Effective Scoring Functions for Semi-Supervised Temporal Action Localization
AB  - Temporal Action Localization (TAL) aims to predict both action category and temporal boundary of action instances in untrimmed videos, i.e., start and end time. Fully-supervised solutions are usually adopted in most existing works, and proven to be effective. One of the practical bottlenecks in these solutions is the large amount of labeled training data required. To reduce expensive human label cost, this paper focuses on a rarely investigated yet practical task named semi-supervised TAL and proposes an effective active learning method, named AL-STAL. We leverage four steps for actively selecting video samples with high informativeness and training the localization model, named Train, Query, Annotate, Append. Two scoring functions that consider the uncertainty of localization model are equipped in AL-STAL, thus facilitating the video sample rank and selection. One takes entropy of predicted label distribution as measure of uncertainty, named Temporal Proposal Entropy (TPE). And the other introduces a new metric based on mutual information between adjacent action proposals and evaluates the informativeness of video samples, named Temporal Context Inconsistency (TCI). To validate the effectiveness of proposed method, we conduct extensive experiments on two benchmark datasets THUMOS’14 and ActivityNet 1.3. Experiment results show that AL-STAL outperforms the existing competitors and achieves satisfying performance compared with fully-supervised learning.
PB  - arXiv
PY  - 2022
ST  - Active Learning with Effective Scoring Functions for Semi-Supervised Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.displa.2023.102434
ER  -


TY  - GEN
AU  - Yang, F.
AU  - Ukita, N.
AU  - Sakti, S.
AU  - Nakamura, S.
TI  - Actor-identified Spatiotemporal Action Detection - Detecting Who Is Doing What in Videos
AB  - The success of deep learning on video Action Recognition (AR) has motivated researchers to progressively promote related tasks from the coarse level to the fine-grained level. Compared with conventional AR that only predicts an action label for the entire video, Temporal Action Detection (TAD) has been investigated for estimating the start and end time for each action in videos. Taking TAD a step further, Spatiotemporal Action Detection (SAD) has been studied for localizing the action both spatially and temporally in videos. However, who performs the action, is generally ignored in SAD, while identifying the actor could also be important. To this end, we propose a novel task, Actor-identified Spatiotemporal Action Detection (ASAD), to bridge the gap between SAD and actor identification. In ASAD, we not only detect the spatiotemporal boundary for instance-level action but also assign the unique ID to each actor. To approach ASAD, Multiple Object Tracking (MOT) and Action Classification (AC) are two fundamental elements. By using MOT, the spatiotemporal boundary of each actor is obtained and assigned to a unique actor identity. By using AC, the action class is estimated within the corresponding spatiotemporal boundary. Since ASAD is a new task, it poses many new challenges that cannot be addressed by existing methods: i) no dataset is specifically created for ASAD, ii) no evaluation metrics are designed for ASAD, iii) current MOT performance is the bottleneck to obtain satisfactory ASAD results. To address those problems, we contribute to i) annotate a new ASAD dataset, ii) propose ASAD evaluation metrics by considering multi-label actions and actor identification, iii) improve the data association strategies in MOT to boost the MOT performance, which leads to better ASAD results. We believe considering actor identification with spatiotemporal action detection could promote the research on video understanding and beyond. The code is available at https://github.com/fandulu/ASAD.
PB  - arXiv
PY  - 2022
ST  - Actor-identified Spatiotemporal Action Detection - Detecting Who Is Doing What in Videos
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v35i18.17934
ER  -


TY  - GEN
AU  - Özcan, F.
AU  - Alkan, A.
TI  - Neural Decoding of Inferior Colliculus Multiunit Activity for Sound Category identification with temporal correlation and deep learning
AB  - Natural sounds are easily perceived and identified by humans and animals. Despite this, the neural transformations that enable sound perception remain largely unknown. Neuroscientists are drawing important conclusions about neural decoding that may eventually aid research into the design of brain-machine interfaces (BCIs). It is thought that the time-frequency correlation characteristics of sounds may be reflected in auditory assembly responses in the midbrain and that this may play an important role in identification of natural sounds. In our study, natural sounds will be predicted from multi-unit activity (MUA) signals collected in the inferior colliculus. The temporal correlation values of the MUA signals are converted into images. We used two different segment sizes and thus generated four subsets for the classification. Using pre-trained convolutional neural networks (CNNs), features of the images were extracted and the type of sound heard was classified. For this, we applied transfer learning from Alexnet, GoogleNet and Squeezenet CNNs. The classifiers support vector machines (SVM), k-nearest neighbour (KNN), Naive Bayes and Ensemble were used. The accuracy, sensitivity, specificity, precision and F1 score were measured as evaluation parameters. Considering the trials one by one in each, we obtained an accuracy of 85.69% with temporal correlation images over 1000 ms windows. Using all trials and removing noise, the accuracy increased to 100%.
PB  - bioRxiv
PY  - 2022
ST  - Neural Decoding of Inferior Colliculus Multiunit Activity for Sound Category identification with temporal correlation and deep learning
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.08.24.505211
ER  -


TY  - GEN
AU  - Ouyang, Y.
AU  - Zhang, T.
AU  - Gu, W.
AU  - Wang, H.
TI  - Adaptive Perception Transformer for Temporal Action Localization
AB  - Temporal action localization aims to predict the boundary and category of each action instance in untrimmed long videos. Most of previous methods based on anchors or proposals neglect the global-local context interaction in entire video sequences. Besides, their multi-stage designs cannot generate action boundaries and categories straightforwardly. To address the above issues, this paper proposes a end-to-end model, called Adaptive Perception transformer (AdaPerFormer for short). Specifically, AdaPerFormer explores a dual-branch attention mechanism. One branch takes care of the global perception attention, which can model entire video sequences and aggregate global relevant contexts. While the other branch concentrates on the local convolutional shift to aggregate intra-frame and inter-frame information through our bidirectional shift operation. The end-to-end nature produces the boundaries and categories of video actions without extra steps. Extensive experiments together with ablation studies are provided to reveal the effectiveness of our design. Our method obtains competitive performance on the THUMOS14 and ActivityNet-1.3 dataset.
PB  - arXiv
PY  - 2022
ST  - Adaptive Perception Transformer for Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tmm.2024.3367599
ER  -


TY  - GEN
AU  - Tang, Y.
AU  - Wu, Y.
AU  - Zhou, P.
AU  - Hu, J.
TI  - Enabling Weakly-Supervised Temporal Action Localization from On-Device Learning of the Video Stream
AB  - Detecting actions in videos have been widely applied in on-device applications such as cars, robots, etc. Practical on-device videos are always untrimmed with both action and background. It is desirable for a model to both recognize the class of action and localize the temporal position where the action happens. Such a task is called temporal action location (TAL), which is always trained on the cloud where multiple untrimmed videos are collected and labeled. It is desirable for a TAL model to continuously and locally learn from new data, which can directly improve the action detection precision while protecting customers’ privacy. However, directly training a TAL model on the device is non-trivial. To train a TAL model which can precisely recognize and localize each action, tremendous video samples with temporal annotations are required. However, annotating videos frame by frame is exorbitantly time-consuming and expensive. Although weakly-supervised temporal action localization (W-TAL) has been proposed to learn from untrimmed videos with only video-level labels, such an approach is also not suitable for on-device learning scenarios. In practical on-device learning applications, data are collected in streaming. For example, the camera on the device keeps collecting video frames for hours or days, and the actions of nearly all classes are included in a single long video stream. Dividing such a long video stream into multiple video segments requires lots of human effort, which hinders the exploration of applying the TAL tasks to realistic on-device learning applications. To enable W-TAL models to learn from a long, untrimmed streaming video, we propose an efficient video learning approach that can directly adapt to new environments. We first propose a self-adaptive video dividing approach with a contrast score-based segment merging approach to convert the video stream into multiple segments. Then, we explore different sampling strategies on the TAL tasks to request as few labels as possible. To the best of our knowledge, we are the first attempt to directly learn from the on-device, long video stream. Experimental results on the THUMOS’14 dataset show that the performance of our approach is comparable to the current W-TAL state-of-the-art (SOTA) work without any laborious manual video splitting.
PB  - arXiv
PY  - 2022
ST  - Enabling Weakly-Supervised Temporal Action Localization from On-Device Learning of the Video Stream
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tcad.2022.3197536
ER  -


TY  - GEN
AU  - Akuoko, O.K.
AU  - Dhikrullahi, S.B.
AU  - Hinne, I.A.
AU  - Wilson, M.D.
AU  - Afrane, Y.A.
TI  - Biting behaviour and the spatio-temporal dynamics of malaria vectors in different ecological zones in Ghana
AB  - Background: Significant decrease in malaria morbidity and mortality have been attained using long-lasting insecticide treated nets and indoor residual spraying. Selective pressure from these control methods influences change in vector bionomics and behavioural pattern. There is a need to understand how insecticide resistance drives behavioural changes within vector species. This study aimed to determine the spatio-temporal dynamics and biting behaviour of malaria vectors in different ecological zones in Ghana in an era of high insecticide use for public health vector control. Methods: Adult mosquitoes were collected during the dry and the rainy seasons in 2017 and 2018 from five study sites in Ghana in different ecological zones. Indoor and outdoor biting mosquitoes were collected per hour from 18:00 to 06:00 hours employing the human landing catches (HLC) technique. Morphological and molecular species identification of vectors were done using identification keys and PCR respectively. Genotyping of insecticide resistant markers was done using the TaqMan SNP genotyping probe-based assays. Detection of P. falciparum sporozoites was determined using PCR. Results: A total of 50,322 mosquitoes belonging to four different genera were collected from all the study sites during the sampling seasons in 2017 and 2018. Among the Anophelines were Anopheles gambiae s.l. 93.16%, (31055/33,334), An. funestus 2.07%, (690/33,334), An. pharoensis 4.63%, (1545/33,334), and An. rufipes 0.13% (44/33,334). Overall, 76.40%, (25,468/33,334) of Anopheles were collected in the rainy season and 23.60%, (7,866/33,334) in the dry season. There was a significant difference (z = 2.410; p = 0.0160) between indoor biting (51.09%; 15,866/31,055) and outdoor biting An. gambiae s.l. (48.91%; 15,189/31,055). The frequency of the Vgsc-1014F mutation was slightly higher in indoor biting mosquitoes (54.90%) than outdoors (45.10%). Overall, forty-four pools (44) of samples were positive for P. falciparum CSP giving an overall sporozoite rate of 0.07%. Conclusion: Anopheles gambiae s.l. were more abundant indoors across all ecological zones of Ghana. The frequency of G119S was higher in indoor than outdoor from all the study sites, but higher sporozoite rates in outdoor mosquitoes in Dodowa and Kpalsogu. There is thus, an urgent need for a supplementary malaria control intervention to control outdoor biting mosquitoes.
PB  - Research Square
PY  - 2022
ST  - Biting behaviour and the spatio-temporal dynamics of malaria vectors in different ecological zones in Ghana
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-1975878/v1
ER  -


TY  - GEN
AU  - Taconet, P.
AU  - Soma, D.D.
AU  - Zogo, B.
AU  - Pennetier, C.
AU  - Moiroux, N.
TI  - Physiological and behavioural resistance of malaria vectors in rural West-Africa: A data mining study to address their fine-scale spatiotemporal heterogeneity, drivers, and predictability
AB  - Insecticide resistance and behavioural adaptation of malaria mosquitoes affect the efficacy of long-lasting insecticide nets - currently the main tool for malaria vector control. To develop and deploy complementary, efficient and cost-effective control interventions, a good understanding of the drivers of these physiological and behavioural traits is needed. In this data-mining exercise, we modelled a set of indicators of physiological resistance to insecticide (prevalence of three target-site mutations) and behavioural resistance phenotypes (early- and late-biting, exophagy) of anopheles mosquitoes in two rural areas of West-Africa, located in Burkina Faso and Cote d’Ivoire. To this aim, we used mosquito field collections along with heterogeneous, multi-source and multi-scale environmental data. The objectives were i) to assess the small-scale spatial and temporal heterogeneity of physiological resistance to insecticide and behavioural resistance phenotypes, ii) to better understand their drivers, and iii) to assess their spatio-temporal predictability, at scales that are consistent with operational action. The explanatory variables covered a wide range of potential environmental determinants of vector resistance to insecticide or behavioural resistance phenotypes: vector control, human availability and nocturnal behaviour, macro and micro-climatic conditions, landscape, etc. The resulting models revealed many statistically significant associations, although their predictive powers were overall weak. We interpreted and discussed these associations in light of several topics of interest, such as: respective contribution of public health and agriculture in the selection of physiological resistances, biological costs associated with physiological resistances, biological mechanisms underlying biting behaviour, and impact of micro-climatic conditions on the time or place of biting. To our knowledge, our work is the first modeling insecticide resistance and feeding behaviour of malaria vectors at such fine spatial scale with such a large dataset of both mosquito and environmental data.
PB  - bioRxiv
PY  - 2022
ST  - Physiological and behavioural resistance of malaria vectors in rural West-Africa
Y2  - 2025/05/05/21:54:31
DO  - 10.24072/pcjournal.367
ER  -


TY  - GEN
AU  - Lv, Z.
AU  - Wang, F.
AU  - Zhang, S.
AU  - Yang, H.
AU  - Wu, F.
TI  - Personalizing Intervened Network for Long-tailed Sequential User Behavior Modeling
AB  - In an era of information explosion, recommendation systems play an important role in people’s daily life by facilitating content exploration. It is known that user activeness, i.e., number of behaviors, tends to follow a long-tail distribution, where the majority of users are with low activeness. In practice, we observe that tail users suffer from significantly lower-quality recommendation than the head users after joint training. We further identify that a model trained on tail users separately still achieve inferior results due to limited data. Though long-tail distributions are ubiquitous in recommendation systems, improving the recommendation performance on the tail users still remains challenge in both research and industry. Directly applying related methods on long-tail distribution might be at risk of hurting the experience of head users, which is less affordable since a small portion of head users with high activeness contribute a considerate portion of platform revenue. In this paper, we propose a novel approach that significantly improves the recommendation performance of the tail users while achieving at least comparable performance for the head users over the base model. The essence of this approach is a novel Gradient Aggregation technique that learns common knowledge shared by all users into a backbone model, followed by separate plugin prediction networks for the head users and the tail users personalization. As for common knowledge learning, we leverage the backward adjustment from the causality theory for deconfounding the gradient estimation and thus shielding off the backbone training from the confounder, i.e., user activeness. We conduct extensive experiments on two public recommendation benchmark datasets and a large-scale industrial datasets collected from the Alipay platform. Empirical studies validate the rationality and effectiveness of our approach.
PB  - arXiv
PY  - 2022
ST  - Personalizing Intervened Network for Long-tailed Sequential User Behavior Modeling
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3394486.3403078
ER  -


TY  - GEN
AU  - Hu, L.
AU  - Liu, S.
AU  - Feng, W.
TI  - Spatial Temporal Graph Attention Network for Skeleton-Based Action Recognition
AB  - It’s common for current methods in skeleton-based action recognition to mainly consider capturing long-term temporal dependencies as skeleton sequences are typically long (>128 frames), which forms a challenging problem for previous approaches. In such conditions, short-term dependencies are few formally considered, which are critical for classifying similar actions. Most current approaches are consisted of interleaving spatial-only modules and temporal-only modules, where direct information flow among joints in adjacent frames are hindered, thus inferior to capture short-term motion and distinguish similar action pairs. To handle this limitation, we propose a general framework, coined as STGAT, to model cross-spacetime information flow. It equips the spatial-only modules with spatial-temporal modeling for regional perception. While STGAT is theoretically effective for spatial-temporal modeling, we propose three simple modules to reduce local spatial-temporal feature redundancy and further release the potential of STGAT, which (1) narrow the scope of self-attention mechanism, (2) dynamically weight joints along temporal dimension, and (3) separate subtle motion from static features, respectively. As a robust feature extractor, STGAT generalizes better upon classifying similar actions than previous methods, witnessed by both qualitative and quantitative results. STGAT achieves state-of-the-art performance on three large-scale datasets: NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400. Code is released.
PB  - arXiv
PY  - 2022
ST  - Spatial Temporal Graph Attention Network for Skeleton-Based Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s10044-023-01179-3
ER  -


TY  - GEN
AU  - Koizumi, K.
AU  - Kunii, N.
AU  - Ueda, K.
AU  - Shimada, S.
AU  - Nakao, M.
TI  - Intracranial Neurofeedback Modulating Neural Activity in the Mesial Temporal Lobe During Memory Encoding: A Pilot Study
AB  - Removal of the mesial temporal lobe (MTL) is an established surgical procedure that leads to seizure freedom in patients with intractable MTL epilepsy; however, it carries the potential risk of memory damage. Neurofeedback (NF), which regulates brain function by converting brain activity into perceptible information and providing feedback, has attracted considerable attention in recent years for its potential as a novel complementary treatment for many neurological disorders. However, no research has attempted to artificially reorganize memory functions by applying NF before resective surgery to preserve memory functions. Thus, this study aimed (1) to construct a memory NF system that used intracranial electrodes to feedback neural activity on the language-dominant side of the MTL during memory encoding and (2) to verify whether neural activity and memory function in the MTL change with NF training. Two intractable epilepsy patients with implanted intracranial electrodes underwent at least five sessions of memory NF training to increase the theta power in the MTL. There was an increase in theta power and a decrease in fast beta and gamma powers in one of the patients in the late stage of memory NF sessions. NF signals were not correlated with memory function. Despite its limitations as a pilot study, to our best knowledge, this study is the first to report that intracranial NF may modulate neural activity in the MTL, which is involved in memory encoding. The findings provide important insights into the future development of NF systems for the artificial reorganization of memory functions.
PB  - Research Square
PY  - 2022
ST  - Intracranial Neurofeedback Modulating Neural Activity in the Mesial Temporal Lobe During Memory Encoding
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s10484-023-09595-1
ER  -


TY  - GEN
AU  - Friedrich, E.V.C.
AU  - Zillekens, I.C.
AU  - Biel, A.L.
AU  - Sauseng, P.
AU  - Schilbach, L.
TI  - Spatio-Temporal Dynamics of Oscillatory Brain Activity during the Observation of Actions and Interactions between Point-light Agents
AB  - Predicting actions from nonverbal cues and using them to optimize one’s response behavior (i.e., interpersonal predictive coding) is essential in everyday social interactions. We aimed to investigate the neural correlates of different cognitive processes evolving over time during interpersonal predictive coding. Thirty-nine participants watched two agents depicted by moving point-light stimuli while an electroencephalogram (EEG) was recorded. One well-recognizable agent performed either a ‘communicative’ or an ‘individual’ action. The second agent either was blended into a cluster of noise dots (i.e., present), or was entirely replaced by noise dots (i.e., absent), which participants had to differentiate. EEG amplitude and coherence analyses for theta, alpha and beta frequency bands revealed a dynamic pattern unfolding over time: Watching communicative actions was associated with enhanced coupling within medial anterior regions involved in social and mentalizing processes and with dorsolateral prefrontal activation indicating a higher deployment of cognitive resources. Trying to detect the agent in the cluster of noise dots without having seen communicative cues was related to enhanced coupling in posterior regions for social perception and visual processing. Observing an expected outcome was modulated by motor system activation. Finally, when the agent was detected correctly, activation in posterior areas for visual processing of socially-relevant features was increased. Taken together, our results demonstrate that it is crucial to consider the temporal dynamics of social interactions and of their neural correlates to better understand interpersonal predictive coding. This could lead to optimized treatment approaches for individuals with problems in social interactions.
PB  - bioRxiv
PY  - 2022
ST  - Spatio-Temporal Dynamics of Oscillatory Brain Activity during the Observation of Actions and Interactions between Point-light Agents
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.08.16.504090
ER  -


TY  - GEN
AU  - Gao, Z.
AU  - Cui, X.
AU  - Zhuo, T.
AU  - Wang, M.
AU  - Chen, S.
TI  - Temporal Action Localization with Multi-temporal Scales
AB  - Temporal action localization plays an important role in video analysis, which aims to localize and classify actions in untrimmed videos. The previous methods often predict actions on a feature space of a single-temporal scale. However, the temporal features of a low-level scale lack enough semantics for action classification while a high-level scale cannot provide rich details of the action boundaries. To address this issue, we propose to predict actions on a feature space of multi-temporal scales. Specifically, we use refined feature pyramids of different scales to pass semantics from high-level scales to low-level scales. Besides, to establish the long temporal scale of the entire video, we use a spatial-temporal transformer encoder to capture the long-range dependencies of video frames. Then the refined features with long-range dependencies are fed into a classifier for the coarse action prediction. Finally, to further improve the prediction accuracy, we propose to use a frame-level self attention module to refine the classification and boundaries of each action instance. Extensive experiments show that the proposed method can outperform state-of-the-art approaches on the THUMOS14 dataset and achieves comparable performance on the ActivityNet1.3 dataset. Compared with A2Net (TIP20, Avg{0.3:0.7}), Sub-Action (CSVT2022, Avg{0.1:0.5}), and AFSD (CVPR21, Avg{0.3:0.7}) on the THUMOS14 dataset, the proposed method can achieve improvements of 12.6%, 17.4% and 2.2%, respectively 1
PB  - arXiv
PY  - 2022
ST  - Temporal Action Localization with Multi-temporal Scales
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr46437.2021.00151
ER  -


TY  - GEN
AU  - Li, Y.
AU  - Gao, C.
AU  - Yao, Q.
AU  - Jin, D.
AU  - Li, Y.
TI  - DisenHCN: Disentangled Hypergraph Convolutional Networks for Spatiotemporal Activity Prediction
AB  - Spatiotemporal activity prediction, aiming to predict user activities at a specific location and time, is crucial for applications like urban planning and mobile advertising. Existing solutions based on tensor decomposition or graph embedding suffer from the following two major limitations: 1) ignoring the fine-grained similarities of user preferences; 2) user’s modeling is entangled. In this work, we propose a hypergraph neural network model called DisenHCN (short for Disentangled Hypergraph Convolutional Networks) to bridge the above gaps. In particular, we first unify the fine-grained user similarity and the complex matching between user preferences and spatiotemporal activity into a heterogeneous hypergraph. We then disentangle the user representations into different aspects (location-aware, time-aware, and activity-aware) and aggregate corresponding aspect’s features on the constructed hypergraph, capturing high-order relations from different aspects and disentangles the impact of each aspect for final prediction. Extensive experiments show that our DisenHCN outperforms the state-of-the-art methods by 14.23% to 18.10% on four real-world datasets. Further studies also convincingly verify the rationality of each component in our DisenHCN.
PB  - arXiv
PY  - 2022
ST  - DisenHCN
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tits.2022.3168879
ER  -


TY  - GEN
AU  - Zhu, B.
AU  - Jiang, X.
AU  - Xu, K.
AU  - Sun, T.
AU  - Chen, S.
TI  - Encoding Temporal Dynamics between Sub-Actions for Action Recognition
AB  - Generally, the progress of human action is not consistent across the video but exhibits a complex temporal structure that consists of several finer-grained sub-actions. Inspired by this observation, we propose a novel video representation named Temporal Dynamic Fisher Vector (TDFV), to characterize the temporal dynamics between the sub-actions involved in an action sequence. The video sequences are first parsed into smooth-changed clips corresponding to sub-actions or backgrounds through adaptive temporal segmentation. A soft attention pooling method is then proposed to aggregate frame-wise features within a snippet into a clip-level representation. Considering that each sub-action can be viewed as a latent state during the evolution of an action, we represent the video sequence by a linear dynamical system (LDS) and treat the clip-level representations as the observations of latent states in LDS. Finally, the Fisher kernel technique is applied over the LDS to encode the temporal dynamics between sub-actions into a compact and fixed-length feature vector, TDFV. Extensive experiments were conducted on the Olympic Sports, Hollywood2, HMDB51, and UCF101 datasets. The recognition performance of the proposed TDFV representation is competitive with state-of-the-art approaches.
PB  - SSRN
PY  - 2022
ST  - Encoding Temporal Dynamics between Sub-Actions for Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icpr.2018.8546263
ER  -


TY  - GEN
AU  - Li, M.
AU  - Wang, T.
AU  - Zhang, H.
AU  - Pu, S.
AU  - Wu, F.
TI  - HERO: HiErarchical spatio-tempoRal reasOning with Contrastive Action Correspondence for End-to-End Video Object Grounding
AB  - Video Object Grounding (VOG) is the problem of associating spatial object regions in the video to a descriptive natural language query. This is a challenging vision-language task that necessitates constructing the correct cross-modal correspondence and modeling the appropriate spatio-temporal context of the query video and caption, thereby localizing the specific objects accurately. In this paper, we tackle this task by a novel framework called HiErarchical spatio-tempoRal reasOning (HERO) with contrastive action correspondence. We study the VOG task at two aspects that prior works overlooked: (1) Contrastive Action Correspondence-aware Retrieval. Notice that the fine-grained video semantics (e.g., multiple actions) is not totally aligned with the annotated language query (e.g., single action), we first introduce the weakly-supervised contrastive learning that classifies the video as action-consistent and action-independent frames relying on the video-caption action semantic correspondence. Such a design can build the fine-grained cross-modal correspondence for more accurate subsequent VOG. (2) Hierarchical Spatio-temporal Modeling Improvement. While transformer-based VOG models present their potential in sequential modality (i.e., video and caption) modeling, existing evidence also indicates that the transformer suffers from the issue of the insensitive spatio-temporal locality. Motivated by that, we carefully design the hierarchical reasoning layers to decouple fully connected multi-head attention and remove the redundant interfering correlations. Furthermore, our proposed pyramid and shifted alignment mechanisms are effective to improve the cross-modal information utilization of neighborhood spatial regions and temporal frames. We conducted extensive experiments to show our HERO outperforms existing techniques by achieving significant improvement on two benchmark datasets.
PB  - arXiv
PY  - 2022
ST  - HERO
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3503161.3548333
ER  -


TY  - GEN
AU  - Serbinowska, S.S.
AU  - Johnson, T.T.
TI  - BehaVerify: Verifying Temporal Logic Specifications for Behavior Trees
AB  - Behavior Trees, which originated in video games as a method for controlling NPCs but have since gained traction within the robotics community, are a framework for describing the execution of a task. BehaVerify is a tool that creates a nuXmv model from a PyTree. For composite nodes, which are standardized, this process is automatic and requires no additional user input. A wide variety of leaf nodes are automatically supported and require no additional user input, but customized leaf nodes will require additional user input to be correctly modeled. BehaVerify can provide a template to make this easier. BehaVerify is able to create a nuXmv model with over 100 nodes and nuXmv was able to verify various non-trivial LTL properties on this model, both directly and via counterexample. The model in question features parallel nodes, selector, and sequence nodes. A comparison with models based on BTCompiler indicates that the models created by BehaVerify perform better.
PB  - arXiv
PY  - 2022
ST  - BehaVerify
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-17108-6_19
ER  -


TY  - GEN
AU  - Rózsa, M.
AU  - Tóth, M.
AU  - Oláh, G.
AU  - Barzó, P.
AU  - Tamás, G.
TI  - Temporal disparity of action potentials triggered in axon initial segments and distal axons in the neocortex
AB  - Neural population activity determines the timing of synaptic inputs, which arrive to dendrites, cell bodies and axon initial segments (AISs) of cortical neurons. Action potential initiation in the AIS (AIS-APs) is driven by input integration, and the phase preference of AIS-APs during network oscillations is characteristic to cell classes. Distal regions of cortical axons do not receive synaptic inputs, yet experimental induction protocols can trigger retroaxonal action potentials (RA-APs) in axons distal from the soma. We report spontaneously occurring RA-APs in human and rodent cortical interneurons that appear uncorrelated to inputs and population activity. Network linked triggering of AIS-APs versus input independent timing of RA-APs of the same interneurons result in disparate temporal contribution of a single cell to in vivo network operation through perisomatic and distal axonal firing.
PB  - bioRxiv
PY  - 2022
ST  - Temporal disparity of action potentials triggered in axon initial segments and distal axons in the neocortex
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.08.09.503303
ER  -


TY  - GEN
AU  - Tang, Y.
AU  - Ge, J.
AU  - Guo, K.
AU  - Hu, H.
AU  - Liang, J.
TI  - Towards Better Utilization of Pseudo Labels for Weakly Supervised Temporal Action Localization
AB  - Weakly supervised temporal action localization (WS-TAL) aims to simultaneously recognize and localize action instances of interest in untrimmed videos with the use of video-level label only. Some works have demonstrated that pseudo labels play an important role for performance improvement in WS-TAL. Since the pseudo labels are inevitably inaccurate, the direct adoption of the noisy labels can lead to inappropriate knowledge transfer. Although some previous studies have shown the benefits of using only the “reliable” pseudo labels, the performance improvement is still limited. In this work, we experimentally analyze how the noise in pseudo labels affects the model performance within the self-distillation framework. Motivated by the finding that the incorrect pseudo labels with large confidence scores have a significant impact on performance, we propose a overconfidence suppression (OCS) strategy to mitigate the effect of the overconfident pseudo labels and thus prevent over-fitting of the student model. In addition, a simplified contrast learning method is utilized to fine-tune the feature representation by increasing the separation of foreground and background snippets. Equipped with the proposed methods, the benefits of pseudo labels can be better exploited and allow the model to achieve state-of-the-art performance on THUMOS’14 and ActivityNet-1.2 benchmarks.
PB  - SSRN
PY  - 2022
ST  - Towards Better Utilization of Pseudo Labels for Weakly Supervised Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4181934
ER  -


TY  - GEN
AU  - Sobral, D.
AU  - Fernandes, A.F.
AU  - Sardoo, A.
AU  - Coelho, A.V.
AU  - Pimentel-Santos, F.M.
TI  - MULTI-OMICS TEMPORAL PROFILING OF AXIAL SPONDYLOARTHRITIS PATIENTS REVEALS AN ASSOCIATION OF THERAPEUTIC RESPONSE TO ADALIMUMAB WITH DISEASE ACTIVITY AND INNATE / ADAPTIVE IMMUNITY
AB  - Background: Axial Spondyloarthritis can lead to significant disability and impairment in quality of life. TNF inhibitors are recommended to patients enduring active disease despite conventional treatment. Nonetheless, up to 40% of patients of patients fail to respond to TNF inhibitors. In this context, it is important to identify as early as possible patients highly likely to respond. This study aims at identifying, among axial spondyloarthritis patients undergoing treatment with the TNF inhibitor adalimumab, early molecular biomarkers differentiating good responders from non-responders after 14 weeks of treatment, as measured by ASAS20. Methods: Peripheral blood RNA sequencing and serum proteins measured by mass spectrometry were evaluated in a cohort of biologic naïve axial spondyloarthritis patients (n = 35), before (baseline) and after (3-5 days, 2 weeks and 14 weeks) treatment with adalimumab. Results from differential expression analysis were used in combination with clinical data to build logistic regression models and random forest models to predict response to adalimumab at baseline. Results: Responders to adalimumab presented higher levels of markers of innate immunity at baseline, mostly related with neutrophils, and lower levels of adaptive immunity markers, particularly B-cells. A logistic regression model incorporating ASDAS-CRP and AFF3, the top differentially expressed gene between responders and non-responders at baseline, enabled an accurate prediction of response to adalimumab in our cohort (AUC=0.96), with random forest models suggesting 80% predictive accuracy. A treatment-associated signature suggests a reduction in inflammatory activity, with C-reactive protein and Haptoglobin showing strong and early decrease in the serum of axial spondyloarthritis patients, while a cluster of apolipoproteins showed increased expression at week 14. Conclusions: Differences in disease activity and/or blood innate/adaptive immune cell type composition at baseline may be a major contributor to response to adalimumab in axial spondyloarthritis, where a model including clinical and blood gene expression variables shows high predictive power. Our results suggest novel molecular biomarkers of response to adalimumab at baseline.
PB  - medRxiv
PY  - 2022
ST  - MULTI-OMICS TEMPORAL PROFILING OF AXIAL SPONDYLOARTHRITIS PATIENTS REVEALS AN ASSOCIATION OF THERAPEUTIC RESPONSE TO ADALIMUMAB WITH DISEASE ACTIVITY AND INNATE / ADAPTIVE IMMUNITY
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.08.02.22278314
ER  -


TY  - GEN
AU  - Li, P.
AU  - Cao, J.
TI  - Prototype Contrastive Learning for Point-Supervised Temporal Action Detection
AB  - Detecting temporal actions in a video with only single-frame annotation in each action instance or segment, a.k.a., point-level supervision, has emerged as a more challenging task recently, compared to fully-supervised setting where per-frame annotations are available during training. Moreover, point-supervised setting provides richer temporal prior knowledge with affordable label cost compared to weak supervision that gives the video-level label. This makes it probably being deployed in wider applications. However, it faces the label sparsity and the serious class-imbalance problem, which motivates us to develop an efficient pseudo-label generation approach to yield both more positive and negative samples as supervision. Particularly, we propose a Prototype Contrastive Learning (PCL) method for point-supervised temporal action detection. PCL aims at explicitly discovering the class relations between labeled and unlabeled frames by adopting prototype learning, and generates pseudo labels by estimating the semantic similarity of pair-wise frames in the embedding space. Meanwhile, it imposes the class relation constraint on the prototypes of action and background by introducing contrastive representation learning, i.e., the prototypes in distinct classes are pushed far away and those within the same class are pulled closer. This allows learning the discriminative feature representations of prototypes that well comply to the data distribution of video frames. Thus, we employ the prototype representations as the hidden pattern proxies of different classes, and take advantage of their semantic relations to generate the pseudo labels for unlabeled samples. Finally, we conduct empirical studies on three benchmarks including GTEA, BEOID, and THUMOS14, and the results have demonstrated the favorable performance of our method.
PB  - SSRN
PY  - 2022
ST  - Prototype Contrastive Learning for Point-Supervised Temporal Action Detection
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4179223
ER  -


TY  - GEN
AU  - Shahtori, N.M.
AU  - Atashzar, S.F.
TI  - Complex Temporal behavior Modeling for Pandemic Spread: Not a Simple Delayed Response!
AB  - One of the significant challenges, when a new virus circulates in a host population, is to detect the outbreak as it arises in a timely fashion and implement the appropriate preventive policies to effectively halt the spread of the disease. The conventional computational epidemic models provide a state-space representation of the dynamic changes of various sub-clusters of a society based on their exposure to the virus and are mostly developed for small-size epidemics. In this work, we reshape and reformulate the conventional computational epidemic modeling approach based on the complex temporal behavior of disease propagation in host populations, inspired by the COVID-19 pandemic. Our new proposed framework allows the construction of transmission rate (p) as a probabilistic function of contributing factors such as virus mutation, immunity waning, and immunity resilience. Our results unravel the interplay between transmission rate, vaccination, virus mutation, immunity loss, and their indirect impacts on the endemic states and waves of the spread. The proposed model provides a robust mathematical framework that allows policy-makers to improve preparedness for curtailing an infectious disease and unfolds the optimal time-frame for vaccination given the available resources and the probability of virus mutation for the current and unforeseen outbreaks.
PB  - medRxiv
PY  - 2022
ST  - Complex Temporal behavior Modeling for Pandemic Spread
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.08.01.22278281
ER  -


TY  - GEN
AU  - Hu, Y.
AU  - Xu, Y.
AU  - Zhang, Y.
AU  - Lu, X.
AU  - Gao, S.
TI  - Camg: Context-Aware Moment Graph Network for Multimodal Temporal Activity Localization Via Language
AB  - Temporal Activity Localization via Language (TALL) is a challenging task for language based video understanding, especially when a video contains multiple moments of interest and the language query has words describing complex context dependencies between the moments. Latest studies have proposed various ways to exploit the temporal context of adjacent moments, but two apparent limitations remained. First, only limited context information was encoded based on RNNs or 2-D convolutions, which highly depended on the pre-sorting of proposals and lacked flexibility. Second, semantically correlated content in different moments was ignored, i.e., semantic context. To address these limitations, we propose a novel GCN-based framework, i.e., Context-Aware Moment Graph (CAMG) network, to jointly model the temporal context and semantic context. GCNs enable the CAMG to capture long-range dependencies with high flexibility. Also, we design a multi-step fusion scheme to aggregate object, motion and textual
PB  - SSRN
PY  - 2022
ST  - Camg
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4176359
ER  -


TY  - GEN
AU  - MacGregor, K.A.
AU  - Lavoie, M.-F.
AU  - Robinson, S.M.C.
AU  - Simard, É.
AU  - McKindsey, C.W.
TI  - Lab and field evaluation of tagging methods for the use of acoustic telemetry to observe sea urchin movement behaviour at ecologically relevant spatiotemporal scales
AB  - Background Acoustic telemetry allows detailed observations of the movement behaviour of many species and as tags get smaller, smaller organisms may be tagged. The number of studies using acoustic telemetry to evaluate marine invertebrate movement is growing, but novel attachment methods include unknowns about the effects of tagging procedures on individual survival and behaviour. This study compared methods of tag attachment on green sea urchins (Strongylocentrotus droebachiensis) to determine the feasibility of using acoustic transmitters to track urchin movement. Four tagging methods were compared in the lab and tag retention, urchin condition, and survival analyzed. Two tagging methods (Dyneema® fishing line and T-bar tags) were evaluated in the field using an existing acoustic telemetry array. Urchins were tagged and the study area revisited one week and two months post-release by scuba divers to estimate movement and tag retention. Results The best methods in the lab, with high tag retention, survival, and minimal effects on urchin condition, were fishing line methods. T-bar tags, although showing high tag retention, caused significant mortality and had deleterious long-term effects on urchin condition and behaviour. After two months in the field, as in the lab, fishing line was a more effective tagging method. Urchins tagged with fishing line showed increased estimates of space occupancy compared to T-bar-tagged urchins and a single fishing-line tagged individual was found by divers in good health after 80 days. Combined, these laboratory and field results demonstrate the feasibility of using acoustic telemetry to observe urchin movement. Conclusions Results strongly suggest that surgical attachment methods that minimize injuries at the attachment site should be prioritized for marine echinoderm tagging studies. Together, lab and field tests indicate that acoustic telemetry is a promising method to examine marine echinoderm movement over ecologically relevant spatial and temporal scales.
PB  - Research Square
PY  - 2022
ST  - Lab and field evaluation of tagging methods for the use of acoustic telemetry to observe sea urchin movement behaviour at ecologically relevant spatiotemporal scales
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-1886921/v1
ER  -


TY  - GEN
AU  - Xiang, W.
AU  - Li, C.
AU  - Wang, B.
AU  - Hua, X.-S.
AU  - Zhang, L.
TI  - Spatiotemporal Self-attention Modeling with Temporal Patch Shift for Action Recognition
AB  - Transformer-based methods have recently achieved great advancement on 2D image-based vision tasks. For 3D video-based tasks such as action recognition, however, directly applying spatiotemporal transformers on video data will bring heavy computation and memory burdens due to the largely increased number of patches and the quadratic complexity of self-attention computation. How to efficiently and effectively model the 3D self-attention of video data has been a great challenge for transformers. In this paper, we propose a Temporal Patch Shift (TPS) method for efficient 3D self-attention modeling in transformers for video-based action recognition. TPS shifts part of patches with a specific mosaic pattern in the temporal dimension, thus converting a vanilla spatial self-attention operation to a spatiotemporal one with little additional cost. As a result, we can compute 3D self-attention using nearly the same computation and memory cost as 2D self-attention. TPS is a plug-and-play module and can be inserted into existing 2D transformer models to enhance spatiotemporal feature learning. The proposed method achieves competitive performance with state-of-the-arts on Something-something V1 & V2, Diving-48, and Kinetics400 while being much more efficient on computation and memory cost. The source code of TPS can be found at https://github.com/MartinXM/TPS.
PB  - arXiv
PY  - 2022
ST  - Spatiotemporal Self-attention Modeling with Temporal Patch Shift for Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-20062-5_36
ER  -


TY  - GEN
AU  - Alafif, T.
AU  - Allahyani, M.
AU  - Alhothali, A.
AU  - Alotaibi, R.
AU  - Barnawi, A.
TI  - HYBRID CLASSIFIERS FOR SPATIO-TEMPORAL REAL-TIME ABNORMAL BEHAVIORS DETECTION, TRACKING, AND RECOGNITION IN MASSIVE HAJJ CROWDS
AB  - Individual abnormal behaviors vary depending on crowd sizes, contexts, and scenes. Challenges such as partial occlusions, blurring, large-number abnormal behavior, and camera viewing occur in large-scale crowds when detecting, tracking, and recognizing individuals with abnormal behaviors. In this paper, our contribution is twofold. First, we introduce an annotated and labeled large-scale crowd abnormal behaviors Hajj dataset (HAJJv2). Second, we propose two methods of hybrid Convolutional Neural Networks (CNNs) and Random Forests (RFs) to detect and recognize Spatio-temporal abnormal behaviors in small and large-scales crowd videos. In small-scale crowd videos, a ResNet-50 pre-trained CNN model is fine-tuned to verify whether every frame is normal or abnormal in the spatial domain. If anomalous behaviors are observed, a motion-based individuals detection method based on the magnitudes and orientations of Horn-Schunck optical flow is used to locate and track individuals with abnormal behaviors. A Kalman filter is employed in large-scale crowd videos to predict and track the detected individuals in the subsequent frames. Then, means, variances, and standard deviations statistical features are computed and fed to the RF to classify individuals with abnormal behaviors in the temporal domain. In large-scale crowds, we fine-tune the ResNet-50 model using YOLOv2 object detection technique to detect individuals with abnormal behaviors in the spatial domain. The proposed method achieves 99.77% and 93.71% of average Area Under the Curves (AUCs) on two public benchmark small-scale crowd datasets, UMN and UCSD, respectively. While the large-scale crowd method achieves 76.08% average AUC using the HAJJv2 dataset. Our method outperforms state-of-the-art methods using the small-scale crowd datasets with a margin of 1.67%, 6.06%, and 2.85% on UMN, UCSD Ped1, and UCSD Ped2, respectively. It also achieves a satisfactory result in the large-scale crowds.
PB  - arXiv
PY  - 2022
ST  - HYBRID CLASSIFIERS FOR SPATIO-TEMPORAL REAL-TIME ABNORMAL BEHAVIORS DETECTION, TRACKING, AND RECOGNITION IN MASSIVE HAJJ CROWDS
Y2  - 2025/05/05/21:54:31
DO  - 10.3390/electronics12051165
ER  -


TY  - GEN
AU  - Li, Z.
AU  - He, L.
AU  - Xu, H.
TI  - Weakly-Supervised Temporal Action Detection for Fine-Grained Videos with Hierarchical Atomic Actions
AB  - Action understanding has evolved into the era of fine granularity, as most human behaviors in real life have only minor differences. To detect these fine-grained actions accurately in a label-efficient way, we tackle the problem of weakly-supervised fine-grained temporal action detection in videos for the first time. Without the careful design to capture subtle differences between fine-grained actions, previous weakly-supervised models for general action detection cannot perform well in the fine-grained setting. We propose to model actions as the combinations of reusable atomic actions which are automatically discovered from data through self-supervised clustering, in order to capture the commonality and individuality of fine-grained actions. The learnt atomic actions, represented by visual concepts, are further mapped to fine and coarse action labels leveraging the semantic label hierarchy. Our approach constructs a visual representation hierarchy of four levels: clip level, atomic action level, fine action class level and coarse action class level, with supervision at each level. Extensive experiments on two large-scale fine-grained video datasets, FineAction and FineGym, show the benefit of our proposed weakly-supervised model for fine-grained action detection, and it achieves state-of-the-art results.
PB  - arXiv
PY  - 2022
ST  - Weakly-Supervised Temporal Action Detection for Fine-Grained Videos with Hierarchical Atomic Actions
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-20080-9_33
ER  -


TY  - GEN
AU  - Eslambolchilar, P.
AU  - Stawarz, K.
AU  - Dias, N.V.
AU  - Knowles, Z.
AU  - Mackintosh, K.A.
TI  - Tangible Data Visualization of Physical Activity for Children and Adolescents: A Qualitative Study of Temporal Transition of Experiences
AB  - Children and adolescents in the UK are increasingly at risk of significant health problems due to physical inactivity. While activity trackers and fitness applications have focused on addressing this problem in youth, poor wear-time compliance and usability and accessibility issues have been frequently reported in the literature as barriers to engagement. Physicalization of data offers an alternative approach to engage with physical activity (PA). In this paper, we present the results of a seven-week qualitative study with 97 primary and secondary school children (8-14 years old). We took a temporal approach to collect children’s and adolescents’ perspectives in short video interviews as they received 3D-printed models representing their faded-weekly PA levels. Our findings showed that children’s and adolescents’ emotional engagement with the models remained high throughout the study, while their reflection on the models and their knowledge of what constitutes PA and its different types evolved over time. The findings from this temporal study suggest that tangible data visualization of PA evokes a number of experiences such as embodied reflection, active learning, emotions, and temporality of PA experience. Therefore, we argue that the motivational impact of regular tangible visualizations as a form of feedback should be considered alongside wearable trackers in addressing childhood inactivity.
PB  - SSRN
PY  - 2022
ST  - Tangible Data Visualization of Physical Activity for Children and Adolescents
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.ijcci.2023.100565
ER  -


TY  - GEN
AU  - Li, C.
AU  - Li, S.
AU  - Gao, Y.
AU  - Zhou, L.
AU  - Li, W.
TI  - Static Graph Convolution with Temporal and Channel-Wise Graph Topology Generation for Skeleton-Based Action Recognition
AB  - Graph convolutional networks (GCNs) are widely used in skeleton based action recognition. It is known that the graph topology is a vital part in GCNs, and different kinds of graph topologies have been proposed for skeleton based action recognition, mostly based on a predefined topology and a dynamically learned one. The predefined topology is based on the human intuition for skeleton (the connectivity of joints) and has not been investigated whether it is optimal. In this paper, we focus on investigating this static graph topology and propose to generate a learned static graph topology for skeleton. To be specific, a temporal frame-wise and channel-wise topology based GCNs (TC-GCNs) are developed, where, instead of using a predefined topology by human, a topology is learned for skeleton based action recognition. The TC-GCNs consist of generating a temporal frame-wise topology and a channel-wise topology to formulate the relationship of skeleton joints in the temporal dimension and channel dimension, respectively. The proposed method can be integrated with the conventional dynamic topology by replacing the predefined graph topology with our generated one. Experimental results show that our method achieves the state-of-the-art performance on two widely used benchmarks, namely the NTU-RGB+D, NTU-RGB+D 120 and UAV-Human.
PB  - SSRN
PY  - 2022
ST  - Static Graph Convolution with Temporal and Channel-Wise Graph Topology Generation for Skeleton-Based Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4170495
ER  -


TY  - GEN
AU  - Cao, W.
AU  - Wen, Z.
AU  - Su, H.
TI  - Spatiotemporal Clustering Analysis and Zonal Prediction Model for Deformation Behavior of Super-High Arch Dams
AB  - Super-high arch dams are affected by similar environmental factors, and there is some spatial and temporal correlation among the deformation measurement points, while the deformation pattern of different parts of the arch dam varies. Using a single-point model for the deformation points or a single spatiotemporal model for the whole arch dam may ignore the spatiotemporal correlation of the measurement points or not consider the spatiotemporal variability between regions. In this study, a novel analytical model for multi-point deformation monitoring of super-arch dams is proposed. By constructing deformation similarity characteristics to characterize the deformation similarity degree, then using the clustering by fast search and find of density peaks method (CFSFDP) to obtain the measurement point regions with similar deformation laws, the deformation laws of different deformation regions are analyzed. Then, based on the spatiotemporal integrated autoregressive moving average model (STARIMA), a spatiotemporal deformation analysis model is constructed for the deformation of super-high arch dams. Through example analysis, the fitting and prediction performance of the model is better than that of the single measurement point model and the spatiotemporal hybrid model, which provides an efficient and convenient new method for deformation monitoring and analysis of ultra-high arch dams.
PB  - SSRN
PY  - 2022
ST  - Spatiotemporal Clustering Analysis and Zonal Prediction Model for Deformation Behavior of Super-High Arch Dams
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4170149
ER  -


TY  - GEN
AU  - Taube, J.C.
AU  - Susswein, Z.
AU  - Bansal, S.
TI  - Spatiotemporal trends in self-reported mask-wearing behavior in the United States: Analysis of a large cross-sectional survey
AB  - Background Face mask-wearing has been identified as an effective strategy to prevent transmission of SARS-CoV-2, yet mask mandates were never imposed nationally in the United States. This decision resulted in a patchwork of local policies and varying compliance potentially generating heterogeneities in the local trajectories of COVID-19 in the U.S. While numerous studies have investigated patterns and predictors of masking behavior nationally, most suffer from survey biases and none have been able to characterize mask-wearing at fine spatial scales across the U.S. through different phases of the pandemic. Objective Urgently needed is a debiased spatiotemporal characterization of mask-wearing behavior in the U.S. This information is critical to further assess the effectiveness of masking, evaluate drivers of transmission at different time points during the pandemic, and guide future public health decisions through, for example, forecasting disease surges. Methods We analyze spatiotemporal masking patterns in over eight million behavioral survey responses from across the United States starting in September 2020 through May 2021. We adjust for sample size and representation using binomial regression models and survey raking, respectively, to produce county-level monthly estimates of masking behavior. We additionally debias self-reported masking estimates using bias measures derived by comparing vaccination data from the same survey to official records at the county-level. Lastly, we evaluate whether individuals’ perceptions of their social environment can serve as a less biased form of behavioral surveillance than self-reported data. Results We find that county-level masking behavior is spatially heterogeneous along an urban-rural gradient, with mask-wearing peaking in winter 2021 and declining sharply through May 2021. Our results identify regions where targeted public health efforts could have been most effective and suggest that individuals’ frequency of mask-wearing may be influenced by national guidance and disease prevalence. We validate our bias-correction approach by comparing debiased self-reported mask-wearing estimates with community-reported estimates, after addressing issues of small sample size and representation. Self-reported behavior estimates are especially prone to social desirability and non-response biases and our findings demonstrate that these biases can be reduced if individuals are asked to report on community rather than self behaviors. Conclusions Our work highlights the importance of characterizing public health behaviors at fine spatiotemporal scales to capture heterogeneities that may drive outbreak trajectories. Our findings also emphasize the need for a standardized approach to incorporating behavioral big data into public health response efforts. Even large surveys are prone to bias; thus, we advocate for a social sensing approach to behavioral surveillance to enable more accurate estimates of health behaviors. Finally, we invite the public health and behavioral research communities to use our publicly available estimates to consider how bias-corrected behavioral estimates may improve our understanding of protective behaviors during crises and their impact on disease dynamics.
PB  - medRxiv
PY  - 2022
ST  - Spatiotemporal trends in self-reported mask-wearing behavior in the United States
Y2  - 2025/05/05/21:54:31
DO  - 10.2196/42128
ER  -


TY  - GEN
AU  - Weng, Y.
AU  - Pan, Z.
AU  - Han, M.
AU  - Chang, X.
AU  - Zhuang, B.
TI  - An Efficient Spatio-Temporal Pyramid Transformer for Action Detection
AB  - The task of action detection aims at deducing both the action category and localization of the start and end moment for each action instance in a long, untrimmed video. While vision Transformers have driven the recent advances in video understanding, it is non-trivial to design an efficient architecture for action detection due to the prohibitively expensive self-attentions over a long sequence of video clips. To this end, we present an efficient hierarchical Spatio-Temporal Pyramid Transformer (STPT) for action detection, building upon the fact that the early self-attention layers in Transformers still focus on local patterns. Specifically, we propose to use local window attention to encode rich local spatio-temporal representations in the early stages while applying global attention modules to capture long-term space-time dependencies in the later stages. In this way, our STPT can encode both locality and dependency with largely reduced redundancy, delivering a promising trade-off between accuracy and efficiency. For example, with only RGB input, the proposed STPT achieves 53.6% mAP on THUMOS14, surpassing I3D+AFSD RGB model by over 10% and performing favorably against state-of-the-art AFSD that uses additional flow features with 31% fewer GFLOPs, which serves as an effective and efficient end-to-end Transformer-based framework for action detection.
PB  - arXiv
PY  - 2022
ST  - An Efficient Spatio-Temporal Pyramid Transformer for Action Detection
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-19830-4_21
ER  -


TY  - GEN
AU  - Peng, J.
AU  - Xing, L.
AU  - Ruan, Z.
AU  - Yang, H.
TI  - Spatio-Temporal Influence of Station-Level Built Environment on Elderly's Metro Travel Behavior
AB  - Although many studies have investigated the effects of built environment on residents' metro travel behavior, few studies have focused on the special group of elderly people and less have considered the effects of spatial interactions over time. Based on six consecutive years of metro smart card data of elderly people in Wuhan, a spatial panel Durbin model was used to estimate the spatial and temporal effects of built environment on elderly people's metro travel behavior. The results show that the built environment and the characteristics of subway stations have significant direct and indirect effects on the elderly passenger flow on weekends and weekdays. These findings help to better understand the characteristics of elderly people's metro travel behavior and the built environment factors that influenced them, and are important for the development of age-friendly metro travel in the context of population aging.
PB  - SSRN
PY  - 2022
ST  - Spatio-Temporal Influence of Station-Level Built Environment on Elderly's Metro Travel Behavior
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4167537
ER  -


TY  - GEN
AU  - Liu, H.
AU  - See, J.
AU  - Lv, W.
AU  - Lin, W.
TI  - Task-adaptive Spatial-Temporal Video Sampler for Few-shot Action Recognition
AB  - A primary challenge faced in few-shot action recognition is inadequate video data for training. To address this issue, current methods in this field mainly focus on devising algorithms at the feature level while little attention is paid to processing input video data. Moreover, existing frame sampling strategies may omit critical action information in temporal and spatial dimensions, which further impacts video utilization efficiency. In this paper, we propose a novel video frame sampler for few-shot action recognition to address this issue, where task-specific spatial-temporal frame sampling is achieved via a temporal selector (TS) and a spatial amplifier (SA). Specifically, our sampler first scans the whole video at a small computational cost to obtain a global perception of video frames. The TS plays its role in selecting top-T frames that contribute most significantly and subsequently. The SA emphasizes the discriminative information of each frame by amplifying critical regions with the guidance of saliency maps. We further adopt task-adaptive learning to dynamically adjust the sampling strategy according to the episode task at hand. Both the implementations of TS and SA are differentiable for end-to-end optimization, facilitating seamless integration of our proposed sampler with most few-shot action recognition methods. Extensive experiments show a significant boost in the performances on various benchmarks including long-term videos. The code is available at https://github.com/R00Kie-Liu/Sampler.
PB  - arXiv
PY  - 2022
ST  - Task-adaptive Spatial-Temporal Video Sampler for Few-shot Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3503161.3547938
ER  -


TY  - GEN
AU  - Kang, T.-K.
AU  - Lee, G.-H.
AU  - Lee, S.-W.
TI  - HTNet: Anchor-free Temporal Action Localization with Hierarchical Transformers
AB  - Temporal action localization (TAL) is a task of identifying a set of actions in a video, which involves localizing the start and end frames and classifying each action instance. Existing methods have addressed this task by using predefined anchor windows or heuristic bottom-up boundary-matching strategies, which are major bottlenecks in inference time. Additionally, the main challenge is the inability to capture long-range actions due to a lack of global contextual information. In this paper, we present a novel anchor-free framework, referred to as HTNet, which predicts a set of hstart time, end time, classi triplets from a video based on a Transformer architecture. After the prediction of coarse boundaries, we refine it through a background feature sampling (BFS) module and hierarchical Transformers, which enables our model to aggregate global contextual information and effectively exploit the inherent semantic relationships in a video. We demonstrate how our method localizes accurate action instances and achieves state-of-the-art performance on two TAL benchmark datasets: THUMOS14 and ActivityNet 1.3. MSC Codes 68T45
PB  - arXiv
PY  - 2022
ST  - HTNet
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/smc53654.2022.9945289
ER  -


TY  - GEN
AU  - Xu, L.
AU  - Wang, Q.
AU  - Lin, X.
AU  - Yuan, L.
TI  - AN EFFICIENT FRAMEWORK FOR FEW-SHOT SKELETON-BASED TEMPORAL ACTION SEGMENTATION
AB  - Temporal action segmentation (TAS) aims to classify and locate actions in the long untrimmed action sequence. With the success of deep learning, many deep models for action segmentation have emerged. However, few-shot TAS is still a challenging problem. This study proposes an efficient framework for the few-shot skeleton-based TAS, including a data augmentation method and an improved model. The data augmentation approach based on motion interpolation is presented here to solve the problem of insufficient data, and can increase the number of samples significantly by synthesizing action sequences. Besides, we concatenate a Connectionist Temporal Classification (CTC) layer with a network designed for skeleton-based TAS to obtain an optimized model. Leveraging CTC can enhance the temporal alignment between prediction and ground truth and further improve the segment-wise metrics of segmentation results. Extensive experiments on both public and self-constructed datasets, including two small-scale datasets and one large-scale dataset, show the effectiveness of two proposed methods in improving the performance of the few-shot skeleton-based TAS task.
PB  - arXiv
PY  - 2022
ST  - AN EFFICIENT FRAMEWORK FOR FEW-SHOT SKELETON-BASED TEMPORAL ACTION SEGMENTATION
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.cviu.2023.103707
ER  -


TY  - GEN
AU  - Rahaman, R.
AU  - Singhania, D.
AU  - Thiery, A.
AU  - Yao, A.
TI  - A Generalized & Robust Framework For Timestamp Supervision in Temporal Action Segmentation
AB  - In temporal action segmentation, Timestamp Supervision requires only a handful of labelled frames per video sequence. For unlabelled frames, previous works rely on assigning hard labels, and performance rapidly collapses under subtle violations of the annotation assumptions. We propose a novel Expectation-Maximization (EM) based approach that leverages the label uncertainty of unlabelled frames and is robust enough to accommodate possible annotation errors. With accurate timestamp annotations, our proposed method produces SOTA results and even exceeds the fully-supervised setup in several metrics and datasets. When applied to timestamp annotations with missing action segments, our method presents stable performance. To further test our formulation's robustness, we introduce the new challenging annotation setup of SkipTag Supervision. This setup relaxes constraints and requires annotations of any fixed number of random frames in a video, making it more flexible than Timestamp Supervision while remaining competitive.
PB  - arXiv
PY  - 2022
ST  - A Generalized & Robust Framework For Timestamp Supervision in Temporal Action Segmentation
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-19772-7_17
ER  -


TY  - GEN
AU  - Bai, Y.
AU  - Zhou, D.
AU  - Zhang, S.
AU  - Long, Y.
AU  - Wang, J.
TI  - Action Quality Assessment with Temporal Parsing Transformer
AB  - Action Quality Assessment(AQA) is important for action understanding and resolving the task poses unique challenges due to subtle visual differences. Existing state-of-the-art methods typically rely on the holistic video representations for score regression or ranking, which limits the generalization to capture fine-grained intra-class variation. To overcome the above limitation, we propose a temporal parsing transformer to decompose the holistic feature into temporal part-level representations. Specifically, we utilize a set of learnable queries to represent the atomic temporal patterns for a specific action. Our decoding process converts the frame representations to a fixed number of temporally ordered part representations. To obtain the quality score, we adopt the state-of-the-art contrastive regression based on the part representations. Since existing AQA datasets do not provide temporal part-level labels or partitions, we propose two novel loss functions on the cross attention responses of the decoder: a ranking loss to ensure the learnable queries to satisfy the temporal order in cross attention and a sparsity loss to encourage the part representations to be more discriminative. Extensive experiments show that our proposed method outperforms prior work on three public AQA benchmarks by a considerable margin.
PB  - arXiv
PY  - 2022
ST  - Action Quality Assessment with Temporal Parsing Transformer
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-19772-7_25
ER  -


TY  - GEN
AU  - Ding, G.
AU  - Yao, A.
TI  - Leveraging Action Affinity and Continuity for Semi-supervised Temporal Action Segmentation
AB  - We present a semi-supervised learning approach to the temporal action segmentation task. The goal of the task is to temporally detect and segment actions in long, untrimmed procedural videos, where only a small set of videos are densely labelled, and a large collection of videos are unlabelled. To this end, we propose two novel loss functions for the unlabelled data: an action affinity loss and an action continuity loss. The action affinity loss guides the unlabelled samples learning by imposing the action priors induced from the labelled set. Action continuity loss enforces the temporal continuity of actions, which also provides frame-wise classification supervision. In addition, we propose an Adaptive Boundary Smoothing (ABS) approach to build coarser action boundaries for more robust and reliable learning. The proposed loss functions and ABS were evaluated on three benchmarks. Results show that they significantly improved action segmentation performance with a low amount (5% and 10%) of labelled data and achieved comparable results to full supervision with 50% labelled data. Furthermore, ABS succeeded in boosting performance when integrated into fully-supervised learning.
PB  - arXiv
PY  - 2022
ST  - Leveraging Action Affinity and Continuity for Semi-supervised Temporal Action Segmentation
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-19833-5_2
ER  -


TY  - GEN
AU  - Tang, Y.
AU  - Liu, X.
AU  - Yu, X.
AU  - Lu, J.
AU  - Zhou, J.
TI  - Learning from Temporal Spatial Cubism for Cross-Dataset Skeleton-based Action Recognition
AB  - Rapid progress and superior performance have been achieved for skeleton-based action recognition recently. In this paper, we investigate this problem under a cross-dataset setting, which is a new, pragmatic and challenging task in real-world scenario. Following the unsupervised domain adaptation (UDA) paradigm, the action labels are only available on a source dataset, but unavailable on a target dataset in the training stage. Different from the conventional adversarial learning based approaches for UDA, we utilize a self-supervision scheme to reduce the domain shift between two skeleton-based action datasets. Our inspiration is drawn from Cubism, an art genre from the early 20th century, which breaks and reassembles the objects to convey a greater context. By segmenting and permuting temporal segments or human body parts, we design two self-supervised learning classification tasks to explore the temporal and spatial dependency of a skeleton-based action and improve the generalization ability of the model. We conduct experiments on six datasets for skeleton-based action recognition, including three large-scale datasets (NTU RGB+D, PKU-MMD and Kinetics) where new cross-dataset settings and benchmarks are established. Extensive results demonstrate that our method outperforms state-of-the-art approaches. The source codes of our model and all the compared methods are available at https://github.com/shanice-l/st-cubism.
PB  - arXiv
PY  - 2022
ST  - Learning from Temporal Spatial Cubism for Cross-Dataset Skeleton-based Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3472722
ER  -


TY  - GEN
AU  - Nag, S.
AU  - Zhu, X.
AU  - Song, Y.-Z.
AU  - Xiang, T.
TI  - Zero-Shot Temporal Action Detection via Vision-Language Prompting
AB  - Existing temporal action detection (TAD) methods rely on large training data including segment-level annotations, limited to recognizing previously seen classes alone during inference. Collecting and annotating a large training set for each class of interest is costly and hence unscalable. Zero-shot TAD (ZS-TAD) resolves this obstacle by enabling a pre-trained model to recognize any unseen action classes. Meanwhile, ZS-TAD is also much more challenging with significantly less investigation. Inspired by the success of zero-shot image classification aided by vision-language (ViL) models such as CLIP, we aim to tackle the more complex TAD task. An intuitive method is to integrate an off-the-shelf proposal detector with CLIP style classification. However, due to the sequential localization (e.g., proposal generation) and classification design, it is prone to localization error propagation. To overcome this problem, in this paper we propose a novel zero-Shot Temporal Action detection model via Vision-LanguagE prompting (STALE). Such a novel design effectively eliminates the dependence between localization and classification by breaking the route for error propagation in-between. We further introduce an interaction mechanism between classification and localization for improved optimization. Extensive experiments on standard ZS-TAD video benchmarks show that our STALE significantly outperforms state-of-the-art alternatives. Besides, our model also yields superior results on supervised TAD over recent strong competitors. The PyTorch implementation of STALE is available on https://github.com/sauradip/STALE.
PB  - arXiv
PY  - 2022
ST  - Zero-Shot Temporal Action Detection via Vision-Language Prompting
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-20062-5_39
ER  -


TY  - GEN
AU  - Nag, S.
AU  - Zhu, X.
AU  - Song, Y.-Z.
AU  - Xiang, T.
TI  - Semi-Supervised Temporal Action Detection with Proposal-Free Masking
AB  - Existing temporal action detection (TAD) methods rely on a large number of training data with segment-level annotations. Collecting and annotating such a training set is thus highly expensive and unscalable. Semi-supervised TAD (SS-TAD) alleviates this problem by leveraging unlabeled videos freely available at scale. However, SS-TAD is also a much more challenging problem than supervised TAD, and consequently much under-studied. Prior SS-TAD methods directly combine an existing proposal-based TAD method and a SSL method. Due to their sequential localization (e.g., proposal generation) and classification design, they are prone to proposal error propagation. To overcome this limitation, in this work we propose a novel Semi-supervised Temporal action detection model based on PropOsal-free Temporal mask (SPOT) with a parallel localization (mask generation) and classification architecture. Such a novel design effectively eliminates the dependence between localization and classification by cutting off the route for error propagation in-between. We further introduce an interaction mechanism between classification and localization for prediction refinement, and a new pretext task for self-supervised model pre-training. Extensive experiments on two standard benchmarks show that our SPOT outperforms state-of-the-art alternatives, often by a large margin. The PyTorch implementation of SPOT is available at https://github.com/sauradip/SPOT
PB  - arXiv
PY  - 2022
ST  - Semi-Supervised Temporal Action Detection with Proposal-Free Masking
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-20062-5_38
ER  -


TY  - GEN
AU  - Shi, D.
AU  - Zhong, Y.
AU  - Cao, Q.
AU  - Li, J.
AU  - Tao, D.
TI  - ReAct: Temporal Action Detection with Relational Queries
AB  - This work aims at advancing temporal action detection (TAD) using an encoder-decoder framework with action queries, similar to DETR, which has shown great success in object detection. However, the framework suffers from several problems if directly applied to TAD: the insufficient exploration of inter-query relation in the decoder, the inadequate classification training due to a limited number of training samples, and the unreliable classification scores at inference. To this end, we first propose a relational attention mechanism in the decoder, which guides the attention among queries based on their relations. Moreover, we propose two losses to facilitate and stabilize the training of action classification. Lastly, we propose to predict the localization quality of each action query at inference in order to distinguish high-quality queries. The proposed method, named ReAct, achieves the state-of-the-art performance on THUMOS14, with much lower computational costs than previous methods. Besides, extensive ablation studies are conducted to verify the effectiveness of each proposed component. The code is available at https://github.com/sssste/React.
PB  - arXiv
PY  - 2022
ST  - ReAct
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-20080-9_7
ER  -


TY  - GEN
AU  - Li, Z.
AU  - Yu, J.
AU  - Ge, Y.
AU  - Chen, Z.
TI  - Forcing the Whole Video as Background: An Adversarial Learning Strategy for Weakly Temporal Action Localization
AB  - With video-level labels, weakly supervised temporal action localization (WTAL) applies a localization-by-classification paradigm to detect and classify the action in untrimmed videos. Due to the characteristic of classification, class-specific background snippets are inevitably mis-activated to improve the discriminability of the classifier in WTAL. To alleviate the disturbance of background, existing methods try to enlarge the discrepancy between action and background through modeling background snippets with pseudo-snippet-level annotations, which largely rely on artificial hypotheticals. Distinct from the previous works, we present an adversarial learning strategy to break the limitation of mining pseudo background snippets. Concretely, the background classification loss forces the whole video to be regarded as the background by a background gradient reinforcement strategy, confusing the recognition model. Reversely, the foreground(action) loss guides the model to focus on action snippets under such conditions. As a result, competition between the two classification losses drives the model to boost its ability for action modeling. Simultaneously, a novel temporal enhancement network is designed to facilitate the model to construct temporal relation of affinity snippets based on the proposed strategy, for further improving the performance of action localization. Finally, extensive experiments conducted on THUMOS14 and ActivityNet1.2 demonstrate the effectiveness of the proposed method.
PB  - arXiv
PY  - 2022
ST  - Forcing the Whole Video as Background
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3503161.3548300
ER  -


TY  - GEN
AU  - Nag, S.
AU  - Zhu, X.
AU  - Song, Y.-Z.
AU  - Xiang, T.
TI  - Proposal-Free Temporal Action Detection via Global Segmentation Mask Learning
AB  - Existing temporal action detection (TAD) methods rely on generating an overwhelmingly large number of proposals per video. This leads to complex model designs due to proposal generation and/or per-proposal action instance evaluation and the resultant high computational cost. In this work, for the first time, we propose a proposal-free Temporal Action detection model via Global Segmentation mask (TAGS). Our core idea is to learn a global segmentation mask of each action instance jointly at the full video length. The TAGS model differs significantly from the conventional proposal-based methods by focusing on global temporal representation learning to directly detect local start and end points of action instances without proposals. Further, by modeling TAD holistically rather than locally at the individual proposal level, TAGS needs a much simpler model architecture with lower computational cost. Extensive experiments show that despite its simpler design, TAGS outperforms existing TAD methods, achieving new state-of-the-art performance on two benchmarks. Importantly, it is ∼ 20× faster to train and ∼ 1.6× more efficient for inference. Our PyTorch implementation of TAGS is available at https://github.com/sauradip/TAGS.
PB  - arXiv
PY  - 2022
ST  - Proposal-Free Temporal Action Detection via Global Segmentation Mask Learning
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-20062-5_37
ER  -


TY  - GEN
AU  - Yang, Y.
AU  - Liang, Y.
AU  - Huang, C.
AU  - Xia, L.
AU  - Li, C.
TI  - Multi-Behavior Hypergraph-Enhanced Transformer for Sequential Recommendation
AB  - Learning dynamic user preference has become an increasingly important component for many online platforms (e.g., video-sharing sites, e-commerce systems) to make sequential recommendations. Previous works have made many efforts to model item-item transitions over user interaction sequences, based on various architectures, e.g., recurrent neural networks and self-attention mechanism. Recently emerged graph neural networks also serve as useful backbone models to capture item dependencies in sequential recommendation scenarios. Despite their effectiveness, existing methods have far focused on item sequence representation with singular type of interactions, and thus are limited to capture dynamic heterogeneous relational structures between users and items (e.g., page view, add-to-favorite, purchase). To tackle this challenge, we design a MultiBehavior Hypergraph-enhanced Transformer framework (MBHT) to capture both short-term and long-term cross-type behavior dependencies. Specifically, a multi-scale Transformer is equipped with low-rank self-attention to jointly encode behavior-aware sequential patterns from fine-grained and coarse-grained levels. Additionally, we incorporate the global multi-behavior dependency into the hypergraph neural architecture to capture the hierarchical long-range item correlations in a customized manner. Experimental results demonstrate the superiority of our MBHT over various state-of-the-art recommendation solutions across different settings. Further ablation studies validate the effectiveness of our model design and benefits of the new MBHT framework. Our implementation code is released at: https://github.com/yuh-yang/MBHT-KDD22.
PB  - arXiv
PY  - 2022
ST  - Multi-Behavior Hypergraph-Enhanced Transformer for Sequential Recommendation
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3534678.3539342
ER  -


TY  - GEN
AU  - Mac, K.-N.C.
AU  - Do, M.N.
AU  - Vo, M.P.
TI  - Efficient Human Vision Inspired Action Recognition using Adaptive Spatiotemporal Sampling
AB  - Adaptive sampling that exploits the spatiotemporal redundancy in videos is critical for always-on action recognition on wearable devices with limited computing and battery resources. The commonly used fixed sampling strategy is not context-aware and may under-sample the visual content, and thus adversely impacts both computation efficiency and accuracy. Inspired by the concepts of foveal vision and pre-attentive processing from the human visual perception mechanism, we introduce a novel adaptive spatiotemporal sampling scheme for efficient action recognition. Our system pre-scans the global scene context at low-resolution and decides to skip or request high-resolution features at salient regions for further processing. We validate the system on EPIC-KITCHENS and UCF-101 datasets for action recognition, and show that our proposed approach can greatly speed up inference with a tolerable loss of accuracy compared with those from state-of-the-art baselines. Source code is available in https://github.com/knmac/adaptive_spatiotemporal.
PB  - arXiv
PY  - 2022
ST  - Efficient Human Vision Inspired Action Recognition using Adaptive Spatiotemporal Sampling
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tip.2023.3310661
ER  -


TY  - GEN
AU  - Dedhia, B.
AU  - Balasubramanian, R.
AU  - Jha, N.K.
TI  - SCouT: Synthetic Counterfactuals via Spatiotemporal Transformers for Actionable Healthcare
AB  - The Synthetic Control method has pioneered a class of powerful data-driven techniques to estimate the counterfactual reality of a unit from donor units. At its core, the technique involves a linear model fitted on the pre-intervention period that combines donor outcomes to yield the counterfactual. However, linearly combining spatial information at each time instance using time-agnostic weights fails to capture important inter-unit and intra-unit temporal contexts and complex nonlinear dynamics of real data. We instead propose an approach to use local spatiotemporal information before the onset of the intervention as a promising way to estimate the counterfactual sequence. To this end, we suggest a Transformer model that leverages particular positional embeddings, a modified decoder attention mask, and a novel pre-training task to perform spatiotemporal sequence-to-sequence modeling. Our experiments on synthetic data demonstrate the efficacy of our method in the typical small donor pool setting and its robustness against noise. We also generate actionable healthcare insights at the population and patient levels by simulating a state-wide public health policy to evaluate its effectiveness, an in silico trial for asthma medications to support randomized controlled trials, and a medical intervention for patients with Friedreich's ataxia to improve clinical decision-making and promote personalized therapy.
PB  - arXiv
PY  - 2022
ST  - SCouT
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3617180
ER  -


TY  - GEN
AU  - Wilson, G.
AU  - Doppa, J.R.
AU  - Cook, D.J.
TI  - Domain Adaptation Under Behavioral and Temporal Shifts for Natural Time Series Mobile Activity Recognition
AB  - Increasingly, human behavior is captured on mobile devices, leading to an increased interest in automated human activity recognition. However, existing datasets typically consist of scripted movements. Our long-term goal is to perform mobile activity recognition in natural settings. We collect a dataset to support this goal with activity categories that are relevant for downstream tasks such as health monitoring and intervention. Because of the large variations present in human behavior, we collect data from many participants across two different age groups. Because human behavior can change over time, we also collect data from participants over a month's time to capture the temporal drift. We hypothesize that mobile activity recognition can benefit from unsupervised domain adaptation algorithms. To address this need and test this hypothesis, we analyze the performance of domain adaptation across people and across time. We then enhance unsupervised domain adaptation with contrastive learning and with weak supervision when label proportions are available.
PB  - arXiv
PY  - 2022
ST  - Domain Adaptation Under Behavioral and Temporal Shifts for Natural Time Series Mobile Activity Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/igarss.2011.6049995
ER  -


TY  - GEN
AU  - Chen, Z.
AU  - Liu, H.
AU  - Guo, T.
AU  - Song, P.
AU  - Tang, H.
TI  - Contrastive Learning from Spatio-Temporal Mixed Skeleton Sequences for Self-Supervised Skeleton-Based Action Recognition
AB  - Self-supervised skeleton-based action recognition with contrastive learning has attracted much attention. Recent literature shows that data augmentation and large sets of contrastive pairs are crucial in learning such representations. In this paper, we found that directly extending contrastive pairs based on normal augmentations brings limited returns in terms of performance, because the contribution of contrastive pairs from the normal data augmentation to the loss get smaller as training progresses. Therefore, we delve into hard contrastive pairs for contrastive learning. Motivated by the success of mixing augmentation strategy which improves the performance of many tasks by synthesizing novel samples, we propose SkeleMixCLR: a contrastive learning framework with a spatio-temporal skeleton mixing augmentation (SkeleMix) to complement current contrastive learning approaches by providing hard contrastive samples. First, SkeleMix utilizes the topological information of skeleton data to mix two skeleton sequences by randomly combing the cropped skeleton fragments (the trimmed view) with the remaining skeleton sequences (the truncated view). Second, a spatio-temporal mask pooling is applied to separate these two views at the feature level. Third, we extend contrastive pairs with these two views. SkeleMixCLR leverages the trimmed and truncated views to provide abundant hard contrastive pairs since they involve some context information from each other due to the graph convolution operations, which allows the model to learn better motion representations for action recognition. Extensive experiments on NTU-RGB+D, NTU120-RGB+D, and PKU-MMD datasets show that SkeleMixCLR achieves state-of-the-art performance. Codes are available at https://github.com/ czhaneva/SkeleMixCLR.
PB  - arXiv
PY  - 2022
ST  - Contrastive Learning from Spatio-Temporal Mixed Skeleton Sequences for Self-Supervised Skeleton-Based Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v37i3.25495
ER  -


TY  - GEN
AU  - Lohse, A.
AU  - Løkkegaard, A.
AU  - Siebner, H.R.
AU  - Meder, D.
TI  - Linking impulsivity to activity levels in pre-supplementary motor area during sequential gambling
AB  - Impulsivity refers to the tendency to act prematurely or without forethought, and excessive impulsivity is a key problem in many neuropsychiatric disorders. Since the pre-supplementary motor area (preSMA) has been implicated in inhibitory control, this region may also contribute to impulsivity. Here, we examined whether functional recruitment of preSMA may contribute to risky choice behavior (state impulsivity) during sequential gambling and its relation to self-reported trait impulsivity. To this end, we performed task-based functional MRI (fMRI) after low-frequency (1 Hz) repetitive transcranial magnetic stimulation (rTMS) of the preSMA. We expected low-frequency rTMS to modulate task-related engagement of the preSMA and hereby, tune the tendency to make risky choices. 24 healthy volunteers (12 females, 19-52 years) received real or sham rTMS on separate days in counterbalanced order. Thereafter, participants performed a sequential gambling task with concurrently increasing stakes and risk during whole-brain fMRI. In the sham-rTMS session, self-reported trait impulsivity scaled positively with state impulsivity (riskier choice behavior) during gambling. The higher the trait-impulsivity, the lower was the task-related increase in preSMA activity with increasingly risky choices. Following real-rTMS, low-impulsivity participants increased their preference for risky choices, while the opposite was true for high-impulsivity participants resulting in an overall decoupling of trait impulsivity and state impulsivity during gambling. This rTMS-induced behavioral shift was mirrored in the rTMS-induced change in preSMA activation. These results provide converging evidence for a causal link between the level of task-related preSMA activity and the propensity for impulsive risk-taking behavior in the context of sequential gambling.
PB  - bioRxiv
PY  - 2022
ST  - Linking impulsivity to activity levels in pre-supplementary motor area during sequential gambling
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.06.28.497876
ER  -


TY  - GEN
AU  - Zhao, Y.
AU  - Song, Y.
TI  - TURNING TO A TEACHER FOR TIMESTAMP SUPERVISED TEMPORAL ACTION SEGMENTATION
AB  - Temporal action segmentation in videos has drawn much attention recently. Timestamp supervision is a cost-effective way for this task. To obtain more information to optimize the model, the existing method generated pseudo frame-wise labels iteratively based on the output of a segmentation model and the timestamp annotations. However, this practice may introduce noise and oscillation during the training, and lead to performance degeneration. To address this problem, we propose a new framework for timestamp supervised temporal action segmentation by introducing a teacher model parallel to the segmentation model to help stabilize the process of model optimization. The teacher model can be seen as an ensemble of the segmentation model, which helps to suppress the noise and to improve the stability of pseudo labels. We further introduce a segmentally smoothing loss, which is more focused and cohesive, to enforce the smooth transition of the predicted probabilities within action instances. The experiments on three datasets show that our method outperforms the state-of-the-art method and performs comparably against the fully-supervised methods at a much lower annotation cost.
PB  - arXiv
PY  - 2022
ST  - TURNING TO A TEACHER FOR TIMESTAMP SUPERVISED TEMPORAL ACTION SEGMENTATION
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icme52920.2022.9859626
ER  -


TY  - GEN
AU  - Herbuela, V.R.D.M.
AU  - Karita, T.
AU  - Toya, A.
AU  - Onishi, E.
AU  - Saeki, T.
TI  - Multilevel linear and temporal analyses on the effects of weather indices on movements and behaviors
AB  - Weather indices significantly affecting movements, affective, and behavioral states remain largely unknown among children with profound or severe intellectual and multiple disabilities (PIMD/IDs). Main, seasonal interaction and 24-hour time-offsets effects of outdoor weather indices on the movements and behaviors of children with PIMD/IDs were explored using hierarchical and general linear models. Caregiver-interpreted facial, body, and limb movements and behaviors of 20 8-to 16-year-old children with PIMD/IDs and simultaneous online-API-and-sensor-collected app-based weather, proximity and time data were collected in 105 single-dyad video-recorded (30-hour) natural-child-caregiver-dyadic interactions over 5 months. Fluctuations in outdoor atmospheric pressure, humidity, cloudiness, wind speed, season, daylength, time (12 to 1 pm), and conditions were predictive of variations in movements and behaviors, which in turn, also responded to increased indoor UV, humidity, and cloudiness levels during winter, and increasing atmospheric pressure and decreasing humidity during fall. While outdoor temperature, atmospheric pressure, humidity, and cloudiness immediately affected variations in movement and behavior outcomes, time-offset wind-related indices had significant delayed effects at several time lags. Evidently, variations in movements and behaviors of children with PIMD/IDs are affected by seasonal variations and current or delayed fluctuating levels of outdoor weather indices which has significant implications for communication and educational interventions.
PB  - Research Square
PY  - 2022
ST  - Multilevel linear and temporal analyses on the effects of weather indices on movements and behaviors
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-1787022/v1
ER  -


TY  - GEN
AU  - Huang, J.
AU  - Jin, H.
AU  - Gong, S.
AU  - Liu, Y.
TI  - Video Activity Localisation with Uncertainties in Temporal Boundary
AB  - Current methods for video activity localisation over time assume implicitly that activity temporal boundaries labelled for model training are determined and precise. However, in unscripted natural videos, different activities mostly transit smoothly, so that it is intrinsically ambiguous to determine in labelling precisely when an activity starts and ends over time. Such uncertainties in temporal labelling are currently ignored in model training, resulting in learning mis-matched video-text correlation with poor generalisation in test. In this work, we solve this problem by introducing Elastic Moment Bounding (EMB) to accommodate flexible and adaptive activity temporal boundaries towards modelling universally interpretable video-text correlation with tolerance to underlying temporal uncertainties in pre-fixed annotations. Specifically, we construct elastic boundaries adaptively by mining and discovering frame-wise temporal endpoints that can maximise the alignment between video segments and query sentences. To enable both more accurate matching (segment content attention) and more robust localisation (segment elastic boundaries), we optimise the selection of frame-wise endpoints subject to segment-wise contents by a novel Guided Attention mechanism. Extensive experiments on three video activity localisation benchmarks demonstrate compellingly the EMB's advantages over existing methods without modelling uncertainty.
PB  - arXiv
PY  - 2022
ST  - Video Activity Localisation with Uncertainties in Temporal Boundary
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-19830-4_41
ER  -


TY  - GEN
AU  - Chen, Z.
AU  - Li, S.
AU  - Yang, B.
AU  - Li, Q.
AU  - Liu, H.
TI  - Multi-Scale Spatial Temporal Graph Convolutional Network for Skeleton-Based Action Recognition
AB  - Graph convolutional networks have been widely used for skeleton-based action recognition due to their excellent modeling ability of non-Euclidean data. As the graph convolution is a local operation, it can only utilize the short-range joint dependencies and short-term trajectory but fails to directly model the distant joints relations and long-range temporal information that are vital to distinguishing various actions. To solve this problem, we present a multi-scale spatial graph convolution (MS-GC) module and a multi-scale temporal graph convolution (MT-GC) module to enrich the receptive field of the model in spatial and temporal dimensions. Concretely, the MS-GC and MT-GC modules decompose the corresponding local graph convolution into a set of subgraph convolution, forming a hierarchical residual architecture. Without introducing additional parameters, the features will be processed with a series of sub-graph convolutions, and each node could complete multiple spatial and temporal aggregations with its neighborhoods. The final equivalent receptive field is accordingly enlarged, which is capable of capturing both short- and long-range dependencies in spatial and temporal domains. By coupling these two modules as a basic block, we further propose a multi-scale spatial temporal graph convolutional network (MST-GCN), which stacks multiple blocks to learn effective motion representations for action recognition. The proposed MST-GCN achieves remarkable performance on three challenging benchmark datasets, NTU RGB+D, NTU-120 RGB+D and Kinetics-Skeleton, for skeleton-based action recognition.
PB  - arXiv
PY  - 2022
ST  - Multi-Scale Spatial Temporal Graph Convolutional Network for Skeleton-Based Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v35i2.16197
ER  -


TY  - GEN
AU  - Kreider-Letterman, G.
AU  - Castillo, A.
AU  - Mahlandt, E.K.
AU  - Goicoechea, S.
AU  - Garcia-Mata, R.
TI  - ARHGAP17 regulates the spatiotemporal activity of Cdc42 at invadopodia
AB  - Cancer cells form actin-rich protrusions called invadopodia that can degrade the extracellular matrix and facilitate tumor invasion and intravasation. Invadopodia formation is regulated by Rho GTPases; however, the molecular mechanisms controlling Rho GTPase signaling at invadopodia are poorly understood. Here, we have identified ARHGAP17, a Cdc42-specific RhoGAP, as a key regulator of invadopodia in breast cancer cells. Using a combination of fixed and live cell imaging, and a Cdc42 sensor optimized for mammalian cells, we have defined a novel ARHGAP17-mediated signaling pathway that controls the spatial and temporal regulation of Cdc42 activity during invadopodia turnover. During assembly, ARHGAP17 localizes to the invadopodia adhesion ring where it restricts the activity of Cdc42 to the invadopodia core. Later, at the start of invadopodia disassembly, ARHGAP17 moves to the core where it inactivates Cdc42 to promote the disassembly of invadopodia in a process that is mediated by its interaction with the Cdc42 effector CIP4. Our results show that ARHGAP17 coordinates when and where Cdc42 is activated during invadopodia assembly and controls the disassembly by terminating the signal.
PB  - bioRxiv
PY  - 2022
ST  - ARHGAP17 regulates the spatiotemporal activity of Cdc42 at invadopodia
Y2  - 2025/05/05/21:54:31
DO  - 10.1083/jcb.202207020
ER  -


TY  - GEN
AU  - Xia, K.
AU  - Wang, L.
AU  - Zhou, S.
AU  - Zheng, N.
AU  - Tang, W.
TI  - Learning to Refactor Action and Co-occurrence Features for Temporal Action Localization
AB  - The main challenge of Temporal Action Localization is to retrieve subtle human actions from various co-occurring ingredients, e.g., context and background, in an untrimmed video. While prior approaches have achieved substantial progress through devising advanced action detectors, they still suffer from these co-occurring ingredients which often dominate the actual action content in videos. In this paper, we explore two orthogonal but complementary aspects of a video snippet, i.e., the action features and the co-occurrence features. Especially, we develop a novel auxiliary task by decoupling these two types of features within a video snippet and recombining them to generate a new feature representation with more salient action information for accurate action localization. We term our method RefactorNet, which first explicitly factorizes the action content and regularizes its co-occurrence features, and then synthesizes a new action-dominated video representation. Extensive experimental results and ablation studies on THUMOS14 and ActivityNet v1.3 demonstrate that our new representation, combined with a simple action detector, can significantly improve the action localization performance.
PB  - arXiv
PY  - 2022
ST  - Learning to Refactor Action and Co-occurrence Features for Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52688.2022.01351
ER  -


TY  - GEN
AU  - Du, J.-R.
AU  - Feng, J.-C.
AU  - Lin, K.-Y.
AU  - Shan, Y.
AU  - Zheng, W.-S.
TI  - Weakly-Supervised Temporal Action Localization by Progressive Complementary Learning
AB  - Weakly Supervised Temporal Action Localization (WSTAL) aims to localize and classify action instances in long untrimmed videos with only video-level category labels. Due to the lack of snippet-level supervision for indicating action boundaries, previous methods typically assign pseudo labels for unlabeled snippets. However, since some action instances of different categories are visually similar, it is non-trivial to exactly label the (usually) one action category for a snippet, and incorrect pseudo labels would impair the localization performance. To address this problem, we propose a novel method from a category exclusion perspective, named Progressive Complementary Learning (ProCL), which gradually enhances the snippet-level supervision. Our method is inspired by the fact that video-level labels precisely indicate the categories that all snippets surely do not belong to, which is ignored by previous works. Accordingly, we first exclude these surely non-existent categories by a complementary learning loss. And then, we introduce the background-aware pseudo complementary labeling in order to exclude more categories for snippets of less ambiguity. Furthermore, for the remaining ambiguous snippets, we attempt to reduce the ambiguity by distinguishing foreground actions from the background. Extensive experimental results show that our method achieves new state-of-the-art performance on two popular benchmarks, namely THUMOS14 and ActivityNet1.3.
PB  - arXiv
PY  - 2022
ST  - Weakly-Supervised Temporal Action Localization by Progressive Complementary Learning
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tcsvt.2024.3456795
ER  -


TY  - GEN
AU  - Hao, Y.
AU  - Shi, Z.
AU  - Liu, Y.
TI  - WIFI-BASED SPATIOTEMPORAL HUMAN ACTION PERCEPTION
AB  - WiFi-based sensing for human activity recognition (HAR) has recently become a hot topic as it brings great benefits when compared with video-based HAR, such as eliminating the demands of line-of-sight (LOS) and preserving privacy. Making the WiFi signals to'see' the action, however, is quite coarse and thus still in its infancy. An end-to-end spatiotemporal WiFi signal neural network (STWNN) is proposed to enable WiFi-only sensing in both line-of-sight and non-line-of-sight scenarios. Especially, the 3D convolution module is able to explore the spatiotemporal continuity of WiFi signals, and the feature self-attention module can explicitly maintain dominant features. In addition, a novel 3D representation for WiFi signals is designed to preserve multi-scale spatiotemporal information. Furthermore, a small wireless-vision dataset (WVAR) is synchronously collected to extend the potential of STWNN to'see' through occlusions. Quantitative and qualitative results on WVAR and the other three public benchmark datasets demonstrate the effectiveness of our approach on both accuracy and shift consistency.
PB  - arXiv
PY  - 2022
ST  - WIFI-BASED SPATIOTEMPORAL HUMAN ACTION PERCEPTION
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icip46576.2022.9897978
ER  -


TY  - GEN
AU  - Li, S.
AU  - Zhang, F.
AU  - Zhao, R.-W.
AU  - Liu, L.
AU  - Hou, J.
TI  - Pyramid Region-based Slot Attention Network for Temporal Action Proposal Generation
AB  - It has been found that temporal action proposal generation, which aims to discover the temporal action instances within the range of the start and end frames in the untrimmed videos, can largely benefit from proper temporal and semantic context exploitation. The latest efforts were dedicated to considering the temporal context and similarity-based semantic context through self-attention modules. However, they still suffer from cluttered background information and limited contextual feature learning. In this paper, we propose a novel Pyramid Region-based Slot Attention (PRSlot) modules to address these issues. Instead of using the similarity computation, our PRSlot module directly learns the local relations in an encoder-decoder manner and generates the representation of a local region enhanced based on the attention over input features called slot. Specifically, upon the input snippet-level features, PRSlot module takes the target snippet as query, its surrounding region as key and then generates slot representations for each query-key slot by aggregating the local snippet context with a parallel pyramid strategy. Based on PRSlot modules, we present a novel Pyramid Region-based Slot Attention Network termed PRSA-Net to learn a unified visual representation with rich temporal and semantic context for better proposal generation. Extensive experiments are conducted on two widely adopted THUMOS14 and ActivityNet-1.3 benchmarks. Our PRSA-Net outperforms other state-of-the-art methods. In particular, we improve the AR@100 from the previous best 50.67% to 56.12% for proposal generation and raise the mAP under 0.5 tIoU from 51.9% to 58.7% for action detection on THUMOS14. Code is available at https://github.com/handhand123/PRSA-Net.
PB  - arXiv
PY  - 2022
ST  - Pyramid Region-based Slot Attention Network for Temporal Action Proposal Generation
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v34i07.6711
ER  -


TY  - GEN
AU  - Matsui, A.
AU  - Ferrara, E.
TI  - Extracting Fast and Slow: User-Action Embedding with Inter-temporal Information
AB  - With the recent development of technology, data on detailed human temporal behaviors has become available. Many methods have been proposed to mine those human dynamic behavior data and revealed valuable insights for research and businesses. However, most methods analyze only sequence of actions and do not study the inter-temporal information such as the time intervals between actions in a holistic manner. While actions and action time intervals are interdependent, it is challenging to integrate them because they have different natures: time and action. To overcome this challenge, we propose a unified method that analyzes user actions with intertemporal information (time interval). We simultaneously embed the user's action sequence and its time intervals to obtain a low-dimensional representation of the action along with intertemporal information. The paper demonstrates that the proposed method enables us to characterize user actions in terms of temporal context, using three real-world data sets. This paper demonstrates that explicit modeling of action sequences and inter-temporal user behavior information enable successful interpretable analysis.
PB  - arXiv
PY  - 2022
ST  - Extracting Fast and Slow
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/ccdc62350.2024.10587547
ER  -


TY  - GEN
AU  - Wang, X.
AU  - Zhang, H.
AU  - Zhang, S.
AU  - Shao, Y.
AU  - Sang, N.
TI  - Context-aware Proposal Network for Temporal Action Detection
AB  - This technical report presents our first place winning solution for temporal action detection task in CVPR-2022 AcitivityNet Challenge. The task aims to localize temporal boundaries of action instances with specific classes in long untrimmed videos. Recent mainstream attempts are based on dense boundary matchings and enumerate all possible combinations to produce proposals. We argue that the generated proposals contain rich contextual information, which may benefits detection confidence prediction. To this end, our method mainly consists of the following three steps: 1) action classification and feature extraction by Slowfast [10], CSN [20], TimeSformer [4], TSP [1], I3D-flow [7], VGGish-audio [11], TPN [33] and ViViT [3]; 2) proposal generation. Our proposed Context-aware Proposal Network (CPN) builds on top of BMN [16], GTAD [32] and PRN [26] to aggregate contextual information by randomly masking some proposal features. 3) action detection. The final detection prediction is calculated by assigning the proposals with corresponding video-level classification results. Finally, we ensemble the results under different feature combination settings and achieve 45.8% performance on the test set, which improves the champion result in CVPR-2021 ActivityNet Challenge [26] by 1.1% in terms of average mAP.
PB  - arXiv
PY  - 2022
ST  - Context-aware Proposal Network for Temporal Action Detection
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tcsvt.2021.3075607
ER  -


TY  - GEN
AU  - Kopsick, J.D.
AU  - Hartzell, K.
AU  - Lazaro, H.
AU  - Hasselmo, M.E.
AU  - Dannenberg, H.
TI  - Temporal dynamics of cholinergic activity in the septo-hippocampal system
AB  - Cholinergic projection neurons in the medial septum and diagonal band of Broca are the major source of cholinergic modulation of hippocampal circuit functions that support neural coding of location and running speed. Changes in cholinergic modulation are known to correlate with changes in brain states, cognitive functions, and behavior. However, whether cholinergic modulation can change fast enough to serve as a potential speed signal in hippocampal and parahippocampal cortices and whether the temporal dynamics in such a signal depend on the presence of visual cues remain unknown. In this study, we use a fiber-photometric approach to quantify the temporal dynamics of cholinergic activity in freely moving mice as a function of the animal’s running speed and visual cues. We show that the population activity of cholinergic neurons in the medial septum and diagonal band of Broca changes fast enough to be aligned well with changes in the animal’s running speed and is strongly and linearly correlated to the logarithm of the animal’s running speed. Intriguingly, the cholinergic modulation remains strongly and linearly correlated to the speed of the animal’s neck movements during periods of stationary activity. Furthermore, we show that cholinergic modulation is unaltered during darkness. Lastly, we identify rearing, a discrete behavior where the mouse stands on its hindlimbs to scan the environment from an elevated perspective, is associated with higher cholinergic activity than expected from neck movements on the horizontal plane alone. Taken together, these data show that temporal dynamics in the cholinergic modulation of hippocampal circuits are fast enough to provide a potential running speed signal in real-time. Moreover, the data show that cholinergic modulation is primarily a function of the logarithm of the animal’s movement speed, both during locomotion and during stationary activity, with no significant interaction with visual inputs. These data advance our understanding of temporal dynamics in cholinergic modulation of hippocampal circuits and their functions in the context of neural coding of location and running speed.
PB  - bioRxiv
PY  - 2022
ST  - Temporal dynamics of cholinergic activity in the septo-hippocampal system
Y2  - 2025/05/05/21:54:31
DO  - 10.3389/fncir.2022.957441
ER  -


TY  - GEN
AU  - Hao, X.
AU  - Chen, J.
AU  - Chen, S.
AU  - Saad, A.
AU  - Hamid, R.
TI  - Scalable Temporal Localization of Sensitive Activities in Movies and TV Episodes
AB  - To help customers make better-informed viewing choices, video-streaming services try to moderate their content and provide more visibility into which portions of their movies and TV episodes contain age-appropriate material (e.g., nudity, sex, violence, or drug-use). Supervised models to localize these sensitive activities require large amounts of clip-level labeled data which is hard to obtain, while weakly-supervised models to this end usually do not offer competitive accuracy. To address this challenge, we propose a novel Coarse2Fine network designed to make use of readily obtainable video-level weak labels in conjunction with sparse clip-level labels of age-appropriate activities. Our model aggregates frame-level predictions to make video-level classifications and is therefore able to leverage sparse clip-level labels along with video-level labels. Furthermore, by performing frame-level predictions in a hierarchical manner, our approach is able to overcome the label-imbalance problem caused due to the rare-occurrence nature of age-appropriate content. We present comparative results of our approach using 41, 234 movies and TV episodes (∼3 years of video-content) from 521 sub-genres and 250 countries making it by far the largest-scale empirical analysis of age-appropriate activity localization in long-form videos ever published. Our approach offers 107.2% relative mAP improvement (from 5.5% to 11.4%) over existing state-of-the-art activity-localization approaches.
PB  - arXiv
PY  - 2022
ST  - Scalable Temporal Localization of Sensitive Activities in Movies and TV Episodes
Y2  - 2025/05/05/21:54:31
DO  - 10.1515/9781438416359-015
ER  -


TY  - GEN
AU  - Kang, P.
AU  - Zhang, X.
AU  - Pang, S.
AU  - Zhao, Y.
AU  - Huang, J.
TI  - Investigation of the Temporal Behavior of the Self-Q-Switched Ho:Gdvo4 Laser Pumped by a Tm-Doped Fiber Laser
AB  - We investigate the temporal behavior of the self-Q-switched Ho:GdVO4 laser at 2074 nm by analyzing the variation of reabsorption region with the incident pump power. The different temporal behaviors of self-Q-switched Ho:GdVO4 laser are observed with increasing the incident pump power in the experiment. When the incident pump power is below 11.7 W, stable fully modulated self-Q-switching pulses are observed, however, once the incident pump power exceeds 11.7 W, the modulation depth drops sharply with increasing pump power until the pulsing phenomenon disappears. Under considering the ground state reabsorption effect, the temporal behavior of self-Q-switched Ho:GdVO4 laser is discussed by calculating the variation of the reabsorption region with incident pump power, and the theoretical analysis can explain the experimental results very well. The maximum average output power of self-Q-switched Ho:GdVO4 laser reaches 1.07 W at an incident pumping power of 11.7 W, corresponding to a pulse width of 1.808 µs and a pulse repetition frequency of 56.53 kHz. To the best of our knowledge, this is the first Ho doped self-Q-switched solid state laser.
PB  - SSRN
PY  - 2022
ST  - Investigation of the Temporal Behavior of the Self-Q-Switched Ho
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.optlastec.2022.108525
ER  -


TY  - GEN
AU  - Broholt, T.H.
AU  - Amato, V.
AU  - Christensen, L.R.L.
AU  - Kristensen, M.H.
AU  - Petersen, S.
TI  - Opportunities and Barriers for Temporal Demand Response as an Action to Challenges in District Heating
AB  - Resent studies assume that district heating (DH) operators can substitute traditional supply-side management initiatives with temporal demand response (DR) from building heating systems to eliminate and/or mitigate operational challenges. The study reported in this paper investigated whether DH operators are currently aligned with this assumption by mapping the current, prevailing, and expected future challenges of DH systems that temporal DR from building space heating systems could mitigate or substitute. An initial literature review identified a need for further investigations into the current attitude of DH companies towards temporal DR from building space heating systems as alternative actions to current challenges. Seven semi-structured interviews with employees of Danish DH companies were therefore conducted leading to insights that could help researchers target the development of temporal DR solutions to mitigate current challenges that are barriers to the transition to low-temperature 4GDH. The attitude of the seven interviewees towards temporal DR as a solution to DH challenges was positive but not representative of the whole DH industry. It is therefore proposed that future work is to conduct a questionnaire survey to analyse the distribution, prevalence, and importance of the findings presented in this paper.
PB  - SSRN
PY  - 2022
ST  - Opportunities and Barriers for Temporal Demand Response as an Action to Challenges in District Heating
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4569666
ER  -


TY  - GEN
AU  - Gupta, V.
AU  - Bedathur, S.
TI  - ProActive: Self-Attentive Temporal Point Process Flows for Activity Sequences
AB  - Any human activity can be represented as a temporal sequence of actions performed to achieve a certain goal. Unlike machine-made time series, these action sequences are highly disparate as the time taken to finish a similar action might vary between different persons. Therefore, understanding the dynamics of these sequences is essential for many downstream tasks such as activity length prediction, goal prediction, etc. Existing neural approaches that model an activity sequence are either limited to visual data or are task-specific, i.e., limited to next action or goal prediction. In this paper, we present ProActive, a neural marked temporal point process (MTPP) framework for modeling the continuous-time distribution of actions in an activity sequence while simultaneously addressing three high-impact problems - next action prediction, sequence-goal prediction, and end-to-end sequence generation. Specifically, we utilize a self-attention module with temporal normalizing flows to model the influence and the inter-arrival times between actions in a sequence. Moreover, for time-sensitive prediction, we perform an early detection of sequence goal via a constrained margin-based optimization procedure. This in-turn allows ProActive to predict the sequence goal using a limited number of actions. Extensive experiments on sequences derived from three activity recognition datasets show the significant accuracy boost of ProActive over the state-of-the-art in terms of action and goal prediction, and the first-ever application of end-to-end action sequence generation.
PB  - arXiv
PY  - 2022
ST  - ProActive
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3534678.3539477
ER  -


TY  - GEN
AU  - Le, T.
AU  - Shlizerman, E.
TI  - STNDT: Modeling Neural Population Activity with a Spatiotemporal Transformer
AB  - Modeling neural population dynamics underlying noisy single-trial spiking activities is essential for relating neural observation and behavior. A recent non-recurrent method - Neural Data Transformers (NDT) - has shown great success in capturing neural dynamics with low inference latency without an explicit dynamical model. However, NDT focuses on modeling the temporal evolution of the population activity while neglecting the rich covariation between individual neurons. In this paper we introduce SpatioTemporal Neural Data Transformer (STNDT), an NDT-based architecture that explicitly models responses of individual neurons in the population across time and space to uncover their underlying firing rates. In addition, we propose a contrastive learning loss that works in accordance with mask modeling objective to further improve the predictive performance. We show that our model achieves state-of-the-art performance on ensemble level in estimating neural activities across four neural datasets, demonstrating its capability to capture autonomous and non-autonomous dynamics spanning different cortical regions while being completely agnostic to the specific behaviors at hand. Furthermore, STNDT spatial attention mechanism reveals consistently important subsets of neurons that play a vital role in driving the response of the entire population, providing interpretability and key insights into how the population of neurons performs computation.
PB  - arXiv
PY  - 2022
ST  - STNDT
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/s0925-2312(02)00780-4
ER  -


TY  - GEN
AU  - Ahmad, N.
AU  - Chow, S.H.-C.
AU  - Leung, H.-F.
TI  - Beyond the Gates of Euclidean Space: Temporal-Discrimination-Fusions and Attention-based Graph Neural Network for Human Activity Recognition
AB  - Human activity recognition (HAR) through wearable devices has received much interest due to its numerous applications in fitness tracking, wellness screening, and supported living. As a result, we have seen a great deal of work in this field. Traditional deep learning (DL) has set a state of the art performance for HAR domain. However, it ignores the data's structure and the association between consecutive time stamps. To address this constraint, we offer an approach based on Graph Neural Networks (GNNs) for structuring the input representation and exploiting the relations among the samples. However, even when using a simple graph convolution network to eliminate this shortage, there are still several limiting factors, such as inter-class activities issues, skewed class distribution, and a lack of consideration for sensor data priority, all of which harm the HAR model's performance. To improve the current HAR model's performance, we investigate novel possibilities within the framework of graph structure to achieve highly discriminated and rich activity features. We propose a model for (1) time-series-graph module that converts raw data from HAR dataset into graphs; (2) Graph Convolutional Neural Networks (GCNs) to discover local dependencies and correlations between neighboring nodes; and (3) self-attention GNN encoder to identify sensors interactions and data priorities. To the best of our knowledge, this is the first work for HAR, which introduces a GNN-based approach that incorporates both the GCN and the attention mechanism. By employing a uniform evaluation method, our framework significantly improves the performance on hospital patient's activities dataset comparatively considered other state of the art baseline methods.
PB  - arXiv
PY  - 2022
ST  - Beyond the Gates of Euclidean Space
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s10044-024-01229-4
ER  -


TY  - GEN
AU  - Deng, B.
AU  - Liu, D.
AU  - Zhao, S.
TI  - TadML: A fast temporal action detection with Mechanics-MLP
AB  - Temporal Action Detection (TAD) involves identifying action categories and their respective start and end frames in lengthy untrimmed videos, with current models utilizing both RGB and optical flow streams that require manual intervention, add computational complexity, and consume time. Moreover, two-stage approaches prioritizing proposal generation in the ini-tial stage result in a substantial reduction in inference speed. To address this, we propose a single-stage anchor-free method that solely utilizes the RGB stream and incorporates a novel Newtonian Mechanics-MLP architec-ture. Our model achieves comparable accuracy to existing state-of-the-art models but with significantly faster inference speeds, clocking in at an av-erage of 4.44 videos per second on THUMOS14. Our approach showcases the potential of MLP in downstream tasks like TAD. The source code is available at https://github.com/BonedDeng/TadML.
PB  - arXiv
PY  - 2022
ST  - TadML
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-50069-5_4
ER  -


TY  - GEN
AU  - Ghaffarpasand, O.
AU  - Pope, F.D.
TI  - Telematics Data for Geospatial and Temporal Mapping of Urban Transport and Mobility: New Insights into Traffic Behaviour and Complexity
AB  - This paper describes a new approach for understanding urban mobility called geospatial and temporal (GeoST) mapping, which translates telematics data into urban mobility data. The approach provides transport flow data at high spatial and temporal resolution. The underlying data of the model is retrieved from a large telematics dataset, which was collected from GPS-connected vehicles during their journeys over the UK’s West Midlands region road network for the years 2016 and 2018. Single journey telematics data were geospatially aggregated and then distributed over GeoST-segments. In total, approximately 354,000 GeoST-segments, covering over 17,700 km of roads over 35 timeslots are parameterized. Urban mobility characteristics are then extracted for every GeoST-segment. Traffic complexity is discussed and GeoST mapping of the average vehicle (traffic flow) speed, and vehicle specific power (VSP) over different road types are analysed. The role of road slope upon VSP is estimated for every GeoST-segment through knowledge of the elevation of the start and end points of segment lengths. Previously, road slope and its effect upon VSP has been ignored in most of the transport and urban planning studies. A series of case studies are presented that highlight the power of the new GeoST dataset over differing spatial and temporal scales. For example, results show that the total vehicle fleet moved faster by an average of 3% in 2016 compared to 2018. The studied roads at weekends are shown to be less safe, compared to weekdays, because of lower adherence to speed limits. By including road slope in VSP calculations, the annually-averaged VSP results differ by +12.6%, +14.3%, and +12.7% for motorways, primary roads, and secondary roads, respectively, when compared to calculations that ignore road slope. Going forwards, the new GeoST approach provides a new tool for improving our understanding of urban mobility and will be of great benefit to multiple stakeholders. With increasing availability of telematics data, the approach described can be extended to further locations worldwide.
PB  - SSRN
PY  - 2022
ST  - Telematics Data for Geospatial and Temporal Mapping of Urban Transport and Mobility
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4129692
ER  -


TY  - GEN
AU  - Souza, B.
AU  - Santos, M.
AU  - Filho, H.B.
AU  - Paulo, P.
AU  - Souza, C.H.
TI  - Spatial and temporal analysis of land use associated with mineral extraction activities in the Jequitinhonha River, Minas Gerais, Brazil
AB  - Remote sensing tools are increasingly common in environmental studies. Since release of the first Landsat satellite series, temporal analysis studies became important resource to understand land surface changing which could happen in natural ways or by anthropogenic. This study aims to understand how the evolution of the environmental impact occurred over 30 years in affected areas by the mineral extraction activity and the changes in the River morphology using Landsat 5 and 8 images. The use of its spectral bands aimed to create Normalized Difference Vegetation Index images in gold and diamond mining areas, located on the banks of the Jequitinhonha River, near the municipality of Diamantina, Minas Gerais. This study identified river styles and editions, pointing out changes in the evolutionary and dynamic picture in the drainage patterns and the sedimentological contribution. Results lead to the conclusion that land use and occupation are basically the main cause of these morphological changes over time.
PB  - Research Square
PY  - 2022
ST  - Spatial and temporal analysis of land use associated with mineral extraction activities in the Jequitinhonha River, Minas Gerais, Brazil
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-1726402/v1
ER  -


TY  - GEN
AU  - Khatibi, V.A.
AU  - Rahdar, M.
AU  - Rezaei, M.
AU  - Behzadi, G.
AU  - Janahmadi, M.
TI  - The glycolysis inhibitor 2-deoxy-D-glucose exerts different neuronal effects at circuit and cellular levels, partially reverses behavioral alterations and does not prevent NADPH diaphorase activity reduction in the intrahippocampal kainic acid model of temporal lobe epilepsy
AB  - Temporal lobe epilepsy is the most drug-resistant type with the highest incidence among the other focal epilepsies. Metabolic manipulations are of great interest among others, glycolysis inhibitors like 2-deoxy d-glucose (2-DG) being the most promising intervention. Here, we sought to investigate the effects of 2-DG treatment on cellular and circuit level electrophysiological properties using patch-clamp and local field potentials recordings and behavioral alterations such as depression, anxiety-like behaviors, and changes in nitric oxide signaling in the intrahippocampal kainic acid model. We found that epileptic animals were less anxious, more depressed, with more locomotion activity. Interestingly, by masking the effect of increased locomotor activity on the parameters of the zero-maze test, no altered anxiety-like behavior was noted in epileptic animals. However, 2-DG could partially reverse the behavioral changes induced by kainic acid. The findings also showed that 2-DG treatment partially suppresses cellular level alterations while failing to reverse circuit-level changes resulting from kainic acid injection. Analysis of NADPH-diaphorase positive neurons in the CA1 area of the hippocampus revealed that the number of positive neurons was significantly reduced in dorsal CA1 of the epileptic animals and 2-DG treatment did not affect the diminishing effect of kainic acid on NADPH-d+ neurons in the CA1 area. In the control group receiving 2-DG, however, an augmented NADPH-d+ cell number was noted. These data suggest that 2-DG cannot suppress epileptiform activity at the circuit-level in this model of epilepsy and therefore, may fail to control the seizures in temporal lobe epilepsy cases.
PB  - Research Square
PY  - 2022
ST  - The glycolysis inhibitor 2-deoxy-D-glucose exerts different neuronal effects at circuit and cellular levels, partially reverses behavioral alterations and does not prevent NADPH diaphorase activity reduction in the intrahippocampal kainic acid model of temporal lobe epilepsy
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-1720319/v1
ER  -


TY  - GEN
AU  - Sui, L.
AU  - Zhang, C.-L.
AU  - Gu, L.
AU  - Han, F.
TI  - A Simple and Efficient Pipeline to Build an End-to-End Spatial-Temporal Action Detector
AB  - Spatial-temporal action detection is a vital part of video understanding. Current spatial-temporal action detection methods mostly use an object detector to obtain person candidates and classify these person candidates into different action categories. So-called two-stage methods are heavy and hard to apply in real-world applications. Some existing methods build one-stage pipelines, But a large performance drop exists with the vanilla one-stage pipeline and extra classification modules are needed to achieve comparable performance. In this paper, we explore a simple and effective pipeline to build a strong one-stage spatial-temporal action detector. The pipeline is composed by two parts: one is a simple end-to-end spatial-temporal action detector. The proposed end-to-end detector has minor architecture changes to current proposal-based detectors and does not add extra action classification modules. The other part is a novel labeling strategy to utilize unlabeled frames in sparse annotated data. We named our model as SE-STAD. The proposed SE-STAD achieves around 2% mAP boost and around 80% FLOPs reduction. Our code will be released at https://github.com/4paradigm-CV/SE-STAD.
PB  - arXiv
PY  - 2022
ST  - A Simple and Efficient Pipeline to Build an End-to-End Spatial-Temporal Action Detector
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/wacv56688.2023.00594
ER  -


TY  - GEN
AU  - Xia, L.
AU  - Huang, C.
AU  - Xu, Y.
AU  - Pei, J.
TI  - Multi-Behavior Sequential Recommendation with Temporal Graph Transformer
AB  - Modeling time-evolving preferences of users with their sequential item interactions, has attracted increasing attention in many online applications. Hence, sequential recommender systems have been developed to learn the dynamic user interests from the historical interactions for suggesting items. However, the interaction pattern encoding functions in most existing sequential recommender systems have focused on single type of user-item interactions. In many real-life online platforms, user-item interactive behaviors are often multi-typed (e.g., click, add-to-favorite, purchase) with complex cross-type behavior inter-dependencies. Learning from informative representations of users and items based on their multi-typed interaction data, is of great importance to accurately characterize the time-evolving user preference. In this work, we tackle the dynamic user-item relation learning with the awareness of multi-behavior interactive patterns. Towards this end, we propose a new Temporal Graph Transformer (TGT) recommendation framework to jointly capture dynamic short-term and long-range user-item interactive patterns, by exploring the evolving correlations across different types of behaviors. The new TGT method endows the sequential recommendation architecture to distill dedicated knowledge for type-specific behavior relational context and the implicit behavior dependencies. Experiments on the real-world datasets indicate that our method TGT consistently outperforms various state-of-the-art recommendation methods. Our model implementation codes are available at https://github.com/akaxlh/TGT.
PB  - arXiv
PY  - 2022
ST  - Multi-Behavior Sequential Recommendation with Temporal Graph Transformer
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tkde.2022.3175094
ER  -


TY  - GEN
AU  - Mu, S.
AU  - Hou, Y.
AU  - Zhao, W.X.
AU  - Li, Y.
AU  - Ding, B.
TI  - ID-Agnostic User Behavior Pre-training for Sequential Recommendation
AB  - Recently, sequential recommendation has emerged as a widely studied topic. Existing researches mainly design effective neural architectures to model user behavior sequences based on item IDs. However, this kind of approach highly relies on user-item interaction data and neglects the attribute- or characteristic-level correlations among similar items preferred by a user. In light of these issues, we propose IDA-SR, which stands for ID-Agnostic User Behavior Pre-training approach for Sequential Recommendation. Instead of explicitly learning representations for item IDs, IDA-SR directly learns item representations from rich text information. To bridge the gap between text semantics and sequential user behaviors, we utilize the pre-trained language model as text encoder, and conduct a pre-training architecture on the sequential user behaviors. In this way, item text can be directly utilized for sequential recommendation without relying on item IDs. Extensive experiments show that the proposed approach can achieve comparable results when only using ID-agnostic item representations, and performs better than baselines by a large margin when fine-tuned with ID information.
PB  - arXiv
PY  - 2022
ST  - ID-Agnostic User Behavior Pre-training for Sequential Recommendation
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-24755-2_2
ER  -


TY  - GEN
AU  - Han, E.
AU  - Choi, Y.J.
AU  - Park, S.
AU  - Lee, S.H.
AU  - Choi, J.
TI  - A Comprehensive Characterization of Temporal Rheotactic Behaviors in Lateral Line Damaged Zebrafish Larvae
AB  - Aquatic animals have rheotaxis that maintains a balance in response to water flow. Mechanical stimulation induced by water flow is sensed through hair cells in lateral line, thereby leading to adapt positions relevant to the environment. Such behaviors of zebrafish larvae include a diverse range of movement patterns in accordance with the extent of the integration of hair cells. Several attempts have been made in numerical rheotactic modeling, most of which have evaluated rheotactic distortions in terms of head angles or behavioral changes with respect to the gradient of the flow velocity. However, the knowledge about how rheotaxis depends on the extent of hair-cell integrity has not been fully investigated and understood. This article aims at a comprehensive characterization of rheotactic behaviors that identify the lateral line integrity via an automated platform that examines multiple objects simultaneously. As a consequence, a commensurable measure for one-dimensional characterization of rheotactic larval behaviors was obtained so that its linear change could be associated with the number of hair cells remaining intact. These findings offered navigational strategies in a way of sensing the flow-velocity gradients and extended an integral understanding that generalizes to a variety of aquatic animal behaviors in moving fluid. Furthermore, this work satisfied calls for an automated analysis platform to conduct such a task that rendered large-scale screening effectively and a biomarker that enabled to distinguish the seriousness of the hair cell damage to screen candidates having some effects in otoprotective drug discovery.
PB  - SSRN
PY  - 2022
ST  - A Comprehensive Characterization of Temporal Rheotactic Behaviors in Lateral Line Damaged Zebrafish Larvae
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4118856
ER  -


TY  - GEN
AU  - Wang, S.
AU  - Zhang, Y.
AU  - Wei, F.
AU  - Zhao, M.
AU  - Jiang, Y.
TI  - Skeleton-based Action Recognition Via Temporal-Channel Aggregation
AB  - Skeleton-based action recognition methods are limited by the semantic extraction of spatio-temporal skeletal maps. However, current methods have difficulty in effectively combining features from both temporal and spatial graph dimensions and tend to be thick on one side and thin on the other. In this paper, we propose a Temporal-Channel Aggregation Graph Convolutional Networks (TCA-GCN) to learn spatial and temporal topologies dynamically and efficiently aggregate topological features in different temporal and channel dimensions for skeleton-based action recognition. We use the Temporal Aggregation module to learn temporal dimensional features and the Channel Aggregation module to efficiently combine spatial dynamic topological features learned using Channel-wise with temporal dynamic topological features. In addition, we extract multi-scale skeletal features on temporal modeling and fuse them with priori skeletal knowledge with an attention mechanism. Extensive experiments show that our model results outperform state-of-the-art methods on the NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.
PB  - arXiv
PY  - 2022
ST  - Skeleton-based Action Recognition Via Temporal-Channel Aggregation
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.eswa.2023.120683
ER  -


TY  - GEN
AU  - Qin, Z.
AU  - Ji, P.
AU  - Kim, D.
AU  - Anwar, S.
AU  - Gedeon, T.
TI  - Strengthening Skeletal Action Recognizers via Leveraging Temporal Patterns
AB  - Skeleton sequences are compact and lightweight. Numerous skeleton-based action recognizers have been proposed to classify human behaviors. In this work, we aim to incorporate components that are compatible with existing models and further improve their accuracy. To this end, we design two temporal accessories: discrete cosine encoding (DCE) and chronological loss (CRL). DCE facilitates models to analyze motion patterns from the frequency domain and meanwhile alleviates the influence of signal noise. CRL guides networks to explicitly capture the sequence’s chronological order. These two components consistently endow many recently-proposed action recognizers with accuracy boosts, achieving new state-of-the-art (SOTA) accuracy on two large datasets.
PB  - arXiv
PY  - 2022
ST  - Strengthening Skeletal Action Recognizers via Leveraging Temporal Patterns
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-25072-9_39
ER  -


TY  - GEN
AU  - Du, D.
AU  - Su, B.
AU  - Li, Y.
AU  - Si, L.
AU  - Shan, Y.
TI  - Do we really need temporal convolutions in action segmentation?
AB  - Action classification has made great progress, but segmenting and recognizing actions from long untrimmed videos remains a challenging problem. Most state-of-the-art methods focus on designing temporal convolution-based models, but the inflexibility of temporal convolutions and the difficulties in modeling long-term temporal dependencies restrict the potential of these models. Transformer-based models with adaptable and sequence modeling capabilities have recently been used in various tasks. However, the lack of inductive bias and the inefficiency of handling long video sequences limit the application of Transformer in action segmentation. In this paper, we design a pure Transformer-based model without temporal convolutions by incorporating temporal sampling, called Temporal U-Transformer (TUT). The U-Transformer architecture reduces complexity while introducing an inductive bias that adjacent frames are more likely to belong to the same class, but the introduction of coarse resolutions results in the misclassification of boundaries. We observe that the similarity distribution between a boundary frame and its neighboring frames depends on whether the boundary frame is the start or end of an action segment. Therefore, we further propose a boundary-aware loss based on the distribution of similarity scores between frames from attention modules to enhance the ability to recognize boundaries. Extensive experiments show the effectiveness of our model.
PB  - arXiv
PY  - 2022
ST  - Do we really need temporal convolutions in action segmentation?
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icme55011.2023.00178
ER  -


TY  - GEN
AU  - Zhang, B.
AU  - Snezhko, A.
AU  - Sokolov, A.
TI  - Guiding self-assembly of active colloids by temporal modulation of activity
AB  - Self-organization phenomena in ensembles of self-propelled particles open pathways to the synthesis of new dynamic states not accessible by traditional equilibrium processes. The challenge is to develop a set of principles that facilitate the control and manipulation of emergent active states. Here, we report that dielectric rolling colloids energized by a pulsating electric field self-organize into alternating square lattices with a lattice constant controlled by the parameters of the field. We combine experiments and simulations to examine spatiotemporal properties of the emergent collective patterns, and investigate the underlying dynamics of the self-organization.We reveal the resistance of the dynamic lattices to compression/expansion stresses leading to a hysteretic behavior of the lattice constant. The general mechanism of pattern synthesis and control in active ensembles via temporal modulation of activity can be applied to other active colloidal systems.
PB  - arXiv
PY  - 2022
ST  - Guiding self-assembly of active colloids by temporal modulation of activity
Y2  - 2025/05/05/21:54:31
DO  - 10.1103/physrevlett.128.018004
ER  -


TY  - GEN
AU  - Zich, C.
AU  - Quinn, A.J.
AU  - Bonaiuto, J.J.
AU  - Ward, N.S.
AU  - Bestmann, S.
TI  - Spatiotemporal organization of human sensorimotor beta burst activity
AB  - Beta oscillations in human sensorimotor cortex are hallmark signatures of healthy and pathological movement. In single trials, beta oscillations include bursts of intermittent, transient periods of high-power activity. These burst events have been linked to a range of sensory and motor processes, but their precise spatial, spectral, and temporal structure remains unclear. Specifically, a role for beta burst activity in information coding and communication suggests spatiotemporal patterns, or travelling wave activity, along specific anatomical gradients. We here show in human magnetoencephalography recordings that burst activity in sensorimotor cortex occurs in planar spatiotemporal wave-like patterns that dominate along two axes either parallel or perpendicular to the central sulcus. Moreover, we find that the two propagation directions are characterised by distinct anatomical and physiological features. Finally, our results suggest that sensorimotor beta bursts occurring before and after a movement share the same generator but can be distinguished by their anatomical, spectral and spatiotemporal characteristics, indicating distinct functional roles.
PB  - bioRxiv
PY  - 2022
ST  - Spatiotemporal organization of human sensorimotor beta burst activity
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.05.19.492617
ER  -


TY  - GEN
AU  - Yang, L.
AU  - Han, J.
AU  - Zhao, T.
AU  - Liu, N.
AU  - Zhang, D.
TI  - Structured Attention Composition for Temporal Action Localization
AB  - Temporal action localization aims at localizing action instances from untrimmed videos. Existing works have designed various effective modules to precisely localize action instances based on appearance and motion features. However, by treating these two kinds of features with equal importance, previous works cannot take full advantage of each modality feature, making the learned model still sub-optimal. To tackle this issue, we make an early effort to study temporal action localization from the perspective of multi-modality feature learning, based on the observation that different actions exhibit specific preferences to appearance or motion modality. Specifically, we build a novel structured attention composition module. Unlike conventional attention, the proposed module would not infer frame attention and modality attention independently. Instead, by casting the relationship between the modality attention and the frame attention as an attention assignment process, the structured attention composition module learns to encode the frame-modality structure and uses it to regularize the inferred frame attention and modality attention, respectively, upon the optimal transport theory. The final frame-modality attention is obtained by the composition of the two individual attentions. The proposed structured attention composition module can be deployed as a plug-and-play module into existing action localization frameworks. Extensive experiments on two widely used benchmarks show that the proposed structured attention composition consistently improves four state-of-the-art temporal action localization methods and builds new state-of-the-art performance on THUMOS14.
PB  - arXiv
PY  - 2022
ST  - Structured Attention Composition for Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tip.2022.3180925
ER  -


TY  - GEN
AU  - Lin, J.
AU  - Wang, F.
AU  - Deng, L.
AU  - Mei, Y.
AU  - Xie, Y.
TI  - The Temporal and Spatial Behaviors of CME Occurrence Rate at Different Latitudes
AB  - The statistical study of the Coronal Mass Ejections (CMEs) is a hot topic in solar physics. To further reveal the temporal and spatial behaviors of the CMEs at different latitudes and heights, we analyzed the correlation and phase relationships between the occurrence rate of CMEs, the Coronal Brightness Index (CBI), and the 10.7-cm solar radio flux (F10.7). We found that the occurrence rate of the CMEs correlates with CBI relatively stronger at high latitudes (≥ 60◦) than low latitudes (≤ 50◦). At low latitudes, the occurrence rate of the CMEs correlates relatively weaker with CBI than F10.7. There is a relatively stronger correlation relationship between CMEs, F10.7, and CBI during Solar Cycle 24 (SC24) than Solar Cycle 23 (SC23). During SC23, the high-latitude CME occurrence rate lags behind F10.7 by three months, and during SC24, the low-latitude CME occurrence rate leads the low-latitude CBI by one month. The correlation coefficient values turn out to be larger when the very faint CMEs are removed from the samples of the CDAW catalog. Based on our results, we may speculate that the source regions of the high/low-latitude CMEs may vary in height, and the process of magnetic energy accumulation and dissipation is from the lower to the upper atmosphere of the Sun. The temporal offsets between different indicators could help us better understand the physical processes responsible for the solar-terrestrial interactions.
PB  - arXiv
PY  - 2022
ST  - The Temporal and Spatial Behaviors of CME Occurrence Rate at Different Latitudes
Y2  - 2025/05/05/21:54:31
DO  - 10.3847/1538-4357/ac6f54
ER  -


TY  - GEN
AU  - Yu, B.
AU  - Hou, Y.
AU  - Guo, Z.
AU  - Gao, Z.
AU  - Li, Y.
TI  - Ftan: Frame-to-Frame Temporal Alignment Network with Contrastive Learning for Few-Shot Action Recognition
AB  - Most current few-shot action recognition approaches follow the metric learning paradigm, measuring the distance of any sub-sequences (frames, any frame combinations or clips) between different actions for classification. However, this disordered distance metric between action sub-sequences ignores the long-term temporal relations of actions, which may result in significant metric deviations. What’s more, the distance metric suffers from the distinctive temporal distribution of different actions, including intra-class temporal offsets and inter-class local similarity. In this paper, a novel few-shot action recognition framework, Frame-to-frame Temporal Alignment Network (FTAN), is proposed to address the above challenges. Specifically, an attention-based temporal alignment (ATA) module is devised to calculate the distance between corresponding frames of different actions along the temporal dimension to achieve frame-to-frame temporal alignment. Meanwhile, the Temporal Context module (TCM) is proposed to increase inter-class diversity by enriching the frame-level feature representation, and the Frames Cyclic Shift Module (FCSM) performs frame-level temporal cyclic shift to reduce intra-class inconsistency. In addition, we present temporal and global contrastive objectives to assist in learning discriminative and class-agnostic visual features. Experimental results show that the proposed architecture achieves the state of the art on HMDB51, UCF101, Something-Something V2 and Kinetics-100 datasets.
PB  - SSRN
PY  - 2022
ST  - Ftan
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4104257
ER  -


TY  - GEN
AU  - Yang, M.
AU  - Chen, G.
AU  - Zheng, Y.-D.
AU  - Lu, T.
AU  - Wang, L.
TI  - BasicTAD: an Astounding RGB-Only Baseline for Temporal Action Detection
AB  - Temporal action detection (TAD) is extensively studied in the video understanding community by generally following the object detection pipeline in images. However, complex designs are not uncommon in TAD, such as two-stream feature extraction, multi-stage training, complex temporal modeling, and global context fusion. In this paper, we do not aim to introduce any novel technique for TAD. Instead, we study a simple, straightforward, yet must-known baseline given the current status of complex design and low detection efficiency in TAD. In our simple baseline (BasicTAD), we decompose the TAD pipeline into several essential components: data sampling, backbone design, neck construction, and detection head. We extensively investigate the existing techniques in each component for this baseline and, more importantly, perform end-to-end training over the entire pipeline thanks to the simplicity of design. As a result, this simple BasicTAD yields an astounding and real-time RGB-Only baseline very close to the state-of-the-art methods with two-stream inputs. In addition, we further improve the BasicTAD by preserving more temporal and spatial information in network representation (termed as PlusTAD). Empirical results demonstrate that our PlusTAD is very efficient and significantly outperforms the previous methods on the datasets of THUMOS14 and FineAction. Meanwhile, we also perform in-depth visualization and error analysis on our proposed method and try to provide more insights into the TAD problem. Our approach can serve as a strong baseline for future TAD research. The code and model are released at https://github.com/MCG-NJU/BasicTAD.
PB  - arXiv
PY  - 2022
ST  - BasicTAD
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.cviu.2023.103692
ER  -


TY  - GEN
AU  - Stergiou, A.
AU  - Damen, D.
TI  - The Wisdom of Crowds: Temporal Progressive Attention for Early Action Prediction
AB  - Early action prediction deals with inferring the ongoing action from partially-observed videos, typically at the outset of the video. We propose a bottleneck-based attention model that captures the evolution of the action, through progressive sampling over fine-to-coarse scales. Our proposed Temporal Progressive (TemPr) model is composed of multiple attention towers, one for each scale. The predicted action label is based on the collective agreement considering confidences of these towers. Extensive experiments over four video datasets showcase state-of-the-art performance on the task of Early Action Prediction across a range of encoder architectures. We demonstrate the effectiveness and consistency of TemPr through detailed ablations.
PB  - arXiv
PY  - 2022
ST  - The Wisdom of Crowds
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52729.2023.01413
ER  -


TY  - GEN
AU  - Zhang, J.
AU  - Peng, H.
AU  - Chen, Z.
AU  - Li, G.
AU  - Huang, J.
TI  - Photocaged activity-based probes for spatiotemporal detection of protein S-sulfenylation in living cells
AB  - Protein S-sulfenylation (protein sulfenic acid), as one of the most significant oxidative post-translational modifications (OxiPTMs), plays a vital role in regulating protein function. A variety of activity-based probes have been developed to profile sulfenic acid in living cells. However, due to the transient presence and low content of sulfenic acid in living cell, high doses of probes are needed to achieve efficient labeling. More importantly, current probes have no temporal control over sulfenic acid labeling. To overcome these limitations, two caged cysteine sulfenic acid probes DYn-2-ONB and DYn-2-Cou with either an o-nitrobenzyl or coumarin protecting group were developed in this study. Both probes can be efficiently uncaged via irradiation to produce the active C-nucleophile probe DYn-2. Labeling assay in living cells demonstrated DYn-2-ONB exhibited better labeling capacity compared with DYn-2, providing it as a powerful tool to detect protein S-sulfenylation in spatio-temporally controllable manner.
PB  - ChemRxiv
PY  - 2022
ST  - Photocaged activity-based probes for spatiotemporal detection of protein S-sulfenylation in living cells
Y2  - 2025/05/05/21:54:31
DO  - 10.26434/chemrxiv-2022-9cw47-v2
ER  -


TY  - GEN
AU  - Xu, M.
AU  - Gundogdu, E.
AU  - Lapin, M.
AU  - Donoser, M.
AU  - Bazzani, L.
TI  - Contrastive Language-Action Pre-training for Temporal Localization
AB  - Long-form video understanding requires designing approaches that are able to temporally localize activities or language. End-to-end training for such tasks is limited by the compute device memory constraints and lack of temporal annotations at large-scale. These limitations can be addressed by pre-training on large datasets of temporally trimmed videos supervised by class annotations. Once the video encoder is pre-trained, it is common practice to freeze it during fine-tuning. Therefore, the video encoder does not learn temporal boundaries and unseen classes, causing a domain gap with respect to the downstream tasks. Moreover, using temporally trimmed videos prevents to capture the relations between different action categories and the background context in a video clip which results in limited generalization capacity. To address these limitations, we propose a novel post-pre-training approach without freezing the video encoder which leverages language. We introduce a masked contrastive learning loss to capture visio-linguistic relations between activities, background video clips and language in the form of captions. Our experiments show that the proposed approach improves the state-of-the-art on temporal action localization, few-shot temporal action localization, and video language grounding tasks.
PB  - arXiv
PY  - 2022
ST  - Contrastive Language-Action Pre-training for Temporal Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52688.2022.01364
ER  -


TY  - GEN
AU  - Xiong, H.
AU  - Yang, D.
AU  - Nian, X.
AU  - Li, Z.
TI  - Spatiotemporal Pyramid Convolution Action Recognition Network Based on Shared Convolution Strategy
AB  - As a task of computer vision, action recognition has attracted extensive research and madegreat progress. However, due to the diversity of temporal and spatial scales of actions,capturing spatiotemporal information with difffferent scales accurately is still challenging.Previous works often use pyramid structures to capture multi-scale information. In thispaper, we propose a new pyramid structure to aggregate features maps of difffferent layers. And the pyramid structure can be used in difffferent backbone networks in a plug andplay way. Due to the action has multi-scale in time and space domain, we choose twostream network as the backbone network to capture multi-scale features. On this basis, wepropose a Spatiotemporal Pyramid Convolution Network (SPCN) which based on sharedconvolution for action recognition, which fuse spatiotemporal features of difffferent layers toenhance the representation ability of difffferent scales. In order to keep the consistency ofaction information in difffferent scale features, the pyramid structures use a shared module,which shares convolution parameters when extracting information from difffferent layer feature maps. We design difffferent output modules to verify the effffectiveness of the pyramidstructure. We evaluate our network on the UCF101 and Kinetics which is a large humanaction recognition dataset. Experiments show that the proposed SPCN outperforms otherleading methods.
PB  - SSRN
PY  - 2022
ST  - Spatiotemporal Pyramid Convolution Action Recognition Network Based on Shared Convolution Strategy
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4082629
ER  -


TY  - GEN
AU  - Fan, Q.
AU  - Kim, D.
AU  - Chen, C.-F.
AU  - Saenko, K.
AU  - Bargal, S.A.
TI  - Temporal Relevance Analysis for Video Action Models
AB  - In this paper, we provide a deep analysis of temporal modeling for action recognition, an important but underexplored problem in the literature. We first propose a new approach to quantify the temporal relationships between frames captured by CNN-based action models based on layer-wise relevance propagation. We then conduct comprehensive experiments and in-depth analysis to provide a better understanding of how temporal modeling is affected by various factors such as dataset, network architecture, and input frames. With this, we further study some important questions for action recognition that lead to interesting findings. Our analysis shows that there is no strong correlation between temporal relevance and model performance; and action models tend to capture local temporal information, but less long-range dependencies. Our codes and models will be publicly available.
PB  - arXiv
PY  - 2022
ST  - Temporal Relevance Analysis for Video Action Models
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-030-03801-4_11
ER  -


TY  - GEN
AU  - Hu, J.
AU  - Wang, B.
AU  - Guo, C.
AU  - Zhuang, L.
AU  - Jiang, Y.
TI  - Estimation of Reliable Proposal Quality for Temporal Action Detection
AB  - Temporal action detection (TAD) aims to locate and recognize the actions in an untrimmed video. Anchor-free methods have made remarkable progress which mainly formulate TAD into two tasks: classification and localization using two separate branches. This paper reveals the temporal misalignment between the two tasks hindering further progress. To address this, we propose a new method that gives insights into moment and region perspectives simultaneously to align the two tasks by acquiring reliable proposal quality. For the moment perspective, Boundary Evaluate Module (BEM) is designed which focuses on local appearance and motion evolvement to estimate boundary quality and adopts a multi-scale manner to deal with varied action durations. For the region perspective, we introduce Region Evaluate Module (REM) which uses a new and efficient sampling method for proposal feature representation containing more contextual information compared with point feature to refine category score and proposal boundary. The proposed Boundary Evaluate Module and Region Evaluate Module (BREM) are generic, and they can be easily integrated with other anchor-free TAD methods to achieve superior performance. In our experiments, BREM is combined with two different frameworks and improves the performance on THUMOS14 by 3.6% and 1.0% respectively, reaching a new state-of-the-art (63.6% average mAP). Meanwhile, a competitive result of 36.2% average mAP is achieved on ActivityNet-1.3 with the consistent improvement of BREM. The codes are released at https://github.com/Junshan233/BREM.
PB  - arXiv
PY  - 2022
ST  - Estimation of Reliable Proposal Quality for Temporal Action Detection
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3503161.3548029
ER  -


TY  - GEN
AU  - Guermal, M.
AU  - Dai, R.
AU  - Brémond, F.
TI  - THORN: Temporal Human-Object Relation Network for Action Recognition
AB  - Most action recognition models treat human activities as unitary events. However, human activities often follow a certain hierarchy. In fact, many human activities are compositional. Also, these actions are mostly human-object interactions. In this paper we propose to recognize human action by leveraging the set of interactions that define an action. In this work, we present an end-to-end network: THORN, that can leverage important human-object and object-object interactions to predict actions. This model is built on top of a 3D backbone network. The key components of our model are: 1) An object representation filter for modeling object. 2) An object relation reasoning module to capture object relations. 3) A classification layer to predict the action labels. To show the robustness of THORN, we evaluate it on EPIC-Kitchen55 and EGTEA Gaze+, two of the largest and most challenging first-person and human-object interaction datasets. THORN achieves state-of-the-art performance on both datasets.
PB  - arXiv
PY  - 2022
ST  - THORN
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icpr56361.2022.9956630
ER  -


TY  - GEN
AU  - Chen, S.
AU  - Chen, C.
AU  - Li, W.
AU  - Tao, X.
AU  - Guo, Y.
TI  - Faster-TAD: Towards Temporal Action Detection with Proposal Generation and Classification in a Unified Network
AB  - Temporal action detection (TAD) aims to detect the semantic labels and boundaries of action instances in untrimmed videos. Current mainstream approaches are multi-step solutions, which fall short in efficiency and flexibility. In this paper, we propose a unified network for TAD, termed Faster-TAD, by re-purposing a Faster-RCNN like architecture. To tackle the unique difficulty in TAD, we make important improvements over the original framework. We propose a new Context-Adaptive Proposal Module and an innovative Fake-Proposal Generation Block. What's more, we use atomic action features to improve the performance. Faster-TAD simplifies the pipeline of TAD and gets remarkable performance on lots of benchmarks, i.e., ActivityNet-1.3 (40.01% mAP), HACS Segments (38.39% mAP), SoccerNet-Action Spotting (54.09% mAP). It outperforms existing single-network detector by a large margin.
PB  - arXiv
PY  - 2022
ST  - Faster-TAD
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-030-60796-8_13
ER  -


TY  - GEN
AU  - Liu, X.
AU  - Bai, S.
AU  - Bai, X.
TI  - An Empirical Study of End-to-End Temporal Action Detection
AB  - Temporal action detection (TAD) is an important yet challenging task in video understanding. It aims to simultaneously predict the semantic label and the temporal interval of every action instance in an untrimmed video. Rather than end-to-end learning, most existing methods adopt a head-only learning paradigm, where the video encoder is pre-trained for action classification, and only the detection head upon the encoder is optimized for TAD. The effect of end-to-end learning is not systematically evaluated. Besides, there lacks an in-depth study on the efficiency-accuracy trade-off in end-to-end TAD. In this paper, we present an empirical study of end-to-end temporal action detection. We validate the advantage of end-to-end learning over head-only learning and observe up to 11% performance improvement. Besides, we study the effects of multiple design choices that affect the TAD performance and speed, including detection head, video encoder, and resolution of input videos. Based on the findings, we build a mid-resolution baseline detector, which achieves the state-of-the-art performance of end-to-end methods while running more than 4× faster. We hope that this paper can serve as a guide for end-to-end learning and inspire future research in this field. Code and models are available at https://github.com/xlliu7/E2E-TAD.
PB  - arXiv
PY  - 2022
ST  - An Empirical Study of End-to-End Temporal Action Detection
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52688.2022.01938
ER  -


TY  - GEN
AU  - Cheng, F.
AU  - Bertasius, G.
TI  - TallFormer: Temporal Action Localization with a Long-memory Transformer
AB  - Most modern approaches in temporal action localization divide this problem into two parts: (i) short-term feature extraction and (ii) long-range temporal boundary localization. Due to the high GPU memory cost caused by processing long untrimmed videos, many methods sacrifice the representational power of the short-term feature extractor by either freezing the backbone or using a small spatial video resolution. This issue becomes even worse with the recent video transformer models, many of which have quadratic memory complexity. To address these issues, we propose TallFormer, a memory-efficient and end-to-end trainable Temporal Action Localization transformer with Long-term memory. Our long-term memory mechanism eliminates the need for processing hundreds of redundant video frames during each training iteration, thus, significantly reducing the GPU memory consumption and training time. These efficiency savings allow us (i) to use a powerful video transformer feature extractor without freezing the backbone or reducing the spatial video resolution, while (ii) also maintaining long-range temporal boundary localization capability. With only RGB frames as input and no external action recognition classifier, TallFormer outperforms previous state-of-the-arts by a large margin, achieving an average mAP of 59.1% on THUMOS14 and 35.6% on ActivityNet-1.3. The code is public available.
PB  - arXiv
PY  - 2022
ST  - TallFormer
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-19830-4_29
ER  -


TY  - GEN
AU  - Hu, H.
AU  - Dong, S.
AU  - Zhao, Y.
AU  - Li, Z.
AU  - Gao, S.
TI  - TransRAC: Encoding Multi-scale Temporal Correlation with Transformers for Repetitive Action Counting
AB  - Counting repetitive actions are widely seen in human activities such as physical exercise. Existing methods focus on performing repetitive action counting in short videos, which is tough for dealing with longer videos in more realistic scenarios. In the data-driven era, the degradation of such generalization capability is mainly attributed to the lack of long video datasets. To complement this margin, we introduce a new large-scale repetitive action counting dataset covering a wide variety of video lengths, along with more realistic situations where action interruption or action inconsistencies occur in the video. Besides, we also provide a fine-grained annotation of the action cycles instead of just counting annotation along with a numerical value. Such a dataset contains 1,451 videos with about 20,000 annotations, which is more challenging. For repetitive action counting towards more realistic scenarios, we further propose encoding multi-scale temporal correlation with transformers that can take into account both performance and efficiency. Furthermore, with the help of fine-grained annotation of action cycles, we propose a density map regressionbased method to predict the action period, which yields better performance with sufficient interpretability. Our proposed method outperforms state-of-the-art methods on all datasets and also achieves better performance on the unseen dataset without fine-tuning. The dataset and code are available https://svip-lab.github.io/dataset/RepCount-dataset.html.
PB  - arXiv
PY  - 2022
ST  - TransRAC
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52688.2022.01843
ER  -


TY  - GEN
AU  - Yan, J.
AU  - Wang, J.
AU  - Li, Q.
AU  - Wang, C.
AU  - Pu, S.
TI  - Weakly Supervised Regional and Temporal Learning for Facial Action Unit Recognition
AB  - Automatic facial action unit (AU) recognition is a challenging task due to the scarcity of manual annotations. To alleviate this problem, a large amount of efforts has been dedicated to exploiting various weakly supervised methods which leverage numerous unlabeled data. However, many aspects with regard to some unique properties of AUs, such as the regional and relational characteristics, are not sufficiently explored in previous works. Motivated by this, we take the AU properties into consideration and propose two auxiliary AU related tasks to bridge the gap between limited annotations and the model performance in a self-supervised manner via the unlabeled data. Specifically, to enhance the discrimination of regional features with AU relation embedding, we design a task of RoI inpainting to recover the randomly cropped AU patches. Meanwhile, a single image based optical flow estimation task is proposed to leverage the dynamic change of facial muscles and encode the motion information into the global feature representation. Based on these two self-supervised auxiliary tasks, local features, mutual relation and motion cues of AUs are better captured in the backbone network. Furthermore, by incorporating semi-supervised learning, we propose an end-to-end trainable framework named weakly supervised regional and temporal learning (WSRTL) for AU recognition. Extensive experiments on BP4D and DISFA demonstrate the superiority of our method and new state-of-the-art performances are achieved.
PB  - arXiv
PY  - 2022
ST  - Weakly Supervised Regional and Temporal Learning for Facial Action Unit Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tmm.2022.3160061
ER  -


TY  - GEN
AU  - Wang, Z.
AU  - Chen, X.
AU  - Zhou, R.
AU  - Dong, Z.
AU  - Wen, J.-R.
TI  - Sequential Recommendation with User Causal Behavior Discovery
AB  - The key of sequential recommendation lies in the accurate item correlation modeling. Previous models infer such information based on item co-occurrences, which may fail to capture the real causal relations, and impact the recommendation performance and explainability. In this paper, we equip sequential recommendation with a novel causal discovery module to capture causalities among user behaviors. Our general idea is firstly assuming a causal graph underlying item correlations, and then we learn the causal graph jointly with the sequential recommender model by fitting the real user behavior data. More specifically, in order to satisfy the causality requirement, the causal graph is regularized by a differentiable directed acyclic constraint. Considering that the number of items in recommender systems can be very large, we represent different items with a unified set of latent clusters, and the causal graph is defined on the cluster level, which enhances the model scalability and robustness. In addition, we provide theoretical analysis on the identifiability of the learned causal graph. To the best of our knowledge, this paper makes a first step towards combining sequential recommendation with causal discovery. For evaluating the recommendation performance, we implement our framework with different neural sequential architectures, and compare them with many state-of-the-art methods based on real-world datasets. Empirical studies manifest that our model can on average improve the performance by about 6.1% and 11.3% on F1 and NDCG, respectively. To evaluate the model explainability, we build a new dataset with human labeled explanations for both quantitative and qualitative analysis.
PB  - arXiv
PY  - 2022
ST  - Sequential Recommendation with User Causal Behavior Discovery
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icde55515.2023.00010
ER  -


TY  - GEN
AU  - Hashiguchi, R.
AU  - Tamaki, T.
TI  - Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition
AB  - Feature shifts have been shown to be useful for action recognition with CNN-based models since Temporal Shift Module (TSM) was proposed. It is based on frame-wise feature extraction with late fusion, and layer features are shifted along the time direction for the temporal interaction. TokenShift, a recent model based on Vision Transformer (ViT), also uses the temporal feature shift mechanism, which, however, does not fully exploit the structure of Multi-head Self-Attention (MSA) in ViT. In this paper, we propose Multi-head Self/Cross-Attention (MSCA), which fully utilizes the attention structure. TokenShift is based on a frame-wise ViT with features temporally shifted with successive frames (at time t+1 and t−1). In contrast, the proposed MSCA replaces MSA in the frame-wise ViT, and some MSA heads attend to successive frames instead of the current frame. The computation cost is the same as the frame-wise ViT and TokenShift as it simply changes the target to which the attention is taken. There is a choice about which of key, query, and value are taken from the successive frames, then we experimentally compared these variants with Kinetics400. We also investigate other variants in which the proposed MSCA is used along the patch dimension of ViT, instead of the head dimension. Experimental results show that a variant, MSCA-KV, shows the best performance and is better than TokenShift by 0.1% and then ViT by 1.2%.
PB  - arXiv
PY  - 2022
ST  - Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3665689.3665697
ER  -


TY  - GEN
AU  - Zhang, H.Z.
AU  - Yu, D.
AU  - Liu, X.
AU  - Ma, C.
AU  - Hu, Z.H.
TI  - Temporal Attention and Spatial Difference Net for Action Recognition
AB  - On existing large action recognition datasets, especially complex action datasets, existing methods still make this field challenging due to video background noise and difficulties in temporal modeling. To alleviate this issue, this paper proposes a new model, called Temporal Attention and Spatial Difference Net (TASDN), with the main objective of capturing multi-scale motion information. The core idea of TASDN is to obtain temporal information by spatial difference module (SDM) and temporal attention module​ (TAM). SDM obtains spatial features similar to optical flow in two stream model based on spatial difference, while Tam captures temporal features based on attention. Specifically, TASDN uses multi-rate video sampling to build a multi-level network. TASDN achieves 51.9% top1 accuracy on the Something-SomethingV1 dataset, which focuses on temporal modeling. In addition, TASDN also excelled on the Something-SomethingV2 and Kinetics-400 data, achieving top1 accuracy of 63.1% and 78.6%, respectively.
PB  - SSRN
PY  - 2022
ST  - Temporal Attention and Spatial Difference Net for Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4057335
ER  -


TY  - GEN
AU  - Gao, J.
AU  - Chen, M.
AU  - Xu, C.
TI  - Fine-grained Temporal Contrastive Learning for Weakly-supervised Temporal Action Localization
AB  - We target at the task of weakly-supervised action localization (WSAL), where only video-level action labels are available during model training. Despite the recent progress, existing methods mainly embrace a localization-by-classification paradigm and overlook the fruitful fine-grained temporal distinctions between video sequences, thus suffering from severe ambiguity in classification learning and classification-to-localization adaption. This paper argues that learning by contextually comparing sequence-to-sequence distinctions offers an essential inductive bias in WSAL and helps identify coherent action instances. Specifically, under a differentiable dynamic programming formulation, two complementary contrastive objectives are designed, including Fine-grained Sequence Distance (FSD) contrasting and Longest Common Subsequence (LCS) contrasting, where the first one considers the relations of various action/background proposals by using match, insert, and delete operators and the second one mines the longest common subsequences between two videos. Both contrasting modules can enhance each other and jointly enjoy the merits of discriminative action-background separation and alleviated task gap between classification and localization. Extensive experiments show that our method achieves state-of-the-art performance on two popular benchmarks. Our code is available at https: // github. com / MengyuanChen21/CVPR2022-FTCL.
PB  - arXiv
PY  - 2022
ST  - Fine-grained Temporal Contrastive Learning for Weakly-supervised Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52688.2022.01937
ER  -


TY  - GEN
AU  - Wu, L.
AU  - Zhang, C.
AU  - Zou, Y.
TI  - SpatioTemporal Focus for Skeleton-based Action Recognition
AB  - Graph convolutional networks (GCNs) are widely adopted in skeleton-based action recognition due to their powerful ability to model data topology. We argue that the performance of recent proposed skeleton-based action recognition methods is limited by the following factors. First, the predefined graph structures are shared throughout the network, lacking the flexibility and capacity to model the multi-grain semantic information. Second, the relations among the global joints are not fully exploited by the graph local convolution, which may lose the implicit joint relevance. For instance, actions such as running and waving are performed by the co-movement of body parts and joints, e.g., legs and arms, however, they are located far away in physical connection. Inspired by the recent attention mechanism, we propose a multi-grain contextual focus module, termed MCF, to capture the action associated relation information from the body joints and parts. As a result, more explainable representations for different skeleton action sequences can be obtained by MCF. In this study, we follow the common practice that the dense sample strategy of the input skeleton sequences is adopted and this brings much redundancy since number of instances has nothing to do with actions. To reduce the redundancy, a temporal discrimination focus module, termed TDF, is developed to capture the local sensitive points of the temporal dynamics. MCF and TDF are integrated into the standard GCN network to form a unified architecture, named STF-Net. It is noted that STF-Net provides the capability to capture robust movement patterns from these skeleton topology structures, based on multi-grain context aggregation and temporal dependency. Extensive experimental results show that our STF-Net significantly achieves state-of-the-art results on three challenging benchmarks NTU RGB+D 60, NTU RGB+D 120, and Kinetics-skeleton.
PB  - arXiv
PY  - 2022
ST  - SpatioTemporal Focus for Skeleton-based Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.patcog.2022.109231
ER  -


TY  - GEN
AU  - Zhang, P.
AU  - Omanska, A.
AU  - Ander, B.P.
AU  - Stamova, B.
AU  - Schumann, C.M.
TI  - Neuron-specific transcriptomic signatures indicate neuroinflammation and altered neuronal activity in ASD temporal cortex
AB  - Autism spectrum disorder (ASD) is a highly heterogeneous disorder, yet transcriptomic profiling of bulk brain tissue has identified substantial convergence among dysregulated genes and pathways in ASD. However, this approach lacks cell-specific resolution. We performed comprehensive transcriptomic analyses on bulk tissue and laser-capture microdissected (LCM) neurons of 59 postmortem human brains (27 ASD and 32 matched controls) in the superior temporal gyrus (STG) ranging from 2-73 years of age. In bulk tissue, synaptic signaling, heat shock protein-related pathways and RNA splicing were significantly altered in ASD. There was age-dependent dysregulation of genes involved in GABA (GAD1 and GAD2) and glutamate (SLC38A1) signaling pathways. In LCM neurons, AP-1 mediated neuroinflammation and insulin/IGF-1 signaling pathways were upregulated in ASD, while mitochondrial function, ribosome and spliceosome components were downregulated. GABA synthesizing enzymes GAD1 and GAD2 were both downregulated in ASD neurons. Alterations in small nucleolar RNAs (snoRNAs) associated with splicing events suggested interplay between snoRNA dysregulation and splicing disruption in neurons of individuals with ASD. Our findings supported the fundamental hypothesis of altered neuronal communication in ASD, demonstrated that inflammation was elevated at least in part in ASD neurons, and may reveal windows of opportunity for biotherapeutics to target the trajectory of gene expression and clinical manifestation of ASD throughout the human lifespan.
PB  - bioRxiv
PY  - 2022
ST  - Neuron-specific transcriptomic signatures indicate neuroinflammation and altered neuronal activity in ASD temporal cortex
Y2  - 2025/05/05/21:54:31
DO  - 10.1073/pnas.2206758120
ER  -


TY  - GEN
AU  - He, B.
AU  - Yang, X.
AU  - Kang, L.
AU  - Zhou, X.
AU  - Shrivastava, A.
TI  - ASM-Loc: Action-aware Segment Modeling for Weakly-Supervised Temporal Action Localization
AB  - Weakly-supervised temporal action localization aims to recognize and localize action segments in untrimmed videos given only video-level action labels for training. Without the boundary information of action segments, existing methods mostly rely on multiple instance learning (MIL), where the predictions of unlabeled instances (i.e., video snippets) are supervised by classifying labeled bags (i.e., untrimmed videos). However, this formulation typically treats snippets in a video as independent instances, ignoring the underlying temporal structures within and across action segments. To address this problem, we propose ASM-Loc, a novel WTAL framework that enables explicit, action-aware segment modeling beyond standard MIL-based methods. Our framework entails three segment-centric components: (i) dynamic segment sampling for compensating the contribution of short actions; (ii) intra- and inter-segment attention for modeling action dynamics and capturing temporal dependencies; (iii) pseudo instance-level supervision for improving action boundary prediction. Furthermore, a multi-step refinement strategy is proposed to progressively improve action proposals along the model training process. Extensive experiments on THUMOS-14 and ActivityNetv1.3 demonstrate the effectiveness of our approach, establishing new state of the art on both datasets. The code and models are publicly available at https://github.com/boheumd/ASM-Loc.
PB  - arXiv
PY  - 2022
ST  - ASM-Loc
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52688.2022.01355
ER  -


TY  - GEN
AU  - Bernhard, H.
AU  - Frederic, S.L.W.V.J.
AU  - Janssen, M.L.F.
AU  - Reithler, J.
AU  - Roberts, M.J.
TI  - Spatiotemporal patterns of sleep spindle activity in human anterior thalamus and cortex
AB  - Sleep spindles (8 - 16 Hz) are transient electrophysiological events during non-rapid eye movement sleep. While sleep spindles are routinely observed in the cortex using scalp electroencephalography (EEG), recordings of their thalamic counterparts have not been widely studied in humans. Based on a few existing studies, it has been hypothesized that spindles occur as largely local phenomena. We investigated intra-thalamic and thalamocortical spindle co-occurrence, which may underlie thalamocortical communication. We obtained scalp EEG and thalamic recordings from 7 patients that received bilateral deep brain stimulation (DBS) electrodes to the anterior thalamus for the treatment of drug resistant focal epilepsy. Spindles were categorized into subtypes based on their main frequency (i.e., slow (10±2 Hz) or fast (14±2 Hz)) and their level of thalamic involvement (spanning one channel, or spreading uni- or bilaterally within the thalamus). For the first time, we contrasted observed spindle patterns with permuted data to estimate random spindle co-occurrence. We found that multichannel spindle patterns were systematically coordinated at the thalamic and thalamocortical level. Importantly, distinct topographical patterns of thalamocortical spindle overlap were associated with slow and fast subtypes of spindles. These observations provide further evidence for coordinated spindle activity in thalamocortical networks.
PB  - bioRxiv
PY  - 2022
ST  - Spatiotemporal patterns of sleep spindle activity in human anterior thalamus and cortex
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.03.25.485812
ER  -


TY  - GEN
AU  - Zhang, C.
AU  - Yang, T.
AU  - Weng, J.
AU  - Wang, J.
AU  - Zou, Y.
TI  - Unsupervised Pre-training for Temporal Action Localization Tasks
AB  - Unsupervised video representation learning has made remarkable achievements in recent years. However, most existing methods are designed and optimized for video classification. These pre-trained models can be sub-optimal for temporal localization tasks due to the inherent discrepancy between video-level classification and clip-level localization. To bridge this gap, we make the first attempt to propose a self-supervised pretext task, coined as Pseudo Action Localization (PAL) to Unsupervisedly Pre-train feature encoders for Temporal Action Localization tasks (UP-TAL). Specifically, we first randomly select temporal regions, each of which contains multiple clips, from one video as pseudo actions and then paste them onto different temporal positions of the other two videos. The pretext task is to align the features of pasted pseudo action regions from two synthetic videos and maximize the agreement between them. Compared to the existing unsupervised video representation learning approaches, our PAL adapts better to downstream TAL tasks by introducing a temporal equivariant contrastive learning paradigm in a temporally dense and scale-aware manner. Extensive experiments show that PAL can utilize large-scale unlabeled video data to significantly boost the performance of existing TAL methods. Our codes and models will be made publicly available at https://github.com/zhang-can/UP-TAL.
PB  - arXiv
PY  - 2022
ST  - Unsupervised Pre-training for Temporal Action Localization Tasks
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52688.2022.01364
ER  -


TY  - GEN
AU  - He, N.
AU  - Zhang, Y.
AU  - Lin, Y.
AU  - Gao, X.
AU  - Jia, P.
TI  - Evolution of barchan dune interactions investigated by a downscaled water tunnel experiment: The temporal characteristics and a soliton-like behavior
AB  - This paper reports a downscaled water tunnel experiment to study the temporal characteristics of a double dune interaction system and the new pattern of dune interaction when the initial mass ratio of the two dunes is large. These topics are useful for a comprehensive understanding of the dune interaction system but were rarely covered before. The turnover time scale under dune interaction is defined, and its time averaged value is found to have a nonmonotonic relationship with the initial mass ratio. A nonmonotonic relationship is also found between the convexity of the downstream dune tip and the initial mass ratio. The stationary points of the two nonmonotonic curves above correspond to the same dune interaction pattern named “exchange-chasing,” which is considered indispensable in the classification map of dune interactions. The upstream dune acts as an energy transmitter between fluid flow and the downstream dune. A soliton-like behavior occurs when the downstream dune enlarges, where a small dune is detached from the downstream dune tip and gets passed by the upstream dune approximately without mass exchange. The activity of such temporary soliton is found to be negatively related with the initial dune spacing and positively related with the initial mass ratio.
PB  - TechRxiv
PY  - 2022
ST  - Evolution of barchan dune interactions investigated by a downscaled water tunnel experiment
Y2  - 2025/05/05/21:54:31
DO  - 10.36227/techrxiv.19391087
ER  -


TY  - GEN
AU  - He, N.
AU  - Zhang, Y.
AU  - Lin, Y.
AU  - Gao, X.
AU  - Jia, P.
TI  - Evolution of barchan dune interactions investigated by a downscaled water tunnel experiment: the temporal characteristics and a soliton-like behavior
AB  - This paper reports a downscaled water tunnel experiment to study the temporal characteristics of a double dune interaction system and the new pattern of dune interaction when the initial mass ratio of the two dunes is large. These topics are useful for a comprehensive understanding of the dune interaction system but were rarely covered before. The turnover time scale under dune interaction is defined, and its time averaged value is found to have a nonmonotonic relationship with the initial mass ratio. A nonmonotonic relationship is also found between the convexity of the downstream dune tip and the initial mass ratio. The stationary points of the two nonmonotonic curves above correspond to the same dune interaction pattern named “exchange-chasing,” which is considered indispensable in the classification map of dune interactions. The upstream dune acts as an energy transmitter between fluid flow and the downstream dune. A soliton-like behavior occurs when the downstream dune enlarges, where a small dune is detached from the downstream dune tip and gets passed by the upstream dune approximately without mass exchange. The activity of such temporary soliton is found to be negatively related with the initial dune spacing and positively related with the initial mass ratio.
PB  - arXiv
PY  - 2022
ST  - Evolution of barchan dune interactions investigated by a downscaled water tunnel experiment
Y2  - 2025/05/05/21:54:31
DO  - 10.36227/techrxiv.19391087
ER  -


TY  - GEN
AU  - Viet, K.V.H.
AU  - Yamazaki, K.
AU  - Truong, S.
AU  - Sugimoto, A.
AU  - Le, N.
TI  - ABN: Agent-Aware Boundary Networks for Temporal Action Proposal Generation
AB  - Temporal action proposal generation (TAPG) aims to estimate temporal intervals of actions in untrimmed videos, which is a challenging yet plays an important role in many tasks of video analysis and understanding. Despite the great achievement in TAPG, most existing works ignore the human perception of interaction between agents and the surrounding environment by applying a deep learning model as a black-box to the untrimmed videos to extract video visual representation. Therefore, it is beneficial and potentially improve the performance of TAPG if we can capture these interactions between agents and the environment. In this paper, we propose a novel framework named Agent-Aware Boundary Network (ABN), which consists of two sub-networks (i) an Agent-Aware Representation Network to obtain both agent-agent and agents-environment relationships in the video representation, and (ii) a Boundary Generation Network to estimate the confidence score of temporal intervals. In the Agent-Aware Representation Network, the interactions between agents are expressed through local pathway, which operates at a local level to focus on the motions of agents whereas the overall perception of the surroundings are expressed through global pathway, which operates at a global level to perceive the effects of agents-environment. Comprehensive evaluations on 20-action THUMOS-14 and 200-action ActivityNet-1.3 datasets with different backbone networks (i.e C3D, SlowFast and Two-Stream) show that our proposed ABN robustly outperforms state-of-the-art methods regardless of the employed backbone network on TAPG. We further examine the proposal quality by leveraging proposals generated by our method onto temporal action detection (TAD) frameworks and evaluate their detection performances. The source code can be found in this URL.
PB  - arXiv
PY  - 2022
ST  - ABN
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/access.2021.3110973
ER  -


TY  - GEN
AU  - Wang, Q.
AU  - Zhang, Y.
AU  - Zheng, Y.
AU  - Pan, P.
TI  - RCL: Recurrent Continuous Localization for Temporal Action Detection
AB  - Temporal representation is the cornerstone of modern action detection techniques. State-of-the-art methods mostly rely on a dense anchoring scheme, where anchors are sampled uniformly over the temporal domain with a discretized grid, and then regress the accurate boundaries. In this paper, we revisit this foundational stage and introduce Recurrent Continuous Localization (RCL), which learns a fully continuous anchoring representation. Specifically, the proposed representation builds upon an explicit model conditioned with video embeddings and temporal coordinates, which ensure the capability of detecting segments with arbitrary length. To optimize the continuous representation, we develop an effective scale-invariant sampling strategy and recurrently refine the prediction in subsequent iterations. Our continuous anchoring scheme is fully differentiable, allowing to be seamlessly integrated into existing detectors, e.g., BMN [20] and G-TAD [41]. Extensive experiments on two benchmarks demonstrate that our continuous representation steadily surpasses other discretized counterparts by ∼2% mAP. As a result, RCL achieves 52.92% mAP@0.5 on THUMOS14 and 37.65% mAP on ActivtiyNet v1.3, outperforming all existing single-model detectors.
PB  - arXiv
PY  - 2022
ST  - RCL
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52688.2022.01320
ER  -


TY  - GEN
AU  - Stuede, M.
AU  - Schappler, M.
TI  - Non-Parametric Modeling of Spatio-Temporal Human Activity Based on Mobile Robot Observations
AB  - This work presents a non-parametric spatio-temporal model for mapping human activity by mobile autonomous robots in a long-term context. Based on Variational Gaussian Process Regression, the model incorporates prior information of spatial and temporal-periodic dependencies to create a continuous representation of human occurrences. The inhomogeneous data distribution resulting from movements of the robot is included in the model via a heteroscedastic likelihood function and can be accounted for as predictive uncertainty. Using a sparse formulation, data sets over multiple weeks and several hundred square meters can be used for model creation. The experimental evaluation, based on multiweek data sets, demonstrates that the proposed approach outperforms the state of the art both in terms of predictive quality and subsequent path planning.
PB  - arXiv
PY  - 2022
ST  - Non-Parametric Modeling of Spatio-Temporal Human Activity Based on Mobile Robot Observations
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iros47612.2022.9982067
ER  -


TY  - GEN
AU  - Bao, W.
AU  - Yu, Q.
AU  - Kong, Y.
TI  - OpenTAL: Towards Open Set Temporal Action Localization
AB  - Temporal Action Localization (TAL) has experienced remarkable success under the supervised learning paradigm. However, existing TAL methods are rooted in the closed set assumption, which cannot handle the inevitable unknown actions in open-world scenarios. In this paper, we, for the first time, step toward the Open Set TAL (OSTAL) problem and propose a general framework OpenTAL based on Evidential Deep Learning (EDL). Specifically, the OpenTAL consists of uncertainty-aware action classification, actionness prediction, and temporal location regression. With the proposed importance-balanced EDL method, classification uncertainty is learned by collecting categorical evidence majorly from important samples. To distinguish the unknown actions from background video frames, the actionness is learned by the positive-unlabeled learning. The classification uncertainty is further calibrated by leveraging the guidance from the temporal localization quality. The OpenTAL is general to enable existing TAL models for open set scenarios, and experimental results on THUMOS14 and ActivityNet1.3 benchmarks show the effectiveness of our method. The code and pre-trained models are released at https://www.rit.edu/actionlab/opental.
PB  - arXiv
PY  - 2022
ST  - OpenTAL
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52688.2022.00299
ER  -


TY  - GEN
AU  - Xu, Y.
AU  - Yang, J.
AU  - Cao, H.
AU  - Wu, M.
AU  - Chen, Z.
TI  - Source-free Video Domain Adaptation by Learning Temporal Consistency for Action Recognition
AB  - Video-based Unsupervised Domain Adaptation (VUDA) methods improve the robustness of video models, enabling them to be applied to action recognition tasks across different environments. However, these methods require constant access to source data during the adaptation process. Yet in many real-world applications, subjects and scenes in the source video domain should be irrelevant to those in the target video domain. With the increasing emphasis on data privacy, such methods that require source data access would raise serious privacy issues. Therefore, to cope with such concern, a more practical domain adaptation scenario is formulated as the Source-Free Video-based Domain Adaptation (SFVDA). Though there are a few methods for Source-Free Domain Adaptation (SFDA) on image data, these methods yield degenerating performance in SFVDA due to the multi-modality nature of videos, with the existence of additional temporal features. In this paper, we propose a novel Attentive Temporal Consistent Network (ATCoN) to address SFVDA by learning temporal consistency, guaranteed by two novel consistency objectives, namely feature consistency and source prediction consistency, performed across local temporal features. ATCoN further constructs effective overall temporal features by attending to local temporal features based on prediction confidence. Empirical results demonstrate the state-of-the-art performance of ATCoN across various cross-domain action recognition benchmarks. Code is provided at https://github.com/xuyu0010/ATCoN.
PB  - arXiv
PY  - 2022
ST  - Source-free Video Domain Adaptation by Learning Temporal Consistency for Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-19830-4_9
ER  -


TY  - GEN
AU  - Huang, L.
AU  - Wang, L.
AU  - Li, H.
TI  - Weakly Supervised Temporal Action Localization via Representative Snippet Knowledge Propagation
AB  - Weakly supervised temporal action localization aims to localize temporal boundaries of actions and simultaneously identify their categories with only video-level category labels. Many existing methods seek to generate pseudo labels for bridging the discrepancy between classification and localization, but usually only make use of limited contextual information for pseudo label generation. To alleviate this problem, we propose a representative snippet summarization and propagation framework. Our method seeks to mine the representative snippets in each video for propagating information between video snippets to generate better pseudo labels. For each video, its own representative snippets and the representative snippets from a memory bank are propagated to update the input features in an intra- and inter-video manner. The pseudo labels are generated from the temporal class activation maps of the updated features to rectify the predictions of the main branch. Our method obtains superior performance in comparison to the existing methods on two benchmarks, THUMOS14 and ActivityNet1.3, achieving gains as high as 1.2% in terms of average mAP on THUMOS14. Our code is available at https://github.com/LeonHLJ/RSKP.
PB  - arXiv
PY  - 2022
ST  - Weakly Supervised Temporal Action Localization via Representative Snippet Knowledge Propagation
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52688.2022.00327
ER  -


TY  - GEN
AU  - Mazzucato, L.
TI  - Neural mechanisms underlying the temporal organization of naturalistic animal behavior
AB  - Naturalistic animal behavior exhibits a strikingly complex organization in the temporal domain, whose variability stems from at least three sources: hierarchical, contextual, and stochastic. What are the neural mechanisms and computational principles generating such complex temporal features? In this review, we provide a critical assessment of the existing behavioral and neurophysiological evidence for these sources of temporal variability in naturalistic behavior. We crystallize recent studies which converge on an emergent mechanistic theory of temporal variability based on attractor neural networks and metastable dynamics, arising from the coordinated interactions between mesoscopic neural circuits. We highlight the crucial role played by structural heterogeneities and by noise arising in mesoscopic circuits. We assess the shortcomings and missing links in the current theoretical and experimental literature and propose new directions of investigations to fill these gaps.
PB  - arXiv
PY  - 2022
ST  - Neural mechanisms underlying the temporal organization of naturalistic animal behavior
Y2  - 2025/05/05/21:54:31
DO  - 10.7554/elife.76577
ER  -


TY  - GEN
AU  - Zhao, C.
AU  - Ramazanova, M.
AU  - Xu, M.
AU  - Ghanem, B.
TI  - SegTAD: Precise Temporal Action Detection via Semantic Segmentation
AB  - Temporal action detection (TAD) is an important yet challenging task in video analysis. Most existing works draw inspiration from image object detection and tend to reformulate it as a proposal generation - classification problem. However, there are two caveats with this paradigm. First, proposals are not equipped with annotated labels, which have to be empirically compiled, thus the information in the annotations is not necessarily precisely employed in the model training process. Second, there are large variations in the temporal scale of actions, and neglecting this fact may lead to deficient representation in the video features. To address these issues and precisely model temporal action detection, we formulate the task of temporal action detection in a novel perspective of semantic segmentation. Owing to the 1-dimensional property of TAD, we are able to convert the coarse-grained detection annotations to fine-grained semantic segmentation annotations for free. We take advantage of them to provide precise supervision so as to mitigate the impact induced by the imprecise proposal labels. We propose an end-to-end framework SegTAD composed of a 1D semantic segmentation network (1D-SSN) and a proposal detection network (PDN). We evaluate SegTAD on two important large-scale datasets for action detection and it shows competitive performance on both datasets.
PB  - arXiv
PY  - 2022
ST  - SegTAD
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-25069-9_37
ER  -


TY  - GEN
AU  - Liarte, D.B.
AU  - Thornton, S.J.
AU  - Schwen, E.
AU  - Chowdhury, D.
AU  - Sethna, J.P.
TI  - Universal scaling for disordered viscoelastic matter II: Collapses, global behavior and spatio-temporal properties
AB  - Disordered viscoelastic materials are ubiquitous and exhibit fascinating invariant scaling properties. In a companion article [1], we have presented comprehensive new results for the critical behavior of the dynamic susceptibility of disordered elastic systems near the onset of rigidity. Here we provide additional details of the derivation of the singular scaling forms of the longitudinal response near both jamming and rigidity percolation. We then discuss global aspects associated with these forms, and make scaling collapse plots for both undamped and overdamped dynamics in both the rigid and floppy phases. We also derive critical exponents, invariant scaling combinations and analytical formulas for universal scaling functions of several quantities such as transverse and density responses, elastic moduli, viscosities, and correlation functions. Finally, we discuss tentative experimental protocols to measure these behaviors in colloidal suspensions.
PB  - arXiv
PY  - 2022
ST  - Universal scaling for disordered viscoelastic matter II
Y2  - 2025/05/05/21:54:31
DO  - 10.1103/physreve.106.l052601
ER  -


TY  - GEN
AU  - Baghali, S.
AU  - Guo, Z.
AU  - Hasan, S.
TI  - Investigating the Spatiotemporal Charging Demand and Travel Behavior of Electric Vehicles Using GPS Data: A Machine Learning Approach
AB  - The increasing market penetration of electric vehicles (EVs) may change the travel behavior of drivers and pose a significant electricity demand on the power system. Since the electricity demand depends on the travel behavior of EVs, which are inherently uncertain, the forecasting of daily charging demand (CD) will be a challenging task. In this paper, we use the recorded GPS data of EVs and conventional gasoline-powered vehicles from the same city to investigate the potential shift in the travel behavior of drivers from conventional vehicles to EVs and forecast the spatiotemporal patterns of daily CD. Our analysis reveals that the travel behavior of EVs and conventional vehicles are similar. Also, the forecasting results indicate that the developed models can generate accurate spatiotemporal patterns of the daily CD.
PB  - arXiv
PY  - 2022
ST  - Investigating the Spatiotemporal Charging Demand and Travel Behavior of Electric Vehicles Using GPS Data
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/pesgm48719.2022.9917046
ER  -


TY  - GEN
AU  - Wang, S.
AU  - Chang, P.
AU  - Wang, N.
AU  - Yuan, K.
AU  - Yang, J.
TI  - Quantitatively determine the dominant driving factors of the spatial-temporal changes of vegetation-impacts of global change and human activity
AB  - The differences of spatial-temporal distribution patterns and dominant driving factors of vegetation evolution in different regions and different historical periods were not clearly. Therefore, this study introduced the gravity center model and geodetector to analyze the spatial-temporal change characteristics and dominant driving factors of vegetation NDVI in China during 1981-2019 from the perspective of geographic divisions. Results showed that: (1) During 1981-2019, the average vegetation coverage in China, as a whole, showed an increasing trend and zones with obviously increasing vegetation NDVI were mainly distributed in the middle reaches of the Yellow River basin and the upper reaches of the Yangtze River. (2) During 1981-2019, the gravity centers of vegetation NDVI, nationally, were mainly concentrated in Yan'an City and Tongchuan City, showing a southward migration trend as a whole, which indicates that the increment and growth rates of the southern were greater than those of the northern part. (3) The growth of vegetation in southern China was primarily affected by temperature, while that of northern China was largely influenced by precipitation. (4) Specific factors, including land use, precipitation and soil types exerted significant influences on the vegetation NDVI, and their interaction could enhance the influence of single factor.
PB  - Research Square
PY  - 2022
ST  - Quantitatively determine the dominant driving factors of the spatial-temporal changes of vegetation-impacts of global change and human activity
Y2  - 2025/05/05/21:54:31
DO  - 10.1515/geo-2022-0374
ER  -


TY  - GEN
AU  - Abbey, A.
AU  - Marmor, Y.
AU  - Shahar, Y.
AU  - Mokryn, O.
TI  - Exploring the effects of activity-preserving time dilation on the dynamic interplay of airborne contagion processes and temporal networks using an interaction-driven model
AB  - Contacts' temporal ordering and dynamics are crucial for understanding the transmission of infectious diseases. We introduce an interaction-driven model of an airborne disease over contact networks. We demonstrate our interaction-driven contagion model, instantiated for COVID-19, over history-maintaining random temporal networks and real-world contacts. We use it to evaluate temporal, spatiotemporal, and spatial social distancing policies. We find that a spatial distancing policy is mainly beneficial at the early stages of a disease. We then continue to evaluate temporal social distancing, that is, timeline dilation that maintains the activity potential. We expand our model to consider the exposure to viral load, which we correlate with meetings' duration. Using real-life contact data, we demonstrate the beneficial effect of timeline dilation on overall infection rates. Our results demonstrate that given the same transmission level, there is a decrease in the disease's infection rate and overall prevalence under timeline dilation conditions. We further show that slow-spreading pathogens (i.e., require more prolonged exposure to infect) spread roughly at the same rate as fast-spreading ones in highly active communities. This is surprising since slower pathogens follow paths that include longer meetings, while faster pathogens can potentially follow paths that include shorter meetings, which are more common. Our results demonstrate that the temporal dynamics of a community have a more significant effect on the spread of the disease than the characteristics of the spreading processes.
PB  - arXiv
PY  - 2022
ST  - Exploring the effects of activity-preserving time dilation on the dynamic interplay of airborne contagion processes and temporal networks using an interaction-driven model
Y2  - 2025/05/05/21:54:31
DO  - 10.1103/physrevlett.112.118702
ER  -


TY  - GEN
AU  - Wang, H.
AU  - Liu, G.
AU  - Zhao, B.
TI  - Tme: Temporal and Motion Enhancement for Action Recognition
AB  - The key to video understanding is to capture temporal information. 3D CNNs can achieve good performance by directly fusing spatial and temporal features but are computationally intensive. Conventional 2D CNNs perform well in image recognition, but their inability to extract temporal features leads to poor performance in video action recognition. In this work, we aim to identify temporal relationships for video action recognition. To this end, we propose a temporal and motion enhancement (TME) module consisting of two submodules: multipath temporal enhancement(MTE) module and a long short-range Motion Enhancement(LSME) module. The MTE module consists of four paths: the residual path, temporal aggregation(TA) path, channel-wise temporal excitation(CTE) path and spatial-attention temporal excitation(STE) path. The residual path makes part of the information in the original activation accessible. The TA path employs convolution on the temporal dimension to characterize temporal representation. The CTE path rebuilds channelwise feature relationships in terms of the temporal aspect. The STE path introduces attention mechanisms and explores the temporal modeling. We aggregate features of different paths by splitting channels, which is efficient for model inference. The LSME module calculates feature-level temporal differences between long and short range frames, which enhances the motion-related features. The TME module is plug-and-play and can be inserted into 2D CNNs to form an effective TMENet with limited extra computational cost. The extensive experiments on several action recognition datasets, such as Kinetics400, Something-Something V1&V2, HMDB51, and UCF101, demonstrate that the TMENet achieves the state-of-the-art performance with high efficiency. The code will be released soon.
PB  - SSRN
PY  - 2022
ST  - Tme
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4039887
ER  -


TY  - GEN
AU  - Kimura, S.
AU  - Srisuknimit, V.
AU  - McCarty, K.
AU  - Kranzusch, P.J.
AU  - Waldor, M.K.
TI  - Sequential action of a tRNA base editor in conversion of cytidine to pseudouridine
AB  - Post-transcriptional RNA editing modulates gene expression in a condition-dependent fashion. We recently discovered C-to-Ψ editing in Vibrio cholerae tRNA. Here, we characterize the biogenesis, regulation, and functions of this previously undescribed RNA editing process. We show that an enzyme, TrcP, mediates the editing of C-to-U followed by the conversion of U to Ψ, consecutively. AlphaFold-2 predicts that TrcP consists of two globular domains, including a novel cytidine deaminase and a pseudouridylase, along with a long helical domain. The latter domain tethers tRNA substrates during both the C-to-U editing and pseudouridylation, likely enabling a substrate channeling mechanism for efficient catalysis all the way to the terminal product. C-to-Ψ editing both requires and suppresses other modifications, creating an interdependent network of modifications in the tRNA anticodon loop that facilitates coupling of tRNA modification states to iron availability. Our findings provide mechanistic insights into an RNA editing process that likely promotes environmental adaptation.
PB  - bioRxiv
PY  - 2022
ST  - Sequential action of a tRNA base editor in conversion of cytidine to pseudouridine
Y2  - 2025/05/05/21:54:31
DO  - 10.1038/s41467-022-33714-x
ER  -


TY  - GEN
AU  - Tang, A.
AU  - Liu, Y.
AU  - Zhang, X.
AU  - Li, Z.
AU  - Mai, B.
TI  - Spatiotemporal Distribution, Partitioning Behavior and Flux of Legacy and Emerging Per- and Polyfluoroalkyl Substances (Pfas) in Surface Water and Sediment from the Largest Freshwater Lake in China
AB  - Thirty-five legacy and emerging per- and polyfluoroalkyl substances (PFAS) were analyzed in surface water and sediments collected from Poyang Lake, the largest freshwater lake in China. The ƩPFAS concentrations ranged from 23 to 1000 ng/L in water dissolved phase, 1.3 to 9.8 ng/L in suspended particulate matters, and 0.26 to 2.9 ng/g dw in sediments. Short-chain and emerging PFAS were predominant in surface water and sediments, replacing perfluorooctanoic acid (PFOA) and perfluorooctane sulfonate (PFOS). The detection frequencies of hexafluoropropylene oxide dimer/trimer acid (HFPO-DA/TA), 6:2 and 8:2 chlorinated polyfluorinated ether sulfonic acids (6:2 and 8:2 Cl-PFESAs), 6:2 fluorotelomer sulfonate (6:2 FTS), and sodium p -perfluorous nonenoxybenzene sulfonate (OBS) in all samples were 100%, indicating that these emerging PFAS have been widely produced and used in this region. The high concentrations of HFPO-DA/TA, 6:2 FTS, 6:2, 8:2 Cl-PFESAs, and OBS in sediments and their higher water–sediment distribution coefficients than those of predecessors (PFOA or PFOS) suggest that lake sediments could be an important long-term sink for these emerging alternatives. The positive matrix factorization model demonstrated that food packaging and textile treatments (50%) and fluoropolymer manufacturing (26% for alternative sources and 8.2% for legacy sources) were the two major sources of PFAS in Poyang Lake. The influx and outflux of total PFAS in Poyang Lake were 9.0 and 12.8 ton/year, respectively, and the OBS flux was estimated for the first time. The results provide insights into the environmental behavior and fate of emerging PFAS in freshwater ecosystems.
PB  - SSRN
PY  - 2022
ST  - Spatiotemporal Distribution, Partitioning Behavior and Flux of Legacy and Emerging Per- and Polyfluoroalkyl Substances (Pfas) in Surface Water and Sediment from the Largest Freshwater Lake in China
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.3971353
ER  -


TY  - GEN
AU  - Ignat, O.
AU  - Castro, S.
AU  - Zhou, Y.
AU  - Shan, D.
AU  - Mihalcea, R.
TI  - When Did It Happen? Duration-informed Temporal Localization of Narrated Actions in Vlogs
AB  - We consider the task of temporal human action localization in lifestyle vlogs. We introduce a novel dataset consisting of manual annotations of temporal localization for 13,000 narrated actions in 1,200 video clips. We present an extensive analysis of this data, which allows us to better understand how the language and visual modalities interact throughout the videos. We propose a simple yet effective method to localize the narrated actions based on their expected duration. Through several experiments and analyses, we show that our method brings complementary information with respect to previous methods, and leads to improvements over previous work for the task of temporal action localization.
PB  - arXiv
PY  - 2022
ST  - When Did It Happen? Duration-informed Temporal Localization of Narrated Actions in Vlogs
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3495211
ER  -


TY  - GEN
AU  - Chou, C.T.
TI  - Using transcription-based detectors to emulate the behaviour of sequential probability ratio-based concentration detectors
AB  - The sequential probability ratio test (SPRT) from statistics is known to have the least mean decision time compared to other sequential or fixed-time tests for given error rates. In some circumstances, cells need to make decisions accurately and quickly, therefore it has been suggested the SPRT may be used to understand the speed-accuracy tradeoff in cellular decision making. It is generally thought that in order for cells to make use of the SPRT, it is necessary to find biochemical circuits that can compute the log-likelihood ratio needed for the SPRT. However, this paper takes a different approach. We recognise that the high-level behaviour of the SPRT is defined by its positive detection or hit rate, and the computation of the log-likelihood ratio is just one way to realise this behaviour. In this paper, we will present a method which uses a transcription-based detector to emulate the hit rate of the SPRT without computing the exact log-likelihood ratio. We consider the problem of using a promoter with multiple binding sites to accurately and quickly detect whether the concentration of a transcription factor is above a target level. We show that it is possible to find binding and unbinding rates of the transcription factor to the promoter’s binding sites so that the probability that the amount of mRNA produced will be higher than a threshold is approximately equal to the hit rate of the SPRT detector. Moreover, we show that the average time that this transcription-based detector needs to make a positive detection is less than or equal to that of the SPRT for a wide range of concentrations. We remark that the last statement does not contradict Wald’s optimality result because our transcription-based detector uses an open-ended test.
PB  - arXiv
PY  - 2022
ST  - Using transcription-based detectors to emulate the behaviour of sequential probability ratio-based concentration detectors
Y2  - 2025/05/05/21:54:31
DO  - 10.1103/physreve.106.054403
ER  -


TY  - GEN
AU  - Tseng, M.-R.
AU  - Gupta, A.
AU  - Tang, C.-K.
AU  - Tai, Y.-W.
TI  - HAA4D: Few-Shot Human Atomic Action Recognition via 3D Spatio-Temporal Skeletal Alignment
AB  - Human actions involve complex pose variations and their 2D projections can be highly ambiguous. Thus 3D spatio-temporal or 4D (i.e., 3D+T) human skeletons, which are photometric and viewpoint invariant, are an excellent alternative to 2D+T skeletons/pixels to improve action recognition accuracy. This paper proposes a new 4D dataset HAA4D which consists of more than 3,300 RGB videos in 300 human atomic action classes. HAA4D is clean, diverse, class-balanced where each class is viewpoint-balanced with the use of 4D skeletons, in which as few as one 4D skeleton per class is sufficient for training a deep recognition model. Further, the choice of atomic actions makes annotation even easier, because each video clip lasts for only a few seconds. All training and testing 3D skeletons in HAA4D are globally aligned, using a deep alignment model to the same global space, making each skeleton face the negative z-direction. Such alignment makes matching skeletons more stable by reducing intraclass variations and thus with fewer training samples per class needed for action recognition. Given the high diversity and skeletal alignment in HAA4D, we construct the first baseline few-shot 4D human atomic action recognition network without bells and whistles, which produces comparable or higher performance than relevant state-of-the-art techniques relying on embedded space encoding without explicit skeletal alignment, using the same small number of training samples of unseen classes.
PB  - arXiv
PY  - 2022
ST  - HAA4D
Y2  - 2025/05/05/21:54:31
DO  - 10.1049/cvi2.12127
ER  -


TY  - GEN
AU  - Ramazanova, M.
AU  - Escorcia, V.
AU  - Heilbron, F.C.
AU  - Zhao, C.
AU  - Ghanem, B.
TI  - OWL (Observe, Watch, Listen): Audiovisual Temporal Context for Localizing Actions in Egocentric Videos
AB  - Egocentric videos capture sequences of human activities from a first-person perspective and can provide rich multimodal signals. However, most current localization methods use third-person videos and only incorporate visual information. In this work, we take a deep look into the effectiveness of audiovisual context in detecting actions in egocentric videos and introduce a simple-yet-effective approach via Observing, Watching, and Listening (OWL). OWL leverages audiovisual information and context for egocentric temporal action localization (TAL). We validate our approach in two large-scale datasets, EPIC-Kitchens, and HOMAGE. Extensive experiments demonstrate the relevance of the audiovisual temporal context. Namely, we boost the localization performance (mAP) over visual-only models by +2.23% and +3.35% in the above datasets.
PB  - arXiv
PY  - 2022
ST  - OWL (Observe, Watch, Listen)
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvprw59228.2023.00516
ER  -


TY  - GEN
AU  - Azadi, R.
AU  - Bohn, S.
AU  - Lopez, E.
AU  - Eldridge, M.
AU  - Afraz, A.
TI  - Behavioral detection of optogenetic stimulation in inferior temporal cortex depends on the image being viewed
AB  - To be able to effectively restore vision by direct cortical stimulation, we need to understand the perceptual events induced by stimulation of high-level visual cortices. We trained macaque monkeys to detect and report optogenetic impulses delivered to their inferior temporal cortices. In a series of experiments, we observed that detection of cortical stimulation is highly dependent on the choice of images presented to the eyes and that detection of cortical stimulation is most difficult when the animal fixates on a blank screen. We show that optogenetic stimulation of object-selective parts of the visual cortex induces perceptual events that are easy to detect, probably as object-dependent distortions of the concurrent contents of vision. These findings invite expanding the scope of visual prosthetics beyond the primary visual cortex.
PB  - Research Square
PY  - 2022
ST  - Behavioral detection of optogenetic stimulation in inferior temporal cortex depends on the image being viewed
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-1331186/v4
ER  -


TY  - GEN
AU  - Jin, J.
AU  - Chen, X.
AU  - Zhang, W.
AU  - Feng, Z.
AU  - Yu, Y.
TI  - Learn over Past, Evolve for Future: Search-based Time-aware Recommendation with Sequential Behavior Data
AB  - The personalized recommendation is an essential part of modern e-commerce, where user’s demands are not only conditioned by their profile but also by their recent browsing behaviors as well as periodical purchases made some time ago. In this paper, we propose a novel framework named Search-based Time-Aware Recommendation (STARec), which captures the evolving demands of users over time through a unified search-based time-aware model. More concretely, we first design a search-based module to retrieve a user’s relevant historical behaviors, which are then mixed up with her recent records to be fed into a time-aware sequential network for capturing her time-sensitive demands. Besides retrieving relevant information from her personal history, we also propose to search and retrieve similar user’s records as an additional reference. All these sequential records are further fused to make the final recommendation. Beyond this framework, we also develop a novel label trick that uses the previous labels (i.e., user’s feedbacks) as the input to better capture the user’s browsing pattern. We conduct extensive experiments on three real-world commercial datasets on click-through-rate prediction tasks against state-of-the-art methods. Experimental results demonstrate the superiority and efficiency of our proposed framework and techniques. Furthermore, results of online experiments on a daily item recommendation platform of Company X show that STARec gains average performance improvement of around 6% and 1.5% in its two main item recommendation scenarios on CTR metric respectively.
PB  - arXiv
PY  - 2022
ST  - Learn over Past, Evolve for Future
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3485447.3512117
ER  -


TY  - GEN
AU  - Di Muri, C.
AU  - Lawson Handley, L.
AU  - Bean, C.W.
AU  - Winfield, I.J.
AU  - Hänfling, B.
TI  - Spatio-temporal monitoring of lake fish spawning activity using environmental DNA metabarcoding
AB  - Determining the timing and location of fish reproductive events is crucial for the implementation of correct management and conservation schemes. Conventional methods used to monitor these events are often unable to assess the spawning activity directly or can be invasive and therefore problematic. This is especially the case when threatened fish populations are the study subject, such as the Arctic charr (Salvelinus alpinus L.) populations in Windermere (Cumbria, UK). Arctic charr populations have been studied in this lake since the 1940s, and the locations and characteristics of spawning grounds have been described in detail using techniques such as hydroacoustics, as well as physical and visual surveys of the lake bottom. Here, in conjunction with established netting surveys, we added an environmental DNA (eDNA) metabarcoding approach to assess the spatial distribution of Arctic charr in the lake throughout the year to test whether this tool could allow us to identify spawning locations and activity. Sampling was carried out between October 2017 and July 2018 at three locations in the lake, covering putative and known spawning sites. eDNA metabarcoding provided accurate spatial and temporal characterisation of Arctic charr spawning events. Peaks of Arctic charr read counts from eDNA metabarcoding were observed during the spawning season and at specific locations of both putative and known spawning sites. Net catches of mature Arctic charr individuals confirmed the association between the Arctic charr spawning activity and the peaks of eDNA metabarcoding read counts. This study demonstrates the ability of eDNA metabarcoding to effectively and efficiently characterize the spatial and temporal nature of fish spawning in lentic systems.
PB  - bioRxiv
PY  - 2022
ST  - Spatio-temporal monitoring of lake fish spawning activity using environmental DNA metabarcoding
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.02.07.478003
ER  -


TY  - GEN
AU  - Ke, L.
AU  - Peng, K.-C.
AU  - Lyu, S.
TI  - Towards To-a-T Spatio-Temporal Focus for skeleton-based action recognition
AB  - Graph Convolutional Networks (GCNs) have been widely used to model the high-order dynamic dependencies for skeleton-based action recognition. Most existing approaches do not explicitly embed the high-order spatio-temporal importance to joints' spatial connection topology and intensity, and they do not have direct objectives on their attention module to jointly learn when and where to focus on in the action sequence. To address these problems, we propose the To-a-T Spatio-Temporal Focus (STF), a skeleton-based action recognition framework that utilizes the spatio-temporal gradient to focus on relevant spatio-temporal features. We first propose the STF modules with learnable gradient-enforced and instance-dependent adjacency matrices to model the high-order spatio-temporal dynamics. Second, we propose three loss terms defined on the gradientbased spatio-temporal focus to explicitly guide the classifier when and where to look at, distinguish confusing classes, and optimize the stacked STF modules. STF outperforms the state-of-the-art methods on the NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets in all 15 settings over different views, subjects, setups, and input modalities, and STF also shows better accuracy on scarce data and dataset shifting settings.
PB  - arXiv
PY  - 2022
ST  - Towards To-a-T Spatio-Temporal Focus for skeleton-based action recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v36i1.19998
ER  -


TY  - GEN
AU  - Knights, E.
AU  - Smith, F.
AU  - Rossit, S.
TI  - The Anterior Temporal Cortex in Action
AB  - Intelligent manipulation of handheld tools marks a major discontinuity between humans and our closest ancestors. Here we identified neural representations about how tools are typically manipulated within left anterior temporal cortex, by shifting a searchlight classifier through whole-brain real action fMRI data when participants grasped 3D-printed tools in ways considered typical for use (i.e., by their handle). These neural representations were automatically evocated as task performance did not require semantic processing. In fact, findings from a behavioural motion-capture experiment confirmed that actions with tools (relative to non-tool) incurred additional processing costs, as would be suspected if semantic areas are being automatically engaged. These results substantiate theories of semantic cognition that claim the anterior temporal cortex combines sensorimotor and semantic content for advanced behaviours like tool manipulation.
PB  - Research Square
PY  - 2022
ST  - The Anterior Temporal Cortex in Action
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-1328133/v1
ER  -


TY  - GEN
AU  - Filtjens, B.
AU  - Vanrumste, B.
AU  - Slaets, P.
TI  - Skeleton-Based Action Segmentation with Multi-Stage Spatial-Temporal Graph Convolutional Neural Networks
AB  - The ability to identify and temporally segment fine-grained actions in motion capture sequences is crucial for applications in human movement analysis. Motion capture is typically performed with optical or inertial measurement systems, which encode human movement as a time series of human joint locations and orientations or their higher-order representations. State-of-the-art action segmentation approaches use multiple stages of temporal convolutions. The main idea is to generate an initial prediction with several layers of temporal convolutions and refine these predictions over multiple stages, also with temporal convolutions. Although these approaches capture long-term temporal patterns, the initial predictions do not adequately consider the spatial hierarchy among the human joints. To address this limitation, we recently introduced multi-stage spatial-temporal graph convolutional neural networks (MS-GCN). Our framework replaces the initial stage of temporal convolutions with spatial graph convolutions and dilated temporal convolutions, which better exploit the spatial configuration of the joints and their long-term temporal dynamics. Our framework was compared to four strong baselines on five tasks. Experimental results demonstrate that our framework is a strong baseline for skeleton-based action segmentation.
PB  - arXiv
PY  - 2022
ST  - Skeleton-Based Action Segmentation with Multi-Stage Spatial-Temporal Graph Convolutional Neural Networks
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tetc.2022.3230912
ER  -


TY  - GEN
AU  - Beasley, R.M.
AU  - Carbone, C.
AU  - Brooker, A.
AU  - Waage, J.
TI  - The Impacts of Humans And Dogs On The Spatial And Temporal Activity of Wildlife In Urban Woodlands
AB  - Humans can derive enormous benefit from the natural environment and the wildlife they see there, but increasing human use may negatively impact wildlife, particularly in urban green spaces. Few studies have focused on the trade-offs between intensive human use and wildlife use of shared green spaces in urban areas. In this paper we investigate the impacts of humans and their dogs on wildlife within an urban green space using camera trap data from Hampstead Heath, London. Spatial and temporal activity of common woodland wildlife species were compared between sites with low and high frequency of visits by humans and dogs. Blackbirds (Turdus merula) were found to be significantly more active at sites with lower visitation rates, while red foxes (Vulpes vulpes) were observed more in areas with higher visitation rates. Birds and grey squirrels (Sciurus carolinensis) showed evidence of temporal displacement away from peak periods of human and dog visits in areas where visitation rates were high. Responses observed to human and dog visits could have implications for wildlife species persistence and community composition in urban woodlands.
PB  - Research Square
PY  - 2022
ST  - The Impacts of Humans And Dogs On The Spatial And Temporal Activity of Wildlife In Urban Woodlands
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-854180/v1
ER  -


TY  - GEN
AU  - Kim, K.
AU  - Gowda, S.N.
AU  - Aodha, O.M.
AU  - Sevilla-Lara, L.
TI  - Capturing Temporal Information in a Single Frame: Channel Sampling Strategies for Action Recognition
AB  - We address the problem of capturing temporal information for video classification in 2D networks, without increasing their computational cost. Existing approaches focus on modifying the architecture of 2D networks (e.g. by including filters in the temporal dimension to turn them into 3D networks, or using optical flow, etc.), which increases computation cost. Instead, we propose a novel sampling strategy, where we re-order the channels of the input video, to capture short-term frame-to-frame changes. We observe that without bells and whistles, the proposed sampling strategy improves performance on multiple architectures (e.g. TSN, TRN, TSM, and MVFNet) and datasets (CATER, Something-Something-V1 and V2), up to 24% over the baseline of using the standard video input. In addition, our sampling strategies do not require training from scratch and do not increase the computational cost of training and testing. Given the generality of the results and the flexibility of the approach, we hope this can be widely useful to the video understanding community.
PB  - arXiv
PY  - 2022
ST  - Capturing Temporal Information in a Single Frame
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/ccisp55629.2022.9974302
ER  -


TY  - GEN
AU  - Hayashi, Y.
AU  - Kobayakawa, K.
AU  - Kobayakawa, R.
TI  - The temporal and contextual stability of activity levels in hippocampal CA1 cells
AB  - Recent long-term optical imaging studies have demonstrated that the activity levels of hippocampal neurons in a familiar environment change on a daily to weekly basis. However, it is unclear whether there is any time-invariant property in the cells’ neural representations. In this study, using miniature fluorescence microscopy, we measured the neural activity of the mouse hippocampus in four different environments every 3 days. Although the activity level of hippocampal neurons fluctuated greatly in each environment across days, we found a significant correlation between the activity levels for different days, and the correlation was higher for averaged activity levels across multiple environments. When the number of environments used for averaging was increased, a higher activity correlation was observed. Furthermore, the number of environments in which a cell showed activity was preserved. Cells that showed place cell activity in many environments had greater spatial information content, and thus carried a higher amount of information about the current position. In contrast, cells that were active only in a small number of environments provided sparse representation for the environment. These results suggest that each cell has not only an inherent activity level but also play a characteristic role in the coding of space.
PB  - bioRxiv
PY  - 2022
ST  - The temporal and contextual stability of activity levels in hippocampal CA1 cells
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.01.24.477445
ER  -


TY  - GEN
AU  - Yang, X.
AU  - Li, Z.-Y.
AU  - Si, L.-H.
AU  - Shen, B.
AU  - Ling, X.
TI  - Right Superior Temporal Gyrus Altered Functional Activity in Patients With Definite Vestibular Migraine
AB  - The study aimed to investigate resting-state functional brain activity alterations in patients with definite vestibular migraine (dVM). Seventeen patients with dVM, 8 patients with migraine, 17 health controls (HCs) were recruited. The amplitude of low frequency fluctuation (ALFF), fractional ALFF (fALFF) and regional homogeneity (ReHo) were calculated to observe the changes in spontaneous brain activity. Then brain regions with altered fALFF were selected for seed-based functional connectivity analysis. Compared with HCs, VM patients showed significantly increased ALFF values in the right temporal lobe (Cluster size = 91 voxels, P=0.002, FWE corrected), and significantly increased ReHo values in the right superior temporal gyrus (STG), middle temporal gyrus (MTG) and inferior temporal gyrus (ITG) (Cluster size = 136 voxels, P=0.013, FWE corrected). Compared with patients with migraine, patients with VM showed significantly increased fALFF values in the right parietal lobe (Cluster size = 43 voxels, P=0.011, FWE corrected) and right frontal lobe (Cluster size =36 voxels, P=0.026, FWE corrected), significantly increased ReHo values in the right thalamus (Cluster size = 92 voxels, P=0.043, FWE corrected). Our findings documented that patients with VM showed enhanced spontaneous functional activity in the right temporal lobe (STG, MTG, and ITG) compared with HCs, and increased spontaneous activity in the right parietal lobe-frontal lobe-thalamus compared with patients with migraine. Patients with VM and migraine both had altered brain function, but the regions involved are different.
PB  - Research Square
PY  - 2022
ST  - Right Superior Temporal Gyrus Altered Functional Activity in Patients With Definite Vestibular Migraine
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-1206351/v1
ER  -


TY  - GEN
AU  - Chen, T.
AU  - Bowers, K.
AU  - Zhu, D.
AU  - Gao, X.
AU  - Cheng, T.
TI  - Spatio-Temporal Stratified Associations between Urban Human Activities and Crime Patterns: A Case Study in San Francisco Around the Covid-19 Stay-at-Home Mandate
AB  - Crime changes have been reported as a result of human routine activity shifting due to containment policies, such as stay-at-home (SAH) mandates during the COVID-19 pandemic. However, the way in which the manifestation of crime in both space and time is affected by dynamic human activities in urban areas has not been explored in depth in empirical studies hitherto. Here, we aim to quantitatively measure the spatio-temporal associations between crime patterns and human activities in the context of an unstable period of the ever-changing socio-demographic backcloth. We propose an analytical framework to detect the dynamic stratified associations between the spatial distributions of human activities and crimes. The results of a case study in San Francisco, United States reveal that the spatial patterns of most crime types are statistically significantly associated with that of human activities zones. Property crime exhibits higher stratified association than violent crime across all temporal scales. Further, the strongest association is obtained with the eight-week time span centered around the SAH order. These findings not only enhance our understanding of the relationships between urban crime and human activities, but also offer insights that tailored crime intervention strategies need to consider human activity variables.
PB  - SSRN
PY  - 2022
ST  - Spatio-Temporal Stratified Associations between Urban Human Activities and Crime Patterns
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4004696
ER  -


TY  - GEN
AU  - Qiu, H.
AU  - Hou, B.
AU  - Ren, B.
AU  - Zhang, X.
TI  - Spatio-Temporal Tuples Transformer for Skeleton-Based Action Recognition
AB  - Capturing the dependencies between joints is critical in skeleton-based action recognition task. Transformer shows great potential to model the correlation of important joints. However, the existing Transformer-based methods cannot capture the correlation of different joints between frames, which the correlation is very useful since different body parts (such as the arms and legs in”long jump”) between adjacent frames move together. Focus on this problem, A novel spatio-temporal tuples Transformer (STTFormer) method is proposed. The skeleton sequence is divided into several parts, and several consecutive frames contained in each part are encoded. And then a spatio-temporal tuples self-attention module is proposed to capture the relationship of different joints in consecutive frames. In addition, a feature aggregation module is introduced between non-adjacent frames to enhance the ability to distinguish similar actions. Compared with the state-of-the-art methods, our method achieves better performance on two large-scale datasets.
PB  - arXiv
PY  - 2022
ST  - Spatio-Temporal Tuples Transformer for Skeleton-Based Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s00371-023-03132-1
ER  -


TY  - GEN
AU  - Ohishi, H.
AU  - Shimada, S.
AU  - Uchino, S.
AU  - Kimura, H.
AU  - Ochiai, H.
TI  - STREAMING-tag system reveals spatiotemporal relationships between transcriptional regulatory factors and transcriptional activity
AB  - Transcription is a dynamic process that stochastically switches between the ON and OFF states. To detect the dynamic relationship among protein clusters of RNA polymerase II (RNAPII) and coactivators, gene loci, and transcriptional activity, we inserted an MS2 repeat, a TetO repeat, and inteins with a selection marker just downstream of the transcription start site (TSS). By optimizing the individual elements, we have developed the Spliced TetO REpeAt, MS2 repeat, and INtein sandwiched reporter Gene tag (STREAMING-tag) system. Clusters of RNAPII and BRD4 were observed proximally to the TSS of Nanog when the gene was transcribed in mouse embryonic stem cells. In contrast, clusters of MED19 and MED22 Mediator subunits were constitutively located near the TSS. Thus, the STREAMING-tag system revealed the spatiotemporal relationships between transcriptional activity and protein clusters near the gene. This powerful tool is useful for quantitatively understanding dynamic transcriptional regulation in living cells.
PB  - bioRxiv
PY  - 2022
ST  - STREAMING-tag system reveals spatiotemporal relationships between transcriptional regulatory factors and transcriptional activity
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.01.06.472721
ER  -


TY  - GEN
AU  - Wang, H.
AU  - Damen, D.
AU  - Mirmehdi, M.
AU  - Perrett, T.
TI  - TVNet: Temporal Voting Network for Action Localization
AB  - We propose a Temporal Voting Network (TVNet) for action localization in untrimmed videos. This incorporates a novel Voting Evidence Module to locate temporal boundaries, more accurately, where temporal contextual evidence is accumulated to predict frame-level probabilities of start and end action boundaries. Our action-independent evidence module is incorporated within a pipeline to calculate confidence scores and action classes. We achieve an average mAP of 34.6% on ActivityNet-1.3, particularly outperforming previous methods with the highest IoU of 0.95. TVNet also achieves mAP of 56.0% when combined with PGCN and 59.1% with MUSES at 0.5 IoU on THUMOS14 and outperforms prior work at all thresholds. Our code is available at https://github.com/hanielwang/TVNet.
PB  - arXiv
PY  - 2022
ST  - TVNet
Y2  - 2025/05/05/21:54:31
DO  - 10.5220/0010868900003124
ER  -


TY  - GEN
AU  - Sun, H.
AU  - Wang, T.
TI  - Toward Causal-Aware RL: State-Wise Action-Refined Temporal Difference
AB  - Although it is well known that exploration plays a key role in Reinforcement Learning (RL), prevailing exploration strategies for continuous control tasks in RL are mainly based on naive isotropic Gaussian noise regardless of the causality relationship between action space and the task and consider all dimensions of actions equally important. In this work, we propose to conduct interventions on the primal action space to discover the causal relationship between the action space and the task reward. We propose the method of State-Wise Action Refined (SWAR), which addresses the issue of action space redundancy and promote causality discovery in RL. We formulate causality discovery in RL tasks as a state-dependent action space selection problem and propose two practical algorithms as solutions. The first approach, TD-SWAR, detects task-related actions during temporal difference learning, while the second approach, Dyn-SWAR, reveals important actions through dynamic model prediction. Empirically, both methods provide approaches to understand the decisions made by RL agents and improve learning efficiency in action-redundant tasks.
PB  - arXiv
PY  - 2022
ST  - Toward Causal-Aware RL
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-981-19-7784-8_4
ER  -


TY  - GEN
AU  - Sadeghian, Z.
AU  - Bayat, M.
AU  - Safari, F.
TI  - Sequential Four-Component Synthesis And Antitumor Activity Screening of Spiro[chromene-indolo[2,1-b]quinazoline] And Spiro[indolo[2,1b]Quinazoline-pyrano[3,2-c]chromene]
AB  - Chemotherapy is one of the most common types of treatment among cancer patients and by using potent chemicals and agents, tumor promotion was inhibited. Despite the usage of many chemical agents in cancer therapy, cancer is still incurable. It seems that the synthesis of new compounds with high efficiency on cancer cells and low side effects on normal cells will remain a critical challenge among researchers in this area. In the present work, a fast and straightforward process for the transformations involving tryptanthrins, malononitrile, some types of CH-acids such as 1,3-cyclohexanedione, dimedone, and 4-hydroxycumarin resulting in preparing spiro[chromene-indolo[2,1-b]quinazoline] and spiro[indolo[2,1-b]quinazoline-pyrano[3,2-c]chromene] derivatives through sequential Knoevenagel/Michael/intramolecular cyclization sequences was reported. at room temperature. This protocol benefits some notable advantages including short reaction time, mild reaction condition, and simple purification, which make it interesting. Furthermore, it was carried out at room temperature, so it is according to green chemistry procedures. Also, antitumor screening of our new synthetic compounds (4a-i) was evaluated on pancreatic cancer cells (Panc1), breast cancer cells (MDA-MB-231), prostate cancer cells (PC3), and normal human adult dermal fibroblast cells (HDF) by using MTT assay using etoposide as a positive control. We found that 50% growth inhibitory concentration (IC50) values of our synthetic compounds were not lower than etoposide against three cancer cell lines.
PB  - Research Square
PY  - 2021
ST  - Sequential Four-Component Synthesis And Antitumor Activity Screening of Spiro[chromene-indolo[2,1-b]quinazoline] And Spiro[indolo[2,1b]Quinazoline-pyrano[3,2-c]chromene]
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-1178670/v1
ER  -


TY  - GEN
AU  - Ceolini, E.
AU  - Kock, R.
AU  - Band, G.P.H.
AU  - Stoet, G.
AU  - Ghosh, A.
TI  - Temporal clusters of age-related behavioral alterations captured in smartphone touchscreen interactions
AB  - Cognitive and behavioral abilities alter across the adult life span. Smartphones engage various cognitive functions and the corresponding touchscreen interactions may help resolve if and how the behavior is systematically structured by aging. Here, in a sample spanning the adult lifespan (16 to 86 years, N = 598, accumulating 355 million interactions) we analyzed a range of interaction intervals - from a few milliseconds to a minute. We used probability distributions to cluster the interactions according to their next inter-touch interval dynamics to discover systematic age-related changes at the distinct temporal clusters. There were age-related behavioral losses at the clusters occupying short intervals (~ 100 ms, R2 ~ 0.8) but gains at the long intervals (~ 4 s, R2 ~ 0.4). These correlates were independent of the years of experience on the phone or the choice of fingers used on the screen. We found further evidence for a compartmentalized influence of aging, as individuals simultaneously demonstrated both accelerated and decelerated aging at distant temporal clusters. In contrast to these strong correlations, cognitive tests probing sensorimotor, working memory, and executive processes revealed rather weak age-related decline. Contrary to the common notion of a simple behavioral decline with age based on conventional cognitive tests, we show that real-world behavior does not simply decline and the nature of aging systematically varies according to the underlying temporal dynamics. Of all the imaginable factors determining smartphone interactions in the real world, age-sensitive cognitive and behavioral processes can dominatingly dictate smartphone temporal dynamics.
PB  - bioRxiv
PY  - 2021
ST  - Temporal clusters of age-related behavioral alterations captured in smartphone touchscreen interactions
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2021.12.24.474105
ER  -


TY  - GEN
AU  - Riquelme, J.L.
AU  - Hemberger, M.
AU  - Laurent, G.
AU  - Gjorgjieva, J.
TI  - Single spikes drive sequential propagation and routing of activity in a cortical network
AB  - Single spikes can trigger repeatable firing sequences in cortical networks. The mechanisms that support reliable propagation of activity from such small events and their functional consequences remain unclear. By constraining a recurrent network model with experimental statistics from turtle cortex, we generate reliable and temporally precise sequences from single spike triggers. We find that rare strong connections support sequence propagation, while dense weak connections modulate propagation reliability. We identify sections of sequences corresponding to divergent branches of strongly connected neurons which can be selectively gated. Applying eternal inputs to specific neurons in the sparse backbone of strong connections can effectively control propagation and route activity within the network. Finally, we demonstrate that concurrent sequences interact reliably, generating a highly combinatorial space of sequence activations. Our results reveal the impact of individual spikes in cortical circuits, detailing how repeatable sequences of activity can be triggered, sustained, and controlled during cortical computations.
PB  - bioRxiv
PY  - 2021
ST  - Single spikes drive sequential propagation and routing of activity in a cortical network
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2021.12.21.473652
ER  -


TY  - GEN
AU  - Wang, H.
AU  - Zhao, B.
AU  - Liu, G.
TI  - Tme: Temporal and Motion Enhancement for Action Recognition
AB  - The key to video understanding is to capture temporal information. 3D CNNs can achieve good performance by directly fusing spatial and temporal features but are computationally intensive. Conventional 2D CNNs perform well in image recognition, but the inability to extract temporal features leads to poor performance in video action recognition. In this work, we aim to catch temporal relationships for video action recognition. To this end, we propose a Temporal and Motion Enhancement (TME) module consisting of two submodules: Multi-path Temporal Enhancement(MTE) module and Long Short-range Motion Enhancement(LSME) module. The MTE module consists of four paths: Residual path, Temporal Aggregation(TA) path, Channel-wise Temporal Excitation(CTE) path and Spatial-attention Temporal Excitation(STE) path. The Residual path makes part of the information in the original activation accessible. The TA path employs convolution on temporal dimension to characterize temporal representation. The CTE path rebuilds channel-wise feature relationships in terms of the temporal aspect. The STE path introduces attention mechanisms and explores the temporal modeling. We aggregate features of different paths by splitting channels, which is efficient for model inference. The LSME module calculates feature-level temporal differences between long and short range frames, which enhances the motion-related features. TME module is plug-and-play and can be inserted into 2D CNNs to form an effective TMENet with limited extra computational cost. The extensive experiments on several action recognition datasets, such as Kinetics400, Something-Something V1\&V2, HMDB51, and UCF101, demonstrate that the TMENet outperforms the state-of-the-art methods with high efficiency. Code will be released soon.
PB  - SSRN
PY  - 2021
ST  - Tme
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4039887
ER  -


TY  - GEN
AU  - Broomé, S.
AU  - Pokropek, E.
AU  - Li, B.
AU  - Kjellström, H.
TI  - Recur, Attend or Convolve? On Whether Temporal Modeling Matters for Cross-Domain Robustness in Action Recognition
AB  - Most action recognition models today are highly parameterized, and evaluated on datasets with appearance-wise distinct classes. It has also been shown that 2D Convolutional Neural Networks (CNNs) tend to be biased toward texture rather than shape in still image recognition tasks [19], in contrast to humans. Taken together, this raises suspicion that large video models partly learn spurious spatial texture correlations rather than to track relevant shapes over time to infer generalizable semantics from their movement. A natural way to avoid parameter explosion when learning visual patterns over time is to make use of recurrence. Biological vision consists of abundant recurrent circuitry, and is superior to computer vision in terms of domain shift generalization. In this article, we empirically study whether the choice of low-level temporal modeling has consequences for texture bias and cross-domain robustness. In order to enable a light-weight and systematic assessment of the ability to capture temporal structure, not revealed from single frames, we provide the Temporal Shape (TS) dataset, as well as modified domains of Diving48 allowing for the investigation of spatial texture bias in video models. The combined results of our experiments indicate that sound physical inductive bias such as recurrence in temporal modeling may be advantageous when robustness to domain shift is important for the task.
PB  - arXiv
PY  - 2021
ST  - Recur, Attend or Convolve? On Whether Temporal Modeling Matters for Cross-Domain Robustness in Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/wacv56688.2023.00418
ER  -


TY  - GEN
AU  - Huayta, J.
AU  - San-Miguel, A.
TI  - Endogenous DAF-16 spatiotemporal activity quantitatively predicts lifespan extension induced by dietary restriction
AB  - In many organisms, dietary restriction (DR) leads to lifespan extension through the activation of cell protection and pro-longevity gene expression programs. In the nematode C. elegans, the DAF-16 transcription factor is a key aging regulator that governs the Insulin/IGF-1 signaling pathway and undergoes translocation from the cytoplasm to the nucleus of cells when animals are exposed to food limitation. In this work, we assess the endogenous activity of DAF-16 under various DR regimes by coupling CRISPR/Cas9-enabled fluorescent tagging of DAF-16 with quantitative image analysis and machine learning. Our results indicate that lifelong DAF-16 endogenous activity is a robust predictor of mean lifespan in C. elegans, and it accounts for 78% of the lifespan variability induced by DR. We found that this lifespan-extending mechanism occurs mainly in the intestine and neurons, and that DR drives DAF-16 activity in unexpected locations such as the germline and intestinal nucleoli.
PB  - bioRxiv
PY  - 2021
ST  - Endogenous DAF-16 spatiotemporal activity quantitatively predicts lifespan extension induced by dietary restriction
Y2  - 2025/05/05/21:54:31
DO  - 10.1038/s42003-023-04562-2
ER  -


TY  - GEN
AU  - Yang, Z.
AU  - Qin, J.
AU  - Huang, D.
TI  - ACGNet: Action complement graph network forweakly-supervised temporal action localization
AB  - Weakly-supervised temporal action localization (WTAL) in untrimmed videos has emerged as a practical but challenging task since only video-level labels are available. Existing approaches typically leverage off-the-shelf segment-level features, which suffer from spatial incompleteness and temporal incoherence, thus limiting their performance. In this paper, we tackle this problem from a new perspective by enhancing segment-level representations with a simple yet effective graph convolutional network, namely action complement graph network (ACGNet). It facilitates the current video segment to perceive spatial-temporal dependencies from others that potentially convey complementary clues, implicitly mitigating the negative effects caused by the two issues above. By this means, the segment-level features are more discriminative and robust to spatial-temporal variations, contributing to higher localization accuracies. More importantly, the proposed ACGNet works as a universal module that can be flexibly plugged into different WTAL frameworks, while maintaining the end-to-end training fashion. Extensive experiments are conducted on the THUMOS'14 and ActivityNet1.2 benchmarks, where the state-of-the-art results clearly demonstrate the superiority of the proposed approach.
PB  - arXiv
PY  - 2021
ST  - ACGNet
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v36i3.20216
ER  -


TY  - GEN
AU  - Huang, Y.
AU  - Liang, M.
TI  - Spatio-temporal Attention Network for Student Action Recognition in Classroom Teaching Videos
AB  - Inspired by the wide application of transformer in computer vision and its excellent ability in temporal feature learning. This paper proposes a novel and efficient spatio-temporal residual attention network for student action recognition in classroom teaching video. It first fuses 2D spatial convolution and 1D temporal convolution to study spatio-temporal feature, then combines the powerful Reformer to better study the deeper spatio-temporal characteristics with visual significance of student classroom action. Based on the spatio-temporal residual attention network, a single person action recognition model in classroom teaching video is proposed. Considering that there are often multiple students in the classroom video scene, on the basis of single person action recognition, combined with object detection and tracking technology, the association of temporal and spatial characteristics of the same student targets is established, so as to realize the multi-student action recognition in classroom video scene. The experimental results on classroom teaching video dataset and public video dataset show that the proposed model achieves higher action recognition performance than the existing excellent models and methods.
PB  - Research Square
PY  - 2021
ST  - Spatio-temporal Attention Network for Student Action Recognition in Classroom Teaching Videos
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-1022972/v1
ER  -


TY  - GEN
AU  - Yang, H.
AU  - Wu, W.
AU  - Wang, L.
AU  - Yao, H.
AU  - Huang, H.
TI  - Temporal action proposal generation with background constraint
AB  - Temporal action proposal generation (TAPG) is a challenging task that aims to locate action instances in untrimmed videos with temporal boundaries. To evaluate the confidence of proposals, the existing works typically predict action score of proposals that are supervised by the temporal Intersection-over-Union (tIoU) between proposal and the ground-truth. In this paper, we innovatively propose a general auxiliary Background Constraint idea to further suppress low-quality proposals, by utilizing the background prediction score to restrict the confidence of proposals. In this way, the Background Constraint concept can be easily plug-and-played into existing TAPG methods (e.g., BMN, GTAD). From this perspective, we propose the Background Constraint Network (BCNet) to further take advantage of the rich information of action and background. Specifically, we introduce an Action-Background Interaction module for reliable confidence evaluation, which models the inconsistency between action and background by attention mechanisms at the frame and clip levels. Extensive experiments are conducted on two popular benchmarks, i.e., ActivityNet-1.3 and THUMOS14. The results demonstrate that our method outperforms state-of-the-art methods. Equipped with the existing action classifier, our method also achieves remarkable performance on the temporal action localization task.
PB  - arXiv
PY  - 2021
ST  - Temporal action proposal generation with background constraint
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v36i3.20212
ER  -


TY  - GEN
AU  - Hwang, J.
AU  - Zhang, H.
AU  - Choi, J.-H.
AU  - Hsieh, C.-J.
AU  - Lee, J.-S.
TI  - Temporal Shuffling for Defending Deep Action Recognition Models against Adversarial Attacks
AB  - Recently, video-based action recognition methods using convolutional neural networks (CNNs) achieve remarkable recognition performance. However, there is still lack of understanding about the generalization mechanism of action recognition models. In this paper, we suggest that action recognition models rely on the motion information less than expected, and thus they are robust to randomization of frame orders. Furthermore, we find that motion monotonicity remaining after randomization also contributes to such robustness. Based on this observation, we develop a novel defense method using temporal shuffling of input videos against adversarial attacks for action recognition models. Another observation enabling our defense method is that adversarial perturbations on videos are sensitive to temporal destruction. To the best of our knowledge, this is the first attempt to design a defense method without additional training for 3D CNN-based video action recognition models.
PB  - arXiv
PY  - 2021
ST  - Temporal Shuffling for Defending Deep Action Recognition Models against Adversarial Attacks
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.neunet.2023.10.033
ER  -


TY  - GEN
AU  - Zhang, Y.
AU  - Li, J.
AU  - Wu, G.
AU  - Wu, Z.
AU  - Jiang, N.
TI  - Temporal transformer networks with self-supervision for action recognition
AB  - In recent years, 2D Convolutional Networks-based video action recognition has encouragingly gained wide popularity; However, constrained by the lack of long-range non-linear temporal relation modeling and reverse motion information modeling, the performance of existing models is, therefore, undercut seriously. To address this urgent problem, we introduce a startling Temporal Transformer Network with Self-supervision (TTSN). Our high-performance TTSN mainly consists of a temporal transformer module and a temporal sequence self-supervision module. Concisely speaking, we utilize the efficient temporal transformer module to model the nonlinear temporal dependencies among non-local frames, which significantly enhances complex motion feature representations. The temporal sequence self-supervision module we employ unprecedentedly adopts the streamlined strategy of “random batch random channel” to reverse the sequence of video frames, allowing robust extractions of motion information representation from inversed temporal dimensions and improving the generalization capability of the model. Extensive experiments on three widely used datasets (HMDB51, UCF101, and Something-something V1) have conclusively demonstrated that our proposed TTSN is promising as it successfully achieves state-of-the-art performance for action recognition.
PB  - arXiv
PY  - 2021
ST  - Temporal transformer networks with self-supervision for action recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/jiot.2023.3257992
ER  -


TY  - GEN
AU  - Thatipelli, A.
AU  - Narayan, S.
AU  - Khan, S.
AU  - Khan, F.S.
AU  - Ghanem, B.
TI  - Spatio-temporal Relation Modeling for Few-shot Action Recognition
AB  - We propose a novel few-shot action recognition framework, STRM, which enhances class-specific feature discriminability while simultaneously learning higher-order temporal representations. The focus of our approach is a novel spatio-temporal enrichment module that aggregates spatial and temporal contexts with dedicated local patch-level and global frame-level feature enrichment sub-modules. Local patch-level enrichment captures the appearance-based characteristics of actions. On the other hand, global frame-level enrichment explicitly encodes the broad temporal context, thereby capturing the relevant object features over time. The resulting spatio-temporally enriched representations are then utilized to learn the relational matching between query and support action sub-sequences. We further introduce a query-class similarity classifier on the patch-level enriched features to enhance class-specific feature discriminability by reinforcing the feature learning at different stages in the proposed framework. Experiments are performed on four few-shot action recognition benchmarks: Kinetics, SSv2, HMDB51 and UCF101. Our extensive ablation study reveals the benefits of the proposed contributions. Furthermore, our approach sets a new state-of-the-art on all four benchmarks. On the challenging SSv2 benchmark, our approach achieves an absolute gain of 3.5% in classification accuracy, as compared to the best existing method in the literature. Our code and models are available at https://github.com/Anirudh257/strm.
PB  - arXiv
PY  - 2021
ST  - Spatio-temporal Relation Modeling for Few-shot Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52688.2022.01933
ER  -


TY  - GEN
AU  - Wang, S.
AU  - Findeisen, L.
AU  - Leptihn, S.
AU  - Hörning, M.
AU  - Nussberger, S.
TI  - Spatiotemporal Stop-and-go Dynamics of the Mitochondrial TOM Core Complex Correlates With Three-state Channel Activity
AB  - Single-molecule studies can reveal phenomena that remain hidden in ensemble measurements. Here we show the correlation between lateral protein diffusion and channel activity of the general protein import pore of mitochondria (TOM-CC) in membranes resting on ultrathin hydrogel films. Using electrode-free optical recordings of ion flux, we find that TOM-CC switches reversibly between three states of ion permeability associated with protein diffusion. Freely diffusing TOM-CC molecules are observed in a high permeability state, while non-moving molecules are in an intermediate and a low permeability state. We explain this behavior by the mechanical binding of the two protruding Tom22 subunits to the hydrogel and a concomitant combinatorial opening and closing of the two β-barrel pores of TOM-CC. TOM-CC could thus be the first β-barrel protein channel to exhibit membrane state-dependent mechanosensitive properties.
PB  - Research Square
PY  - 2021
ST  - Spatiotemporal Stop-and-go Dynamics of the Mitochondrial TOM Core Complex Correlates With Three-state Channel Activity
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-1135723/v1
ER  -


TY  - GEN
AU  - Grand, M.
AU  - Pellier, D.
AU  - Fiorino, H.
TI  - TempAMLSI: Temporal action model learning based on grammar induction
AB  - Hand-encoding PDDL domains is generally accepted as difficult, tedious and error-prone. The difficulty is even greater when temporal domains have to be encoded. Indeed, actions have a duration and their effects are not instantaneous. In this paper, we present TempAMLSI, an algorithm based on the AMLSI approach able to learn temporal domains. TempAMLSI is based on the classical assumption done in temporal planning that it is possible to convert a non-temporal domain into a temporal domain. TempAMLSI is the first approach able to learn temporal domain with single hard envelope and Cushing’s intervals. We show experimentally that TempAMLSI is able to learn accurate temporal domains, i.e., temporal domain that can be used directly to solve new planning problem, with different forms of action concurrency.
PB  - arXiv
PY  - 2021
ST  - TempAMLSI
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/icaps.v32i1.19847
ER  -


TY  - GEN
AU  - Wein, S.
AU  - Schüller, A.
AU  - Tomé, A.M.
AU  - Greenlee, M.W.
AU  - Lang, E.W.
TI  - Forecasting Brain Activity Based on Models of Spatio-Temporal Brain Dynamics: A Comparison of Graph Neural Network Architectures
AB  - Comprehending the interplay between spatial and temporal characteristics of neural dynamics can contribute to our understanding of information processing in the human brain. Graph neural networks (GNNs) provide a new possibility to interpret graph structured signals like those observed in complex brain networks. In our study we compare different spatio-temporal GNN architectures and study their ability to model neural activity distributions obtained in functional MRI (fMRI) studies. We evaluate the performance of the GNN models on a variety of scenarios in MRI studies and also compare it to a VAR model, which is currently often used for directed functional connectivity analysis. We show that by learning localized functional interactions on the anatomical substrate, GNN based approaches are able to robustly scale to large network studies, even when available data are scarce. By including anatomical connectivity as the physical substrate for information propagation, such GNNs also provide a multi-modal perspective on directed connectivity analysis, offering a novel possibility to investigate the spatio-temporal dynamics in brain networks.
PB  - arXiv
PY  - 2021
ST  - Forecasting Brain Activity Based on Models of Spatio-Temporal Brain Dynamics
Y2  - 2025/05/05/21:54:31
DO  - 10.1162/netn_a_00252
ER  -


TY  - GEN
AU  - Chen, G.
AU  - Zheng, Y.-D.
AU  - Wang, L.
AU  - Lu, T.
TI  - DCAN: Improving temporal action detection via dual context aggregation
AB  - Temporal action detection aims to locate the boundaries of action in the video. The current method based on boundary matching enumerates and calculates all possible boundary matchings to generate proposals. However, these methods neglect the long-range context aggregation in boundary prediction. At the same time, due to the similar semantics of adjacent matchings, local semantic aggregation of densely-generated matchings cannot improve semantic richness and discrimination. In this paper, we propose the end-to-end proposal generation method named Dual Context Aggregation Network (DCAN) to aggregate context on two levels, namely, boundary level and proposal level, for generating high-quality action proposals, thereby improving the performance of temporal action detection. Specifically, we design the Multi-Path Temporal Context Aggregation (MTCA) to achieve smooth context aggregation on boundary level and precise evaluation of boundaries. For matching evaluation, Coarse-to-fine Matching (CFM) is designed to aggregate context on the proposal level and refine the matching map from coarse to fine. We conduct extensive experiments on ActivityNet v1.3 and THUMOS-14. DCAN obtains an average mAP of 35.39% on ActivityNet v1.3 and reaches mAP 54.14% at IoU@0.5 on THUMOS-14, which demonstrates DCAN can generate high-quality proposals and achieve state-of-the-art performance. We release the code at https://github.com/cg1177/DCAN.
PB  - arXiv
PY  - 2021
ST  - DCAN
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v36i1.19900
ER  -


TY  - GEN
AU  - Dai, R.
AU  - Das, S.
AU  - Kahatapitiya, K.
AU  - Ryoo, M.S.
AU  - Brémond, F.
TI  - MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection
AB  - Action detection is a significant and challenging task, especially in densely-labelled datasets of untrimmed videos. Such data consist of complex temporal relations including composite or co-occurring actions. To detect actions in these complex settings, it is critical to capture both short-term and long-term temporal information efficiently. To this end, we propose a novel ‘ConvTransformer’ network for action detection: MS-TCT1. This network comprises of three main components: (1) a Temporal Encoder module which explores global and local temporal relations at multiple temporal resolutions, (2) a Temporal Scale Mixer module which effectively fuses multi-scale features, creating a unified feature representation, and (3) a Classification module which learns a center-relative position of each action instance in time, and predicts frame-level classification scores. Our experimental results on multiple challenging datasets such as Charades, TSU and MultiTHUMOS, validate the effectiveness of the proposed method, which outperforms the state-of-the-art methods on all three datasets.
PB  - arXiv
PY  - 2021
ST  - MS-TCT
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52688.2022.01941
ER  -


TY  - GEN
AU  - Yang, Z.
AU  - An, G.
TI  - STSM: Spatio-temporal Shift module for efficient action recognition
AB  - The modeling, computational cost, and accuracy of traditional Spatio-temporal networks are the three most concentrated research topics in video action recognition. The traditional 2D convolution has a low computational cost, but it cannot capture the time relationship; the convolutional neural networks (CNNs) model based on 3D convolution can obtain good performance, but its computational cost is high, and the amount of parameters is large. In this paper, we propose a plug-and-play Spatio-temporal Shift Module (STSM), which is a generic module that is both effective and high-performance. Specifically, after STSM is inserted into other networks, the performance of the network can be improved without increasing the number of calculations and parameters. In particular, when the network is 2D CNNs, our STSM module allows the network to learn efficient Spatio-temporal features. We conducted extensive evaluations of the proposed module, conducted numerous experiments to study its effectiveness in video action recognition, and achieved state-of-the-art results on the kinetics-400 and Something-Something V2 datasets.
PB  - arXiv
PY  - 2021
ST  - STSM
Y2  - 2025/05/05/21:54:31
DO  - 10.3390/math10183290
ER  -


TY  - GEN
AU  - Yang, L.
AU  - Huang, Y.
AU  - Sugano, Y.
AU  - Sato, Y.
TI  - Stacked temporal attention: Improving first-person action recognition by emphasizing discriminative clips
AB  - First-person action recognition is a challenging task in video understanding. Because of strong ego-motion and a limited field of view, many backgrounds or noisy frames in a first-person video can distract an action recognition model during its learning process. To encode more discriminative features, the model needs to have the ability to focus on the most relevant part of the video for action recognition. Previous works explored to address this problem by applying temporal attention but failed to consider the global context of the full video, which is critical for determining the relatively significant parts. In this work, we propose a simple yet effective Stacked Temporal Attention Module (STAM) to compute temporal attention based on the global knowledge across clips for emphasizing the most discriminative features. We achieve this by stacking multiple self-attention layers. Instead of naive stacking, which is experimentally proven to be ineffective, we carefully design the input to each self-attention layer so that both local and global context of the video is considered during generating the temporal attention weights. Experiments demonstrate that our proposed STAM can be built on top of most existing backbones and boost the performance in various datasets.
PB  - arXiv
PY  - 2021
ST  - Stacked temporal attention
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s12652-021-02940-4
ER  -


TY  - GEN
AU  - Singhania, D.
AU  - Rahaman, R.
AU  - Yao, A.
TI  - Iterative frame-level representation learning and classification for semi-supervised temporal action segmentation
AB  - Temporal action segmentation classifies the action of each frame in (long) video sequences. Due to the high cost of framewise labeling, we propose the first semi-supervised method for temporal action segmentation. Our method hinges on unsupervised representation learning, which, for temporal action segmentation, poses unique challenges. Actions in untrimmed videos vary in length and have unknown labels and start/end times. Ordering of actions across videos may also vary. We propose a novel way to learn frame-wise representations from temporal convolutional networks (TCNs) by clustering input features with added time-proximity condition and multiresolution similarity. By merging representation learning with conventional supervised learning, we develop an "Iterative-Contrast-Classify (ICC)" semi-supervised learning scheme. With more labelled data, ICC progressively improves in performance; ICC semi-supervised learning, with 40% labelled videos, performs similar to fully-supervised counterparts. Our ICC improves MoF by {+1.8, +5.6, +2.5}% on Breakfast, 50Salads and GTEA respectively for 100% labelled videos.
PB  - arXiv
PY  - 2021
ST  - Iterative frame-level representation learning and classification for semi-supervised temporal action segmentation
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v36i2.20124
ER  -


TY  - GEN
AU  - Zeng, R.
AU  - Huang, W.
AU  - Tan, M.
AU  - Huang, J.
AU  - Gan, C.
TI  - Graph convolutional module for temporal action localization in videos
AB  - Temporal action localization, which requires a machine to recognize the location as well as the category of action instances in videos, has long been researched in computer vision. The main challenge of temporal action localization lies in that videos are usually long and untrimmed with diverse action contents involved. Existing state-of-the-art action localization methods divide each video into multiple action units (i.e., proposals in two-stage methods and segments in one-stage methods) and then perform action recognition/regression on each of them individually, without explicitly exploiting their relations during learning. In this paper, we claim that the relations between action units play an important role in action localization, and a more powerful action detector should not only capture the local content of each action unit but also allow a wider field of view on the context related to it. To this end, we propose a general graph convolutional module (GCM) that can be easily plugged into existing action localization methods, including two-stage and one-stage paradigms. To be specific, we first construct a graph, where each action unit is represented as a node and their relations between two action units as an edge. Here, we use two types of relations, one for capturing the temporal connections between different action units, and the other one for characterizing their semantic relationship. Particularly for the temporal connections in two-stage methods, we further explore two different kinds of edges, one connecting the overlapping action units and the other one connecting surrounding but disjointed units. Upon the graph we built, we then apply graph convolutional networks (GCNs) to model the relations among different action units, which is able to learn more informative representations to enhance action localization. Experimental results show that our GCM consistently improves the performance of existing action localization methods, including two-stage methods (e.g., CBR [15] and R-C3D [47]) and one-stage methods (e.g., D-SSAD [22]), verifying the generality and effectiveness of our GCM. Moreover, with the aid of GCM, our approach significantly outperforms the state-of-the-art on THUMOS14 (50.9% versus 42.8%). Augmentation experiments on ActivityNet also verify the efficacy of modeling the relationships between action units.
PB  - arXiv
PY  - 2021
ST  - Graph convolutional module for temporal action localization in videos
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tpami.2021.3090167
ER  -


TY  - GEN
AU  - Cao, T.
AU  - He, W.
AU  - Chen, Y.
AU  - Kong, X.
AU  - Tian, X.
TI  - Spatiotemporal Characteristics of Soil Enzyme Activity in Subtropical Forests: The in Situ Evidence from 2d Zymography Images
AB  - Soil enzymes play a central role in organic matter decomposition and nutrient cycling in forest ecosystems. Because of methodological limitations, it has been difficult to experimentally verify the spatiotemporal characteristics of in situ enzymes in heterogeneous soil, especially in forest ecosystems. Using soil zymography, we investigated the seasonal dynamics of enzymes along the soil profiles (to a 28 cm depth) in three forests, a pure Quercus variabilis (oak) forest, a pure Pinus massoniana (pine) forest and a mixed forest (oak + pine), in southern China. For the three forest soils, we observed the same seasonal tendencies for hotspot areas of all enzymes that were positively correlated with soil temperature and moisture. Specifically, the hotspot areas of all enzymes were the largest in summer (21.9% ± 7.1%), the smallest in autumn (6.1% ± 4.9%), and medium-sized in spring (13.4% ± 5.9%) and winter (12.5% ± 7.7%). In addition, the hotspot areas of all enzymes were generally the largest (6.5%-33.3%) in the mixed forest, followed by those in the pure oak (1.3%-28.5%) and pine (0.2%-22.8%) forests. In the mixed forest, there were synergistic effects on the hotspot areas of β-1,4-N-ace-tyl-glucosaminidase and acid and alkaline phosphatase throughout the 28 cm soil profile, especially under dry (autumn) and cold (winter) climate conditions. These results suggest that the negative effects of drought and cold on enzymatic hotspot areas could be mitigated in the mixed forest. The positive effect of the mixed forest on the enzymatic hotspot areas was mainly attributed to the increases in the microbial biomass and total soil faunal abundance. Notably, our data of enzymatic hotspot areas accurately indicated the dynamics of the enzyme activities measured by the classical method in the three forest soils. The in situ zymography approach used in this study provided the first spatiotemporal evidence of the distributions of enzyme activities in forest soils. Unlike traditional methods of measuring enzyme activity that provide only data points, zymography is convenient and provides 2D spatiotemporal data, which are of paramount importance for understanding nutrient cycling and ecosystem functioning.
PB  - SSRN
PY  - 2021
ST  - Spatiotemporal Characteristics of Soil Enzyme Activity in Subtropical Forests
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.3967013
ER  -


TY  - GEN
AU  - Kahatapitiya, K.
AU  - Ren, Z.
AU  - Li, H.
AU  - Ryoo, M.S.
AU  - Hua, G.
TI  - Weakly-guided Self-supervised Pretraining for Temporal Activity Detection
AB  - Temporal Activity Detection aims to predict activity classes per frame, in contrast to video-level predictions in Activity Classification (i.e., Activity Recognition). Due to the expensive frame-level annotations required for detection, the scale of detection datasets is limited. Thus, commonly, previous work on temporal activity detection resorts to fine-tuning a classification model pretrained on large-scale classification datasets (e.g., Kinetics-400). However, such pretrained models are not ideal for downstream detection, due to the disparity between the pretraining and the downstream fine-tuning tasks. In this work, we propose a novel weakly-guided self-supervised pretraining method for detection. We leverage weak labels (classification) to introduce a self-supervised pretext task (detection) by generating frame-level pseudo labels, multi-action frames, and action segments. Simply put, we design a detection task similar to downstream, on large-scale classification data, without extra annotations. We show that the models pretrained with the proposed weakly-guided self-supervised detection task outperform prior work on multiple challenging activity detection benchmarks, including Charades and MultiTHUMOS. Our extensive ablations further provide insights on when and how to use the proposed models for activity detection. Code is available at github.com/kkahatapitiya/SSDet.
PB  - arXiv
PY  - 2021
ST  - Weakly-guided Self-supervised Pretraining for Temporal Activity Detection
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v37i1.25189
ER  -


TY  - GEN
AU  - Xiao, J.
AU  - Jing, L.
AU  - Zhang, L.
AU  - Yuille, A.
AU  - Li, Y.
TI  - Learning from Temporal Gradient for Semi-supervised Action Recognition
AB  - Semi-supervised video action recognition tends to enable deep neural networks to achieve remarkable performance even with very limited labeled data. However, existing methods are mainly transferred from current image-based methods (e.g., FixMatch). Without specifically utilizing the temporal dynamics and inherent multimodal attributes, their results could be suboptimal. To better leverage the encoded temporal information in videos, we introduce temporal gradient as an additional modality for more attentive feature extraction in this paper. To be specific, our method explicitly distills the fine-grained motion representations from temporal gradient (TG) and imposes consistency across different modalities (i.e., RGB and TG). The performance of semi-supervised action recognition is significantly improved without additional computation or parameters during inference. Our method achieves the state-of-the-art performance on three video action recognition benchmarks (i.e., Kinetics-400, UCF-101, and HMDB-51) under several typical semi-supervised settings (i.e., different ratios of labeled data). Code is made available at https://github.com/lambert-x/video-semisup.
PB  - arXiv
PY  - 2021
ST  - Learning from Temporal Gradient for Semi-supervised Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52688.2022.00325
ER  -


TY  - GEN
AU  - Yang, L.
AU  - Han, J.
AU  - Zhao, T.
AU  - Zhang, D.
AU  - Chen, J.
TI  - Background-click supervision for temporal action localization
AB  - Weakly supervised temporal action localization aims at learning the instance-level action pattern from the video-level labels, where a significant challenge is action-context confusion. To overcome this challenge, one recent work builds an action-click supervision framework. It requires similar annotation costs but can steadily improve the localization performance when compared to the conventional weakly supervised methods. In this paper, by revealing that the performance bottleneck of the existing approaches mainly comes from the background errors, we find that a stronger action localizer can be trained with labels on the background video frames rather than those on the action frames. To this end, we convert the action-click supervision to the background-click supervision and develop a novel method, called BackTAL. Specifically, BackTAL implements two-fold modeling on the background video frames, i.e. the position modeling and the feature modeling. In position modeling, we not only conduct supervised learning on the annotated video frames but also design a score separation module to enlarge the score differences between the potential action frames and backgrounds. In feature modeling, we propose an affinity module to measure frame-specific similarities among neighboring frames and dynamically attend to informative neighbors when calculating temporal convolution. Extensive experiments on three benchmarks are conducted, which demonstrate the high performance of the established BackTAL and the rationality of the proposed background-click supervision. Code is available at https://github.com/VividLe/BackTAL.
PB  - arXiv
PY  - 2021
ST  - Background-click supervision for temporal action localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tpami.2021.3132058
ER  -


TY  - GEN
AU  - Liu, W.
AU  - Tekin, B.
AU  - Coskun, H.
AU  - Fua, P.
AU  - Pollefeys, M.
TI  - Learning to align sequential actions in the wild
AB  - State-of-the-art methods for self-supervised sequential action alignment rely on deep networks that find correspondences across videos in time. They either learn frame-to-frame mapping across sequences, which does not leverage temporal information, or assume monotonic alignment between each video pair, which ignores variations in the order of actions. As such, these methods are not able to deal with common real-world scenarios that involve background frames or videos that contain non-monotonic sequence of actions. In this paper, we propose an approach to align sequential actions in the wild that involve diverse temporal variations. To this end, we propose an approach to enforce temporal priors on the optimal transport matrix, which leverages temporal consistency, while allowing for variations in the order of actions. Our model accounts for both monotonic and non-monotonic sequences and handles background frames that should not be aligned. We demonstrate that our approach consistently outperforms the state-of-the-art in self-supervised sequential action representation learning on four different benchmark datasets.
PB  - arXiv
PY  - 2021
ST  - Learning to align sequential actions in the wild
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52688.2022.00222
ER  -


TY  - GEN
AU  - Frieswijk, K.
AU  - Zino, L.
AU  - Cao, M.
TI  - Modelling the effect of vaccination and human behaviour on the spread of epidemic diseases on temporal networks
AB  - Motivated by the increasing number of COVID-19 cases that have been observed in many countries after the vaccination and relaxation of non-pharmaceutical interventions, we propose a mathematical model on time-varying networks for the spread of recurrent epidemic diseases in a partially vaccinated population. The model encapsulates several realistic features, such as the different effectiveness of the vaccine against transmission and development of severe symptoms, testing practices, the possible implementation of non-pharmaceutical interventions to reduce the transmission, isolation of detected individuals, and human behaviour. Using a mean-field approach, we analytically derive the epidemic threshold of the model and, if the system is above such a threshold, we compute the epidemic prevalence at the endemic equilibrium. These theoretical results show that precautious human behaviour and effective testing practices are key toward avoiding epidemic outbreaks. Interestingly, we found that, in many realistic scenarios, vaccination is successful in mitigating the outbreak by reducing the prevalence of seriously ill patients, but it could be a double-edged sword, whereby in some cases it might favour resurgent outbreaks, calling for higher testing rates, more cautiousness and responsibility among the population, or the reintroduction of non-pharmaceutical interventions to achieve complete eradication.
PB  - arXiv
PY  - 2021
ST  - Modelling the effect of vaccination and human behaviour on the spread of epidemic diseases on temporal networks
Y2  - 2025/05/05/21:54:31
DO  - 10.23919/ecc55457.2022.9838287
ER  -


TY  - GEN
AU  - Yambem, S.D.
AU  - Jain, M.
TI  - Temporal variation in the behaviour of a cooperatively breeding bird, Jungle Babbler (Argya striata) at diel and seasonal scale
AB  - Time is an important and limited resource that can drive the trade-off between various essential activities in the lives of animals. Group-living animals need to perform different behaviour to meet their individual needs and also participate in group activities. They must, therefore, partition the available time between these activities which may vary considerably with environmental and ecological conditions. We examined time-activity budget of a cooperative passerine, Jungle Babbler (Argya striata) and how their behaviour vary across diel and seasonal scales. A repertoire of 13 behaviour was recorded of which 12 behaviour that occur throughout the year were examined further in detail. This included individual behaviour such as foraging, grooming, rest, shower and group behaviour such as allogrooming, movement, play, sentinel, mobbing and inter-group fight. Our results indicate that most of the time (about 70%) was spent performing individual behaviour and the remaining time (about 30%) was allocated to social behaviour. We also found almost all behaviour varied across diel and seasonal scale with respect to proportion of time spent performing them. This highlights the impact of environmental factors on how animals partition their time to perform various activities. Our study also lays the foundation for future studies examining the role of ecological factors such as habitat type and predation pressure in driving these patterns of behaviour in Jungle Babblers.
PB  - bioRxiv
PY  - 2021
ST  - Temporal variation in the behaviour of a cooperatively breeding bird, Jungle Babbler (Argya striata) at diel and seasonal scale
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s42965-022-00254-w
ER  -


TY  - GEN
AU  - Abeywardena, K.
AU  - Sumanthiran, S.
AU  - Jayasundara, S.
AU  - Rodrigo, R.
AU  - Jayasekara, P.
TI  - KORSAL: Key-point Detection based Online Real-Time Spatio-Temporal Action Localization
AB  - Real-time and online action localization in a video is a critical yet highly challenging problem. Accurate action localization requires utilization of both temporal and spatial information. Recent attempts achieve this by using computationally intensive 3D CNN architectures or highly redundant two-stream architectures with optical flow, making them both unsuitable for real-time, online applications. To accomplish activity localization under highly challenging real-time constraints, we propose utilizing fast and efficient key-point based bounding box prediction to spatially localize actions. We then introduce a tube-linking algorithm that maintains the continuity of action tubes temporally in the presence of occlusions. Further, we eliminate the need for a two-stream architecture by combining temporal and spatial information into a cascaded input to a single network, allowing the network to learn from both types of information. Temporal information is efficiently extracted using a structural similarity index map as opposed to computationally intensive optical flow. Despite the simplicity of our approach, our lightweight end-to-end architecture achieves state-of-the-art frame-mAP of 74.7% on the challenging UCF101-24 dataset, demonstrating a performance gain of 6.4% over the previous best online methods. We also achieve state-of-the-art video-mAP results compared to both online and offline methods. Moreover, our model achieves a frame rate of 41.8 FPS, which is a 10.7% improvement over contemporary real-time methods.
PB  - arXiv
PY  - 2021
ST  - KORSAL
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/ccece58730.2023.10288973
ER  -


TY  - GEN
AU  - Alsawadi, M.S.
AU  - Rio, M.
TI  - Skeleton-split framework using spatial temporal graph convolutional networks for action recogntion
AB  - There has been a dramatic increase in the volume of videos and their related content uploaded to the internet. Accordingly, the need for efficient algorithms to analyse this vast amount of data has attracted significant research interest. An action recognition system based upon human body motions has been proven to interpret videos’ contents accurately. This work aims to recognize activities of daily living using the ST-GCN model, providing a comparison between four different partitioning strategies: spatial configuration partitioning, full distance split, connection split, and index split. To achieve this aim, we present the first implementation of the ST-GCN framework upon the HMDB-51 dataset. We have achieved 48.88% top-1 accuracy by using the connection split partitioning approach. Through experimental simulation, we show that our proposals have achieved the highest accuracy performance on the UCF-101 dataset using the ST-GCN framework than the state-of-the-art approach. Finally, accuracy of 73.25% top-1 is achieved by using the index split partitioning strategy.
PB  - arXiv
PY  - 2021
ST  - Skeleton-split framework using spatial temporal graph convolutional networks for action recogntion
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/biosmart54244.2021.9677634
ER  -


TY  - GEN
AU  - Waudby, C.A.
AU  - Alvarez-Teijeiro, S.
AU  - Suppinger, S.
AU  - Christodoulou, J.
AU  - Mylona, A.
TI  - An intrinsic temporal order of c-Jun N-terminal phosphorylation regulates its activity by orchestrating co-factor recruitment
AB  - Protein phosphorylation is a major regulatory mechanism of cellular signalling. The c-Jun proto-oncoprotein is phosphorylated at four residues within its transactivation domain (TAD) by the JNK family kinases, but the functional significance of c-Jun multisite phosphorylation has remained elusive. Here we show that c-Jun phosphorylation by JNK exhibits a defined temporal kinetics, with serine63 and serine73 being phosphorylated more rapidly than threonine91 and threonine93. We identified the positioning of the phosphorylation sites relative to the kinase docking motif, and their primary sequence, as the main factors controlling phosphorylation kinetics. Functional analysis revealed three c-Jun phosphorylation states: unphosphorylated c-Jun recruits the Mbd3 repressor, serine63/73 doubly-phosphorylated c-Jun binds to the Tcf4 co-activator, whereas the fully phosphorylated form disfavours Tcf4 binding attenuating JNK signalling. Thus, c-Jun phosphorylation encodes multiple functional states that drive a complex signalling response from a single JNK input.
PB  - bioRxiv
PY  - 2021
ST  - An intrinsic temporal order of c-Jun N-terminal phosphorylation regulates its activity by orchestrating co-factor recruitment
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2021.11.03.465096
ER  -


TY  - GEN
AU  - Kaku, A.
AU  - Liu, K.
AU  - Parnandi, A.
AU  - Schambra, H.
AU  - Fernandez-Granda, C.
TI  - Sequence-to-sequence modeling for action identification at high temporal resolution
AB  - Automatic action identification from video and kinematic data is an important machine learning problem with applications ranging from robotics to smart health. Most existing works focus on identifying coarse actions such as running, climbing, or cutting a vegetable, which have relatively long durations. This is an important limitation for applications that require identification of subtle motions at high temporal resolution. For example, in stroke recovery, quantifying rehabilitation dose requires differentiating motions with sub-second durations. Our goal is to bridge this gap. To this end, we introduce a large-scale, multimodal dataset, StrokeRehab, as a new action-recognition benchmark that includes subtle short-duration actions labeled at a high temporal resolution. These short-duration actions are called functional primitives, and consist of reaches, transports, repositions, stabilizations, and idles. The dataset consists of high-quality Inertial Measurement Unit sensors and video data of 41 stroke-impaired patients performing activities of daily living like feeding, brushing teeth, etc. We show that current state-of-the-art models based on segmentation produce noisy predictions when applied to these data, which often leads to overcounting of actions. To address this, we propose a novel approach for high-resolution action identification, inspired by speech-recognition techniques, which is based on a sequence-to-sequence model that directly predicts the sequence of actions. This approach outperforms current state-of-the-art methods on the StrokeRehab dataset, as well as on the standard benchmark datasets 50Salads, Breakfast, and Jigsaws.
PB  - arXiv
PY  - 2021
ST  - Sequence-to-sequence modeling for action identification at high temporal resolution
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.neucom.2023.126617
ER  -


TY  - GEN
AU  - Radevski, G.
AU  - Moens, M.-F.
AU  - Tuytelaars, T.
TI  - Revisiting spatio-temporal layouts for compositional action recognition
AB  - Recognizing human actions is fundamentally a spatio-temporal reasoning problem, and should be, at least to some extent, invariant to the appearance of the human and the objects involved. Motivated by this hypothesis, in this work, we take an object-centric approach to action recognition. Multiple works have studied this setting before, yet it remains unclear (i) how well a carefully crafted, spatio-temporal layout-based method can recognize human actions, and (ii) how, and when, to fuse the information from layout- and appearance-based models. The main focus of this paper is compositional/few-shot action recognition, where we advocate the usage of multi-head attention (proven to be effective for spatial reasoning) over spatio-temporal layouts, i.e., configurations of object bounding boxes. We evaluate different schemes to inject video appearance information to the system, and benchmark our approach on background cluttered action recognition. On the Something-Else and Action Genome datasets, we demonstrate (i) how to extend multi-head attention for spatio-temporal layout-based action recognition, (ii) how to improve the performance of appearance-based models by fusion with layout-based models, (iii) that even on non-compositional background-cluttered video datasets, a fusion between layout- and appearance-based models improves the performance.
PB  - arXiv
PY  - 2021
ST  - Revisiting spatio-temporal layouts for compositional action recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-981-97-8511-7_30
ER  -


TY  - GEN
AU  - Kazakos, E.
AU  - Huh, J.
AU  - Nagrani, A.
AU  - Zisserman, A.
AU  - Damen, D.
TI  - With a little help from my temporal context: Multimodal egocentric action recognition
AB  - In egocentric videos, actions occur in quick succession. We capitalise on the action’s temporal context and propose a method that learns to attend to surrounding actions in order to improve recognition performance. To incorporate the temporal context, we propose a transformer-based multimodal model that ingests video and audio as input modalities, with an explicit language model providing action sequence context to enhance the predictions. We test our approach on EPIC-KITCHENS and EGTEA datasets reporting state-of-the-art performance. Our ablations showcase the advantage of utilising temporal context as well as incorporating audio input modality and language model to rescore predictions. Code and models at: https://github.com/ekazakos/MTCN.
PB  - arXiv
PY  - 2021
ST  - With a little help from my temporal context
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccv51070.2023.00481
ER  -


TY  - GEN
AU  - Chen, T.
AU  - Wang, S.
AU  - Zhou, D.
AU  - Guan, Y.
TI  - LSTA-Net: Long short-term Spatio-Temporal Aggregation Network for skeleton-based action recognition
AB  - Modelling various spatio-temporal dependencies is the key to recognising human actions in skeleton sequences. Most existing methods excessively relied on the design of traversal rules or graph topologies to draw the dependencies of the dynamic joints, which is inadequate to reflect the relationships of the distant yet important joints. Furthermore, due to the locally adopted operations, the important long-range temporal information is therefore not well explored in existing works. To address this issue, in this work we propose LSTA-Net: a novel Long short-term Spatio-Temporal Aggregation Network, which can effectively capture the long/short-range dependencies in a spatio-temporal manner. We devise our model into a pure factorised architecture which can alternately perform spatial feature aggregation and temporal feature aggregation. To improve the feature aggregation effect, a channel-wise attention mechanism is also designed and employed. Extensive experiments were conducted on three public benchmark datasets, and the results suggest that our approach can capture both long-and-short range dependencies in the space and time domain, yielding higher results than other state-of-the-art methods.1
PB  - arXiv
PY  - 2021
ST  - LSTA-Net
Y2  - 2025/05/05/21:54:31
DO  - 10.12694/scpe.v20i3.1545
ER  -


TY  - GEN
AU  - Bin, T.
AU  - Bin, M.
AU  - Guoqing, Z.
AU  - Siyu, C.
AU  - Jian, L.
TI  - Spatio-temporal patterns of fitness behavior in beijing based on social media data
AB  - Using social media data, this paper employs FastAI, Latent Dirichlet Allocation (LDA) and other text mining techniques coupled with GIS spatial analysis methods to study temporal and spatial patterns of tness behavior of residents in Beijing, China, from the perspective of residents’ daily behavior. Using LDA theme model technology, it is found that tness activities can be divided into four types: running-based tness; riding-based tness; tness in sports venue; and tness under professional guidance. Emotional analysis revealed that, residents can get a better tness experience in sports venues. There are also obvious differences in the spatio-temporal distribution of the different tness behaviors. Fitness behavior of Beijing residents has a multi-center spatial distribution pattern, with a wide coverage in northern city areas but obvious aggregation areas in southern city areas. In terms of temporal patterns, the residents' tness frequency shows an obvious periodic distribution (weekly and 24 hours). And there are obvious differences in the time distribution of tness behaviors for each theme. Additionally, based on the attribution analysis of a geodetector, it is found that the spatial distribution of tness behavior of residents is mainly affected by factors such as catering services, education and culture, companies and public facilities.
PB  - Research Square
PY  - 2021
ST  - Spatio-temporal patterns of fitness behavior in beijing based on social media data
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-967228/v1
ER  -


TY  - GEN
AU  - Balzer, E.L.
AU  - Torres-Giese, E.
TI  - Sequential motion planning assisted by group actions
AB  - We study higher analogues of effective and effectual topological complexity of spaces equipped with a group action. These are G-homotopy invariant and are motivated by the (higher) motion planning problem of G-spaces for which their group action is thought of as an external system assisting the motion planning. Related to this interpretation we define what we call orbital topological complexity, which is also a G-homotopy invariant that provides an upper bound for the topological complexity of the quotient space by the group action. We apply these concepts to actions of the group of order two on orientable surfaces and spheres.
PB  - arXiv
PY  - 2021
ST  - Sequential motion planning assisted by group actions
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s40590-021-00319-1
ER  -


TY  - GEN
AU  - Dai, R.
AU  - Das, S.
AU  - Brémond, F.
TI  - CTRN: Class-Temporal Relational Network for Action Detection
AB  - Action detection is an essential and challenging task, especially for densely labelled datasets of untrimmed videos. There are many real-world challenges in those datasets, such as composite action, co-occurring action, and high temporal variation of instance duration. For handling these challenges, we propose to explore both the class and temporal relations of detected actions. In this work, we introduce an end-to-end network: Class-Temporal Relational Network (CTRN). It contains three key components: (1) The Representation Transform Module filters the class-specific features from the mixed representations to build a graph structured data. (2) The Class-Temporal Module models the class and temporal relations in a sequential manner. (3) G-classifier leverages the privileged knowledge of the snippet-wise co-occurring action pairs to further improve the co-occurring action detection. We evaluate CTRN on three challenging densely labelled datasets and achieve state-of-the-art performance, reflecting the effectiveness and robustness of our method.
PB  - arXiv
PY  - 2021
ST  - CTRN
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-20080-9_7
ER  -


TY  - GEN
AU  - Vo Ho Viet, K.
AU  - Joo, H.
AU  - Yamazaki, K.
AU  - Tran, M.-T.
AU  - Le, N.
TI  - AEI: Actors-environment interaction with adaptive attention for temporal action proposals generation
AB  - Humans typically perceive the establishment of an action in a video through the interaction between an actor and the surrounding environment. An action only starts when the main actor in the video begins to interact with the environment, while it ends when the main actor stops the interaction. Despite the great progress in temporal action proposal generation, most existing works ignore the aforementioned fact and leave their model learning to propose actions as a black-box. In this paper, we make an attempt to simulate that ability of a human by proposing Actor Environment Interaction (AEI) network to improve the video representation for temporal action proposals generation. AEI contains two modules, i.e., perception-based visual representation (PVR) and boundary-matching module (BMM). PVR represents each video snippet by taking human-human relations and humans-environment relations into consideration using the proposed adaptive attention mechanism. Then, the video representation is taken by BMM to generate action proposals. AEI is comprehensively evaluated in ActivityNet-1.3 and THUMOS-14 datasets, on temporal action proposal and detection tasks, with two boundary-matching architectures (i.e., CNN-based and GCN-based) and two classifiers (i.e., Unet and P-GCN). Our AEI robustly outperforms the state-of-the-art methods with remarkable performance and generalization for both temporal action proposal generation and temporal action detection. Source code is available at.
PB  - arXiv
PY  - 2021
ST  - AEI
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s11263-022-01702-9
ER  -


TY  - GEN
AU  - Nag, S.
AU  - Zhu, X.
AU  - Xiang, T.
TI  - Few-shot temporal action localization with query adaptive transformer
AB  - Existing temporal action localization (TAL) works rely on a large number of training videos with exhaustive segment-level annotation, preventing them from scaling to new classes. As a solution to this problem, few-shot TAL (FS-TAL) aims to adapt a model to a new class represented by as few as a single video. Exiting FS-TAL methods assume trimmed training videos for new classes. However, this setting is not only unnatural – actions are typically captured in untrimmed videos, but also ignores background video segments containing vital contextual cues for foreground action segmentation. In this work, we first propose a new FS-TAL setting by proposing to use untrimmed training videos. Further, a novel FS-TAL model is proposed which maximizes the knowledge transfer from training classes whilst enabling the model to be dynamically adapted to both the new class and each video of that class simultaneously. This is achieved by introducing a query adaptive Transformer in the model. Extensive experiments on two action localization benchmarks demonstrate that our method can outperform all the state-of-the-art alternatives significantly in both single-domain and cross-domain scenarios. The source code can be found in https://github.com/sauradip/fewshotQAT
PB  - arXiv
PY  - 2021
ST  - Few-shot temporal action localization with query adaptive transformer
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4341961
ER  -


TY  - GEN
AU  - Wang, Y.
AU  - Ma, J.
AU  - Chen, X.
AU  - Du, C.
TI  - Resting brain activity emerges from wave propagating along spatiotemporal varying hyper-structural connectome
AB  - How spontaneous brain activities emerge from the structural connectivity (SC) has puzzled researchers for a long time. The underlying mechanism still remains largely unknown. Previous studies on modeling the resting-state human brain functional connectivity (FC) are normally based on the relatively static structural connectome directly and very few of them concern about the dynamic spatiotemporal variability of FC. Here we establish an explicit wave equation to describe the spontaneous cortical neural activities based on the high-order hypergraph representation of SC. Theoretical solution shows that the dynamic couplings between brain regions fluctuates in the form of an exponential wave regulated by the spatiotemporal varying Laplacian of the hyper-structural connectome (hSC), which orchestrates the cortical activities propagating in both space and time. Ultimately, we present a possible mechanism of how negative correlations emerge during the fluctuation of the hypergraph Laplacian of SC, which helps to further understand the fundamental role of SC in shaping the entire pattern of FC with a new perspective. Comprehensive tests on four connectome datasets with different resolutions confirm our theory and findings.
PB  - bioRxiv
PY  - 2021
ST  - Resting brain activity emerges from wave propagating along spatiotemporal varying hyper-structural connectome
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2021.10.11.464009
ER  -


TY  - GEN
AU  - Liu, X.
AU  - Zhou, X.
AU  - Zeng, Y.
AU  - Kendrick, K.M.
AU  - Becker, B.
TI  - Medial Prefrontal and Occipito-Temporal Activity at Encoding Determine Enhanced Recognition of Threatening Faces after 1.5 Years
AB  - Studies demonstrated that faces with threatening emotional expressions are better remembered than non-threatening faces. However, whether this memory advantage persists over years and which neural systems underlie such an effect remains unknown. Here, we employed an individual difference approach to examine whether the neural activity during incidental encoding was associated with differential recognition of faces with emotional expressions (angry, fearful, happy, sad and neutral) after a retention interval of > 1.5 years (N = 89). Behaviorally, we found a better recognition for threatening (angry, fearful) versus non-threatening (happy and neutral) faces after a > 1.5 years delay, which was driven by forgetting of non-threatening faces compared with immediate recognition after encoding. Multivariate principal component analysis (PCA) on the behavioral responses further confirmed the discriminative recognition performance between threatening and non-threatening faces. A voxel-wise whole-brain analysis on the concomitantly acquired functional magnetic imaging (fMRI) data during incidental encoding revealed that neural activity in bilateral inferior occipital gyrus (IOG) and ventromedial prefrontal/orbitofrontal cortex (vmPFC/OFC) was associated with the individual differences in the discriminative emotional face recognition performance measured by an innovative behavioral pattern similarity analysis (BPSA) based on inter-subject correlation (ISC). The left fusiform face area (FFA) was additionally determined using a regionally focused analysis. Overall, the present study provides evidence that threatening facial expressions lead to persistent face recognition over periods of > 1.5 years and differential encoding-related activity in the medial prefrontal cortex and occipito-temporal cortex may underlie this effect.
PB  - Research Square
PY  - 2021
ST  - Medial Prefrontal and Occipito-Temporal Activity at Encoding Determine Enhanced Recognition of Threatening Faces after 1.5 Years
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-895998/v1
ER  -


TY  - GEN
AU  - Cheung, B.C.H.
AU  - Hodgson, L.
AU  - Segall, J.E.
AU  - Wu, M.
TI  - Spatial and temporal dynamics of RhoA activities of single breast tumor cells in a 3D environment revealed by a machine learning-assisted FRET technique
AB  - One of the hallmarks of cancer cells is their exceptional ability to migrate within the extracellular matrix (ECM) for gaining access to the circulatory system, a critical step of cancer metastasis. RhoA, a small GTPase, is known to be a key molecular switch that toggles between actomyosin contractility and lamellipodial protrusion during cell migration. Current understanding of RhoA activity in cell migration has been largely derived from studies of cells plated on a two-dimensional (2D) substrate using a FRET biosensor. There has been increasing evidence that cells behave differently in a more physiologically relevant three-dimensional (3D) environment, however, studies of RhoA activities in 3D have been hindered by low signal-to-noise ratio in fluorescence imaging. In this paper, we present a machine learning-assisted FRET technique to follow the spatiotemporal dynamics of RhoA activities of single breast tumor cells (MDA-MB-231) migrating in a 3D as well as a 2D environment using a RhoA biosensor. We found that RhoA activity is more polarized along the long axis of the cell for single cells migrating on 2D fibronectin-coated glass versus those embedded in 3D collagen matrices. In particular, RhoA activities of cells in 2D exhibit a distinct front-to-back and back-to-front movement during migration in contrast to those in 3D. Finally, regardless of dimensionality, RhoA polarization is found to be correlated with cell shape.
PB  - arXiv
PY  - 2021
ST  - Spatial and temporal dynamics of RhoA activities of single breast tumor cells in a 3D environment revealed by a machine learning-assisted FRET technique
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.yexcr.2021.112939
ER  -


TY  - GEN
AU  - Schaffer, E.S.
AU  - Mishra, N.
AU  - Whiteway, M.R.
AU  - Abbott, L.F.
AU  - Axel, R.
TI  - Flygenvectors: The spatial and temporal structure of neural activity across the fly brain
AB  - What are the spatial and temporal scales of brainwide neuronal activity, and how do activities at different scales interact? We used SCAPE microscopy to image a large fraction of the central brain of adult Drosophila melanogaster with high spatiotemporal resolution while flies engaged in a variety of behaviors, including running, grooming and flailing. This revealed neural representations of behavior on multiple spatial and temporal scales. The activity of most neurons across the brain correlated (or, in some cases, anticorrelated) with running and flailing over timescales that ranged from seconds to almost a minute. Grooming elicited a much weaker global response. Although these behaviors accounted for a large fraction of neural activity, residual activity not directly correlated with behavior was high dimensional. Many dimensions of the residual activity reflect the activity of small clusters of spatially organized neurons that may correspond to genetically defined cell types. These clusters participate in the global dynamics, indicating that neural activity reflects a combination of local and broadly distributed components. This suggests that microcircuits with highly specified functions are provided with knowledge of the larger context in which they operate, conferring a useful balance of specificity and flexibility.
PB  - bioRxiv
PY  - 2021
ST  - Flygenvectors
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2021.09.25.461804
ER  -


TY  - GEN
AU  - Sporn, S.
AU  - Chen, X.
AU  - Galea, J.M.
TI  - The dissociable effects of reward on sequential motor behaviour
AB  - Reward has consistently been shown to enhance motor performance however its beneficial effects appear to be largely unspecific. While reward has been shown to invigorate performance, it also enhances learning and/or retention. Therefore, a mechanistic account of the effects of reward on motor behaviour is lacking. Here we tested the hypothesis that these distinct reward-based improvements are driven by dissociable reward types: explicit reward (i.e. money) and performance feedback (i.e. points). Experiment 1 showed that explicit reward instantaneously improved movement times (MT) using a novel sequential reaching task. In contrast, performance-based feedback led to learning-related improvements. Importantly, pairing both maximised MT performance gains and accelerated movement fusion. Fusion describes an optimisation process during which neighbouring sequential movements blend together to form singular actions. Results from experiment 2 served as a replication and showed that fusion led to enhanced performance speed whilst also improving movement efficiency through increased smoothness. Finally, experiment 3 showed that these improvements in performance persist for 24 hours even without reward availability. This highlights the dissociable impact of explicit reward and performance feedback, with their combination maximising performance gains and leading to stable improvements in the speed and efficiency of sequential actions.
PB  - bioRxiv
PY  - 2021
ST  - The dissociable effects of reward on sequential motor behaviour
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2021.09.17.460761
ER  -


TY  - GEN
AU  - Wu, J.
AU  - Sun, P.
AU  - Chen, S.
AU  - Ma, L.
AU  - Luo, P.
TI  - Towards high-quality temporal action detection with sparse proposals
AB  - Temporal Action Detection (TAD) is an essential and challenging topic in video understanding, aiming to localize the temporal segments containing human action instances and predict the action categories. The previous works greatly rely upon dense candidates either by designing varying anchors or enumerating all the combinations of boundaries on video sequences; therefore, they are related to complicated pipelines and sensitive hand-crafted designs. Recently, with the resurgence of Transformer, query-based methods have tended to become the rising solutions for their simplicity and flexibility. However, there still exists a performance gap between query-based methods and well-established methods. In this paper, we identify the main challenge lies in the large variants of action duration and the ambiguous boundaries for short action instances; nevertheless, quadratic-computational global attention prevents query-based methods to build multi-scale feature maps. Towards high-quality temporal action detection, we introduce Sparse Proposals to interact with the hierarchical features. In our method, named SP-TAD, each proposal attends to a local segment feature in the temporal feature pyramid. The local interaction enables utilization of high-resolution features to preserve action instances details. Extensive experiments demonstrate the effectiveness of our method, especially under high tIoU thresholds. E.g., we achieve the state-of-the-art performance on THUMOS14 (45.7% on mAP@0.6, 33.4% on mAP@0.7 and 53.5% on mAP@Avg) and competitive results on ActivityNet-1.3 (32.99% on mAP@Avg). Code will be made available at https://github.com/wjn922/SP-TAD.
PB  - arXiv
PY  - 2021
ST  - Towards high-quality temporal action detection with sparse proposals
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/ispacs57703.2022.10082820
ER  -


TY  - GEN
AU  - Huang, T.
AU  - Luo, Y.
AU  - Jiang, Q.
AU  - Yang, H.
AU  - Huang, C.
TI  - Synergistic Impacts of Climate Change and Human Activities on Spatiotemporal Organic Nitrogen Burial Variation in a Plateau Lake in Southwest China
AB  - The concentration and sources of organic nitrogen (ON) in lake sediment significantly affect the lake nitrogen cycle. However, the influencing factors and contributors to the ON accumulation rate (ONAR) are unclear. In this study, tree sediment cores from northern, eastern, and southern Dianchi Lake (DC-N, DC-E, and DC-S, respectively), sampled in July 2014, were used to study the effects of autochthonous and allochthonous sources on ON. The results showed that ON and the ONAR increased 2.4–5.1 and 2.6–4.8 times, respectively, from1900 to2000, especially since the 1980s, at which point algal blooms occurred more frequently. The ON contents decreased in the order: DC-S > DC-N > DC-E, whereas the ONAR values followed the order: DC-N > DC-S > DC-E, suggesting that the ONAR was influenced by ON content as well as depositional environmental conditions. The total concentrations of n-alkanes (n-C12 to n-C34) ranged from 4719.4 ng g− 1 to 61,959.6 ng g− 1 in the three sediment cores, each of which exhibited different n-alkanes characteristic variation with vertical depth. The sources of ON were mainly allochthonous (soil erosion and terrestrial plants) and autochthonous (algal and aquatic plants) in DC-S and DC-N, respectively, whereas they were primarily mixed planktonic and terrestrial sources in DC-E. Using the stochastic impacts by regression on population, affluence, and technology model to further examine the ONAR values revealed that 1% increase in temperature and nitrogen fertilizer can increase the ONAR by 73.8–86.2% and 73.2–151.3% in all sediments, especially in DC-S and DC-E. However, a 1% increase in construction area could reduce the ONAR by 2.4–14.2%, especially in DC-N. Overall, climate change and human activities determine the spatial and temporal ONAR variation in Dianchi Lake.
PB  - Research Square
PY  - 2021
ST  - Synergistic Impacts of Climate Change and Human Activities on Spatiotemporal Organic Nitrogen Burial Variation in a Plateau Lake in Southwest China
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-811547/v1
ER  -


TY  - GEN
AU  - Tamura, H.
TI  - The temporal pattern of spiking activity of a thalamic neuron are related to the amplitude of the cortical local field potential
AB  - Neuron activity in the sensory cortices mainly depends on feedforward thalamic inputs. High-frequency activity of a thalamic input can be temporally integrated by a neuron in the sensory cortex and is likely to induce larger depolarization. However, feedforward inhibition (FFI) and depression of excitatory synaptic transmission in thalamocortical pathways attenuate depolarization induced by the latter part of high-frequency spiking activity and the temporal summation may not be effective. The spiking activity of a thalamic neuron in a specific temporal pattern may circumvent FFI and depression of excitatory synapses. The present study determined the relationship between the temporal pattern of spiking activity of a single thalamic neuron and the degree of cortical activation as well as that between the firing rate of spiking activity of a single thalamic neuron and the degree of cortical activation. Spiking activity of a thalamic neuron was recorded extracellularly from the lateral geniculate nucleus (LGN) in male Long-Evans rats. Degree of cortical activation was assessed by simultaneous recording of local field potential (LFP) from the visual cortex. A specific temporal pattern appearing in three consecutive spikes of an LGN neuron induced larger cortical LFP modulation than high-frequency spiking activity during a short period. These findings indicate that spiking activity of thalamic inputs is integrated by a synaptic mechanism sensitive to an input temporal pattern.
PB  - bioRxiv
PY  - 2021
ST  - The temporal pattern of spiking activity of a thalamic neuron are related to the amplitude of the cortical local field potential
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2021.09.08.459532
ER  -


TY  - GEN
AU  - Xing, H.
AU  - Xue, Y.
AU  - Zhou, M.
AU  - Burschka, D.
TI  - Robust event detection based on spatio-temporal latent action unit using skeletal information
AB  - This paper proposes a novel dictionary learning approach to detect event anomalities using skeletal information extracted from RGBD video. The event action is represented as several latent action atoms and composed of latent spatial and temporal attributes. We aim to construct a network able to learn from few examples and also rules defined by the user. The skeleton frames are clustered by an initial K-means method. Each skeleton frame is assigned with a varying weight parameter and fed into our Gradual Online Dictionary Learning (GODL) algorithm. During the training process, outlier frames will be gradually filtered by reducing the weight that is inversely proportional to a cost. To strictly distinguish the event action from similar actions and robustly acquire its action units, we build a latent unit temporal structure for each sub-action. We validate the method at the example of fall event detection on NTU RGB+D dataset, because it provides a benchmark available for comparison. We present the experimental validation of the achieved accuracy, recall, and precision. Our approach achieves the best performance in precision and accuracy of human fall event detection, compared with other existing dictionary learning methods. Our method remains the highest accuracy and the lowest variance, with increasing noise ratio.
PB  - arXiv
PY  - 2021
ST  - Robust event detection based on spatio-temporal latent action unit using skeletal information
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iros51168.2021.9636553
ER  -


TY  - GEN
AU  - Basu, D.
AU  - Sendhilnathan, N.
AU  - Murthy, A.
TI  - Neck motor unit activity displays neural signatures of temporal control during sequential saccade planning
AB  - Goal-directed behavior involves the transformation of neural movement plans into appropriate muscle activity patterns. Studies involving single saccades have shown that a rapid, direct pathway links saccade planning in frontal eye fields (FEF) to neck muscle activity. It is unknown if the rapid connection between FEF and neck muscle is maintained during sequential saccade planning. We show that sequence planning signals in the FEF are preserved in the neck EMG, although the activity is delayed specifically for the second saccade. Our results suggest that while the direct link between FEF and neck muscle facilitates downstream continuation of FEF response patterns, an indirect route exists through an inhibitory control center like the basal ganglia, limiting the information flow during processing of saccade sequences. Thus, the indirect and direct pathways from the FEF may function together to enable rapid synchronous, but controlled eye-head responses to sequential gaze shifts.
PB  - bioRxiv
PY  - 2021
ST  - Neck motor unit activity displays neural signatures of temporal control during sequential saccade planning
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2021.08.31.458347
ER  -


TY  - GEN
AU  - Beltzung, B.
AU  - Martinet, L.
AU  - MacIntosh, A.J.J.
AU  - Pelé, M.
AU  - Sueur, C.
TI  - To draw or not to draw: Understanding the temporal organization of drawing behaviour using fractal analyses
AB  - Studies on drawing often focused on spatial aspects of the finished products. Here, the drawing behaviour was studied by analysing its intermittent process, between drawing (i.e. marking a surface) and interruption (i.e. a pause in the marking gesture). To assess how this intermittence develops with age, we collected finger-drawings on a touchscreen by 185 individuals (children and adults). We measured the temporal structure of each drawing sequence to determine its complexity. To do this, we applied temporal fractal estimators to each drawing time series before combining them in a Principal Component Analysis procedure. The youngest children (3 years-old) drew in a more stereotypical way with long-range dependence detected in their alternations between states. Among older children and adults, the complexity of drawing sequences increased showing a less predictable behaviour as their drawings become more detailed and figurative. This study improves our understanding of the temporal aspects of drawing behaviour, and contributes to an objective understanding of its ontogeny.
PB  - bioRxiv
PY  - 2021
ST  - To draw or not to draw
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2021.08.29.458053
ER  -


TY  - GEN
AU  - Juarez, B.
AU  - Kong, M.-S.
AU  - Jo, Y.S.
AU  - Soden, M.E.
AU  - Zweifel, L.S.
TI  - Temporal scaling of dopamine neuron firing and dopamine release by distinct ion channels shape behavior
AB  - Despite the widely known role of dopamine in reinforcement learning, how the patterns of dopamine release that are critical to the acquisition, performance, and extinction of conditioned responses are generated is poorly resolved. Here, we demonstrate that the coordinated actions of two ion channels, Kv4.3 and BKCa1.1, control the pattern of dopamine neuron firing and dopamine release on different time scales to regulate separate phases of reinforced behavior in mice. Inactivation of Kv4.3 in VTA dopamine neurons increases ex vivo pacemaker activity and excitability that is associated with increased in vivo ramping dynamics prior to lever press in a learned instrumental response paradigm. Loss of Kv4.3 enhances performance of the learned response and facilitates extinction. In contrast, loss of BKCa1.1 increases burst firing and phasic dopamine release that enhances learning of an instrumental response. Inactivation of BKCa1.1 enhances extinction burst lever pressing in early extinction training that is associated with increased reward prediction error signals. These data demonstrate that temporally distinct patterns of dopamine release are governed by the intrinsic regulators of the cell to shape behavior.
PB  - bioRxiv
PY  - 2021
ST  - Temporal scaling of dopamine neuron firing and dopamine release by distinct ion channels shape behavior
Y2  - 2025/05/05/21:54:31
DO  - 10.1126/sciadv.adg8869
ER  -


TY  - GEN
AU  - Sadique, F.
AU  - Sengupta, S.
TI  - Modeling and analyzing attacker behavior in IoT botnet using Temporal Convolution Network (TCN)
AB  - Traditional reactive approach of blacklisting botnets fails to adapt to the rapidly evolving landscape of cyberattacks. An automated and proactive approach to detect and block botnet hosts will immensely benefit the industry. Behavioral analysis of attackers is shown to be effective against a wide variety of attack types. Previous works, however, focus solely on anomalies in network traffic to detect bots and botnet. In this work we take a more robust approach of analyzing the heterogeneous events including network traffic, file download events, SSH logins and chain of commands input by attackers in a compromised host. We have deployed several honeypots to simulate Linux shells and allowed attackers access to the shells. We have collected a large dataset of heterogeneous threat events from the honeypots. We have then combined and modeled the heterogeneous threat data to analyze attacker behavior. Then we have used a deep learning architecture called a Temporal Convolutional Network (TCN) to do sequential and predictive analysis on the data. A prediction accuracy of 85 - 97% validates our data model as well as our analysis methodology. In this work, we have also developed an automated mechanism to collect and analyze these data. For the automation we have used CYbersecurity information Exchange (CYBEX). Finally, we have compared TCN with Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) and have showed that TCN outperforms LSTM and GRU for the task at hand.
PB  - arXiv
PY  - 2021
ST  - Modeling and analyzing attacker behavior in IoT botnet using Temporal Convolution Network (TCN)
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.cose.2022.102714
ER  -


TY  - GEN
AU  - Talidou, A.
AU  - Frankland, P.W.
AU  - Mabbott, D.
AU  - Lefebvre, J.
TI  - Learning to be on time: Temporal coordination of neural dynamics by activity-dependent myelination
AB  - Activity-dependent myelination is the mechanism by which myelin changes as a function of neural activity, and plays a fundamental role in brain plasticity. Mediated by structural changes in glia, activity-dependent myelination regulates axonal conduction velocity. It remains unclear how neural activity impacts myelination to orchestrate the timing of neural signaling. We developed a model of spiking neurons enhanced with neuron-glia feedback. Inspired by experimental data and use-dependent synaptic plasticity, we introduced a learning rule, called the Activity-Dependent Myelination (ADM) rule, by which conduction velocity scales with firing rates. We found that the ADM rule implements a homeostatic control mechanism that promotes and preserves synchronization. ADM-mediated plasticity was found to optimize synchrony by compensating for variability in axonal lengths by scaling conduction velocity in an axon-specific way. This property was maintained even when the network structure is altered. We further explored how external stimuli interact with the ADM rule to trigger bidirectional and reversible changes in conduction delays. These results highlight the role played by activity-dependent myelination in synchronous neural communication and brain plasticity.
PB  - bioRxiv
PY  - 2021
ST  - Learning to be on time
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2021.08.17.456520
ER  -


TY  - GEN
AU  - Deshpande, S.S.
AU  - Smith, G.A.
AU  - van Drongelen, W.
TI  - Third-order motifs are sufficient to fully and uniquely characterize spatiotemporal neural network activity
AB  - Neuroscientific analyses balance between capturing the brain’s complexity and expressing that complexity in meaningful and understandable ways. Here we present a novel approach that fully characterizes neural network activity and does so by uniquely transforming raw signals into easily interpretable and biologically relevant metrics of network behavior. We first prove that third-order, or triple, correlation describes network activity in its entirety using the triple correlation uniqueness (TCU) theorem. Triple correlation quantifies the relationships among three events separated by spatial and temporal lags, which are triplet motifs. Classifying these motifs by their event sequencing leads to fourteen qualitatively distinct motif classes that embody well-studied network behaviors such as synchrony, feedback, feedforward, convergence, and divergence. Within these motif classes, the summed triple correlations provide novel metrics of network behavior, as well as being inclusive of commonly used analyses. We demonstrate the power of this approach on a range of networks with increasingly obscured signals, from ideal noiseless simulations to noisy experimental data. This approach can be easily applied to any recording modality, so existing neural datasets are ripe for reanalysis. Triple correlation is an accessible signal processing tool with a solid theoretical foundation capable of revealing previously elusive information within recordings of neural networks.
PB  - bioRxiv
PY  - 2021
ST  - Third-order motifs are sufficient to fully and uniquely characterize spatiotemporal neural network activity
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2021.08.16.456546
ER  -


TY  - GEN
AU  - Ma, Y.
AU  - Zhou, B.
AU  - Wang, R.
AU  - Wang, P.
TI  - Multi-stage Factorized Spatio-Temporal Representation for RGB-D Action and Gesture Recognition
AB  - RGB-D action and gesture recognition remain an interesting topic in human-centered scene understanding, primarily due to the multiple granularities and large variation in human motion. Although many RGB-D based action and gesture recognition approaches have demonstrated remarkable results by utilizing highly integrated spatio-temporal representations across multiple modalities (i.e., RGB and depth data), they still encounter several challenges. Firstly, vanilla 3D convolution makes it hard to capture fine-grained motion differences between local clips under different modalities. Secondly, the intricate nature of highly integrated spatio-temporal modeling can lead to optimization difficulties. Thirdly, duplicate and unnecessary information can add complexity and complicate entangled spatio-temporal modeling. To address the above issues, we propose an innovative heuristic architecture called Multi-stage Factorized Spatio-Temporal (MFST) for RGB-D action and gesture recognition. The proposed MFST model comprises a 3D Central Difference Convolution Stem (CDC-Stem) module and multiple factorized spatio-temporal stages. The CDC-Stem enriches fine-grained temporal perception, and the multiple hierarchical spatio-temporal stages construct dimension-independent higher-order semantic primitives. Specifically, the CDC-Stem module captures bottom-level spatio-temporal features and passes them successively to the following spatio-temporal factored stages to capture the hierarchical spatial and temporal features through the Multi-Scale Convolution and Transformer (MSC-Trans) hybrid block and Weight-shared Multi-Scale Transformer (WMS-Trans) block. The seamless integration of these innovative designs results in a robust spatio-temporal representation that outperforms state-of-the-art approaches on RGB-D action and gesture recognition datasets.
PB  - arXiv
PY  - 2023
ST  - Multi-stage Factorized Spatio-Temporal Representation for RGB-D Action and Gesture Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3581783.3612301
ER  -


TY  - GEN
AU  - Strafforello, O.
AU  - Liu, X.
AU  - Schutte, K.
AU  - van Gemert, J.
TI  - Video BagNet: short temporal receptive fields increase robustness in long-term action recognition
AB  - Previous work on long-term video action recognition relies on deep 3D-convolutional models that have a large temporal receptive field (RF). We argue that these models are not always the best choice for temporal modeling in videos. A large temporal receptive field allows the model to encode the exact sub-action order of a video, which causes a performance decrease when testing videos have a different sub-action order. In this work, we investigate whether we can improve the model robustness to the sub-action order by shrinking the temporal receptive field of action recognition models. For this, we design Video BagNet, a variant of the 3D ResNet-50 model with the temporal receptive field size limited to 1, 9, 17 or 33 frames. We analyze Video BagNet on synthetic and real-world video datasets and experimentally compare models with varying temporal receptive fields. We find that short receptive fields are robust to sub-action order changes, while larger temporal receptive fields are sensitive to the sub-action order.
PB  - arXiv
PY  - 2023
ST  - Video BagNet
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccvw60793.2023.00023
ER  -


TY  - GEN
AU  - Bahrami, E.
AU  - Francesca, G.
AU  - Gall, J.
TI  - How Much Temporal Long-Term Context is Needed for Action Segmentation?
AB  - Modeling long-term context in videos is crucial for many fine-grained tasks including temporal action segmentation. An interesting question that is still open is how much long-term temporal context is needed for optimal performance. While transformers can model the long-term context of a video, this becomes computationally prohibitive for long videos. Recent works on temporal action segmentation thus combine temporal convolutional networks with self-attentions that are computed only for a local temporal window. While these approaches show good results, their performance is limited by their inability to capture the full context of a video. In this work, we try to answer how much long-term temporal context is required for temporal action segmentation by introducing a transformer-based model that leverages sparse attention to capture the full context of a video. We compare our model with the current state of the art on three datasets for temporal action segmentation, namely 50Salads, Breakfast, and Assembly101. Our experiments show that modeling the full context of a video is necessary to obtain the best performance for temporal action segmentation.
PB  - arXiv
PY  - 2023
ST  - How Much Temporal Long-Term Context is Needed for Action Segmentation?
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccv51070.2023.00950
ER  -


TY  - GEN
AU  - Li, X.
AU  - Wang, S.
AU  - Huang, R.
AU  - Gowda, M.
AU  - Kesidis, G.
TI  - Temporal-Distributed Backdoor Attack Against Video Based Action Recognition
AB  - Deep neural networks (DNNs) have achieved tremendous success in various applications including video action recognition, yet remain vulnerable to backdoor attacks (Trojans). The backdoor-compromised model will mis-classify to the target class chosen by the attacker when a test instance (from a non-target class) is embedded with a specific trigger, while maintaining high accuracy on attack-free instances. Although there are extensive studies on backdoor attacks against image data, the susceptibility of video-based systems under backdoor attacks remains largely unexplored. Current studies are direct extensions of approaches proposed for image data, e.g., the triggers are independently embedded within the frames, which tend to be detectable by existing defenses. In this paper, we introduce a simple yet effective backdoor attack against video data. Our proposed attack, adding perturbations in a transformed domain, plants an imperceptible, temporally distributed trigger across the video frames, and is shown to be resilient to existing defensive strategies. The effectiveness of the proposed attack is demonstrated by extensive experiments with various well-known models on two video recognition benchmarks, UCF101 and HMDB51, and a sign language recognition benchmark, Greek Sign Language (GSL) dataset. We delve into the impact of several influential factors on our proposed attack and identify an intriguing effect termed "collateral damage" through extensive studies.
PB  - arXiv
PY  - 2023
ST  - Temporal-Distributed Backdoor Attack Against Video Based Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v38i4.28104
ER  -


TY  - GEN
AU  - Kim, J.
AU  - Lee, M.
AU  - Heo, J.-P.
TI  - Self-Feedback DETR for Temporal Action Detection
AB  - Temporal Action Detection (TAD) is challenging but fundamental for real-world video applications. Recently, DETR-based models have been devised for TAD but have not performed well yet. In this paper, we point out the problem in the self-attention of DETR for TAD; the attention modules focus on a few key elements, called temporal collapse problem. It degrades the capability of the encoder and decoder since their self-attention modules play no role. To solve the problem, we propose a novel framework, Self-DETR, which utilizes cross-attention maps of the decoder to reactivate self-attention modules. We recover the relationship between encoder features by simple matrix multiplication of the cross-attention map and its transpose. Likewise, we also get the information within decoder queries. By guiding collapsed self-attention maps with the guidance map calculated, we settle down the temporal collapse of self-attention modules in the encoder and decoder. Our extensive experiments demonstrate that Self-DETR resolves the temporal collapse problem by keeping high diversity of attention over all layers. Moreover, it is validated that our simple framework achieves a new state-of-the-art performance on THUMOS14 and outperforms all the DETR-based approaches on ActivityNet-v1.3.
PB  - arXiv
PY  - 2023
ST  - Self-Feedback DETR for Temporal Action Detection
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccv51070.2023.00944
ER  -


TY  - GEN
AU  - Lin, J.
AU  - Shan, R.
AU  - Zhu, C.
AU  - Yu, Y.
AU  - Zhang, W.
TI  - ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation
AB  - With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data quality of testing samples, which greatly reduces the difficulty for LLMs to extract the essential knowledge from user behavior sequences. As for few-shot recommendation, we further design retrieval-enhanced instruction tuning (ReiT) by adopting SUBR as a data augmentation technique for training samples. Specifically, we develop a mixed training dataset consisting of both the original data samples and their retrieval-enhanced counterparts. We conduct extensive experiments on three real-world public datasets to demonstrate the superiority of ReLLa compared with existing baseline models, as well as its capability for lifelong sequential behavior comprehension.
PB  - arXiv
PY  - 2023
ST  - ReLLa
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3589334.3645467
ER  -


TY  - GEN
AU  - Han, S.
AU  - Park, Y.
AU  - Lee, M.
AU  - An, J.
AU  - Lee, D.
TI  - Enhancing Spatiotemporal Traffic Prediction through Urban Human Activity Analysis
AB  - Traffic prediction is one of the key elements to ensure the safety and convenience of citizens. Existing traffic prediction models primarily focus on deep learning architectures to capture spatial and temporal correlation. They often overlook the underlying nature of traffic. Specifically, the sensor networks in most traffic datasets do not accurately represent the actual road network exploited by vehicles, failing to provide insights into the traffic patterns in urban activities. To overcome these limitations, we propose an improved traffic prediction method based on graph convolution deep learning algorithms. We leverage human activity frequency data from National Household Travel Survey to enhance the inference capability of a causal relationship between activity and traffic patterns. Despite making minimal modifications to the conventional graph convolutional recurrent networks and graph convolutional transformer architectures, our approach achieves state-of-the-art performance without introducing excessive computational overhead.
PB  - arXiv
PY  - 2023
ST  - Enhancing Spatiotemporal Traffic Prediction through Urban Human Activity Analysis
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3583780.3614867
ER  -


TY  - GEN
AU  - Ye, J.
AU  - Liang, J.
TI  - Spatial-Temporal Alignment Network for Action Recognition
AB  - This paper studies introducing viewpoint invariant feature representations in existing action recognition architecture. Despite significant progress in action recognition, efficiently handling geometric variations in large-scale datasets remains challenging. To tackle this problem, we propose a novel Spatial-Temporal Alignment Network (STAN), which explicitly learns geometric invariant representations for action recognition. Notably, the STAN model is light-weighted and generic, which could be plugged into existing action recognition models (e.g., MViTv2) with a low extra computational cost. We test our STAN model on widely-used datasets like UCF101 and HMDB51. The experimental results show that the STAN model can consistently improve the state-of-the-art models in action recognition tasks in trained-from-scratch settings.
PB  - arXiv
PY  - 2023
ST  - Spatial-Temporal Alignment Network for Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1117/12.2644209
ER  -


TY  - GEN
AU  - Lu, C.
AU  - Mak, M.-W.
AU  - Li, R.
AU  - Chi, Z.
AU  - Fu, H.
TI  - Progression-Guided Temporal Action Detection in Videos
AB  - We present a novel framework, Action Progression Network (APN), for temporal action detection (TAD) in videos. The framework locates actions in videos by detecting the action evolution process. To encode the action evolution, we quantify a complete action process into 101 ordered stages (0%, 1%, ..., 100%), referred to as action progressions. We then train a neural network to recognize the action progressions. The framework detects action boundaries by detecting complete action processes in the videos, e.g., a video segment with detected action progressions closely follow the sequence 0%, 1%, ..., 100%. The framework offers three major advantages: (1) Our neural networks are trained end-to-end, contrasting conventional methods that optimize modules separately; (2) The APN is trained using action frames exclusively, enabling models to be trained on action classification datasets and robust to videos with temporal background styles differing from those in training; (3) Our framework effectively avoids detecting incomplete actions and excels in detecting long-lasting actions due to the fine-grained and explicit encoding of the temporal structure of actions. Leveraging these advantages, the APN achieves competitive performance and significantly surpasses its counterparts in detecting long-lasting actions. With an IoU threshold of 0.5, the APN achieves a mean Average Precision (mAP) of 58.3% on the THUMOS14 dataset and 98.9% mAP on the DFMAD70 dataset.
PB  - arXiv
PY  - 2023
ST  - Progression-Guided Temporal Action Detection in Videos
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/access.2024.3451503
ER  -


TY  - GEN
AU  - Teeti, I.
AU  - Bhargav, R.S.
AU  - Singh, V.
AU  - Banerjee, B.
AU  - Cuzzolin, F.
TI  - Temporal DINO: A Self-supervised Video Strategy to Enhance Action Prediction
AB  - The emerging field of action prediction - the task of forecasting action in a video sequence - plays a vital role in various computer vision applications such as autonomous driving, activity analysis and human-computer interaction. Despite significant advancements, accurately predicting future actions remains a challenging problem due to high dimensionality, complex dynamics and uncertainties inherent in video data. Traditional supervised approaches require large amounts of labelled data, which is expensive and time-consuming to obtain. This paper introduces a novel self-supervised video strategy for enhancing action prediction inspired by DINO (self-distillation with no labels). The approach, named Temporal-DINO, employs two models; a ‘student’ processing past frames; and a ‘teacher’ processing both past and future frames, enabling a broader temporal context. During training, the teacher guides the student to learn future context by only observing past frames. The strategy is evaluated on ROAD dataset for the action prediction downstream task using 3D-ResNet, Transformer, and LSTM architectures. The experimental results showcase significant improvements in prediction performance across these architectures, with our method achieving an average enhancement of 9.9% Precision Points (PP), which highlights its effectiveness in enhancing the backbones’ capabilities of capturing long-term dependencies. Furthermore, our approach demonstrates efficiency in terms of the pretraining dataset size and the number of epochs required. This method overcomes limitations present in other approaches, including the consideration of various backbone architectures, addressing multiple prediction horizons, reducing reliance on hand-crafted augmentations, and streamlining the pretraining process into a single stage. These findings highlight the potential of our approach in diverse video-based tasks such as activity recognition, motion planning, and scene understanding.
PB  - arXiv
PY  - 2023
ST  - Temporal DINO
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccvw60793.2023.00352
ER  -


TY  - GEN
AU  - Liu, Z.
AU  - Gerritsen, J.
AU  - Smidt, H.
AU  - Zoetendal, E.G.
TI  - Impact of Donor Individuality, Temporal Variation, and Culture Medium Type on Microbiota Composition and Metabolic Activity in Human Fecal Batch Culture
AB  - Fecal batch culture (FBC) studies often rely on a single fecal sample collection and the use of one type of medium for cultivation, bringing challenges to the interpretation of results and the comparison between studies. This study investigated the impact of donor individuality, temporal variation and culture medium type on microbiota composition and metabolic activity in an FBC setting with the fiber polydextrose (PDX) as carbon and energy source. FBCs were inoculated with fecal microbiota from three healthy donors sampled at three different days (day 1, 2 and 30), using either basal or rich culture medium with PDX as carbon source. Microbiota composition and metabolic activity were determined after 0, 6, 12, and 24 h of incubation. Microbiota composition variation explained by donor individuality dropped from 51% to 16% during incubation, while that explained by medium and PDX supplementation increased from 0% to 17% and 20%, respectively. Independent of the medium, the genera Erysipelotrichaceae UCG-003, Blautia and Fusicatenibacter were stimulated by PDX supplementation. In basal medium Bacteroides and Anaerostipes grew better, whereas Bifidobacterium, Faecalibacterium and Megasphaera grew better in rich medium. Metabolite variation was explained up to 50% by PDX supplementation during incubations, with butyrate being produced at the highest concentrations among all metabolites. Temporal variation explained less than 3% of the variation in both microbiota and metabolite composition. In conclusion, in this study donor individuality had the most profound impact on microbiota succession while medium and PDX supplementation had larger impacts on metabolic activity in FBCs.
PB  - bioRxiv
PY  - 2023
ST  - Impact of Donor Individuality, Temporal Variation, and Culture Medium Type on Microbiota Composition and Metabolic Activity in Human Fecal Batch Culture
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.08.07.552316
ER  -


TY  - GEN
AU  - Sanghavi, S.
AU  - Kar, K.
TI  - Distinct roles of putative excitatory and inhibitory neurons in the macaque inferior temporal cortex in core object recognition behavior
AB  - A spatially distributed population of neurons in the macaque inferior temporal (IT) cortex supports object recognition behavior, but the cell-type specificity of the population in forming "behaviorally sufficient" object decodes remain unclear. To address this, we recorded neural signals from the macaque IT cortex and compared the object identity information and the alignment of decoding strategies derived from putative inhibitory (Inh) and excitatory (Exc) neurons to the monkeys' behavior. We observed that while Inh neurons represented significant category information, decoding strategies based on Exc neural population activity outperformed those from Inh neurons in overall accuracy and their image-level match to the monkeys' behavioral reports. Interestingly, both Exc and Inh responses explained a fraction of unique variance of the monkeys' behavior, demonstrating a distinct role of the two cell types in generating object identity solutions for a downstream readout. We observed that current artificial neural network (ANN) models of primate ventral stream, designed with AI goals of performance optimization on image categorization, better predict Exc neurons (and its contribution to object recognition behavior) than Inh neurons. Beyond, the refinement of linking propositions between IT and object recognition behavior, our results guide the development of more biologically constrained brain models by offering novel cell-type specific neural benchmarks.
PB  - bioRxiv
PY  - 2023
ST  - Distinct roles of putative excitatory and inhibitory neurons in the macaque inferior temporal cortex in core object recognition behavior
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.08.01.551579
ER  -


TY  - GEN
AU  - Wang, P.
AU  - Zeng, F.
AU  - Qian, Y.
TI  - A Survey on Deep Learning-based Spatio-temporal Action Detection
AB  - Spatio-temporal action detection (STAD) aims to classify the actions present in a video and localize them in space and time. It has become a particularly active area of research in computer vision because of its explosively emerging real-world applications, such as autonomous driving, visual surveillance, entertainment, etc. Many efforts have been devoted in recent years to building a robust and effective framework for STAD. This paper provides a comprehensive review of the state-of-the-art deep learning-based methods for STAD. Firstly, a taxonomy is developed to organize these methods. Next, the linking algorithms, which aim to associate the frame- or clip-level detection results together to form action tubes, are reviewed. Then, the commonly used benchmark datasets and evaluation metrics are introduced, and the performance of state-of-the-art models is compared. At last, this paper is concluded, and a set of potential research directions of STAD are discussed.
PB  - arXiv
PY  - 2023
ST  - A Survey on Deep Learning-based Spatio-temporal Action Detection
Y2  - 2025/05/05/21:54:31
DO  - 10.1142/s0219691323500662
ER  -


TY  - GEN
AU  - Sanghavi, S.
AU  - Kar, K.
TI  - Distinct Roles of Putative Excitatory and Inhibitory Neurons in the Macaque Inferior Temporal Cortex in Core Object Recognition Behavior
AB  - A spatially distributed population of neurons in the macaque inferior temporal (IT) cortex supports object recognition behavior, but the cell-type specificity of the population in forming "behaviorally sufficient" object decodes remain unclear. To address this, we recorded neural signals from the macaque IT cortex and compared the object identity information and the alignment of decoding strategies derived from putative inhibitory (Inh) and excitatory (Exc) neurons to the monkeys’ behavior. We observed that while Inh neurons represented significant category information, decoding strategies based on Exc neural population activity outperformed those from Inh neurons in overall accuracy and their image-level match to the monkeys’ behavioral reports. Interestingly, both Exc and Inh responses explained a fraction of unique variance of the monkeys’ behavior, demonstrating a distinct role of the two cell types in generating object identity solutions for a downstream readout. We observed that current artificial neural network (ANN) models of primate ventral stream, designed with AI goals of performance optimization on image categorization, better predict Exc neurons (and its contribution to object recognition behavior) than Inh neurons. Beyond, the refinement of linking propositions between IT and object recognition behavior, our results guide the development of more biologically constrained brain models by offering novel cell-type specific neural benchmarks.
PB  - SSRN
PY  - 2023
ST  - Distinct Roles of Putative Excitatory and Inhibitory Neurons in the Macaque Inferior Temporal Cortex in Core Object Recognition Behavior
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.08.01.551579
ER  -


TY  - GEN
AU  - Tang, X.
AU  - Fan, J.
AU  - Luo, C.
AU  - Zhang, M.
AU  - Yang, Z.
TI  - DDG-Net: Discriminability-Driven Graph Network for Weakly-supervised Temporal Action Localization
AB  - Weakly-supervised temporal action localization (WTAL) is a practical yet challenging task. Due to large-scale datasets, most existing methods use a network pretrained in other datasets to extract features, which are not suitable enough for WTAL. To address this problem, researchers design several modules for feature enhancement, which improve the performance of the localization module, especially modeling the temporal relationship between snippets. However, all of them omit that ambiguous snippets deliver contradictory information, which would reduce the discriminability of linked snippets. Considering this phenomenon, we propose Discriminability-Driven Graph Network (DDG-Net), which explicitly models ambiguous snippets and discriminative snippets with well-designed connections, preventing the transmission of ambiguous information and enhancing the discriminability of snippet-level representations. Additionally, we propose feature consistency loss to prevent the assimilation of features and drive the graph convolution network to generate more discriminative representations. Extensive experiments on THUMOS14 and ActivityNet1.2 benchmarks demonstrate the effectiveness of DDG-Net, establishing new state-of-the-art results on both datasets. Source code is available at https://github.com/XiaojunTang22/ICCV2023-DDGNet.
PB  - arXiv
PY  - 2023
ST  - DDG-Net
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccv51070.2023.00609
ER  -


TY  - GEN
AU  - Bresson, G.
AU  - Etienne, J.-M.
AU  - Lacroix, G.
TI  - Nighttime Light Pollution and Economic Activities: A Spatio-Temporal Model with Common Factors for US Counties
AB  - Excessive nighttime light is known to have detrimental effects on health and on the environment (fauna and flora). The paper investigates the link between nighttime light pollution and economic growth, air pollution, and urban density. We propose a county model of consumption which accounts for spatial interactions. The model naturally leads to a dynamic general nesting spatial model with unknown common factors. The model is estimated with data for 3071 continental US counties from 2012-2019 using a quasi-maximum likelihood estimator. Short run and long run county marginal effects emphasize the importance of spillover effects on radiance levels. Counties with high levels of radiance are less sensitive to additional growth than low-level counties. This has implications for policies that have been proposed to curtail nighttime light pollution.
PB  - SSRN
PY  - 2023
ST  - Nighttime Light Pollution and Economic Activities
Y2  - 2025/05/05/21:54:31
DO  - 10.54932/wakk9634
ER  -


TY  - GEN
AU  - Hosseinian Ghamsari, F.S.
AU  - Rasekhi, A.
AU  - Faghihzadeh, E.
AU  - Farrahi, H.
TI  - Task-dependent brain activity in generalized anxiety disorder determined by Bayesian spatiotemporal single case model
AB  - Background Data obtained from functional magnetic resonance imaging (fMRI) have a complex structure. Considering the special features of this type of data in analyses is of particular importance. Previous studies on generalized anxiety disorder (GAD) as a prevalent mental disorder using functional neuroimaging have had conflicting results. Results In this study, we apply a Bayesian spatiotemporal model to this type of data that considers both spatial and temporal dependence among regions, which is one of the most essential features to consider. In this single-subject study, we analyzed data from a patient with GAD and a healthy participant. Both participants are 24-year-old women who are assigned an emotion reactivity task (matching neutral and negative facial expressions) inside a scanner. The spatial Bayesian variable selection method is used to detect blood oxygen level-dependent activation in fMRI data. Activation areas in neutral and negative facial expressions are provided for both participants by a posterior probability map. The results of our study show a greater level of activity in the GAD participant in comparison to the healthy participant in responding to the negative matching task. Conclusion The GAD patient showed more neural activity in response to negative facial expressions than the healthy participant in brain regions related to emotional response in the areas of the frontal pole, middle frontal gyrus, insular cortex, and frontal orbital cortex. Moreover, the inferior frontal gyrus in the patient with GAD showed more reaction to negative emotional stimuli.
PB  - Research Square
PY  - 2023
ST  - Task-dependent brain activity in generalized anxiety disorder determined by Bayesian spatiotemporal single case model
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3185580/v1
ER  -


TY  - GEN
AU  - De Bortoli, M.
AU  - Chrpa, L.
AU  - Gebser, M.
AU  - Steinbauer-Wagner, G.
TI  - Enhancing Temporal Planning by Sequential Macro-actions (Extended Version)
AB  - Temporal planning is an extension of classical planning involving concurrent execution of actions and alignment with temporal constraints. Durative actions along with invariants allow for modeling domains in which multiple agents operate in parallel on shared resources. Hence, it is often important to avoid resource conflicts, where temporal constraints establish the consistency of concurrent actions and events. Unfortunately, the performance of temporal planning engines tends to sharply deteriorate when the number of agents and objects in a domain gets large. A possible remedy is to use macro-actions that are well-studied in the context of classical planning. In temporal planning settings, however, introducing macro-actions is significantly more challenging when the concurrent execution of actions and shared use of resources, provided the compliance to temporal constraints, should not be suppressed entirely. Our work contributes a general concept of sequential temporal macro-actions that guarantees the applicability of obtained plans, i.e., the sequence of original actions encapsulated by a macro-action is always executable. We apply our approach to several temporal planners and domains, stemming from the International Planning Competition and RoboCup Logistics League. Our experiments yield improvements in terms of obtained satisficing plans as well as plan quality for the majority of tested planners and domains.
PB  - arXiv
PY  - 2023
ST  - Enhancing Temporal Planning by Sequential Macro-actions (Extended Version)
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-43619-2_40
ER  -


TY  - GEN
AU  - Ushiyama, J.
AU  - Ideriha, T.
TI  - Behavioral fluctuation reflecting theta-rhythmic activation of sequential working memory
AB  - Sequential working memory, the ability to actively maintain sequential information, is essential for our cognition and has been considered to be represented rhythmically within the theta (3–7 Hz) range. In the current study, we predicted that if the sequential information is truly activated rhythmically, there should be periodic fluctuation in our behavior where the easiness/quickness to recall the information rises and falls according to the theta rhythm. We conducted detailed analyses on reaction times (RT) for retrieving sequential and non-sequential information in six experiments (total n = 110). The results revealed that the RT for recalling the sequential information showed fluctuation in the theta range and was significantly stronger than that observed when sequential order was not necessary to remember. Taken together, we revealed that our behavior displayed theta-rhythmic fluctuation when recalling sequential information in a relatively large sample, supporting theta phase-dependent coding of sequential working memory.
PB  - Research Square
PY  - 2023
ST  - Behavioral fluctuation reflecting theta-rhythmic activation of sequential working memory
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3118828/v1
ER  -


TY  - GEN
AU  - Dong, B.
AU  - Mahapatra, S.
AU  - Clark, M.G.
AU  - Crim, G.
AU  - Zhang, C.
TI  - Spatiotemporal precise optical manipulation of intracellular molecular activities
AB  - Controlling chemical processes in live cells is a challenging task. The spatial heterogeneity of biochemical reactions in cells is often overlooked by conventional means of incubating cells with desired chemicals. A comprehensive understanding of spatially diverse biochemical processes requires precise control over molecular activities at the subcellular level. Herein, we develop a closed-loop optoelectronic control system that allows the manipulation of biomolecular activities in live cells at high spatiotemporal precision. Chemical-selective fluorescence signals are utilized to command lasers that trigger specific chemical reactions or control the activation of photoswitchable inhibitors at desired targets. We demonstrate the capability to selectively produce reactive oxygen species (ROS) solely at targeted organelles using blue light. Notably, the induction of ROS in the endoplasmic reticulum leads to a more pronounced disruption of tubulin polymerization and a reduction in green fluorescent protein signals, in comparison to that in lipid droplets. Moreover, when combined with a photoswitchable inhibitor, we selectively inhibit tubulin polymerization within subcellular compartments. This technology enables spatiotemporal control over chemical processes and drug activities, exclusively at desired targets, while minimizing undesired effects on non-targeted locations.
PB  - bioRxiv
PY  - 2023
ST  - Spatiotemporal precise optical manipulation of intracellular molecular activities
Y2  - 2025/05/05/21:54:31
ER  -


TY  - GEN
AU  - Tubino, R.R.
AU  - Cazabet, R.
AU  - Tovanich, N.
AU  - Robardet, C.
TI  - Temporal and Geographical Analysis of Real Economic Activities in the Bitcoin Blockchain
AB  - We study the real economic activity in the Bitcoin blockchain that involves transactions from/to retail users rather than between organizations such as marketplaces, exchanges, or other services. We first introduce a heuristic method to classify Bitcoin players into three main categories: Frequent Receivers (FR), Neighbors of FR, and Others. We show that most real transactions involve Frequent Receivers, representing a small fraction of the total value exchanged according to the blockchain, but a significant fraction of all payments, raising concerns about the centralization of the Bitcoin ecosystem. We also conduct a weekly pattern analysis of activity, providing insights into the geographical location of Bitcoin users and allowing us to quantify the bias of a well-known dataset for actor identification.
PB  - arXiv
PY  - 2023
ST  - Temporal and Geographical Analysis of Real Economic Activities in the Bitcoin Blockchain
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-74643-7_17
ER  -


TY  - GEN
AU  - Kim, S.
AU  - Kim, Y.
AU  - Kim, D.
TI  - Analysis on the Temporal Scaling Behavior of Extreme Rainfall in Korean Peninsula Based on High-Resolution Radar-Based Precipitation Data
AB  - To overcome the limitations of relying solely on ground precipitation gauges, this study utilizes radar precipitation data to investigate the temporal scaling behavior of true extreme rainfall values in South Korea. For this, the raw radar reflectivity data undergoes extensive processing, including bias correction and outlier removal, to obtain accurate high-resolution precipitation fields for the period of 2010 to 2019, with a spatial resolution of 1km and a temporal resolution of 10 minutes. The focus of the investigation is on the relationship between maximum precipitation and its duration, specifically the probable maximum precipitation (PMP). The findings of the study are as follows: (1) PMP values estimated based on radar rainfall data are significantly greater than those estimated based on the ground gauge network across all investigated durations. Most radar-based PMPs greater than gauge-based ones were observed at mountainous area where ground-gauge network is sparse; (2) The relationship between PMP and duration deviates significantly from a power-law distribution. This deviation is primarily caused by unusual short-duration extreme rainfall events, which may not be adequately captured by ground gauge networks but can be better observed using weather radar. For this, the degree of deviation from the power-law distribution is greater for radar precipitation compared to ground-gauge observations; (3) The power-law relationship becomes stronger for lower-quantile rainfall values (e.g., 99% quantile). This is because these lower quantile rainfall values are influenced by various rainfall generation mechanisms with distinct characteristic timescales; (4) For durations ranging from 10 minutes to 6 hours, the East Asian Monsoon season leads to greater extreme rainfall compared to typhoons. However, for longer durations, typhoons result in greater rainfall. This suggests that the East Asian Monsoon season and typhoons are associated with floods in urban and riverine environments, respectively; (5) The inclusion of Jeju Island in the analysis reveals that PMPs for most durations are observed on Jeju Island, primarily caused by typhoon events. Considering that climate change is expected to induce a northward shift in typhoon paths, appropriate flood defense measures should be implemented, especially in the southern part of the Korean Peninsula.
PB  - SSRN
PY  - 2023
ST  - Analysis on the Temporal Scaling Behavior of Extreme Rainfall in Korean Peninsula Based on High-Resolution Radar-Based Precipitation Data
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4512972
ER  -


TY  - GEN
AU  - Qin, Y.
AU  - Chen, L.
AU  - Ben, X.
AU  - Yang, M.
TI  - You Watch Once More: A More Effective CNN Architecture for Video Spatio-Temporal Action Localization
AB  - The task of spatio-temporal action localization (STAL) needs to detect the action and position of individuals in the scene. Many works focus on how to improve the accuracy, but they usually ignore inference speed and practical applications. To address the above problems, we propose a new end-to-end spatio-temporal action localization network called You Watch Once More (YWOM). In this work, there are three measures proposed to improve the accuracy of positioning and recognition while guaranteeing the inference speed. First, a new feature fusion mechanism based on frequency channel attention (FCA) is proposed, which can effectively fuse the features extracted by different backbones. In addition, a new loss function is proposed to speed up the regression and convergence of the bounding box. Specifically, the SIOU regression loss function instead of the smooth L1 loss function is applied to help the model converge stably. Moreover, a lateral connection mechanism is designed to apply more backbones to our network structure. The experimental results demonstrate that YWOM can achieve online inference speed and has good performance of spatio-temporal action localization tasks. YWOM has superiority over other related works including YOWO on the UCF101-24 dataset. The frame-mAP and the video-mAP (0.2) are improved by 4.23% and 1.63% respectively.
PB  - Research Square
PY  - 2023
ST  - You Watch Once More
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3163610/v1
ER  -


TY  - GEN
AU  - Yang, S.
AU  - Liu, J.
AU  - Lu, S.
AU  - Hwa, E.M.
AU  - Kot, A.C.
TI  - One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching
AB  - One-shot skeleton action recognition, which aims to learn a skeleton action recognition model with a single training sample, has attracted increasing interest due to the challenge of collecting and annotating large-scale skeleton action data. However, most existing studies match skeleton sequences by comparing their feature vectors directly which neglects spatial structures and temporal orders of skeleton data. This paper presents a novel one-shot skeleton action recognition technique that handles skeleton action recognition via multi-scale spatialtemporal feature matching. We represent skeleton data at multiple spatial and temporal scales and achieve optimal feature matching from two perspectives. The first is multi-scale matching which captures the scale-wise semantic relevance of skeleton data at multiple spatial and temporal scales simultaneously. The second is cross-scale matching which handles different motion magnitudes and speeds by capturing sample-wise relevance across multiple scales. Extensive experiments over three large-scale datasets (NTU RGB+D, NTU RGB+D 120, and PKU-MMD) show that our method achieves superior one-shot skeleton action recognition, and outperforms SOTA consistently by large margins.
PB  - arXiv
PY  - 2023
ST  - One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tpami.2024.3363831
ER  -


TY  - GEN
AU  - Wen, Y.
AU  - Tang, Z.
AU  - Pang, Y.
AU  - Ding, B.
AU  - Liu, M.
TI  - Interactive Spatiotemporal Token Attention Network for Skeleton-based General Interactive Action Recognition
AB  - Recognizing interactive action plays an important role in human-robot interaction and collaboration. Previous methods use late fusion and co-attention mechanism to capture interactive relations, which have limited learning capability or inefficiency to adapt to more interacting entities. With assumption that priors of each entity are already known, they also lack evaluations on a more general setting addressing the diversity of subjects. To address these problems, we propose an Interactive Spatiotemporal Token Attention Network (ISTA-Net), which simultaneously model spatial, temporal, and interactive relations. Specifically, our network contains a tokenizer to partition Interactive Spatiotemporal Tokens (ISTs), which is a unified way to represent motions of multiple diverse entities. By extending the entity dimension, ISTs provide better interactive representations. To jointly learn along three dimensions in ISTs, multi-head self-attention blocks integrated with 3D convolutions are designed to capture inter-token correlations. When modeling correlations, a strict entity ordering is usually irrelevant for recognizing interactive actions. To this end, Entity Rearrangement is proposed to eliminate the orderliness in ISTs for interchangeable entities. Extensive experiments on four datasets verify the effectiveness of ISTA-Net by outperforming state-of-the-art methods. Our code is publicly available at https://github.com/Necolizer/ISTA-Net.
PB  - arXiv
PY  - 2023
ST  - Interactive Spatiotemporal Token Attention Network for Skeleton-based General Interactive Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iros55552.2023.10342472
ER  -


TY  - GEN
AU  - Wasim, S.T.
AU  - Khattak, M.U.
AU  - Naseer, M.
AU  - Shah, M.
AU  - Khan, F.S.
TI  - Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition
AB  - Recent video recognition models utilize Transformer models for long-range spatio-temporal context modeling. Video transformer designs are based on self-attention that can model global context at a high computational cost. In comparison, convolutional designs for videos offer an efficient alternative but lack long-range dependency modeling. Towards achieving the best of both designs, this work proposes Video-FocalNet, an effective and efficient architecture for video recognition that models both local and global contexts. Video-FocalNet is based on a spatiotemporal focal modulation architecture that reverses the interaction and aggregation steps of self-attention for better efficiency. Further, the aggregation step and the interaction step are both implemented using efficient convolution and element-wise multiplication operations that are computationally less expensive than their self-attention counterparts on video representations. We extensively explore the design space of focal modulation-based spatiotemporal context modeling and demonstrate our parallel spatial and temporal encoding design to be the optimal choice. Video-FocalNets perform favorably well against the state-of-the-art transformer-based models for video recognition on five large-scale datasets (Kinetics-400, Kinetics-600, SS-v2, Diving-48, and ActivityNet-1.3) at a lower computational cost. Our code/models are released at https://github.com/TalalWasim/Video-FocalNets.
PB  - arXiv
PY  - 2023
ST  - Video-FocalNets
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccv51070.2023.01267
ER  -


TY  - GEN
AU  - Gupta, V.
AU  - Bedathur, S.
TI  - Tapestry of Time and Actions: Modeling Human Activity Sequences using Temporal Point Process Flows
AB  - Human beings always engage in a vast range of activities and tasks that demonstrate their ability to adapt to different scenarios. These activities can range from the simplest daily routines, like walking and sitting, to multi-level complex endeavors such as cooking a four-course meal. Any human activity can be represented as a temporal sequence of actions performed to achieve a certain goal. Unlike the time series datasets extracted from electronics or machines, these action sequences are highly disparate in their nature - the time to finish a sequence of actions can vary between different persons. Therefore, understanding the dynamics of these sequences is essential for many downstream tasks such as activity length prediction, goal prediction, next-action recommendation, etc. Existing neural network-based approaches that learn a continuous-time activity sequence (or CTAS) are limited to the presence of only visual data or are designed specifically for a particular task, i.e., limited to next action or goal prediction. In this paper, we present ProActive, a neural marked temporal point process (MTPP) framework for modeling the continuous-time distribution of actions in an activity sequence while simultaneously addressing three high-impact problems - next action prediction, sequence-goal prediction, and end-to-end sequence generation. Specifically, we utilize a self-attention module with temporal normalizing flows to model the influence and the inter-arrival times between actions in a sequence. Moreover, for time-sensitive prediction, we perform an early detection of sequence goal via a constrained margin-based optimization procedure. This in-turn allows ProActive to predict the sequence goal using a limited number of actions. In addition, we propose a novel addition over the ProActive model that can handle variations in the order of actions, i.e., different methods of achieving a given goal. We demonstrate that this variant can learn the order in which the person or actor prefers to do their actions. Extensive experiments on sequences derived from three activity recognition datasets show the significant accuracy boost of ProActive over the state-of-the-art in terms of action and goal prediction, and the first-ever application of end-to-end action sequence generation.
PB  - arXiv
PY  - 2023
ST  - Tapestry of Time and Actions
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3650045
ER  -


TY  - GEN
AU  - Zhang, X.
AU  - Cheng, Y.
AU  - Bi, X.
AU  - Cao, R.
AU  - Zhang, L.
TI  - TKMBR: Temporal Knowledge Graph-based Multi-Behavior Recommendation for E-commerce
AB  - Striving to enhance predictive performance by leveraging auxiliary behaviors, multi-behavior recommendation models have emerged in the realm of e-commerce. These models aim to address the diversity and effectiveness of interactive behaviors. While some methods have shown promising effects, they still exhibit certain limitations, such as overlooking dynamic nature of user interactions. In this paper, we present TKMBR, a multi-behavior recommendation framework based on a temporal knowledge graph in e-commerce. TKMBR incorporates a temporal knowledge graph to capture the temporal dynamics of user behaviors, which allows for the identification of underlying temporal patterns and the capturing of evolving user preferences over time. To augment the understanding of user preferences, heterogeneous signals are integrated and an item-side information knowledge graph is constructed based on various user-item interactions. Moreover, contrastive learning tasks are employed to alleviate the issue of data sparsity. Finally, we evaluate the performance of our approach on two representative recommendation datasets using standard metrics with HR and NDCG. Experimental results demonstrate the effectiveness of TKMBR in improving recommendation quality.
PB  - Research Square
PY  - 2023
ST  - TKMBR
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3144279/v1
ER  -


TY  - GEN
AU  - Chappa, N.V.R.
AU  - Nguyen, P.
AU  - Nelson, A.H.
AU  - Dobbs, P.D.
AU  - Luu, K.
TI  - Sogar: Self-Supervised Spatiotemporal Attention-Based Social Group Activity Recognition
AB  - This paper introduces a novel approach to Social Group Activity Recognition (SoGAR) using Self-supervised Transformers network that can effectively utilize unlabeled video data. To extract spatio-temporal information, we created local and global views with varying frame rates. Our self-supervised objective ensures that features extracted from contrasting views of the same video were consistent across spatio-temporal domains. Our proposed approach is efficient in using transformer-based encoders to alleviate the weakly supervised setting of group activity recognition. By leveraging the benefits of transformer models, our approach can model long-term relationships along spatio-temporal dimensions. Our proposed SoGAR method achieved state-of-the-art results on three group activity recognition benchmarks, namely JRDB-PAR, NBA, and Volleyball datasets, surpassing the current numbers in terms of F1-score, MCA, and MPCA metrics.
PB  - SSRN
PY  - 2023
ST  - Sogar
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4504147
ER  -


TY  - GEN
AU  - Duma, G.M.
AU  - Pellegrino, G.
AU  - Rabuffo, G.
AU  - Bonanni, P.
AU  - Sorrentino, P.
TI  - Altered spread of waves of activities at large scale is influenced by cortical thickness organization in temporal lobe epilepsy: a MRI-hdEEG study
AB  - Temporal lobe epilepsy (TLE) is a brain network disorder characterized by alterations at both the structural and the functional level. It remains unclear how structure and function are related and whether this has any clinical relevance. In the present work, we adopted a novel methodological approach investigating how network structural features influence the large-scale dynamics. The functional network was defined by the spatio-temporal spreading of aperiodic bursts of activations (neuronal avalanches), as observed utilizing high-density electroencephalography (hdEEG) in TLE patients. The structural network was modeled as the region-based thickness covariance. Loosely speaking, we quantified the similarity of the cortical thickness of any two brain regions, both across groups, and at the individual level, the latter utilizing a novel approach to define the personalized covariance network (pCN). In order to compare the structural and functional networks (at the nodal level), we studied the correlation between the probability that a wave of activity would propagate from a source to a target region, and the similarity of the source region thickness as compared to other target brain regions. Building on the recent evidence that large-waves of activities pathologically spread through the epileptogenic network in TLE, also during resting state, we hypothesize that the structural cortical organization might influence such altered spatio-temporal dynamics. We observed a stable cluster of structure-function correlation in the bilateral limbic areas across subjects, highlighting group specific features for left, right and bilateral TLE. The involvement of contralateral areas was observed in unilateral TLE. We showed that in temporal lobe epilepsy alterations of structural and functional networks pair in the regions where seizures propagate and are linked to disease severity. In this study we leveraged on a well-defined model of neurological disease and pushed forward personalization approaches potentially useful in clinical practice. Finally, the methods developed here could be exploited to investigate the relationship between structure-function networks at subject level in other neurological conditions.
PB  - bioRxiv
PY  - 2023
ST  - Altered spread of waves of activities at large scale is influenced by cortical thickness organization in temporal lobe epilepsy
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.07.05.547809
ER  -


TY  - GEN
AU  - Yang, N.
AU  - Zhang, W.
AU  - Qu, S.
AU  - Yu, R.
AU  - Zhao, Y.
TI  - Influence of mining activities on hydrological processes in Pingshuo mining district, Loess Plateau: Insights from spatio-temporal variations of δD and δ18O
AB  - Under the influence of mining activities, investigating hydrological processes is an important cornerstone of water resources and eco-environment protection. In this study, the stables isotopic compositions (δD and δ18O) of surface water and groundwater in five periods were analyzed to identify the formation and evolution of surface water and groundwater in mining district. The δD and δ18O indicate that the water bodies in the study area is mainly recharged by local precipitation and undergo evaporation. There is a close hydraulic connection between surface water, mine water and groundwater, the deep confined water is affected by the surface water and phreatic water due to the developing water-conducting fractures. The recharge of deep groundwater to the overlying aquifer may increase resulting in gradually obvious “oxygen drift” with the continuous development of fractures, which will decrease the degree of “oxygen drift” as the fractures reclogged. The δ18O of phreatic water in each period is depleted in southeast and enriched in northwest of the study area. The δ18O of confined water has an obvious spatial variation in the wet season due to the combined effects of the opencast working and underground mining, and a uniform spatial variation in the dry season. The difference in spatial variation of δ18O between phreatic and confined water may attribute to the regeneration rate, and the stable isotopic compositions in phreatic water is easier to recover than that of confined water after mining disturbance. This finding provides important information about hydrological characteristics of Pingshuo mining district.
PB  - Research Square
PY  - 2023
ST  - Influence of mining activities on hydrological processes in Pingshuo mining district, Loess Plateau
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-2891980/v1
ER  -


TY  - GEN
AU  - Wu, Z.
AU  - Ma, N.
AU  - Wang, C.
AU  - Xu, G.
AU  - Li, M.
TI  - Spatial-Temporal Hypergraph Based on Dual-Stage Attention Network for Multi-View Data Lightweight Action Recognition
AB  - For the problems of irrelevant frames and high model complexity in action recognition, this paper proposes a Spatial-Temporal Hypergraph based on Dual-Stage Attention Network (STHG-DAN) for multi-view data lightweight action recognition. It includes two stages: Temporal Attention Mechanism based on Trainable Threshold (TAM-TT) and Hypergraph Convolution based on Dynamic Spatial-Temporal Attention Mechanism (HG-DSTAM). In the first stage, TAM-TT uses a learning threshold to extract key frames from multi-view videos; In the second stage, HG-DSTAM divides the human joints into three parts: trunk, hand and leg to build spatial hypergraphs, extracts the higher-order features of the multi-view spatial hypergraphs of human joints, inputs them into the dynamic spatial-temporal attention mechanism, and learns the intra frame correlation of multi-view data between the joint features of body parts, which can obtain the significant areas of action; We use multi-scale convolution operation and depth separable network, which can realize efficient action recognition with a few trainable parameters. We experiment on the NTU-RGB+D dataset and the imitating traffic police gesture dataset, the performance and accuracy of the model is better than the existing algorithms, effectively improving the machine and human body language interaction cognitive ability.
PB  - SSRN
PY  - 2023
ST  - Spatial-Temporal Hypergraph Based on Dual-Stage Attention Network for Multi-View Data Lightweight Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4498134
ER  -


TY  - GEN
AU  - Morales-Álvarez, A.
AU  - Valdés-Cuervo, A.A.
AU  - Parra-Pérez, L.G.
AU  - Grijalva-Quiñonez, C.S.
TI  - Authoritative Parenting and Adolescents’ Digital Citizenship Behaviors: Sequential Mediation of Parental Fairness and Self-Disclosure
AB  - Background. Parenting-related factors are associated with adolescents’ online behavior. However, the relationship between the adopted parenting style and digital citizenship behaviors, as well as the variables underlying this relationship, needs to be clarified. Hence, this study examined the relationship between authoritative parenting and digital citizenship behaviors and explored the mediating roles of perceived parental fairness and self-disclosure in adolescents. Methods. A total of 709 Mexican (52% female, M = 14.6, SD = 1.90) middle and high school students in northwest and southwest Mexico were recruited for this study. Latent structural equation modeling was performed to assess the direct and indirect relationships between variables controlling for gender. Results. The results showed a positive association between authoritative parenting, parental fairness, self-disclosure, and digital citizenship behavior. In addition, parental fairness, self-disclosure, and digital citizenship behaviors. In addition, parental fairness and self-disclosure partially mediated the association between authoritative parenting and digital citizenship. Conclusions. These findings suggest that authoritative parenting and adolescents’ perceptions of parental fairness play an important role in adolescents' self-disclosure and development of digital citizenship behaviors.
PB  - Research Square
PY  - 2023
ST  - Authoritative Parenting and Adolescents’ Digital Citizenship Behaviors
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3093585/v1
ER  -


TY  - GEN
AU  - Mojtahed, H.
AU  - Rao, R.
AU  - Sarkar, M.
TI  - Instantaneous Interbeat Interval Temporal Modeling Based on Physical Activity
AB  - Background: The heartbeat is a crucial vital sign that provides an effective tool for diagnosing various health issues. The ANS regulates the heartbeat. Physical activity (PA) causes a response in this system and subsequently affects the heart rhythm. A continuous change in heart rate can be an indication of a change in PA. This study explores the effect of various PAs on InterBeat Interval (IBI) prediction and identifies the most accurate models specific to activities.Method: Several machine learning and deep learning methods are deployed to model piecewise stationary sections of IBI of subjects engaged in various PAs, i.e., running, walking, and sitting. The IBI for each of these PAs are grouped into two categories (i) training and (ii) testing sets. The error change across these models are then compared.Results: The AR model predicted the Run and Walk series with the least error, whereas the GLM IG model performed best when predicting sitting. The CNN model demonstrated the lowest variance when trained and evaluated across various activities. Moreover, the KNN and GB models, due to higher error when tested across activities, are better indicators of PA changes. When examining the results relative to stationary/non-stationary groups, models trained on non-stationary IBI series generated a greater increase in error when tested with irrelevant activity. In contrast, models trained on Walk predicting Run demonstrated a reduction in error for stationary IBI series.Conclusions: The findings suggest that the prediction error can be minimized in most train/test scenarios by training the models on data relevant to PA.
PB  - SSRN
PY  - 2023
ST  - Instantaneous Interbeat Interval Temporal Modeling Based on Physical Activity
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4462207
ER  -


TY  - GEN
AU  - Wang, Y.
AU  - Wang, Y.
AU  - Li, W.
AU  - Li, M.
AU  - Wang, Y.
TI  - Temporal expectation makes our action more cautious: Evidence from unconscious processing
AB  - Unconscious motor inhibition is thought to be automatically generated when individuals encounter potentially disturbing information, so it can make the individual's actions cautious and as minimally disturbed as possible. Temporal expectation is a top-down active preparation for future events that can enhance relevant cognitive processing in the expected temporal frame. To gain further insight into how temporal expectation amplifies unconscious motor inhibition, two experiments were conducted in micro- and macro-expectation contexts, respectively, and found stronger motor inhibition when the occurrence time of a subliminal stimulus could be anticipated. The results confirm our proposed expectation-amplifier hypothesis. Specifically, greater motor readiness is elicited when individuals are highly prepared for upcoming stimuli. If the stimuli are identified as disturbing information (e.g., subliminally interrupted rather than constant stimuli), the cognitive monitoring system will automatically generate reinforcing inhibition to suppress motor readiness, temporal expectation thus makes the individual’s actions more cautious.
PB  - Research Square
PY  - 2023
ST  - Temporal expectation makes our action more cautious
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3057713/v1
ER  -


TY  - GEN
AU  - Zheng, R.
AU  - Wang, X.
AU  - Sun, Y.
AU  - Daume III, H.
AU  - Huang, F.
TI  - TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning
AB  - Despite recent progress in reinforcement learning (RL) from raw pixel data, sample inefficiency continues to present a substantial obstacle. Prior works have attempted to address this challenge by creating self-supervised auxiliary tasks, aiming to enrich the agent's learned representations with control-relevant information for future state prediction. However, these objectives are often insufficient to learn representations that can represent the optimal policy or value function, and they often consider tasks with small, abstract discrete action spaces and thus overlook the importance of action representation learning in continuous control. In this paper, we introduce TACO: Temporal Action-driven COntrastive Learning, a simple yet powerful temporal contrastive learning approach that facilitates the concurrent acquisition of latent state and action representations for agents. TACO simultaneously learns a state and an action representation by optimizing the mutual information between representations of current states paired with action sequences and representations of the corresponding future states. Theoretically, TACO can be shown to learn state and action representations that encompass sufficient information for control, thereby improving sample efficiency. For online RL, TACO achieves 40% performance boost after one million environment interaction steps on average across nine challenging visual continuous control tasks from Deepmind Control Suite. In addition, we show that TACO can also serve as a plug-and-play module adding to existing offline visual RL methods to establish the new state-of-the-art performance for offline visual RL across offline datasets with varying quality.
PB  - arXiv
PY  - 2023
ST  - TACO
Y2  - 2025/05/05/21:54:31
DO  - 10.1371/journal.pone.0265456
ER  -


TY  - GEN
AU  - Cao, C.
AU  - Yang, C.
AU  - Li, S.
TI  - Discovering Intrinsic Spatial-Temporal Logic Rules to Explain Human Actions
AB  - We propose a logic-informed knowledge-driven modeling framework for human movements by analyzing their trajectories. Our approach is inspired by the fact that human actions are usually driven by their intentions or desires, and are influenced by environmental factors such as the spatial relationships with surrounding objects. In this paper, we introduce a set of spatial-temporal logic rules as knowledge to explain human actions. These rules will be automatically discovered from observational data. To learn the model parameters and the rule content, we design an expectation-maximization (EM) algorithm, which treats the rule content as latent variables. The EM algorithm alternates between the E-step and M-step: in the E-step, the posterior distribution over the latent rule content is evaluated; in the M-step, the rule generator and model parameters are jointly optimized by maximizing the current expected log-likelihood. Our model may have a wide range of applications in areas such as sports analytics, robotics, and autonomous cars, where understanding human movements are essential. We demonstrate the model’s superior interpretability and prediction performance on pedestrian and NBA basketball player datasets, both achieving promising results.
PB  - arXiv
PY  - 2023
ST  - Discovering Intrinsic Spatial-Temporal Logic Rules to Explain Human Actions
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3132847.3133014
ER  -


TY  - GEN
AU  - Yang, J.
AU  - Wang, K.
AU  - Zhao, L.
AU  - Dai, K.
AU  - Li, R.
TI  - Yowov2: A Real-Time Multi-Level Detection Framework for Spatio-Temporal Action Detection
AB  - Designing a real-time framework for the spatio-temporal action detection task is still a challenge. In this paper, we propose a novel real-time action detection framework, YOWOv2. In this new framework, YOWOv2 takes advantage of both the 3D backbone and 2D backbone for accurate action detection. A multi-level detection pipeline is designed to detect action instances of different scales. To achieve this goal, we carefully build a simple and efficient 2D backbone with a feature pyramid network to extract different levels of classification features and regression features. For the 3D backbone, we adopt the existing efficient 3D CNN to save development time. By combining 3D backbones and 2D backbones of different sizes, we design a YOWOv2 family including YOWOv2-Tiny, YOWOv2-Medium, and YOWOv2-Large. We also introduce the popular dynamic label assignment strategy and anchor-free mechanism to make the YOWOv2 consistent with the advanced model architecture design. With our improvement, YOWOv2 is significantly superior to YOWO, and can still keep real-time detection. Without any bells and whistles, YOWOv2 achieves 87.0% frame mAP and 52.8% video mAP with over 20 FPS on the UCF101-24. On the AVA, YOWOv2 achieves 21.7% frame mAP with over 20 FPS. Our code is available on https://github.com/yjh0410/YOWOv2.
PB  - SSRN
PY  - 2023
ST  - Yowov2
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4485402
ER  -


TY  - GEN
AU  - Shi, J.
AU  - Li, D.
AU  - Yang, J.
AU  - Shen, C.
TI  - Title: Spatio-temporal Dynamics of Gross Ecosystem Product (GEP) and the Impacts of Human Activities Disturbance in an Ecologically Fragile Region from 1990 to 2020
AB  - Context: As an index of ecological well-being, Gross Ecosystem Product (GEP) estimates the value of final ecosystem services or the direct benefits people derive from the ecosystem. Objectives: In this research, we accounted for GEP and quantified the impacts of human activities on GEP in Shanxi, an ecologically fragile area in China, from 1990 to 2020. Methods: We associated all kinds of non-spatial data with spatial data and employed the local indicators of spatial association, the Sankey diagram, and the empirical orthogonal decomposition (EOF) to explore the spatio-temporal dynamic properties of GEP. The transfer matrix and gravity model were used to measure the response of the GEP to disturbance from human activities due to urbanization. Results: The results show that: (1) excluding 2010, the GEP possesses a growth trend and increased from 117.65 billion Chinese yuan (CNY) to 4594.89 billion CNY; (2) contrary to the steady growth of the GEP, the regions with high GEP generally tended to decrease, and the Green Gold Index (GGI) tended to increase and then decrease; (3) the spatial distribution of GEP in Shanxi is restricted, and there is a tendency for this restriction to decrease over time; (4) the decade from 2005 to 2015 has the fewest changes in the GEP of Shanxi; (5) the GEP field has a globally consistent type and a high-value-low-value inverse phase-type in the variation of the spatial distribution, and the first type accounts for 61.74% of the total variance in the EOF; (6) the variation of GEP in different cities may differ significantly over time, and the cities with more disturbance from human activities have lower GEP or higher variance in GEP; (7) the disturbance of residential land has a more significant impact on the GEP than the disturbance of industrial and mining land in Shanxi. Conclusion: Our research could provide important insights into ecological assessment in an ecologically fragile region, thus providing a policy basis for the conservation and better use of environmental resources.
PB  - Research Square
PY  - 2023
ST  - Title
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3052088/v1
ER  -


TY  - GEN
AU  - Acar, D.
AU  - Tunçcan, E.
AU  - Selek, S.N.
AU  - Özdemir, S.
AU  - Süyen, G.G.
TI  - Effect of Dual Leucine Zipper Kinase Inhibitor GNE-3511 on Epileptogenesis and Cognitive and Behavioral Changes in a Temporal Lobe Epilepsy Model in Mice
AB  - Inflammation and neuronal loss are key factors in the pathophysiology of epilepsy. It is known that activation of Dual Leucine Zipper Kinase (DLK) in chronic neurodegenerative diseases causes neuron death and axon degeneration through apoptotic pathways. Genetic deletion of DLK (dual leucine zipper kinase, MAP3K12) or pharmacological inhibition by GNE-3511 or GNE-8505 was found to have benefits in mouse models of Alzheimer's and ALS; however, there is no previous study that uses DLK inhibitors in epilepsy models. Therefore, the aim of our study is to suppress the DLK/JNK pathway with the DLK inhibitor GNE-3511 in the temporal lobe epilepsy model induced by pilocarpine and to determine the effect of this inhibition on cognitive and behavioural defects, as well as epileptogenesis. Open field, elevated plus maze, and Morris water maze tests were used to assess locomotor activity, anxiety and learning and memory, respectively. Following decapitation the hippocampi were removed and, histopathological and biochemical analyses were performed. Both treatment groups had less anxiety and locomotor activity as well as decreased impairment in memory and learning performance compared to SE group (p<0.001). Both doses of GNE-3511 prevented the spontaneous recurrent seizures in a dose dependent manner with respect to the SE group (p<0.01 and p<0.001, respectively). Histological examination of the hippocampus revealed a neuroprotective effect of both doses of treatment in the CA1 area and dentate gyrus (p<0.0001). Thus, DLK inhibitor GNE-3511 was found to be effective in preventing epileptogenesis, neuronal loss and cognitive and behavioural deficits due to pilocarpine induced SE.
PB  - bioRxiv
PY  - 2023
ST  - Effect of Dual Leucine Zipper Kinase Inhibitor GNE-3511 on Epileptogenesis and Cognitive and Behavioral Changes in a Temporal Lobe Epilepsy Model in Mice
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.06.11.544443
ER  -


TY  - GEN
AU  - Glaeser-Khan, S.
AU  - Savalia, N.K.
AU  - Cressy, J.
AU  - Kwan, A.C.
AU  - Kaye, A.P.
TI  - Spatiotemporal organization of prefrontal norepinephrine influences neuronal activity
AB  - Norepinephrine (NE), a neuromodulator released by locus coeruleus neurons throughout cortex, influences arousal and learning through extra-synaptic vesicle exocytosis. While NE within cortical regions has been viewed as a homogenous field, recent studies have demonstrated heterogeneous axonal dynamics and advances in GPCR-based fluorescent sensors permit direct observation of the local dynamics of NE at cellular scale. To investigate how the spatiotemporal dynamics of NE release in the PFC affect neuronal firing, we employed in-vivo two-photon imaging of layer 2/3 of PFC in order to observe fine-scale neuronal calcium and NE dynamics concurrently. We found that local and global NE fields can decouple from one another, providing a substrate for local NE spatiotemporal activity patterns. Optic flow analysis revealed putative release and reuptake events which can occur at the same location, albeit at different times, indicating the potential to create a heterogeneous NE field. Utilizing generalized linear models, we demonstrated that cellular Ca2+ fluctuations are influenced by both the local and global NE field. However, during periods of local/global NE field decoupling, the local field drives cell firing dynamics rather than the global field. These findings underscore the significance of localized, phasic NE fluctuations for structuring cell firing, which may provide local neuromodulatory control of cortical activity.
PB  - bioRxiv
PY  - 2023
ST  - Spatiotemporal organization of prefrontal norepinephrine influences neuronal activity
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.06.09.544191
ER  -


TY  - GEN
AU  - Qashou, A.
AU  - Yousef, S.
TI  - Microgrid TestBed for Temporal Forecasting by Converting Stochastic Behaviour into a Stable Pattern
AB  - The malfunction variables of power stations are related to the areas of weather, physical structure, control, and load behavior. To predict temporal power failure is difficult due to their unpredictable characteristics. As high accuracy is normally required, the estimation of failures of short-term temporal prediction is highly difficult. This study presents a method for converting stochastic behavior into a stable pattern, which can subsequently be used in a short-term estimator. For this conversion, K-means clustering is employed, followed by Long-Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) algorithms are used to perform the Short-term estimation. The environment, the operation, and the generated signal factors are all simulated using mathematical models. Weather parameters and load samples have been collected as part of a dataset. Monte-Carlo simulation using MATLAB programming has been used to conduct experimental estimation of failures. The estimated failures of the experiment are then compared with the actual system temporal failures and found to be in good match. Therefore, for any future power grid there is a ready testbed to estimate the future failures.
PB  - Research Square
PY  - 2023
ST  - Microgrid TestBed for Temporal Forecasting by Converting Stochastic Behaviour into a Stable Pattern
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-2899186/v1
ER  -


TY  - GEN
AU  - Valencia-Esquivel, J.I.
AU  - Kiere, L.M.
AU  - Osorio-Beristain, M.
TI  - Temporal trade-off between territorial and thermoregulatory behaviors of a generalist lizard in a dry forest
AB  - Avoiding dangerously hot body temperatures is important for survival, but animals may perform reproductive behaviors at the expense of behaviors used to cool down (or vice-versa), resulting in a thermoregulation-reproduction trade-off. Although this trade-off has been demonstrated in semi-aquatic animals, it has not been studied in terrestrial ectotherms. This is an important research gap given the importance of survival-reproduction trade-offs in evolutionary ecology and the pace of habitat warming due to vegetation loss and global climate change. We explored this trade-off in territorial males of the lizard Sceloporus ochoterenae, which mates during the hot-dry season in seasonally dry tropical forest. We first confirmed the existence of a temporal trade-off between performing push-ups (a territorial behavioral display) versus sheltering in the shade (thermoregulatory behavior), then used confirmatory path analysis to explore how it is affected by vegetation cover, microclimate temperature, and the presence of a conspecific intruder. We found that territories with less vegetation cover had higher microclimate temperatures, where focal males spent more time performing push-ups at the expense of sheltering in the shade. Focal males also spent more time performing push-ups the longer an intruder was present, who was also affected by the environmental variables. Territorial males spent more time in sunny spots when performing push-ups despite the potential for overheating, perhaps because the display is more effective when performed in the open. The potential effects of continued habitat warming on this trade-off vary widely, including intensifying it, driving lizards to change their daily activity rhythms, and chronic overheating.
PB  - Research Square
PY  - 2023
ST  - Temporal trade-off between territorial and thermoregulatory behaviors of a generalist lizard in a dry forest
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s00265-024-03478-z
ER  -


TY  - GEN
AU  - Chiba, K.
TI  - Spatiotemporal Variations in Seismic Activity in and Around the Focal Region of the 2021 M7.3 and 2022 M7.4 Fukushima-Oki Earthquakes, Japan
AB  - The spatiotemporal evolution of seismic activity is presented for a broad region surrounding the focal areas of the 2021 M7.3 and 2022 M7.4 Fukushima-Oki earthquakes, which occurred within the subducting slab off the Pacific coast of Fukushima Prefecture, northeastern Japan. This study investigates the spatiotemporal variations in seismic activity during the periods before the 2021 M7.3 earthquake, between the 2021 M7.3 and 2022 M7.4 earthquakes, and after the 2022 M7.4 earthquake using theb-value of the Gutenberg–Richter relation, the aftershock decay rate (p-value), and changes in the seismicity rate (Z-value). The study area is also divided into two depth sections to investigate the depth variations in these seismicity parameters relative to the plate interface. The b-values in the deeper section (intraslab) are generally lower than those in the shallower section (around the plate interface) throughout the entire analysis period, including the hypocentral areas of the M7.3 and M7.4 earthquakes. The aftershock decay rates for the M7.3 and M7.4 earthquakes also show depth-dependent characteristics, with a slower decay rate (p < 1.0) at many grid nodes in the deeper section than in the shallower section. Furthermore, seismic quiescence was noted in the hypocentral area of the M7.3 earthquake about two years before the occurrence of this mainshock. The locations of the M7.3 and M7.4 earthquakes around the down-dip edge of the slip area of the 2011 M9.0 Tohoku earthquake suggests that the variations in seismic activity detected in this study mainly reflect stress increases due to the coseismic slip and postseismic deformation of this great earthquake. The present study suggests that the effect of viscoelastic relaxation is a dominant factor in the deeper section. Furthermore, the variations in seismicity may also reflect heterogeneous structures within the slab.
PB  - SSRN
PY  - 2023
ST  - Spatiotemporal Variations in Seismic Activity in and Around the Focal Region of the 2021 M7.3 and 2022 M7.4 Fukushima-Oki Earthquakes, Japan
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4463908
ER  -


TY  - GEN
AU  - Ogunjo, S.T.
AU  - Rabiu, A.B.
TI  - Spatio-temporal influence of solar activity on global air temperature
AB  - Previous studies on the impact and influence of solar activity on terrestrial weather has yielded contradictory results in literatures. Present study presents, on a global scale, the correlation between surface air temperature and two solar activity indices (Sunspot number, 'Rz', and solar radio flux at 10.7, 'F10.7') at different time scales during solar cycle 23. Global air temperature has higher correlation values of ±0.8 with F10.7 compared to Rz (±0.3). Our results showed hemispheric delineation of the correlation between air temperature and solar activity with negative correlation in the southern hemisphere and positive correlation in the northern hemisphere. At the onset of the solar cycle, this hemispheric delineation pattern was prevalent, however, an inverse hemispheric delineation was observed at the recession of the solar cycle.
PB  - arXiv
PY  - 2023
ST  - Spatio-temporal influence of solar activity on global air temperature
Y2  - 2025/05/05/21:54:31
DO  - 10.31401/sungeo.2022.02.05
ER  -


TY  - GEN
AU  - Ren, H.
AU  - Yang, W.
AU  - Zhang, T.
AU  - Zhang, Y.
TI  - Proposal-based Multiple Instance Learning for Weakly-supervised Temporal Action Localization
AB  - Weakly-supervised temporal action localization aims to localize and recognize actions in untrimmed videos with only video-level category labels during training. Without instance-level annotations, most existing methods follow the Segment-based Multiple Instance Learning (S-MIL) framework, where the predictions of segments are supervised by the labels of videos. However, the objective for acquiring segment-level scores during training is not consistent with the target for acquiring proposal-level scores during testing, leading to suboptimal results. To deal with this problem, we propose a novel Proposal-based Multiple Instance Learning (P-MIL) framework that directly classifies the candidate proposals in both the training and testing stages, which includes three key designs: 1) a surrounding contrastive feature extraction module to suppress the discriminative short proposals by considering the surrounding contrastive information, 2) a proposal completeness evaluation module to inhibit the low-quality proposals with the guidance of the completeness pseudo labels, and 3) an instance-level rank consistency loss to achieve robust detection by leveraging the complementarity of RGB and FLOW modalities. Extensive experimental results on two challenging benchmarks including THUMOS14 and ActivityNet demonstrate the superior performance of our method. Our code is available at github.com/RenHuan1999/CVPR2023 P-MIL.
PB  - arXiv
PY  - 2023
ST  - Proposal-based Multiple Instance Learning for Weakly-supervised Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52729.2023.00237
ER  -


TY  - GEN
AU  - Langle-Chimal, O.D.
AU  - Merrill, S.C.
AU  - Clark, E.M.
AU  - Smith, J.M.
AU  - Cheney, N.
TI  - Temporal Evolution of Risk Behavior in a Disease Spread Simulation
AB  - Human behavior is a dynamic process that evolves with experience. Understanding the evolution of individual’s risk propensity is critical to design public health interventions to propitiate the adoption of better biosecurity protocols and thus, prevent the transmission of an infectious disease. Using an experimental game that simulates the spread of a disease in a network of porcine farms, we measure how learning from experience affects the risk aversion of over 1000 players. We used a fully automated approach to segment the players into four categories based on the temporal trends of their game plays and compare their overall game performance. We found that the risk tolerant group is 50% more likely to incur an infection than the risk averse one. We also find that while all individuals decrease the amount of time it takes to make decisions as they become more experienced at the game, we find a group of players with constant decision strategies who rapidly decrease their time to make a decision and a second context-aware decision group that contemplates longer before decisions while presumably performing a real-time risk assessment. The behavioral strategies employed by players in this simulated setting could be used in the future as an early warning signal to identify undesirable biosecurity-related risk aversion preferences, or changes in behavior, which may allow for targeted interventions to help mitigate them. MSC Codes ACM-class: F.2.2, I.2.7
PB  - arXiv
PY  - 2023
ST  - Temporal Evolution of Risk Behavior in a Disease Spread Simulation
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-67447-1_11
ER  -


TY  - GEN
AU  - Shao, J.
AU  - Wang, X.
AU  - Quan, R.
AU  - Yang, J.
AU  - Yang, Y.
TI  - Action Sensitivity Learning for Temporal Action Localization
AB  - Temporal action localization (TAL), which involves recognizing and locating action instances, is a challenging task in video understanding. Most existing approaches directly predict action classes and regress offsets to boundaries, while overlooking the discrepant importance of each frame. In this paper, we propose an Action Sensitivity Learning framework (ASL) to tackle this task, which aims to assess the value of each frame and then leverage the generated action sensitivity to recalibrate the training procedure. We first introduce a lightweight Action Sensitivity Evaluator to learn the action sensitivity at the class level and instance level, respectively. The outputs of the two branches are combined to reweight the gradient of the two sub-tasks. Moreover, based on the action sensitivity of each frame, we design an Action Sensitive Contrastive Loss to enhance features, where the action-aware frames are sampled as positive pairs to push away the action-irrelevant frames. The extensive studies on various action localization benchmarks (i.e., MultiThumos, Charades, Ego4D-Moment Queries v1.0, Epic-Kitchens 100, Thumos14 and ActivityNet1.3) show that ASL surpasses the state-of-the-art in terms of average-mAP under multiple types of scenarios, e.g., single-labeled, densely-labeled and egocentric.
PB  - arXiv
PY  - 2023
ST  - Action Sensitivity Learning for Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccv51070.2023.01238
ER  -


TY  - GEN
AU  - Brands, A.M.
AU  - Mathar, D.
AU  - Peters, J.
TI  - Signatures of heuristic-based directed exploration in two-step sequential decision task behaviour
AB  - Processes formalized in classic Reinforcement Learning (RL) theory, such as model-based (MB) control and exploration strategies have proven fertile in cognitive and computational neuroscience, as well as computational psychiatry. Dysregulations in MB control and exploration and their neurocomputational underpinnings play a key role across several psychiatric disorders. Yet, computational accounts mostly study these processes in isolation. The current study extended standard hybrid models of a widely-used sequential RL-task (two-step task; TST) employed to measure MB control. We implemented and compared different computational model extensions for this task to quantify potential exploration mechanisms. In two independent data sets spanning two different variants of the task, an extension of a classical hybrid RL model with a heuristic-based exploration mechanism provided the best fit, and revealed a robust positive effect of directed exploration on choice probabilities in stage one of the task. Posterior predictive checks further showed that the extended model reproduced choice patterns present in both data sets. Results are discussed with respect to implications for computational psychiatry and the search for neurocognitive endophenotypes.
PB  - bioRxiv
PY  - 2023
ST  - Signatures of heuristic-based directed exploration in two-step sequential decision task behaviour
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.05.22.541443
ER  -


TY  - GEN
AU  - Zhang, Y.
AU  - Mitelut, C.
AU  - Arpin, D.J.
AU  - Murphy, T.
AU  - Saxena, S.
TI  - Behavioral Classification of Sequential Neural Activity Using Time Varying Recurrent Neural Networks
AB  - Shifts in data distribution across time can strongly affect early classification of time-series data. When decoding behavior from neural activity, early detection of behavior may help in devising corrective neural stimulation before the onset of behavior. Recurrent Neural Networks (RNNs) are common models for sequence data. However, standard RNNs are not able to handle data with temporal distributional shifts to guarantee robust classification across time. To enable the network to utilize all temporal features of the neural input data, and to enhance the memory of an RNN, we propose a novel approach: RNNs with time-varying weights, here termed Time-Varying RNNs (TV-RNNs). These models are able to not only predict the class of the time-sequence correctly but also lead to accurate classification earlier in the sequence than standard RNNs. In this work, we focus on early sequential classification of brain-wide neural activity across time using TV-RNNs applied to a variety of neural data from mice and humans, as subjects perform motor tasks. Finally, we explore the contribution of different brain regions on behavior classification using SHapley Additive exPlanation (SHAP) value, and find that the somatosensory and premotor regions play a large role in behavioral classification.
PB  - bioRxiv
PY  - 2023
ST  - Behavioral Classification of Sequential Neural Activity Using Time Varying Recurrent Neural Networks
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.05.10.540244
ER  -


TY  - GEN
AU  - Veillette, J.P.
AU  - Lopes, P.
AU  - Nusbaum, H.C.
TI  - Temporal Dynamics of Brain Activity Predicting Sense of Agency over Muscle Movements
AB  - Our muscles are the primary means through which we affect the external world, and the sense of agency (SoA) over the action through those muscles is fundamental to our self-awareness. However, SoA research to date has focused almost exclusively on agency over action outcomes rather than over the musculature itself, as it was believed that SoA over the musculature could not be manipulated directly. Drawing on methods from human-computer interaction and adaptive experimentation, we use human-in-the-loop Bayesian optimization to tune the timing of electrical muscle stimulation so as to robustly elicit a sense of agency over electrically-actuated muscle movements in male and female human subjects. We use time-resolved decoding of subjects’ EEG to estimate the time course of neural activity which predicts reported agency on a trial-by-trial basis. Like paradigms which assess SoA over action consequences, we found that the late (post-conscious) neural activity predicts SoA. Unlike typical paradigms, however, we also find patterns of early (sensorimotor) activity with distinct temporal dynamics predicts agency over muscle movements, suggesting that the “neural correlates of agency” may depend on the level of abstraction (i.e., direct sensorimotor feedback vs. downstream consequences) most relevant to a given agency judgement. Moreover, fractal analysis of the EEG suggests that SoA-contingent dynamics of neural activity may modulate the sensitivity of the motor system to external input.
PB  - bioRxiv
PY  - 2023
ST  - Temporal Dynamics of Brain Activity Predicting Sense of Agency over Muscle Movements
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.05.06.539706
ER  -


TY  - GEN
AU  - Wang, X.
AU  - Katsaggelos, A.K.
TI  - Video-Specific Query-Key Attention Modeling for Weakly-Supervised Temporal Action Localization
AB  - Weakly-supervised temporal action localization aims to identify and localize the action instances in the untrimmed videos with only video-level action labels. When humans watch videos, we can adapt our abstract-level knowledge about actions in different video scenarios and detect whether some actions are occurring. In this paper, we mimic how humans do and bring a new perspective for locating and identifying multiple actions in a video. We propose a network named VQK-Net with a video-specific query-key attention modeling that learns a unique query for each action category of each input video. The learned queries not only contain the actions' knowledge features at the abstract level but also have the ability to fit this knowledge into the target video scenario, and they will be used to detect the presence of the corresponding action along the temporal dimension. To better learn these action category queries, we exploit not only the features of the current input video but also the correlation between different videos through a novel video-specific action category query learner worked with a query similarity loss. Finally, we conduct extensive experiments on three commonly used datasets (THUMOS14, ActivityNet1.2, and ActivityNet1.3) and achieve state-of-the-art performance.
PB  - arXiv
PY  - 2023
ST  - Video-Specific Query-Key Attention Modeling for Weakly-Supervised Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr42600.2020.00109
ER  -


TY  - GEN
AU  - Rajendiran, R.
AU  - Roy, D.
AU  - Fernando, B.
TI  - Modelling Spatio-Temporal Interactions For Compositional Action Recognition
AB  - Humans have the natural ability to recognize actions even if the objects involved in the action or the background are changed. Humans can abstract away the action from the appearance of the objects which is referred to as compositionality of actions. We focus on this compositional aspect of action recognition to impart human-like generalization abilities to video action-recognition models. First, we propose an interaction model that captures both fine-grained and long-range interactions between hands and objects. Frame-wise hand-object interactions capture fine-grained movements, while long-range interactions capture broader context and disambiguate actions across time. Second, in order to provide additional contextual cues to differentiate similar actions, we infuse the interaction tokens with global motion information from video tokens. The final global motion refined interaction tokens are used for compositional action recognition. We show the effectiveness of our interaction-centric approach on the compositional Something-Else dataset where we obtain a new state-of-the-art result outperforming recent object-centric methods by a significant margin.
PB  - arXiv
PY  - 2023
ST  - Modelling Spatio-Temporal Interactions For Compositional Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-981-97-8511-7_30
ER  -


TY  - GEN
AU  - Li, Y.
AU  - Ajloon, F.H.
AU  - Wang, X.
AU  - Ma, X.
AU  - Wang, W.
TI  - Temporal Effects of Thinning on Soil Organic Carbon and Carbon Cycling-Related Enzyme Activities in Oak-Pine Mixed Forests
AB  - Thinning, a common practice in forest management, has complex effects on soil organic carbon dynamics. In this study, we examined the effects of pre-commercial thinning on soil organic carbon (SOC) and carbon cycling-related enzyme activities in a thinning chronosequence (4-12-year recovery period) of oak-pine mixed forests in the Qinling Mountains, China. Three thinning treatments, including the control (CK) with no thinning, T2018 (thinning 4 years ago), and T2010 (thinning 12 years ago), were applied to the oak-pine mixed forests. The soil physicochemical properties, SOC, microbial biomass carbon (MBC), easily oxidizable organic carbon (EOC), dissolved organic carbon (DOC), and carbon cycling-related enzyme activities (hydrolase: β-glucosidase [BG], cellobiohydrolase [CBH], and invertase [INV]) were measured. Our results indicated that T2018 significantly decreased SOC by an average of 51.67% relative to that of CK, whereas T2010 induced gradually returned to untreated status. EOC, MBC, BG, and CBH were significantly lower in T2018 and T2010 than in CK, whereas MBC was significantly higher in T2010 than in T2018. There were no significant differences in DOC or INV among treatments. The structural equation model showed that thinning regulated SOC mainly by changing soil water content, MBC, and carbon cycling-related enzyme activities. These findings provide a solid foundation for understanding the response of SOC to forest thinning.
PB  - SSRN
PY  - 2023
ST  - Temporal Effects of Thinning on Soil Organic Carbon and Carbon Cycling-Related Enzyme Activities in Oak-Pine Mixed Forests
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4435387
ER  -


TY  - GEN
AU  - Li, G.
AU  - Cheng, D.
AU  - Ding, X.
AU  - Wang, X.
AU  - Gao, X.
TI  - Boosting Weakly-Supervised Temporal Action Localization with Text Information
AB  - Due to the lack of temporal annotation, current Weakly-supervised Temporal Action Localization (WTAL) methods are generally stuck into over-complete or incomplete localization. In this paper, we aim to leverage the text information to boost WTAL from two aspects, i.e., (a) the discriminative objective to enlarge the inter-class difference, thus reducing the over-complete; (b) the generative objective to enhance the intra-class integrity, thus finding more complete temporal boundaries. For the discriminative objective, we propose a Text-Segment Mining (TSM) mechanism, which constructs a text description based on the action class label, and regards the text as the query to mine all class-related segments. Without the temporal annotation of actions, TSM compares the text query with the entire videos across the dataset to mine the best matching segments while ignoring irrelevant ones. Due to the shared sub-actions in different categories of videos, merely applying TSM is too strict to neglect the semantic-related segments, which results in incomplete localization. We further introduce a generative objective named Video-text Language Completion (VLC), which focuses on all semantic-related segments from videos to complete the text sentence. We achieve the state-of-the-art performance on THUMOS14 and ActivityNet1.3. Surprisingly, we also find our proposed method can be seamlessly applied to existing methods, and improve their performances with a clear margin. The code is available at https://github.com/lgzlIlIlI/Boosting-WTAL.
PB  - arXiv
PY  - 2023
ST  - Boosting Weakly-Supervised Temporal Action Localization with Text Information
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52729.2023.01026
ER  -


TY  - GEN
AU  - Mesnildrey, Q.
AU  - Aksenov, A.
AU  - D'Ambra, M.R.
AU  - Volpert, V.
AU  - Beuter, A.
TI  - Investigating Spatiotemporal Dynamics of Cortical Activity During Language Production in the Healthy and Lesioned Brain
AB  - Efficient language production requires rapid interactions between different brain areas. These interactions can be severely affected by brain lesions. However, the neurophysiological correlates of the spatiotemporal dynamics during language production are not well understood. The current pilot study explores differences in spatiotemporal cortical dynamics between five subjects with post-stroke aphasia and five control subjects. Electroencephalography was recorded during picture naming in both groups. Average-based analyses (event-related potential (ERP), frequency-specific Global Field Power (GFP)), reveal a strong synchronization of cortical oscillations, especially within the first 600ms post-stimulus, with a time shift between participants with aphasia and control subjects. ERPs and the corresponding brain microstates indicate coordinated brain activity alternating mainly between frontal and occipital zones. This behavior can be described as standing waves between two main sources. At the single-trial scale, traveling waves (TW) were identified from both phase and amplitude analyses. The spatiotemporal distribution of amplitude TW reveals subject-specific organization of several interconnected hubs. In patients with aphasia this spatial organization of TW reveals zones with no TW notably in the vicinity of stroke lesions. The present results provide important hints for the hypothesis that TW contribute to the synchronization and communication between different brain areas especially by interconnecting cortical hubs. Moreover, our findings show that cortical dynamics is affected by brain lesions.
PB  - bioRxiv
PY  - 2023
ST  - Investigating Spatiotemporal Dynamics of Cortical Activity During Language Production in the Healthy and Lesioned Brain
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.04.27.538530
ER  -


TY  - GEN
AU  - Chappa, N.V.R.
AU  - Nguyen, P.
AU  - Nelson, A.H.
AU  - Dobbs, P.D.
AU  - Luu, K.
TI  - SoGAR: Self-supervised Spatiotemporal Attention-based Social Group Activity Recognition
AB  - This paper introduces a novel approach to Social Group Activity Recognition (SoGAR) using Self-supervised Transformers network that can effectively utilize unlabeled video data. To extract spatio-temporal information, we create local and global views with varying frame rates. Our self-supervised objective ensures that features extracted from contrasting views of the same video are consistent across spatio-temporal domains. Our proposed approach efficiently uses transformer-based encoders to alleviate the weakly supervised setting of group activity recognition. By leveraging the benefits of transformer models, our approach can model long-term relationships along spatio-temporal dimensions. Our proposed SoGAR method achieves state-of-the-art results on three group activity recognition benchmarks, namely JRDB-PAR, NBA, and Volleyball datasets, surpassing the current state-of-the-art in terms of F1-score, MCA, and MPCA metrics.
PB  - arXiv
PY  - 2023
ST  - SoGAR
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4504147
ER  -


TY  - GEN
AU  - Li, G.
AU  - De Cheng
AU  - Ding, X.
AU  - Li, J.
AU  - Gao, X.
TI  - Weakly-Supervised Temporal Action Localization with Bidirectional Semantic Consistency Constraint
AB  - Weakly Supervised Temporal Action Localization (WTAL) aims to classify and localize temporal boundaries of actions for the video, given only video-level category labels in the training datasets. Due to the lack of boundary information during training, existing approaches formulate WTAL as a classification problem, i.e., generating the temporal class activation map (T-CAM) for localization. However, with only classification loss, the model would be sub-optimized, i.e., the action-related scenes are enough to distinguish different class labels. Regarding other actions in the action-related scene (i.e., the scene same as positive actions) as co-scene actions, this sub-optimized model would misclassify the co-scene actions as positive actions. To address this misclassification, we propose a simple yet efficient method, named bidirectional semantic consistency constraint (Bi-SCC), to discriminate the positive actions from co-scene actions. The proposed Bi-SCC firstly adopts a temporal context augmentation to generate an augmented video that breaks the correlation between positive actions and their co-scene actions in the inter-video; Then, a semantic consistency constraint (SCC) is used to enforce the predictions of the original video and augmented video to be consistent, hence suppressing the co-scene actions. However, we find that this augmented video would destroy the original temporal context. Simply applying the consistency constraint would affect the completeness of localized positive actions. Hence, we boost the SCC in a bidirectional way to suppress co-scene actions while ensuring the integrity of positive actions, by cross-supervising the original and augmented videos. Finally, our proposed Bi-SCC can be applied to current WTAL approaches, and improve their performance. Experimental results show that our approach outperforms the state-of-the-art methods on THUMOS14 and ActivityNet. The code is available at https://github.com/lgzlIlIlI/BiSCC.
PB  - arXiv
PY  - 2023
ST  - Weakly-Supervised Temporal Action Localization with Bidirectional Semantic Consistency Constraint
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tnnls.2023.3266062
ER  -


TY  - GEN
AU  - Gritsenko, A.
AU  - Xiong, X.
AU  - Djolonga, J.
AU  - Schmid, C.
AU  - Arnab, A.
TI  - End-to-End Spatio-Temporal Action Localisation with Video Transformers
AB  - The most performant spatio-temporal action localisation models use external person proposals and complex external memory banks. We propose a fully end-to-end, purely-transformer based model that directly ingests an input video, and outputs tubelets - a sequence of bounding boxes and the action classes at each frame. Our flexible model can be trained with either sparse bounding-box supervision on individual frames, or full tubelet annotations. And in both cases, it predicts coherent tubelets as the output. Moreover, our end-to-end model requires no additional pre-processing in the form of proposals, or post-processing in terms of non-maximal suppression. We perform extensive ablation experiments, and significantly advance the state-of-the-art results on four different spatio-temporal action localisation benchmarks with both sparse keyframes and full tubelet annotations.
PB  - arXiv
PY  - 2023
ST  - End-to-End Spatio-Temporal Action Localisation with Video Transformers
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52733.2024.01739
ER  -


TY  - GEN
AU  - Zhang, N.
AU  - Yuan, R.
TI  - Climate and Human Activities Dominate the Spatial Heterogeneity and Temporal Change of Ecological Risks in China's Nature Reserves
AB  - Nature reserves are an effective strategy for biodiversity conservation globally. However, research on ecological risk and driving factors of China's nature reserves are limited. Based on the Ecological Risk Index we estimated the ecological risk to 665 of China's nature reserves across levels, climatic zones, and ecosystem type from 1990 to 2020, at five-year intervals. We also explored how the driving factors of human activities and climate affected the spatial heterogeneity and temporal change of ecological risk in China's nature reserves. Our results showed that overall the ecological risk to China's nature reserves has decreased over the past three decades, with the most pronounced decline between 1990 and 2000. The ecological risk of coastline nature reserves has increased, however, while the ecological risk to other ecosystem types has decreased. Climate is the dominant factor driving spatial heterogeneity of ecological risk in China's nature reserves, with a contribution rate of 78.01%. Temporal change of ecological risk in China's nature reserves is primarily impacted by human activities in 84.96% of China's nature reserves. Only 2.40% of China's nature reserves were at a higher ecological risk in 2020. Our study reveals spatial heterogeneity and temporal change of ecological risks in China's nature reserves, highlighting ecological risk assessments of nature reserves should consider the restrictive effect of climate background. The decline in human activity will improve the ecological risk of the nature reserve. This study provides scientific guidance for the sustainable development of China's nature reserve system.
PB  - SSRN
PY  - 2023
ST  - Climate and Human Activities Dominate the Spatial Heterogeneity and Temporal Change of Ecological Risks in China's Nature Reserves
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4421346
ER  -


TY  - GEN
AU  - Huang, H.
AU  - Wang, Y.
AU  - Cai, M.
AU  - Wen, F.
AU  - Hu, X.
TI  - Adaptive Temporal Compression for Reduction of Computational Complexity in Human Behavior Recognition
AB  - The research on video analytics especially in the area of human behavior recognition has become increasingly popular recently. It is widely applied in virtual reality, video surveillance, and video retrieval. With the advancement of deep learning algorithms and computer hardware, the conventional two-dimensional convolution technique for training video models has been replaced by three-dimensional convolution, which enables the extraction of spatio-temporal features. Specifically, the use of 3D convolution in human behavior recognition has been the subject of growing interest. However, the increased dimensionality has led to challenges such as the dramatic increase in the number of parameters, increased time complexity, and a strong dependence on GPUs for effective spatio-temporal feature extraction. The training speed can be considerably slow without the support of powerful GPU hardware. To address these issues, this study proposes an Adaptive Temporal Compression (ATC) module that serves as a standalone module and can be integrated into existing architectures. The ATC module effectively reduces GPU computing load and time complexity with negligible loss of accuracy, thereby facilitating real-time human behavior recognition.
PB  - Research Square
PY  - 2023
ST  - Adaptive Temporal Compression for Reduction of Computational Complexity in Human Behavior Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1038/s41598-024-61286-x
ER  -


TY  - GEN
AU  - Zhou, J.
AU  - Huang, L.
AU  - Wang, L.
AU  - Liu, S.
AU  - Li, H.
TI  - Improving Weakly Supervised Temporal Action Localization by Bridging Train-Test Gap in Pseudo Labels
AB  - The task of weakly supervised temporal action localization targets at generating temporal boundaries for actions of interest, meanwhile the action category should also be classified. Pseudo-label-based methods, which serve as an effective solution, have been widely studied recently. However, existing methods generate pseudo labels during training and make predictions during testing under different pipelines or settings, resulting in a gap between training and testing. In this paper, we propose to generate high-quality pseudo labels from the predicted action boundaries. Nevertheless, we note that existing post-processing, like NMS, would lead to information loss, which is insufficient to generate high-quality action boundaries. More importantly, transforming action boundaries into pseudo labels is quite challenging, since the predicted action instances are generally overlapped and have different confidence scores. Besides, the generated pseudo-labels can be fluctuating and inaccurate at the early stage of training. It might repeatedly strengthen the false predictions if there is no mechanism to conduct self-correction. To tackle these issues, we come up with an effective pipeline for learning better pseudo labels. Firstly, we propose a Gaussian weighted fusion module to preserve information of action instances and obtain high-quality action boundaries. Second, we formulate the pseudo-label generation as an optimization problem under the constraints in terms of the confidence scores of action instances. Finally, we introduce the idea of Δ pseudo labels, which enables the model with the ability of self-correction. Our method achieves superior performance to existing methods on two benchmarks, THUMOS14 and ActivityNet1.3, achieving gains of 1.9% on THUMOS14 and 3.7% on ActivityNet1.3 in terms of average mAP. Our code is available at https://github.com/zhou745/GauFuse-WSTAL.git.
PB  - arXiv
PY  - 2023
ST  - Improving Weakly Supervised Temporal Action Localization by Bridging Train-Test Gap in Pseudo Labels
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52729.2023.02203
ER  -


TY  - GEN
AU  - Aboah, A.
AU  - Bagci, U.
AU  - Mussah, A.R.
AU  - Owor, N.J.
AU  - Adu-Gyamfi, Y.
TI  - DeepSegmenter: Temporal Action Localization for Detecting Anomalies in Untrimmed Naturalistic Driving Videos
AB  - Identifying unusual driving behaviors exhibited by drivers during driving is essential for understanding driver behavior and the underlying causes of crashes. Previous studies have primarily approached this problem as a classification task, assuming that naturalistic driving videos come discretized. However, both activity segmentation and classification are required for this task due to the continuous nature of naturalistic driving videos. The current study therefore departs from conventional approaches and introduces a novel methodological framework, DeepSegmenter, that simultaneously performs activity segmentation and classification in a single framework. The proposed framework consists of four major modules namely Data Module, Activity Segmentation Module, Classification Module and Postprocessing Module. Our proposed method won 8th place in the 2023 AI City Challenge, Track 3, with an activity overlap score of 0.5426 on experimental validation data. The experimental results demonstrate the effectiveness, efficiency, and robustness of the proposed system. The code is available at https://github.com/aboah1994/DeepSegment.git.
PB  - arXiv
PY  - 2023
ST  - DeepSegmenter
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvprw59228.2023.00565
ER  -


TY  - GEN
AU  - Kwon, J.
AU  - Kim, S.
AU  - Dong-Kyum, K.
AU  - Cha, M.
AU  - Lee, C.J.
TI  - Title: SUBTLE: An unsupervised platform with temporal link embedding that maps animal behavior
AB  - While huge strides have recently been made in language-based machine learning, the ability of artificial systems to comprehend the sequences that comprise animal behavior has been lagging behind. In contrast, humans instinctively recognize behaviors by finding similarities in behavioral sequences. Here, we develop an unsupervised behavior-mapping framework, SUBTLE (spectrogram-UMAP-based temporal-link embedding), to capture comparable behavioral repertoires from 3D action skeletons. To find the best embedding method, we devise a temporal proximity index as a metric to gauge temporal representation in the behavioral embedding space. The method achieves the best performance compared to current embedding strategies. Its spectrogram-based UMAP clustering not only identifies subtle inter-group differences but also matches human-annotated labels. SUBTLE framework automates the tasks of both identifying behavioral repertoires like walking, grooming, standing, and rearing, and profiling individual behavior signatures like subtle inter-group differences by age. SUBTLE highlights the importance of temporal representation in the behavioral embedding space for human-like behavioral categorization.
PB  - bioRxiv
PY  - 2023
ST  - Title
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s11263-024-02072-0
ER  -


TY  - GEN
AU  - Li, X.
AU  - Zhao, H.
AU  - Gu, B.
AU  - Chen, Y.
AU  - Chen, Y.
TI  - Spatio-Temporal Characteristics of Carbon Source/Sink and Their Relationships to Climatic Factors and Human Activities in China's Coastal Terrestrial Ecosystems
AB  - The terrestrial ecosystem in China's coastal areas has unique resources and geographical characteristics, which can directly respond to climate change and is typically affected by human activities. It is typically used to determine how different factors affect carbon sources/sinks’ characteristics. Based on a long-term data set from 2000 to 2020, the net ecosystem productivity (NEP) of vegetation in China’s coastal terrestrial ecosystems was measured, and the spatiotemporal changes of carbon sources/sinks in China's coastal areas and their responses to temperature, precipitation, and human activities were analyzed. The result shows that the annual average carbon NEP of China's coastal terrestrial ecosystem is 412.61 gC/m2. This spatial distribution pattern is high in the south and low in the north, decreasing from offshore to land. The carbon sink dominates the vegetation ecosystem’s carbon cycle. Overall, the NEP has been on the rise, showing spatial variation characterized by significant regional differences. The NEP value of vegetation in at least half of coastal areas will show a different trend in the future from the current. Climate impacts terrestrial ecosystems’ carbon source/sink in coastal areas. 45.89% of the regions have a significant positive correlation with precipitation; 36.13% of the regions exhibit a significant negative correlation. The regions with a significant negative correlation with temperature accounted for 42.91%. The regions with a high positive correlation accounted for 40.29%. Human activities have a dual impact on ecosystem carbon sources/sinks. Human activities in Northeast China, North China, and Western Guangxi have significantly increased carbon sources. This accounts for 66.45% of the whole region. On the contrary, southern Jiangsu and central Guangdong have significantly increased carbon sinks, accounting for 25.44%.
PB  - SSRN
PY  - 2023
ST  - Spatio-Temporal Characteristics of Carbon Source/Sink and Their Relationships to Climatic Factors and Human Activities in China's Coastal Terrestrial Ecosystems
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4416061
ER  -


TY  - GEN
AU  - Huang, W.-J.
AU  - Yeh, J.-H.
AU  - Chen, M.-H.
AU  - Faure, G.J.
AU  - Lai, S.-H.
TI  - Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection
AB  - The goal of spatial-temporal action detection is to determine the time and place where each person's action occurs in a video and classify the corresponding action category. Most of the existing methods adopt fully-supervised learning, which requires a large amount of training data, making it very difficult to achieve zero-shot learning. In this paper, we propose to utilize a pre-trained visual-language model to extract the representative image and text features, and model the relationship between these features through different interaction modules to obtain the interaction feature. In addition, we use this feature to prompt each label to obtain more appropriate text feature. Finally, we calculate the similarity between the interaction feature and the text feature for each label to determine the action category. Our experiments on J-HMDB and UCF101-24 datasets demonstrate that the proposed interaction module and prompting make the visual-language features better aligned, thus achieving excellent accuracy for zero-shot spatio-temporal action detection. The code will be available at https://github.com/webber2933/iCLIP.
PB  - arXiv
PY  - 2023
ST  - Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccvw60793.2023.00036
ER  -


TY  - GEN
AU  - Cho, S.-H.
AU  - Park, S.H.
TI  - Modeling Daily Travel Choices in an Activity- based Framework considering Spatiotemporal Constraints
AB  - Activity-based models (ACBMs) are developed to estimate individual travel patterns and improve the accuracy of forecasting aggregated travel demand. Two essential elements of ACBM are the decision to travel, often known as mobility, and the travel choice purpose of the journey. This study aims to develop sequential models of daily mobility and trip purpose choice using an activity-based framework that incorporates some useful features of the activity-based perspective. Random utility maximization (RUM)-based mobility and trip purpose choice models explain key elements of individuals’ daily activity patterns, incorporating spatiotemporal and socio-demographic characteristics. Based on the national household travel survey data collected in South Korea in 2016, these two models were estimated to represent 70% of the full sample, and applied to the remaining 30%. The mobility model reveals that licensed drivers and high-income travelers are more likely to travel, whereas the elderly and homemakers are less likely to do so. The previously selected trip purpose as well as the total number of trips for each purpose during the departure time of the trip influence the choice of the current trip. Demand forecasting and the policy implications of the empirical results are discussed.
PB  - Research Square
PY  - 2023
ST  - Modeling Daily Travel Choices in an Activity- based Framework considering Spatiotemporal Constraints
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-2699621/v1
ER  -


TY  - GEN
AU  - Santos, A.R.G.
AU  - Mathur, S.
AU  - García, R.A.
AU  - Cunha, M.S.
AU  - Amard, L.
TI  - Temporal variation of the photometric magnetic activity for the Sun and Kepler solar-like stars
AB  - Context. The photometric time series of solar-like stars can exhibit rotational modulation, that is, brightness variations due to active regions co-rotating with the stellar surface. These signatures allow us to constrain properties of stellar rotation and magnetic activity. Aims. In this work we investigate the behavior, particularly the variability in terms of strength, of the photometric magnetic activity of Kepler solar-like stars and compare it with that of the Sun. Methods. We adopted the photometric magnetic activity proxy, Sph, which was computed with a cadence of five times the rotation period (Prot). The average Sph was taken as the mean activity level, and the standard deviation was taken as a measure of the temporal variation of the magnetic activity over the Kepler observations. We also analyzed Sun-as-a-star photometric data from VIRGO (Variability of Solar Irradiance and Gravity Oscillations). Sun-like stars were selected from a very narrow parameter space around the solar properties, according to the recent Gaia-Kepler stellar properties catalog and the latest Kepler rotation catalog. We also looked into KIC 8006161 (HD 173701), a very active metal-rich G dwarf, and we compared its magnetic activity to that of stars with similar stellar fundamental parameters. Results. We find that the amplitude of Sph variability is strongly correlated with its mean value, independent of spectral type. An equivalent relationship has previously been found for ground-based observations of chromospheric activity emission and magnetic field strength, but in this work we show that photometric Kepler data also present the same behavior. While, depending on the phase of the cycle, the Sun is among the less active stars, we find that the Sph properties are consistent with those observed in Kepler Sun-like stars. KIC 8006161 is, however, among the most active of its peers, which tend to be metal-rich. This results from an underlying relationship between Prot and metallicity and supports the following interpretation of the magnetic activity of KIC 8006161: its strong activity is a consequence of its high metallicity, which affects the depth of the convection zone and, consequently, the efficiency of the dynamo.
PB  - arXiv
PY  - 2023
ST  - Temporal variation of the photometric magnetic activity for the Sun and Kepler solar-like stars
Y2  - 2025/05/05/21:54:31
DO  - 10.1051/0004-6361/202245430
ER  -


TY  - GEN
AU  - Wang, P.
AU  - Ling, H.
TI  - DIR-AS: Decoupling Individual Identification and Temporal Reasoning for Action Segmentation
AB  - Fully supervised action segmentation works on framewise action recognition with dense annotations and often suffers from the over-segmentation issue. Existing works have proposed a variety of solutions such as boundary-aware networks, multi-stage refinement, and temporal smoothness losses. However, most of them take advantage of frame-wise supervision, which cannot effectively tackle the evaluation metrics with different granularities. In this paper, for the desirable large receptive field, we first develop a novel localglobal attention mechanism with temporal pyramid dilation and temporal pyramid pooling for efficient multi-scale attention. Then we decouple two inherent goals in action segmentation, i.e., (1) individual identification solved by frame-wise supervision, and (2) temporal reasoning tackled by action set prediction. Afterward, an action alignment module fuses these different granularity predictions, leading to more accurate and smoother action segmentation. We achieve state-of-the-art accuracy, e.g., 82.8% (+2.6%) on GTEA and 74.7% (+1.2%) on Breakfast, which demonstrates the effectiveness of our proposed method, accompanied by extensive ablation studies. The code will be made available later.
PB  - arXiv
PY  - 2023
ST  - DIR-AS
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icme57554.2024.10688247
ER  -


TY  - GEN
AU  - Ilaboya, I.R.
AU  - Atikpo, E.
AU  - Omosefe, E.B.
AU  - AmbroseAgabi, E.E.
TI  - Spatio-Temporal Distribution and Environmental Risk Assessment for Heavy Metal Pollution of Soil from Cemetery Activities in a Typical Urban City in South-South Nigeria
AB  - Cemeteries are frequently found within residential settings in Nigeria and are not thought to pose a substantial environmental threat. Since no known publications have looked at the interaction between soil pollution occasioned by cemetery activities and the natural environment in south-south Nigeria, this article presents a study of particular interest. Using the stainless steel auger, composite soil samples were collected from the topsoil to 15cm depth of 8 randomly selected points each from 3 cemeteries in Benin City, Nigeria to make a total 24 sampling points. Control sample was obtained from the center of the city which is an equal distance from each studied cemetery. The samples were digested and analyzed for elemental composition using SKYRAY EDXRF model EDX3600B. Meta pollution induced deterioration level of the soil was studied using risk assessment models (contamination factor and pollution load index) the data were also subjected to descriptive statistics, Pearson’s product moment correlation coefficient, hierarchical cluster analysis and geospatial analysis. Nine (Al, Si, Fe, Ni, Cu, Zn, Mo, Sn and Sb) out of the 27 metals analyzed had reasonable level of concentration and therefore, were used as indicators for pollution risk assessment. No evidence of heavy metal pollution around the cemetery environment was observed. The contamination factor outcome of as Al (0.00001-0.00001), Si (0.04996–0.06226), Fe (0.27817–0.62971), Ni (0.00511-0.00610), Cu (0.00395-0.00400), Zn (0.00106-0.01440), Mo (0.19450-0.25450), Sn (0.03572-0.03822), and Sb (0.17636-0.18848) indicated low level of contamination. Pollution load index values for all the cemeteries were < 1, showing baseline levels of low metal pollution around the vicinity of the cemeteries. The Pearson’s product moment correlation analysis revealed a significant positive correlation between Si and Zn at 1% significant level with (P-value) of 0.008 (P< 0.01), Fe and Ni with p-value of 0.001 and r value of 0.917, Fe and Cu with p-value of 0.008 and r value of 0.847, Fe and Zn with p-value of 0.005 and r value of 0.868, Fe and Sn with p-value of 0.000 and r value of 0.967.
PB  - SSRN
PY  - 2023
ST  - Spatio-Temporal Distribution and Environmental Risk Assessment for Heavy Metal Pollution of Soil from Cemetery Activities in a Typical Urban City in South-South Nigeria
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4408492
ER  -


TY  - GEN
AU  - Koida, K.
AU  - Komatsu, H.
TI  - Effect of microstimulation of the inferior temporal cortex on color judgment behavior
AB  - Within the anterior inferior temporal cortex (AIT) there is a cluster of color selective neurons whose activities correlate with color discrimination behavior. To examine the causal relationship between the activities of these neurons and behavior, we applied electrical microstimulation to modulate neuronal activities within the AIT. We trained monkeys to perform a color judgment task and evaluated the effect of microstimulation in terms of the horizontal shift of the psychometric function. We found large effects of microstimulation on color discrimination behavior, predominantly within a subregion of the AIT. The cortical extent where microstimulation modulated behavior correlated with the presence of color-selective neurons. Unexpectedly, the direction of the modulation of color judgment evoked by microstimulation correlated negatively with the preference of the neurons around the stimulation site. These results support the existence of an anterior inferior temporal color-selective area (AITC) and its causal relationship with the color perception of these animals.
PB  - bioRxiv
PY  - 2023
ST  - Effect of microstimulation of the inferior temporal cortex on color judgment behavior
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.03.31.535032
ER  -


TY  - GEN
AU  - Zhu, X.
AU  - Huang, P.-Y.
AU  - Liang, J.
AU  - de Melo, C.M.
AU  - Hauptmann, A.
TI  - STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition
AB  - We study the problem of human action recognition using motion capture (MoCap) sequences. Unlike existing techniques that take multiple manual steps to derive standardized skeleton representations as model input, we propose a novel Spatial-Temporal Mesh Transformer (STMT) to directly model the mesh sequences. The model uses a hierarchical transformer with intra-frame off-set attention and inter-frame self-attention. The attention mechanism allows the model to freely attend between any two vertex patches to learn non-local relationships in the spatial-temporal domain. Masked vertex modeling and future frame prediction are used as two self-supervised tasks to fully activate the bi-directional and auto-regressive attention in our hierarchical transformer. The proposed method achieves state-of-the-art performance compared to skeleton-based and point-cloud-based models on common MoCap benchmarks. Code is available at https://github.com/zgzxy001/ STMT.
PB  - arXiv
PY  - 2023
ST  - STMT
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52729.2023.00153
ER  -


TY  - GEN
AU  - Liu, Y.
AU  - Li, X.
AU  - Luo, Z.
AU  - Zhou, W.
TI  - JCDNet: Joint of Common and Definite phases Network for Weakly Supervised Temporal Action Localization
AB  - Weakly-supervised temporal action localization aims to localize action instances in untrimmed videos with only video-level supervision. We witness that different actions record common phases, e.g., the run-up in the HighJump and LongJump. These different actions are defined as conjoint actions, whose rest parts are definite phases, e.g., leaping over the bar in a HighJump. Compared with the common phases, the definite phases are more easily localized in existing researches. Most of them formulate this task as a Multiple Instance Learning paradigm, in which the common phases are tended to be confused with the background, and affect the localization completeness of the conjoint actions. To tackle this challenge, we propose a Joint of Common and Definite phases Network (JCDNet) by improving feature discriminability of the conjoint actions. Specifically, we design a Class-Aware Discriminative module to enhance the contribution of the common phases in classification by the guidance of the coarse definite-phase features. Besides, we introduce a temporal attention module to learn robust action-ness scores via modeling temporal dependencies, distinguishing the common phases from the background. Extensive experiments on three datasets (THUMOS14, ActivityNetv1.2, and a conjoint-action subset) demonstrate that JCDNet achieves competitive performance against the state-of-the-art methods.
PB  - arXiv
PY  - 2023
ST  - JCDNet
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tmm.2021.3073235
ER  -


TY  - GEN
AU  - Lee, P.
AU  - Kim, T.
AU  - Shim, M.
AU  - Wee, D.
AU  - Byun, H.
TI  - Decomposed Cross-modal Distillation for RGB-based Temporal Action Detection
AB  - Temporal action detection aims to predict the time intervals and the classes of action instances in the video. Despite the promising performance, existing two-stream models exhibit slow inference speed due to their reliance on computationally expensive optical flow. In this paper, we introduce a decomposed cross-modal distillation framework to build a strong RGB-based detector by transferring knowledge of the motion modality. Specifically, instead of direct distillation, we propose to separately learn RGB and motion representations, which are in turn combined to perform action localization. The dual-branch design and the asymmetric training objectives enable effective motion knowledge transfer while preserving RGB information intact. In addition, we introduce a local attentive fusion to better exploit the multimodal complementarity. It is designed to preserve the local discriminability of the features that is important for action localization. Extensive experiments on the benchmarks verify the effectiveness of the proposed method in enhancing RGB-based action detectors. Notably, our framework is agnostic to backbones and detection heads, bringing consistent gains across different model combinations.
PB  - arXiv
PY  - 2023
ST  - Decomposed Cross-modal Distillation for RGB-based Temporal Action Detection
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52729.2023.00235
ER  -


TY  - GEN
AU  - Khalil, K.
AU  - Lanza, V.
AU  - Manceau, D.
AU  - Aziz-Alaoui, M.A.
AU  - Provitolo, D.
TI  - Analysis of a Spatio-Temporal Advection-Diffusion Model for Human Behaviors During a Catastrophic Event
AB  - In this work, using the theory of first-order macroscopic crowd models, we introduce a compartmental advection-diffusion type model, describing the spatio-temporal dynamics of a population in different human behaviors (alert, panic and control behaviors) during a catastrophic event. For this model, we prove the local existence, uniqueness and regularity of a solution, as well as the positivity and boundedness of this solution that allows the global existence. Then, in order to study the spatio-temporal behavioral dynamics of a population during a disaster event, we present several numerical simulations for different scenarios of evacuation.
PB  - SSRN
PY  - 2023
ST  - Analysis of a Spatio-Temporal Advection-Diffusion Model for Human Behaviors During a Catastrophic Event
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4399373
ER  -


TY  - GEN
AU  - Chen, B.
AU  - Shvetsova, N.
AU  - Rouditchenko, A.
AU  - Glass, J.
AU  - Kuehne, H.
TI  - What, when, and where? Self-Supervised Spatio-Temporal Grounding in Untrimmed Multi-Action Videos from Narrated Instructions
AB  - Spatio-temporal grounding describes the task of localizing events in space and time, e.g., in video data, based on verbal descriptions only. Models for this task are usually trained with human-annotated sentences and bounding box supervision. This work addresses this task from a multimodal supervision perspective, proposing a framework for spatio-temporal action grounding trained on loose video and subtitle supervision only, without human annotation. To this end, we combine local representation learning, which focuses on leveraging fine-grained spatial information, with a global representation encoding that captures higher-level representations and incorporates both in a joint approach. To evaluate this challenging task in a real-life setting, a new benchmark dataset is proposed, providing dense spatio-temporal grounding annotations in long, untrimmed, multi-action instructional videos for over 5K events. We evaluate the proposed approach and other methods on the proposed and standard downstream tasks, showing that our method improves over current baselines in various settings, including spatial, temporal, and untrimmed multi-action spatio-temporal grounding.
PB  - arXiv
PY  - 2023
ST  - What, when, and where? Self-Supervised Spatio-Temporal Grounding in Untrimmed Multi-Action Videos from Narrated Instructions
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52733.2024.01743
ER  -


TY  - GEN
AU  - Yang, H.
AU  - van de Kreeke, T.
AU  - Van Eyndhoven, L.C.
AU  - Tel, J.
TI  - Temporal perturbation of STAT1/2 activity reveals dynamic ligand discrimination of type I interferon signaling
AB  - Type-I interferon (IFN-I) subtypes signal through the same IFNα receptor (IFNAR), and initiate temporal STAT1/2 activation to orchestrate innate and adaptive immunity. It remains unknown how IFNAR discriminates between subtypes (e.g., IFNα and IFNβ), and how STAT1/2 signaling is affected by time-varying inputs. Here, we utilize our microfluidic system and live-cell imaging to quantify STAT1/2 activation dynamics in a reporter fibroblast model. Population-averaged and single-cell analyses reveal distinct STAT1/2 responses to various IFNα and IFNβ inputs. Upon continuous stimulation, cells show less sensitivity but more sustained responses to IFNα over IFNβ. A short IFNα pulse induces nearly homogeneous STAT1/2 dynamics, in contrast to heterogeneous responses in IFNβ-pulsed cells. Distinct STAT1/2 refractory states emerge upon exposure to repeated IFN-I pulses, while alternating pulse stimulation reveals that IFNβ can revoke STAT1/2 refractoriness caused by IFNα, but not vice versa. These findings highlight the differences between IFNα and IFNβ signaling and how they can elicit distinct temporal cellular behaviors during viral infection.
PB  - bioRxiv
PY  - 2023
ST  - Temporal perturbation of STAT1/2 activity reveals dynamic ligand discrimination of type I interferon signaling
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.03.27.534340
ER  -


TY  - GEN
AU  - Grzyb, A.
AU  - Wolna-Maruwka, A.
AU  - Łukowiak, R.
AU  - Ceglarek, J.
AU  - Niewiadomska, A.
TI  - Spatial and Temporal Differentiation of Soil Biochemical Activity and its Relationship with Nitrogen Resources During the Vegetation Period of Selected Crops
AB  - Understanding the spatial-temporal variability of soil enzymatic activity and its relationship with nitrogen resources in the soil and crop yield is of key importance in assessing the functioning of the soil ecosystem. The aim of the study was to determine the spatial and temporal variability of the activity of soil enzymes, such as acid (PAC) and alkaline (PAL) phosphatases, urease (URE) and protease (PROT), the content of N-NH4, N-NO3, phosphorus, pH, moisture, as well as crop yield on a conventionally managed farmland of 40 ha. During the two-year experiment, soil samples were collected from 37 measurement points. Wheat was the first tested crop, followed by oilseed rape. It was shown that all the tested soil parameters showed temporal and spatial variability, a significant amount of them were significantly higher in July. The creation of raster maps showing the distribution of the tested parameters allowed for the observation of considerable activity of PAC, PAL, URE and PROT, as well as a high application of N-NO3 in the southern part of the field, during the growth of the plants. The statistical analysis revealed a negative interaction between the N-NH4 and N-NO3 and the urease in the soil under the cultivation of plants. pH and the percentage of moisture in the soil also had higher values in the south of the field. This pointed to the existence of separate production zones in the south-central part of the field, characterized by a higher yield of wheat and rape. On the basis of the conducted research, it was unequivocally stated that the values of enzymatic and chemical parameters of the soil were reflected in the size of the yield obtained, which allows conclusions to be drawn with respect to the rational management of nitrogen in the production process, laying the foundations for precision agriculture.
PB  - SSRN
PY  - 2023
ST  - Spatial and Temporal Differentiation of Soil Biochemical Activity and its Relationship with Nitrogen Resources During the Vegetation Period of Selected Crops
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4401494
ER  -


TY  - GEN
AU  - Nag, S.
AU  - Zhu, X.
AU  - Deng, J.
AU  - Song, Y.-Z.
AU  - Xiang, T.
TI  - DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion
AB  - We propose a new formulation of temporal action detection (TAD) with denoising diffusion, DiffTAD in short. Taking as input random temporal proposals, it can yield action proposals accurately given an untrimmed long video. This presents a generative modeling perspective, against previous discriminative learning manners. This capability is achieved by first diffusing the ground-truth proposals to random ones (i.e., the forward/noising process) and then learning to reverse the noising process (i.e., the backward/denoising process). Concretely, we establish the denoising process in the Transformer decoder (e.g., DETR) by introducing a temporal location query design with faster convergence in training. We further propose a cross-step selective conditioning algorithm for inference acceleration. Extensive evaluations on ActivityNet and THUMOS show that our DiffTAD achieves top performance compared to previous art alternatives. The code will be made available at https://github.com/sauradip/DiffusionTAD.
PB  - arXiv
PY  - 2023
ST  - DiffTAD
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccv51070.2023.00951
ER  -


TY  - GEN
AU  - Bhaskar, D.
AU  - Moore, J.
AU  - Zhang, Y.
AU  - Pittenger, C.
AU  - Krishnaswamy, S.
TI  - Neuro-GSTH: A Geometric Scattering and Persistent Homology Framework for Uncovering Spatiotemporal Signatures in Neural Activity
AB  - Understanding how neurons communicate and coordinate their activity is essential for unraveling the brain’s complex functionality. To analyze the intricate spatiotemporal dynamics of neural signaling, we developed Geometric Scattering Trajectory Homology (neuro-GSTH), a novel framework that captures time-evolving neural signals and encodes them into low-dimensional representations. GSTH integrates geometric scattering transforms, which extract multiscale features from brain signals modeled on anatomical graphs, with t-PHATE, a manifold learning method that maps the temporal evolution of neural activity. Topological descriptors from computational homology are then applied to characterize the global structure of these neural trajectories, enabling the quantification and differentiation of spatiotemporal brain dynamics. We demonstrate the power of neuro-GSTH in neuroscience by applying it to both simulated and biological neural datasets. First, we used neuro-GSTH to analyze neural oscillatory behavior in the Kuramoto model, revealing its capacity to track the synchronization of neural circuits as coupling strength increases. Next, we applied neuro-GSTH to neural recordings from the visual cortex of mice, where it accurately reconstructed visual stimulus patterns such as sinusoidal gratings. Neuro-GSTH-derived neural trajectories enabled precise classification of stimulus properties like spatial frequency and orientation, significantly outperforming traditional methods in capturing the underlying neural dynamics. These findings demonstrate that neuro-GSTH effectively identifies neural motifs—distinct patterns of spatiotemporal activity—providing a powerful tool for decoding brain activity across diverse tasks, sensory inputs, and neurological disorders. Neuro-GSTH thus offers new insights into neural communication and dynamics, advancing our ability to map and understand complex brain functions.
PB  - bioRxiv
PY  - 2023
ST  - Neuro-GSTH
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.03.22.533807
ER  -


TY  - GEN
AU  - Deng, A.
AU  - Yang, T.
AU  - Chen, C.
TI  - A Large-scale Study of Spatiotemporal Representation Learning with a New Benchmark on Action Recognition
AB  - The goal of building a benchmark (suite of datasets) is to provide a unified protocol for fair evaluation and thus facilitate the evolution of a specific area. Nonetheless, we point out that existing protocols of action recognition could yield partial evaluations due to several limitations. To comprehensively probe the effectiveness of spatiotemporal representation learning, we introduce BEAR, a new BEnchmark on video Action Recognition. BEAR is a collection of 18 video datasets grouped into 5 categories (anomaly, gesture, daily, sports, and instructional), which covers a diverse set of real-world applications. With BEAR, we thoroughly evaluate 6 common spatiotemporal models pre-trained by both supervised and self-supervised learning. We also report transfer performance via standard finetuning, few-shot finetuning, and unsupervised domain adaptation. Our observation suggests that the current state-of-the-art cannot solidly guarantee high performance on datasets close to real-world applications, and we hope BEAR can serve as a fair and challenging evaluation benchmark to gain insights on building next-generation spatiotemporal learners. Our dataset, code, and models are released at: https://github.com/AndongDeng/BEAR
PB  - arXiv
PY  - 2023
ST  - A Large-scale Study of Spatiotemporal Representation Learning with a New Benchmark on Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccv51070.2023.01876
ER  -


TY  - GEN
AU  - Xu, Y.
AU  - Wang, Z.
AU  - Du, B.
TI  - Leveraging Spatial Residual Attention and Temporal Markov Networks for Video Action Understanding
AB  - The effective use of temporal relationships while extracting fertile spatial features is the key to video action understanding. Video action understanding is a challenging visual task because it generally necessitates not only the features of individual key frames but also the contextual understanding of the entire video and the relationships among key frames. Temporal relationships pose a challenge to video action understanding. However, existing 3D convolutional neural network approaches are limited, with a great deal of redundant spatial and temporal information. In this paper, we present a novel twostream approach that incorporates Spatial Residual Attention and Temporal Markov (SRATM) to learn complementary features to achieve stronger video action understanding performance. Specifically, the spatial residual attention network captures effective spatial feature representation. Then, the temporal Markov network enhances the model by learning the temporal relationships via conducting probabilistic logic calculation among frames in a video. Finally, we conduct extensive experiments on three realistic video datasets, namely, Something-Something-V1, Something-Something-V2, and Diving48, and the experimental results demonstrate that the proposed SRATM method achieves competitive results.
PB  - SSRN
PY  - 2023
ST  - Leveraging Spatial Residual Attention and Temporal Markov Networks for Video Action Understanding
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.neunet.2023.10.047
ER  -


TY  - GEN
AU  - Yun, W.
AU  - Qi, M.
AU  - Wang, C.
AU  - Ma, H.
TI  - Weakly-Supervised Temporal Action Localization by Inferring Salient Snippet-Feature
AB  - Weakly-supervised temporal action localization aims to locate action regions and identify action categories in untrimmed videos simultaneously by taking only video-level labels as the supervision. Pseudo label generation is a promising strategy to solve the challenging problem, but the current methods ignore the natural temporal structure of the video that can provide rich information to assist such a generation process. In this paper, we propose a novel weakly-supervised temporal action localization method by inferring salient snippet-feature. First, we design a saliency inference module that exploits the variation relationship between temporal neighbor snippets to discover salient snippet-features, which can reflect the significant dynamic change in the video. Secondly, we introduce a boundary refinement module that enhances salient snippet-features through the information interaction unit. Then, a discrimination enhancement module is introduced to enhance the discriminative nature of snippet-features. Finally, we adopt the refined snippet-features to produce high-fidelity pseudo labels, which could be used to supervise the training of the action localization network. Extensive experiments on two publicly available datasets, i.e., THUMOS14 and ActivityNet v1.3, demonstrate our proposed method achieves significant improvements compared to the state-of-the-art methods. Our source code is available at https://github.com/wuli55555/ISSF.
PB  - arXiv
PY  - 2023
ST  - Weakly-Supervised Temporal Action Localization by Inferring Salient Snippet-Feature
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v38i7.28516
ER  -


TY  - GEN
AU  - Ju, C.
AU  - Li, Z.
AU  - Zhao, P.
AU  - Wang, Y.
AU  - Xie, W.
TI  - Multi-modal Prompting for Low-Shot Temporal Action Localization
AB  - In this paper, we consider the problem of temporal action localization under low-shot (zero-shot & few-shot) scenario, with the goal of detecting and classifying the action instances from arbitrary categories within some untrimmed videos, even not seen at training time. We adopt a Transformer-based two-stage action localization architecture with class-agnostic action proposal, followed by open-vocabulary classification. We make the following contributions. First, to compensate image-text foundation models with temporal motions, we improve category-agnostic action proposal by explicitly aligning embeddings of optical flows, RGB and texts, which has largely been ignored in existing low-shot methods. Second, to improve open-vocabulary action classification, we construct classifiers with strong discriminative power, i.e., avoid lexical ambiguities. To be specific, we propose to prompt the pre-trained CLIP text encoder either with detailed action descriptions (acquired from large-scale language models), or visually-conditioned instance-specific prompt vectors. Third, we conduct thorough experiments and ablation studies on THUMOS14 and ActivityNet1.3, demonstrating the superior performance of our proposed model, outperforming existing state-of-the-art approaches by one significant margin.
PB  - arXiv
PY  - 2023
ST  - Multi-modal Prompting for Low-Shot Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4341961
ER  -


TY  - GEN
AU  - Wang, Z.
AU  - Song, S.
AU  - Luo, C.
AU  - Xie, W.
AU  - Shen, L.
TI  - Spatio-Temporal AU Relational Graph Representation Learning For Facial Action Units Detection
AB  - This paper presents our Facial Action Units (AUs) detection submission to the fifth Affective Behavior Analysis in-the-wild Competition (ABAW). Our approach consists of three main modules: (i) a pre-trained facial representation encoder which produce a strong facial representation from each input face image in the input sequence; (ii) an AU-specific feature generator that specifically learns a set of AU features from each facial representation; and (iii) a spatio-temporal graph learning module that constructs a spatio-temporal graph representation. This graph representation describes AUs contained in all frames and predicts the occurrence of each AU based on both the modeled spatial information within the corresponding face and the learned temporal dynamics among frames. The experimental results show that our approach outperformed the baseline and the spatio-temporal graph representation learning allows our model to generate the best results among all ablated systems. Our model ranks at the 4th place in the AU recognition track at the 5th ABAW Competition. Our code is publicly available at https://github.com/wzh125/ABAW-5 . MSC Codes 68T40
PB  - arXiv
PY  - 2023
ST  - Spatio-Temporal AU Relational Graph Representation Learning For Facial Action Units Detection
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.neucom.2024.129106
ER  -


TY  - GEN
AU  - Zou, P.
AU  - Wen, K.
AU  - Wang, R.
AU  - Peng, Y.
AU  - Sun, X.
TI  - Spatial-temporal Transformer for Affective Behavior Analysis
AB  - The in-the-wild affective behavior analysis has been an important study. In this paper, we submit our solutions for the 5th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW), which includes V-A Estimation, Facial Expression Classification and AU Detection Sub-challenges. We propose a Transformer Encoder with Multi-Head Attention framework to learn the distribution of both the spatial and temporal features. Besides, there are virious effective data augmentation strategies employed to alleviate the problems of sample imbalance during model training. The results fully demonstrate the effectiveness of our proposed model based on the Aff-Wild2 dataset.
PB  - arXiv
PY  - 2023
ST  - Spatial-temporal Transformer for Affective Behavior Analysis
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.patrec.2023.08.010
ER  -


TY  - GEN
AU  - Swann, A.
AU  - Murphy, N.
AU  - Lijffijt, M.
AU  - Tamman, A.
AU  - Kosten, T.
TI  - Temporal Architecture of Suicide: Interacting Immediate and Long-Term Action Regulation with History of Medically Severe Suicide Attempt
AB  - Imminent suicide is difficult to predict. Suicide usually occurs on the first attempt. Suicide may consist of suicidal crises, with fluctuating and unpredictable risk, superimposed on long-term latent susceptibility. Risk characteristics are consistent with interacting long-term (sensitization) and short-term (impulsivity) mechanisms: sensitization to stress/trauma perceived as inescapable, or to addiction, can increase latent action-impulsivity, disinhibiting high-risk behavior including suicidal ideation and aggression. Survived medically severe suicide attempts are associated with severe morbidity and premature death. Therefore, it is necessary to identify characteristics that may predict a first attempt. We used Bayesian logistic regression and path analysis to identify direct and indirect (via suicidal ideation-worst (SSI-W)) predictors of MSSA in 28 adult survivors of MSSA and 23 symptomatically and demographically similar non-attempters (NA). SSI-W increased odds ratio (OR) for MSSA; Cumulative Adversity and action-impulsivity increased OR for MSSA independent of SSI-W. SSI-W mediated increased MSSA by aggression, alcohol use, and depression. Childhood Trauma Minimization/Denial increased MSSA directly but correlated negatively with SSI-W. MSSA required latent aggression or impulsivity combined with stress-sensitization and increased action-impulsivity. These interacting characteristics are potential targets for identification and preventive treatment of risk for suicide regardless of attempt history.
PB  - Research Square
PY  - 2023
ST  - Temporal Architecture of Suicide
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-2536527/v1
ER  -


TY  - GEN
AU  - Cao, C.
AU  - Wang, Y.
AU  - Lu, Y.
AU  - Zhang, X.
AU  - Zhang, Y.
TI  - Co-Occurrence Matters: Learning Action Relation for Temporal Action Localization
AB  - Temporal action localization (TAL) is a prevailing task due to its great application potential. Existing works in this field mainly suffer from two weaknesses: (1) They often neglect the multi-label case and only focus on temporal modeling. (2) They ignore the semantic information in class labels and only use the visual information. To solve these problems, we propose a novel Co-Occurrence Relation Module (CORM) that explicitly models the co-occurrence relationship between actions. Besides the visual information, it further utilizes the semantic embeddings of class labels to model the co-occurrence relationship. The CORM works in a plug-and-play manner and can be easily incorporated with the existing sequence models. By considering both visual and semantic co-occurrence, our method achieves high multi-label relationship modeling capacity. Meanwhile, existing datasets in TAL always focus on low-semantic atomic actions. Thus we construct a challenging multi-label dataset UCF-Crime-TAL that focuses on high-semantic actions by annotating the UCF-Crime dataset at frame level and considering the semantic overlap of different events. Extensive experiments on two commonly used TAL datasets, i.e., MultiTHUMOS and TSU, and our newly proposed UCF-Crime-TAL demenstrate the effectiveness of the proposed CORM, which achieves state-of-the-art performance on these datasets.
PB  - arXiv
PY  - 2023
ST  - Co-Occurrence Matters
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tcsvt.2023.3321508
ER  -


TY  - GEN
AU  - Tang, T.N.
AU  - Kim, K.
AU  - Sohn, K.
TI  - TemporalMaxer: Maximize Temporal Context with only Max Pooling for Temporal Action Localization
AB  - Temporal Action Localization (TAL) is a challenging task in video understanding that aims to identify and localize actions within a video sequence. Recent studies have emphasized the importance of applying long-term temporal context modeling (TCM) blocks to the extracted video clip features such as employing complex self-attention mechanisms. In this paper, we present the simplest method ever to address this task and argue that the extracted video clip features are already informative to achieve outstanding performance without sophisticated architectures. To this end, we introduce TemporalMaxer, which minimizes long-term temporal context modeling while maximizing information from the extracted video clip features with a basic, parameter-free, and local region operating max-pooling block. Picking out only the most critical information for adjacent and local clip embeddings, this block results in a more efficient TAL model. We demonstrate that TemporalMaxer outperforms other state-of-the-art methods that utilize long-term TCM such as self-attention on various TAL datasets while requiring significantly fewer parameters and computational resources. The code for our approach is publicly available at https://github.com/TuanTNG/ TemporalMaxer.
PB  - arXiv
PY  - 2023
ST  - TemporalMaxer
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3567828
ER  -


TY  - GEN
AU  - Goldbraikh, A.
AU  - Shubi, O.
AU  - Rubin, O.
AU  - Pugh, C.M.
AU  - Laufer, S.
TI  - MS-TCRNet: Multi-Stage Temporal Convolutional Recurrent Networks for Action Segmentation Using Sensor-Augmented Kinematics
AB  - Action segmentation is a challenging task in high-level process analysis, typically performed on video or kinematic data obtained from various sensors. This work presents two contributions related to action segmentation on kinematic data. Firstly, we introduce two versions of Multi-Stage Temporal Convolutional Recurrent Networks (MS-TCRNet), specifically designed for kinematic data. The architectures consist of a prediction generator with intra-stage regularization and Bidirectional LSTM or GRU-based refinement stages. Secondly, we propose two new data augmentation techniques, World Frame Rotation and Hand Inversion, which utilize the strong geometric structure of kinematic data to improve algorithm performance and robustness. We evaluate our models on three datasets of surgical suturing tasks: the Variable Tissue Simulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS) Dataset, both of which are open surgery simulation datasets collected by us, as well as the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), a well-known benchmark in robotic surgery. Our methods achieved state-of-the-art performance. code: https://github.com/AdamGoldbraikh/MS-TCRNet
PB  - arXiv
PY  - 2023
ST  - MS-TCRNet
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.patcog.2024.110778
ER  -


TY  - GEN
AU  - Shakeel, M.
AU  - Brockmann, A.
TI  - Temporal effects of sugar intake on fly local search and honey bee dance behaviour
AB  - Honey bees communicate navigational information of profitable food to nestmates via dance, a small scale walking pattern. Hungry flies and honey bee foragers initiate a sugar-elicited local search that involves path integration and show similarities with dance behaviour. Using a comparative approach, we explored the temporal dynamics of initiation of local search and dance in flies and honey bees, respectively. Passive displacement experiments showed that feeding and initiation of search can be spatially dissociated in both species. Sugar intake increased the probability to initiate a search but onset of walking starts the path integration system guiding the search. When prevented from walking, the motivation to begin a path integration-based search was sustained for 3 min after sugar intake in flies and bees. In flies, the behavioural parameters of search were significantly reduced for 3 min but were higher than flies that were given no sugar stimulus, indicating some degree of meander. These results suggest that sugar elicits two independent behavioural responses: path integration and increased turning, and initiation and duration of path integration system is temporally more restricted. Honey bee dance experiments demonstrated that the motivation of foragers to initiate dance was sustained for 15 min, whereas the number of circuits declined after 3 min. Based on our findings, we propose that the food-intake during foraging has the capability to activate the path integration system in flies and honey bees, and this interaction might have been elaborated during evolution to guide the walking pattern of the honey bee dance.
PB  - bioRxiv
PY  - 2023
ST  - Temporal effects of sugar intake on fly local search and honey bee dance behaviour
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s00359-023-01670-6
ER  -


TY  - GEN
AU  - Shi, D.
AU  - Zhong, Y.
AU  - Cao, Q.
AU  - Li, J.
AU  - Tao, D.
TI  - TriDet: Temporal Action Detection with Relative Boundary Modeling
AB  - In this paper, we present a one-stage framework TriDet for temporal action detection. Existing methods often suffer from imprecise boundary predictions due to the ambiguous action boundaries in videos. To alleviate this problem, we propose a novel Trident-head to model the action boundary via an estimated relative probability distribution around the boundary. In the feature pyramid of TriDet, we propose an efficient Scalable-Granularity Perception (SGP) layer to mitigate the rank loss problem of self-attention that takes place in the video features and aggregate information across different temporal granularities. Benefiting from the Trident-head and the SGP-based feature pyramid, TriDet achieves state-of-the-art performance on three challenging benchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational costs, compared to previous methods. For example, TriDet hits an average mAP of 69.3% on THUMOS14, outperforming the previous best by 2.5%, but with only 74.6% of its latency. The code is released to https://github.com/dingfengshi/TriDet.
PB  - arXiv
PY  - 2023
ST  - TriDet
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52729.2023.01808
ER  -


TY  - GEN
AU  - Gluszek-Kustusz, A.
AU  - Craske, B.
AU  - Legal, T.
AU  - McHugh, T.
AU  - Welburn, J.P.I.
TI  - Phosphorylation controls spatial and temporal activities of motor-PRC1 complexes to complete mitosis
AB  - During mitosis, spindle architecture alters as chromosomes segregate to daughter cells. The microtubule crosslinker Protein Required for Cytokinesis 1 (PRC1) is essential for spindle stability, chromosome segregation and completion of cytokinesis, but how it recruits motors to the central spindle to coordinate the segregation of chromosomes is unknown. Here, we combine structural and cell biology approaches to show that the human CENP-E motor, which is essential for chromosome capture and alignment by microtubules, binds to PRC1 through a conserved hydrophobic motif. This binding mechanism is also used by Kinesin-4 Kif4A:PRC1. Using in vitro reconstitution, we demonstrate that CENP-E slides antiparallel PRC1-crosslinked microtubules. We find that the regulation of CENP-E -PRC1 interaction is spatially and temporally coupled with relocalization to overlapping microtubules in anaphase. Finally, we demonstrate that the PRC1:microtubule motor interaction is essential in anaphase to control chromosome partitioning, retain central spindle integrity and ensure cytokinesis. Taken together our findings reveal the molecular basis for the cell cycle regulation of motor-PRC1 complexes to couple chromosome segregation and cytokinesis.
PB  - bioRxiv
PY  - 2023
ST  - Phosphorylation controls spatial and temporal activities of motor-PRC1 complexes to complete mitosis
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.03.11.531660
ER  -


TY  - GEN
AU  - Chakraborty, A.
AU  - Rao, R.R.
TI  - On Temporal and Spatial Behaviors of CBRS
AB  - The recently established Citizens Broadband Radio Service (CBRS) has attracted attention because it accommodates incumbents, auctioned license holders, and unlicensed users. Incumbents retain unconstrained transmission rights, but when inactive, the unused spectrum is shared between the Primary Access License (PAL) users and the General Authorized Access (GAA) users. The spectrum sharing is controlled by a cloud-based centralized administrator, Spectrum Access System (SAS), which uses an environmental sensing capability network to establish transmission rights for PAL and GAA users without hindering the incumbents. This paper reports findings from GAA CBRS Devices’ (CBSDs’) deployments in California’s San Diego county, which has numerous incumbents and characterizes the temporal and spatial behavior of CBRS. Based on measured temporal data, a Markov model is shown to be effective in estimating the steady state and hitting time probabilities for spectrum availability. We also recorded the availability of the CBRS spectrum, advertised by the SAS at various locations in San Diego, and found evidence of undisclosed obfuscation in the reporting of spectrum availability. We then developed two strategies that maximize the entropy of the Markov Chain’s hidden states and provide more significant obfuscation than the undisclosed actions of the SAS.
PB  - TechRxiv
PY  - 2023
ST  - On Temporal and Spatial Behaviors of CBRS
Y2  - 2025/05/05/21:54:31
DO  - 10.36227/techrxiv.22248058
ER  -


TY  - GEN
AU  - Ahmed, T.
AU  - Rizvi, S.T.H.
AU  - Kanwal, N.
TI  - Transforming Spatio-Temporal Self-Attention Using Action Embedding for Skeleton-Based Action Recognition
AB  - Over the past few years, skeleton-based action recognition has attracted great success because skeleton data is immune to illumination variation, view-point variation, background clutter, scaling, and camera motion. However, effectively modeling the latent information of skeleton data is a challenging problem. In this paper, we propose a novel idea of action embedding with a self-attention transformer for skeleton-based action recognition. Our proposed method encodes action embedding using the relationship between distant body joints (e.g., joints of both hands move togetherfor performing clapping action) and thus corresponds to spatial attention. Whilst interdependencies between body joints are modeled using a transformer network. Our method works in a single-stream (end-to-end) fashion, where MLP is used for classification. We carry out an ablation study and evaluate our model on a small-scale SYSU-3D dataset and large-scale NTU-RGB+D and NTU-RGB+D 120 datasets where the results demonstrate that our method surpasses state-of-the-art performance.
PB  - SSRN
PY  - 2023
ST  - Transforming Spatio-Temporal Self-Attention Using Action Embedding for Skeleton-Based Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.jvcir.2023.103892
ER  -


TY  - GEN
AU  - Lin, W.
AU  - Kukleva, A.
AU  - Possegger, H.
AU  - Kuehne, H.
AU  - Bischof, H.
TI  - TAEC: Unsupervised Action Segmentation with Temporal-Aware Embedding and Clustering
AB  - Temporal action segmentation in untrimmed videos has gained increased attention recently. However, annotating action classes and frame-wise boundaries is extremely time consuming and cost intensive, especially on large-scale datasets. To address this issue, we propose an unsupervised approach for learning action classes from untrimmed video sequences. In particular, we propose a temporal embedding network that combines relative time prediction, feature reconstruction, and sequence-to-sequence learning, to preserve the spatial layout and sequential nature of the video features. A two-step clustering pipeline on these embedded feature representations then allows us to enforce temporal consistency within, as well as across videos. Based on the identified clusters, we decode the video into coherent temporal segments that correspond to semantically meaningful action classes. Our evaluation on three challenging datasets shows the impact of each component and, furthermore, demonstrates our state-of-the-art unsupervised action segmentation results.
PB  - arXiv
PY  - 2023
ST  - TAEC
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icme57554.2024.10688143
ER  -


TY  - GEN
AU  - Raviteja Chappa, N.V.S.
AU  - Nguyen, P.
AU  - Nelson, A.H.
AU  - Dobbs, P.D.
AU  - Luu, K.
TI  - SPARTAN: Self-supervised Spatiotemporal Transformers Approach to Group Activity Recognition
AB  - In this paper, we propose a new, simple, and effective Self-supervised Spatio-temporal Transformers (SPARTAN) approach to Group Activity Recognition (GAR) using unlabeled video data. Given a video, we create local and global Spatio-temporal views with varying spatial patch sizes and frame rates. The proposed self-supervised objective aims to match the features of these contrasting views representing the same video to be consistent with the variations in spatiotemporal domains. To the best of our knowledge, the proposed mechanism is one of the first works to alleviate the weakly supervised setting of GAR using the encoders in video transformers. Furthermore, using the advantage of transformer models, our proposed approach supports long-term relationship modeling along spatio-temporal dimensions. The proposed SPARTAN approach performs well on two group activity recognition benchmarks, including NBA and Volleyball datasets, by surpassing the state-of-the-art results by a significant margin in terms of MCA and MPCA metrics.
PB  - arXiv
PY  - 2023
ST  - SPARTAN
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvprw59228.2023.00544
ER  -


TY  - GEN
AU  - Song, Q.
AU  - Zhou, Y.
AU  - Hu, M.
AU  - Liu, C.
TI  - Faster Learning of Temporal Action Proposal via Sparse Multilevel Boundary Generator
AB  - Temporal action localization in videos presents significant challenges in the field of computer vision. While the boundary-sensitive method has been widely adopted, its limitations include incomplete use of intermediate and global information, as well as an inefficient proposal feature generator. To address these challenges, we propose a novel framework, Sparse Multilevel Boundary Generator (SMBG), which enhances the boundary-sensitive method with boundary classification and action completeness regression. SMBG features a multi-level boundary module that enables faster processing by gathering boundary information at different lengths. Additionally, we introduce a sparse extraction confidence head that distinguishes information inside and outside the action, further optimizing the proposal feature generator. To improve the synergy between multiple branches and balance positive and negative samples, we propose a global guidance loss. Our method is evaluated on two popular benchmarks, ActivityNet-1.3 and THUMOS14, and is shown to achieve state-of-the-art performance, with a better inference speed (2.47xBSN++, 2.12xDBG). These results demonstrate that SMBG provides a more efficient and simple solution for generating temporal action proposals. Our proposed framework has the potential to advance the field of computer vision and enhance the accuracy and speed of temporal action localization in video analysis.The code and models are made available at https://github.com/zhouyang-001/SMBG-for-temporal-action-proposal.
PB  - arXiv
PY  - 2023
ST  - Faster Learning of Temporal Action Proposal via Sparse Multilevel Boundary Generator
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s11042-023-15308-x
ER  -


TY  - GEN
AU  - Xian, R.
AU  - Wang, X.
AU  - Manocha, D.
TI  - MITFAS: Mutual Information based Temporal Feature Alignment and Sampling for Aerial Video Action Recognition
AB  - We present a novel approach for action recognition in UAV videos. Our formulation is designed to handle occlusion and viewpoint changes caused by the movement of a UAV. We use the concept of mutual information to compute and align the regions corresponding to human action or motion in the temporal domain. This enables our recognition model to learn from the key features associated with the motion. We also propose a novel frame sampling method that uses joint mutual information to acquire the most informative frame sequence in UAV videos. We have integrated our approach with X3D and evaluated the performance on multiple datasets. In practice, we achieve 18.9% improvement in Top-1 accuracy over current state-of-the-art methods on UAV-Human [30], 7.3% improvement on Drone-Action [41], and 7.16% improvement on NEC Drones [7]. The code is available at https://github.com/Ricky-Xian/MITFAS
PB  - arXiv
PY  - 2023
ST  - MITFAS
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/wacv57701.2024.00649
ER  -


TY  - GEN
AU  - Wang, X.
AU  - Xian, R.
AU  - Guan, T.
AU  - Bera, A.
AU  - Manocha, D.
TI  - AZTR: Aerial Video Action Recognition with Auto Zoom and Temporal Reasoning
AB  - We propose a novel approach for aerial video action recognition. Our method is designed for videos captured using UAVs and can run on edge or mobile devices. We present a learning-based approach that uses customized auto zoom to automatically identify the human target and scale it appropriately. This makes it easier to extract the key features and reduces the computational overhead. We also present an efficient temporal reasoning algorithm to capture the action information along the spatial and temporal domains within a controllable computational cost. Our approach has been implemented and evaluated both on the desktop with high-end GPUs and on the low power Robotics RB5 Platform for robots and drones. In practice, we achieve 6.1-7.4% improvement over SOTA in Top-1 accuracy on the RoCoG-v2 dataset, 8.3-10.4% improvement on the UAV-Human dataset and 3.2% improvement on the Drone Action dataset.
PB  - arXiv
PY  - 2023
ST  - AZTR
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icra48891.2023.10160564
ER  -


TY  - GEN
AU  - Guresti, B.
AU  - Vanlioglu, A.
AU  - Ure, N.K.
TI  - IQ-Flow: Mechanism Design for Inducing Cooperative Behavior to Self-Interested Agents in Sequential Social Dilemmas
AB  - Achieving and maintaining cooperation between agents to accomplish a common objective is one of the central goals of Multi-Agent Reinforcement Learning (MARL). Nevertheless in many real-world scenarios, separately trained and specialized agents are deployed into a shared environment, or the environment requires multiple objectives to be achieved by different coexisting parties. These variations among specialties and objectives are likely to cause mixed motives that eventually result in a social dilemma where all the parties are at a loss. In order to resolve this issue, we propose the Incentive Q-Flow (IQ-Flow) algorithm, which modifies the system's reward setup with an incentive regulator agent such that the cooperative policy also corresponds to the self-interested policy for the agents. Unlike the existing methods that learn to incentivize self-interested agents, IQ-Flow does not make any assumptions about agents' policies or learning algorithms, which enables the generalization of the developed framework to a wider array of applications. IQ-Flow performs an offline evaluation of the optimality of the learned policies using the data provided by other agents to determine cooperative and self-interested policies. Next, IQ-Flow uses meta-gradient learning to estimate how policy evaluation changes according to given incentives and modifies the incentive such that the greedy policy for cooperative objective and self-interested objective yield the same actions. We present the operational characteristics of IQ-Flow in Iterated Matrix Games. We demonstrate that IQ-Flow outperforms the state-of-the-art incentive design algorithm in Escape Room and 2-Player Cleanup environments. We further demonstrate that the pretrained IQ-Flow mechanism significantly outperforms the performance of the shared reward setup in the 2-Player Cleanup environment.
PB  - arXiv
PY  - 2023
ST  - IQ-Flow
Y2  - 2025/05/05/21:54:31
DO  - 10.4995/thesis/10251/90417
ER  -


TY  - GEN
AU  - Jiang, Y.
AU  - Chen, H.
AU  - Ko, H.
TI  - Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition
AB  - Recently, skeleton-based human action has become a hot research topic because the compact representation of human skeletons brings new blood to this research domain. As a result, researchers began to notice the importance of using RGB or other sensors to analyze human action by extracting skeleton information. Leveraging the rapid development of deep learning (DL), a significant number of skeleton-based human action approaches have been presented with fine-designed DL structures recently. However, a well-trained DL model always demands high-quality and sufficient data, which is hard to obtain without costing high expenses and human labor. In this paper, we introduce a novel data augmentation method for skeleton-based action recognition tasks, which can effectively generate high-quality and diverse sequential actions. In order to obtain natural and realistic action sequences, we propose denoising diffusion probabilistic models (DDPMs) that can generate a series of synthetic action sequences, and their generation process is precisely guided by a spatial-temporal transformer (ST-Trans). Experimental results show that our method outperforms the state-of-the-art (SOTA) motion generation approaches on different naturality and diversity metrics. It proves that its high-quality synthetic data can also be effectively deployed to existing action recognition models with significant performance improvement.
PB  - arXiv
PY  - 2023
ST  - Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cac53003.2021.9728206
ER  -


TY  - GEN
AU  - Liu, Z.
AU  - Wang, L.
AU  - Zhou, D.
AU  - Ding, E.
AU  - Fan, R.
TI  - Temporal Segment Transformer for Action Segmentation
AB  - Recognizing human actions from untrimmed videos is an important task in activity understanding, and poses unique challenges in modeling long-range temporal relations. Recent works adopt a predict-and-refine strategy which converts an initial prediction to action segments for global context modeling. However, the generated segment representations are often noisy and exhibit inaccurate segment boundaries, over-segmentation and other problems. To deal with these issues, we propose an attention based approach which we call temporal segment transformer, for joint segment relation modeling and denoising. The main idea is to denoise segment representations using attention between segment and frame representations, and also use inter-segment attention to capture temporal correlations between segments. The refined segment representations are used to predict action labels and adjust segment boundaries, and a final action segmentation is produced based on voting from segment masks. We show that this novel architecture achieves state-of-the-art accuracy on the popular 50Salads, GTEA and Breakfast benchmarks. We also conduct extensive ablations to demonstrate the effectiveness of different components of our design.
PB  - arXiv
PY  - 2023
ST  - Temporal Segment Transformer for Action Segmentation
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.imavis.2022.104567
ER  -


TY  - GEN
AU  - Luo, J.
AU  - Zhou, L.
AU  - Zhu, G.
AU  - Yang, B.
AU  - Wang, J.
TI  - TEMPORAL-CHANNEL TOPOLOGY ENHANCED NETWORK FOR SKELETON-BASED ACTION RECOGNITION
AB  - Skeleton-based action recognition has become popular in recent years due to its efficiency and robustness. Most current methods adopt graph convolutional network (GCN) for topology modeling, but GCN-based methods are limited in long-distance correlation modeling and generalizability. In contrast, the potential of convolutional neural network (CNN) for topology modeling has not been fully explored. In this paper, we propose a novel CNN architecture, Temporal-Channel Topology Enhanced Network (TCTE-Net), to learn spatial and temporal topologies for skeleton-based action recognition. The TCTE-Net consists of two modules: the Temporal-Channel Focus module, which learns a temporal-channel focus matrix to identify the most critical feature representations, and the Dynamic Channel Topology Attention module, which dynamically learns spatial topological features, and fuses them with an attention mechanism to model long-distance channel-wise topology. We conduct experiments on NTU RGB+D, NTU RGB+D 120, and FineGym datasets. TCTE-Net shows state-of-the-art performance compared to CNN-based methods and achieves superior performance compared to GCN-based methods. The code is available at https://github.com/aikuniverse/TCTENet.
PB  - arXiv
PY  - 2023
ST  - TEMPORAL-CHANNEL TOPOLOGY ENHANCED NETWORK FOR SKELETON-BASED ACTION RECOGNITION
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-981-99-8429-9_9
ER  -


TY  - GEN
AU  - Ramesh, S.
AU  - Dall’Alba, D.
AU  - Gonzalez, C.
AU  - Fiorini, P.
AU  - Padoy, N.
TI  - Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition
AB  - Automatic recognition of fine-grained surgical activities, called steps, is a challenging but crucial task for intelligent intra-operative computer assistance. The development of current vision-based activity recognition methods relies heavily on a high volume of manually annotated data. This data is difficult and time-consuming to generate and requires domain-specific knowledge. In this work, we propose to use coarser and easier-to-annotate activity labels, namely phases, as weak supervision to learn step recognition with fewer step annotated videos. We introduce a step-phase dependency loss to exploit the weak supervision signal. We then employ a Single-Stage Temporal Convolutional Network (SS-TCN) with a ResNet-50 backbone, trained in an end-to-end fashion from weakly annotated videos, for temporal activity segmentation and recognition. We extensively evaluate and show the effectiveness of the proposed method on a large video dataset consisting of 40 laparoscopic gastric bypass procedures and the public benchmark CATARACTS containing 50 cataract surgeries.
PB  - arXiv
PY  - 2023
ST  - Weakly Supervised Temporal Convolutional Networks for Fine-grained Surgical Activity Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tmi.2023.3262847
ER  -


TY  - GEN
AU  - Siddique, L.A.
AU  - Junhai, R.
AU  - Reza, T.
AU  - Khan, S.S.
AU  - Rahman, T.
TI  - Analysis of Real-Time Hostile Activitiy Detection from Spatiotemporal Features Using Time Distributed Deep CNNs, RNNs and Attention-Based Mechanisms
AB  - Real-time video surveillance, through CCTV camera systems has become essential for ensuring public safety which is a priority today. Although CCTV cameras help a lot in increasing security, these systems require constant human interaction and monitoring. To eradicate this issue, intelligent surveillance systems can be built using deep learning video classification techniques that can help us automate surveillance systems to detect violence as it happens. In this research, we explore deep learning video classification techniques to detect violence as they are happening. Traditional image classification techniques fall short when it comes to classifying videos as they attempt to classify each frame separately for which the predictions start to flicker. Therefore, many researchers are coming up with video classification techniques that consider spatiotemporal features while classifying. However, deploying these deep learning models with methods such as skeleton points obtained through pose estimation and optical flow obtained through depth sensors, are not always practical in an IoT environment. Although these techniques ensure a higher accuracy score, they are computationally heavier. Keeping these constraints in mind, we experimented with various video classification and action recognition techniques such as ConvLSTM, LRCN (with both custom CNN layers and VGG-16 as feature extractor) CNNTransformer and C3D. We achieved a test accuracy of 80% on ConvLSTM, 83.33% on CNN-BiLSTM, 70% on VGG16-BiLstm, 76.76% on CNN-Transformer and 80% on C3D.
PB  - arXiv
PY  - 2023
ST  - Analysis of Real-Time Hostile Activitiy Detection from Spatiotemporal Features Using Time Distributed Deep CNNs, RNNs and Attention-Based Mechanisms
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/ipas55744.2022.10053001
ER  -


TY  - GEN
AU  - Xiong, Z.
AU  - Liu, D.
AU  - Zhou, P.
AU  - Zhu, J.
TI  - TRACKING OBJECTS AND ACTIVITIES WITH ATTENTION FOR TEMPORAL SENTENCE GROUNDING
AB  - Temporal sentence grounding (TSG) aims to localize the temporal segment which is semantically aligned with a natural language query in an untrimmed video. Most existing methods extract frame-grained features or object-grained features by 3D ConvNet or detection network under a conventional TSG framework, failing to capture the subtle differences between frames or to model the spatio-temporal behavior of core persons/objects. In this paper, we introduce a new perspective to address the TSG task by tracking pivotal objects and activities to learn more fine-grained spatio-temporal behaviors. Specifically, we propose a novel Temporal Sentence Tracking Network (TSTNet), which contains (A) a Cross-modal Targets Generator to generate multi-modal templates and search space, filtering objects and activities, and (B) a Temporal Sentence Tracker to track multi-modal targets for modeling the targets' behavior and to predict query-related segment. Extensive experiments and comparisons with state-of-the-arts are conducted on challenging benchmarks: Charades-STA and TACoS. And our TSTNet achieves the leading performance with a considerable real-time speed.
PB  - arXiv
PY  - 2023
ST  - TRACKING OBJECTS AND ACTIVITIES WITH ATTENTION FOR TEMPORAL SENTENCE GROUNDING
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icassp49357.2023.10096206
ER  -


TY  - GEN
AU  - Zhong, W.
AU  - Zheng, M.
AU  - Tang, D.
AU  - Feng, X.
AU  - Qin, B.
TI  - STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training
AB  - Although large-scale video-language pre-training models, which usually build a global alignment between the video and the text, have achieved remarkable progress on various downstream tasks, the idea of adopting fine-grained information during the pre-training stage is not well explored. In this work, we propose STOA-VLP, a pre-training framework that jointly models object and action information across spatial and temporal dimensions. More specifically, the model regards object trajectories across frames and multiple action features from the video as fine-grained features. Besides, We design two auxiliary tasks to better incorporate both kinds of information into the pre-training process of the video-language model. The first is the dynamic object-text alignment task, which builds a better connection between object trajectories and the relevant noun tokens. The second is the spatial-temporal action set prediction, which guides the model to generate consistent action features by predicting actions found in the text. Extensive experiments on three downstream tasks (video captioning, text-video retrieval, and video question answering) demonstrate the effectiveness of our proposed STOA-VLP (e.g. 3.7 Rouge-L improvements on MSRVTT video captioning benchmark, 2.9% accuracy improvements on MSVD video question answering benchmark, compared to previous approaches).
PB  - arXiv
PY  - 2023
ST  - STOA-VLP
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v37i3.25483
ER  -


TY  - GEN
AU  - Zhou, Y.
AU  - Duan, H.
AU  - Rao, A.
AU  - Su, B.
AU  - Wang, J.
TI  - Self-supervised Action Representation Learning from Partial Spatio-Temporal Skeleton Sequences
AB  - Self-supervised learning has demonstrated remarkable capability in representation learning for skeleton-based action recognition. Existing methods mainly focus on applying global data augmentation to generate different views of the skeleton sequence for contrastive learning. However, due to the rich action clues in the skeleton sequences, existing methods may only take a global perspective to learn to discriminate different skeletons without thoroughly leveraging the local relationship between different skeleton joints and video frames, which is essential for real-world applications. In this work, we propose a Partial Spatio-Temporal Learning (PSTL) framework to exploit the local relationship from a partial skeleton sequences built by a unique spatio-temporal masking strategy. Specifically, we construct a negative-sample-free triplet steam structure that is composed of an anchor stream without any masking, a spatial masking stream with Central Spatial Masking (CSM), and a temporal masking stream with Motion Attention Temporal Masking (MATM). The feature cross-correlation matrix is measured between the anchor stream and the other two masking streams, respectively. (1) Central Spatial Masking discards selected joints from the feature calculation process, where the joints with a higher degree of centrality have a higher possibility of being selected. (2) Motion Attention Temporal Masking leverages the motion of action and remove frames that move faster with a higher possibility. Our method achieves state-of-the-art performance on NTURGB+D 60, NTURGB+D 120 and PKU-MMD under various downstream tasks. Furthermore, to simulate the real-world scenarios, a practical evaluation is performed where some skeleton joints are lost in downstream tasks. In contrast to previous methods that suffer from large performance drops, our PSTL can still achieve remarkable results under this challenging setting, validating the robustness of our method. Our code is available at https://github.com/YujieOuO/PSTL.git.
PB  - arXiv
PY  - 2023
ST  - Self-supervised Action Representation Learning from Partial Spatio-Temporal Skeleton Sequences
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v37i3.25495
ER  -


TY  - GEN
AU  - Wang, S.
AU  - Zhang, Y.
AU  - Qi, H.
AU  - Zhao, M.
AU  - Jiang, Y.
TI  - DYNAMIC SPATIAL-TEMPORAL HYPERGRAPH CONVOLUTIONAL NETWORK FOR SKELETON-BASED ACTION RECOGNITION
AB  - Skeleton-based action recognition relies on the extraction of spatial-temporal topological information. Hypergraphs can establish prior unnatural dependencies for the skeleton. However, the existing methods only focus on the construction of spatial topology and ignore the time-point dependence. This paper proposes a dynamic spatial-temporal hypergraph convolutional network (DST-HCN) to capture spatial-temporal information for skeleton-based action recognition. DST-HCN introduces a time-point hypergraph (TPH) to learn relationships at time points. With multiple spatial static hypergraphs and dynamic TPH, our network can learn more complete spatial-temporal features. In addition, we use the high-order information fusion module (HIF) to fuse spatial-temporal information synchronously. Extensive experiments on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets show that our model achieves state-of-the-art, especially compared with hypergraph methods.
PB  - arXiv
PY  - 2023
ST  - DYNAMIC SPATIAL-TEMPORAL HYPERGRAPH CONVOLUTIONAL NETWORK FOR SKELETON-BASED ACTION RECOGNITION
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icme55011.2023.00367
ER  -


TY  - GEN
AU  - Yang, J.
AU  - Dai, K.
TI  - YOWOv2: A Stronger yet Efficient Multi-level Detection Framework for Real-time Spatio-temporal Action Detection
AB  - Designing a real-time framework for the spatiotemporal action detection task is still a challenge. In this paper, we propose a novel real-time action detection framework, YOWOv2. In this new framework, YOWOv2 takes advantage of both the 3D backbone and 2D backbone for accurate action detection. A multi-level detection pipeline is designed to detect action instances of different scales. To achieve this goal, we carefully build a simple and efficient 2D backbone with a feature pyramid network to extract different levels of classification features and regression features. For the 3D backbone, we adopt the existing efficient 3D CNN to save development time. By combining 3D backbones and 2D backbones of different sizes, we design a YOWOv2 family including YOWOv2-Tiny, YOWOv2-Medium, and YOWOv2-Large. We also introduce the popular dynamic label assignment strategy and anchor-free mechanism to make the YOWOv2 consistent with the advanced model architecture design. With our improvement, YOWOv2 is significantly superior to YOWO, and can still keep real-time detection. Without any bells and whistles, YOWOv2 achieves 87.0% frame mAP and 52.8% video mAP with over 20 FPS on the UCF101-24. On the AVA, YOWOv2 achieves 21.7% frame mAP with over 20 FPS. Our code is available on https://github.com/yjh0410/YOWOv2.
PB  - arXiv
PY  - 2023
ST  - YOWOv2
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-981-96-0774-7_3
ER  -


TY  - GEN
AU  - Sabetta, Z.
AU  - Krishna, G.
AU  - Curry, T.
AU  - Adelson, P.D.
AU  - Thomas, T.C.
TI  - Aging with TBI vs. Aging: 6-month temporal profiles for neuropathology and astrocyte activation converge in behaviorally relevant thalamocortical circuitry of male and female rats
AB  - Traumatic brain injury (TBI) manifests late-onset and persisting clinical symptoms with implications for sex differences and increased risk for the development of age-related neurodegenerative diseases. Few studies have evaluated chronic temporal profiles of neuronal and glial pathology that include sex as a biological variable. After experimental diffuse TBI, late-onset and persisting somatosensory hypersensitivity to whisker stimulation develops at one-month post-injury and persists to at least two months post-injury in male rats, providing an in vivo model to evaluate the temporal profile of pathology responsible for morbidity. Whisker somatosensation is dependent on signaling through the thalamocortical relays of the whisker barrel circuit made up of glutamatergic projections between the ventral posteromedial nucleus of the thalamus (VPM) and primary somatosensory barrel cortex (S1BF) with inhibitory (GABA) innervation from the thalamic reticular nucleus (TRN) to the VPM. To evaluate the temporal profiles of pathology, male and female Sprague Dawley rats (n = 5-6/group) were subjected to sham surgery or midline fluid percussion injury (FPI). At 7-, 56-, and 168-days post-injury (DPI), brains were processed for amino-cupric silver stain and glial fibrillary acidic protein (GFAP) immunoreactivity, where pixel density of staining was quantified to determine the temporal profile of neuropathology and astrocyte activation in the VPM, S1BF, and TRN. FPI induced significant neuropathology in all brain regions at 7 DPI. At 168 DPI, neuropathology remained significantly elevated in the VPM and TRN, but returned to sham levels in the S1BF. GFAP immunoreactivity was increased as a function of FPI and DPI, with an FPI × DPI interaction in all regions and an FPI × Sex interaction in the S1BF. The interactions were driven by increased GFAP immunoreactivity in shams over time in the VPM and TRN. In the S1BF, GFAP immunoreactivity increased at 7 DPI and declined to age-matched sham levels by 168 DPI, while GFAP immunoreactivity in shams significantly increased between 7 and 168 days. The FPI × Sex interaction was driven by an overall greater level of GFAP immunoreactivity in FPI males compared to FPI females. Increased GFAP immunoreactivity was associated with an increased number of GFAP-positive soma, predominantly at 7 DPI. Overall, these findings indicate that FPI, time post-injury, sex, region, and aging with injury differentially contribute to chronic changes in neuronal pathology and astrocyte activation after diffuse brain injury. Thus, our results highlight distinct patterns of pathological alterations associated with the development and persistence of morbidity that supports chronic neuropathology, especially within the thalamus. Further, data indicate a convergence between TBI-induced and age-related pathology where further investigation may reveal a role for divergent astrocytic phenotypes associated with increased risk for neurodegenerative diseases.
PB  - bioRxiv
PY  - 2023
ST  - Aging with TBI vs. Aging
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.02.06.527058
ER  -


TY  - GEN
AU  - Xu, B.
AU  - Shu, X.
TI  - Spatiotemporal Decouple-and-Squeeze Contrastive Learning for Semi-Supervised Skeleton-based Action Recognition
AB  - Contrastive learning has been successfully leveraged to learn action representations for addressing the problem of semi-supervised skeleton-based action recognition. However, most contrastive learning-based methods only contrast global features mixing spatiotemporal information, which confuses the spatial- and temporal-specific information reflecting different semantic at the frame level and joint level. Thus, we propose a novel Spatiotemporal Decouple-and-Squeeze Contrastive Learning (SDS-CL) framework to comprehensively learn more abundant representations of skeleton-based actions by jointly contrasting spatial-squeezing features, temporal-squeezing features, and global features. In SDS-CL, we design a new Spatiotemporal-decoupling Intra-Inter Attention (SIIA) mechanism to obtain the spatiotemporal-decoupling attentive features for capturing spatiotemporal specific information by calculating spatial- and temporal-decoupling intra-attention maps among joint/motion features, as well as spatial- and temporal-decoupling inter-attention maps between joint and motion features. Moreover, we present a new Spatial-squeezing Temporal-contrasting Loss (STL), a new Temporal-squeezing Spatial-contrasting Loss (TSL), and the Global-contrasting Loss (GL) to contrast the spatial-squeezing joint and motion features at the frame level, temporal-squeezing joint and motion features at the joint level, as well as global joint and motion features at the skeleton level. Extensive experimental results on four public datasets show that the proposed SDS-CL achieves performance gains compared with other competitive methods.
PB  - arXiv
PY  - 2023
ST  - Spatiotemporal Decouple-and-Squeeze Contrastive Learning for Semi-Supervised Skeleton-based Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tnnls.2023.3247103
ER  -


TY  - GEN
AU  - Barberis, L.
AU  - Simien, C.
AU  - Marin, R.
AU  - Kembro, J.
TI  - Behavior dynamics at high temporal resolutions: the relevance of a right scale for sampling
AB  - Many species used in behavioral studies are small vertebrates with high metabolic rates and potentially enhanced temporal resolution of perception. Nevertheless, the temporal organization of behaviors at fast time scales (≤ 1s) has received little attention. Herein, we studied the temporal organization of behaviors at short time scales to gain insight into behavioral dynamics and to rethink how behavioral events are defined. We statistically explored high-resolution quail (Coturnix japonica) datasets encompassing 17 coarse- and fine-grained defined behaviors. We show that for the majority of these behaviors, events last predominately < 300ms and can be shorter than 70ms. Insufficient sampling resolution, even in the order of 1s, of behaviors that involve spatial displacement (e.g. walking) yields distorted probability distributions of event durations and overestimation of event durations. Contrarily, behaviors without spatial displacement (e.g. vigilance) maintain non-Gaussian, power-law-type distributions indicative of long-term memory, independently of the sampling resolution evaluated. Since data probability distributions reflect underlying biological processes, our results highlight the importance of quantification of behavioral dynamics based on the temporal scale pertinent to the species, and data distribution. We propose a hierarchical model that links diverse types of behavioral definitions and distributions, and paves the way towards a statistical framework for defining behaviors.
PB  - Research Square
PY  - 2023
ST  - Behavior dynamics at high temporal resolutions
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-2510699/v1
ER  -


TY  - GEN
AU  - Howard, R.
AU  - Kunze, L.
TI  - Evaluating Temporal Observation-Based Causal Discovery Techniques Applied to Road Driver Behaviour
AB  - Autonomous robots are required to reason about the behaviour of dynamic agents in their environment. The creation of models to describe these relationships is typically accomplished through the application of causal discovery techniques. However, as it stands observational causal discovery techniques struggle to adequately cope with conditions such as causal sparsity and non-stationarity typically seen during online usage in autonomous agent domains. Meanwhile, interventional techniques are not always feasible due to domain restrictions. In order to better explore the issues facing observational techniques and promote further discussion of these topics we carry out a benchmark across 10 contemporary observational temporal causal discovery methods in the domain of autonomous driving. By evaluating these methods upon causal scenes drawn from real world datasets in addition to those generated synthetically we highlight where improvements need to be made in order to facilitate the application of causal discovery techniques to the aforementioned use-cases. Finally, we discuss potential directions for future work that could help better tackle the difficulties currently experienced by state of the art techniques.
PB  - arXiv
PY  - 2023
ST  - Evaluating Temporal Observation-Based Causal Discovery Techniques Applied to Road Driver Behaviour
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iv55152.2023.10186705
ER  -


TY  - GEN
AU  - Keisham, K.
AU  - Jalali, A.
AU  - Lee, M.
TI  - Multi-Level Alignment for Few-Shot Temporal Action Localization
AB  - Temporal action localization (TAL) which aims to localize actions occurring in a long untrimmed video, requires a large number of annotated training data. However, in real-life applications, it is very expensive to obtain segment-level annotations for large-scale datasets and there also exists an incomprehensible number of action classes that are not practical. To overcome this challenge, we present a novel few-shot learning method that localizes temporal action for previously unseen novel classes with only a few training samples. Unlike previous methods that do not exploit the alignment of visual information at each temporal location, we propose a novel multi-level encoder cosine-similarity alignment module that implicitly learns the spatiotemporal context alignment for long untrimmed videos. Towards this objective, our proposed method adopts an episodic-based training scheme to learn the alignment of similar video snippets between videos belonging to the same class with few training examples. At test time, this learned aligned context information is then adapted to novel unseen classes. Experimental results on two standard datasets ActivityNet1.3 and THUMOS-14 show that our proposed method outperforms other state-of-the-art methods for few-shot temporal action localization with single and multiple action instances on the ActivityNet-1.3 dataset and achieves competitive results on the THUMOS-14 dataset.
PB  - SSRN
PY  - 2023
ST  - Multi-Level Alignment for Few-Shot Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.ins.2023.119618
ER  -


TY  - GEN
AU  - Li, Y.
AU  - Wu, J.
AU  - Fang, A.
AU  - Li, W.
TI  - A Dual Pipeline with Spatio-Temporal Attention Fusion Approach for Human Activity Recognition
AB  - Sensor-based Human Activity Recognition (SHAR) has gained more attention due to the rapid development of the Internet of Things (IoT). The critical issue for SHAR is rescuing the performance bottleneck from expensive feature engineering. Recent works explore this problem by combining deep learning with SHAR, which leads to a critical challenge in designing a suitable model structure to learn an informative representation. In this work, we introduce a dual pipeline with a spatio-temporal attention fusion approach, termed the ST-attention dual pipeline, to address this problem. The ST-attention dual pipeline is a hybrid neural network with an attention mechanism. It utilizes each pipeline to capture long-term dependencies and hierarchical information representations, fusing them by the ST-attention generated across spatial and temporal dimensions to improve presentation capabilities. Extensive experiments conducted on three benchmark datasets, e.g., OPPORTUNITY, PAMAP2, and USC-HAD, demonstrate the superiority of our ST-attention dual pipeline. We also evaluate the performance of our spatio-temporal attention mechanism over other fusion methods.
PB  - SSRN
PY  - 2023
ST  - A Dual Pipeline with Spatio-Temporal Attention Fusion Approach for Human Activity Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4341183
ER  -


TY  - GEN
AU  - Cleland, L.D.
AU  - Rowland, H.M.
AU  - Mazzà, C.
AU  - Saal, H.P.
TI  - Complexity of spatiotemporal plantar pressure patterns during everyday behaviours
AB  - The human foot sole is the primary interface with the external world during balance and walking, and also provides important tactile information on the state of contact. However, prior studies on plantar pressure have focused mostly on summary metrics such as overall force or centre of pressure under limited conditions. Here, we recorded spatiotemporal plantar pressure patterns with high spatial resolution while participants completed a wide range of daily activities, including balancing, locomotion, and jumping tasks. Contact area differed across task categories, but was only moderately correlated with the overall force experienced by the foot sole. The centre of pressure was often located outside the contact area or in locations experiencing relatively low pressure, and therefore a result of disparate contact regions spread widely across the foot. Non-negative matrix factorisation revealed low-dimensional spatial complexity that increased during interaction with unstable surfaces. Additionally, pressure patterns at the heel and metatarsals decomposed into separately located and robustly identifiable components, jointly capturing most variance in the signal. These results suggest optimal sensor placements to capture task-relevant spatial information and provide insight into how pressure varies spatially on the foot sole during a wide variety of natural behaviours.
PB  - bioRxiv
PY  - 2023
ST  - Complexity of spatiotemporal plantar pressure patterns during everyday behaviours
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.01.27.525870
ER  -


TY  - GEN
AU  - Pang, C.
AU  - Lu, X.
AU  - Lyu, L.
TI  - Skeleton-based Action Recognition through Contrasting Two-Stream Spatial-Temporal Networks
AB  - For pursuing accurate skeleton-based action recognition, most prior methods use the strategy of combining Graph Convolution Networks (GCNs) with attention-based methods in a serial way. However, they regard the human skeleton as a complete graph, resulting in less variations between different actions (e.g., the connection between the elbow and head in action “clapping hands”). For this, we propose a novel Contrastive GCN-Transformer Network (ConGT) which fuses the spatial and temporal modules in a parallel way. The ConGT involves two parallel streams: Spatial-Temporal Graph Convolution stream (STG) and Spatial-Temporal Transformer stream (STT). The STG is designed to obtain action representations maintaining the natural topology structure of the human skeleton. The STT is devised to acquire action representations containing the global relationships among joints. Since the action representations produced from these two streams contain different characteristics, and each of them knows little information of the other, we introduce the contrastive learning paradigm to guide their output representations of the same sample to be as close as possible in a self-supervised manner. Through the contrastive learning, they can learn information from each other to enrich the action features by maximizing the mutual information between the two types of action representations. To further improve action recognition accuracy, we introduce the Cyclical Focal Loss (CFL) which can focus on confident training samples in early training epochs, with an increasing focus on hard samples during the middle epochs. We conduct experiments on three benchmark datasets, which demonstrate that our model achieves state-of-the-art performance in action recognition.
PB  - arXiv
PY  - 2023
ST  - Skeleton-based Action Recognition through Contrasting Two-Stream Spatial-Temporal Networks
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tmm.2023.3239751
ER  -


TY  - GEN
AU  - Guo, J.
AU  - Lin, L.
AU  - Aqeel, M.M.
AU  - Richards, E.A.
AU  - Delp, E.J.
TI  - Joint Temporal Patterns By Integrating Diet and Physical Activity
AB  - Both diet and physical activity are associated with obesity and chronic diseases such as diabetes and metabolic syndrome. Early efforts in connecting dietary and physical activity behaviors to generate patterns rarely considered the use of time. In this paper, we propose a distance-based cluster analysis approach to find joint temporal diet and physical activity patterns among U.S. adults ages 20-65. Dynamic Time Warping (DTW) generalized to multi-dimensions is combined with commonly used clustering methods to generate unbiased partitioning of the National Health and Nutrition Examination Survey 2003-2006 (NHANES) dataset. The clustering results are evaluated using visualization of the clusters, the Silhouette Index, and the associations between clusters and health status indicators based on multivariate regression models. Our experiments indicate that the integration of diet, physical activity, and time has the potential to discover joint temporal patterns with association to health.
PB  - medRxiv
PY  - 2023
ST  - Joint Temporal Patterns By Integrating Diet and Physical Activity
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.01.23.23284780
ER  -


TY  - GEN
AU  - Guo, J.
AU  - Aqeel, M.M.
AU  - Lin, L.
AU  - Richards, E.A.
AU  - Delp, E.J.
TI  - Cluster Analysis to Find Temporal Physical Activity Patterns Among US Adults
AB  - Physical activity (PA) is known to be a risk factor for obesity and chronic diseases such as diabetes and metabolic syndrome. Few attempts have been made to pattern the time of physical activity while incorporating intensity and duration in order to determine the relationship of this multi-faceted behavior with health. In this paper, we explore a distance-based approach for clustering daily physical activity time series to estimate temporal physical activity patterns among U.S. adults (ages 20-65) from the National Health and Nutrition Examination Survey 2003-2006 (NHANES). A number of distance measures and distance-based clustering methods were investigated and compared using various metrics. These metrics include the Silhouette and the Dunn Index (internal criteria), and the associations of the clusters with health status indicators (external criteria). Our experiments indicate that using a distance-based cluster analysis approach to estimate temporal physical activity patterns through the day, has the potential to describe the complexity of behavior rather than characterizing physical activity patterns solely by sums or labels of maximum activity levels.
PB  - medRxiv
PY  - 2023
ST  - Cluster Analysis to Find Temporal Physical Activity Patterns Among US Adults
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/ichi57859.2023.00038
ER  -


TY  - GEN
AU  - Song, D.
AU  - Zhu, B.
AU  - Zhao, J.
AU  - Han, J.
TI  - Human-Like Lane-Changing Spatiotemporal Target Decision Strategy Based on Driving Behavior Generation Mechanism
AB  - Human-like lane changing (LC) is important for the human-like driving of intelligent vehicles (IVs). In this study, we conducted semantic segmentation and deconstruction of an LC behavior generation mechanism and defined LC spatiotemporal targets, which are explicit LC behavioral features that can indirectly express the implicit generation mechanism. Then we constructed a human-like LC spatiotemporal target decision strategy (HLSTDS) based on driving behavior generation mechanism. HLSTDS unifies human-like LC decision-making and LC behavioral feature modeling for the same framework, realizes a feature-driven human-like LC spatiotemporal target decision, and provides a guarantee for human-like LC of IVs. Specifically, we designed a strategy framework for HLSTDS that matches the generation mechanism of LC behavior and designed reward functions that match the cognitive and behavioral characteristics of human drivers. Under the guidance of LC spatiotemporal targets, the discrete expected trajectory space is opened, and the resizing and pruning of the expected trajectory space are carried out based on statistics and safety constraints. Thus, the sampling efficiency and safety of HLSTDS are improved, while the dimension disaster is avoided. In HLSTDS, different reward functions and expected trajectory spaces are set for anticipation and relaxation, which are two different LC stages. the authenticity of the interaction between the HLSTDS and the environment is ensured by a traffic vehicle trajectory prediction. The weights of the human-like reward functions are recovered for HLSTDS from the HighD dataset by maximum entropy inverse reinforcement learning. Finally, the results of a verification test prove that the HLSTDS can accurately determine the LC spatiotemporal targets for different vehicle states and traffic situations. Additionally, the results of a comparative verification with other methods and a generalization verification on the NGSIM dataset reveal the advantages of HLSTDS for LC spatiotemporal target decision accuracy and generalization ability.
PB  - SSRN
PY  - 2023
ST  - Human-Like Lane-Changing Spatiotemporal Target Decision Strategy Based on Driving Behavior Generation Mechanism
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4330303
ER  -


TY  - GEN
AU  - Khatibi, V.A.
AU  - Salimi, M.
AU  - Rahdar, M.
AU  - Behzadi, G.
AU  - Janahmadi, M.
TI  - The electrophysiological signature of dorsal hippocampus-basolateral amygdala circuit in anxiety-like behavior in the intrahippocampal kainic acid mice model of temporal lobe epilepsy: With emphasis on the impact of glycolysis inhibition
AB  - Pharmacoresistant temporal lobe epilepsy affects millions of people around the world with uncontrolled seizures and comorbidities, like anxiety, being the most problematic aspects calling for novel therapeutic procedures. The intrahippocampal kainic acid model of temporal lobe epilepsy is an appropriate rodent model to evaluate the effects of novel interventions, including glycolysis inhibition, on epilepsy-induced alterations. Here, we investigated kainic acid-induced changes in the dorsal hippocampus (dHPC) and basolateral amygdala (BLA) circuit and the efficiency of a glycolysis inhibitor, 2-deoxy D-glucose (2-DG), in resetting such alterations using simultaneous LFP recording and elevated zero-maze test. dHPC theta and gamma powers were lower in epileptic groups, both in the baseline and anxiogenic conditions. BLA theta power was higher in baseline condition while it was lower in anxiogenic condition in epileptic animals and 2-DG could reverse it. dHPC-BLA coherence was altered only in anxiogenic condition and 2-DG could reverse it only in gamma frequency. This coherence was significantly correlated with the time in which the animals exposed themselves to the anxiogenic condition. Further, theta-gamma phase-locking was lower in epileptic groups in the dHPC-BLA circuit and 2-DG could considerably increase it.
PB  - Research Square
PY  - 2023
ST  - The electrophysiological signature of dorsal hippocampus-basolateral amygdala circuit in anxiety-like behavior in the intrahippocampal kainic acid mice model of temporal lobe epilepsy
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-2362215/v1
ER  -


TY  - GEN
AU  - Amaris, G.
AU  - Vesely, S.
AU  - Hess, S.
AU  - Klöckner, C.A.
TI  - Can Competing Demands Affect Pro-Environmental Behaviour: A Study of the Impact of Exposure to Partly Related Sequential Experiments
AB  - The study of human behaviour is central to the development of appropriate policies for sustainability. We argue that mathematical models of human choice behaviour may produce biased results if they fail to account for the possibility of spillover effects, in particular the possibility that individual behaviour may change as a result of competing demands, such as in the sequential exposure to partly related choice contexts. Using a sample of 751 individuals and a carefully constructed experiment, we develop mathematical models that jointly explain the choice between different pro-environmental actions and the willingness to donate money for environmental causes. We find that the strength of preferences for behavioural changes leading to greater CO2 reductions is (causally) shaped by participants previously considering other similar behavioural changes. The kind of spillover effects we find are relatively complex and often subtle, and thus warrant further replication studies. Our study demonstrates that choices can be influenced by the type of scenarios, the type of information that is displayed, and the order of the experiments which helped us identify a type of spillover effects.
PB  - SSRN
PY  - 2023
ST  - Can Competing Demands Affect Pro-Environmental Behaviour
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.ecolecon.2023.108023
ER  -


TY  - GEN
AU  - Xing, J.
AU  - Wang, M.
AU  - Liu, Y.
AU  - Mu, B.
TI  - Revisiting the Spatial and Temporal Modeling for Few-Shot Action Recognition
AB  - Spatial and temporal modeling is one of the most core aspects of few-shot action recognition. Most previous works mainly focus on long-term temporal relation modeling based on high-level spatial representations, without considering the crucial low-level spatial features and short-term temporal relations. Actually, the former feature could bring rich local semantic information, and the latter feature could represent motion characteristics of adjacent frames, respectively. In this paper, we propose SloshNet, a new framework that revisits the spatial and temporal modeling for few-shot action recognition in a finer manner. First, to exploit the low-level spatial features, we design a feature fusion architecture search module to automatically search for the best combination of the low-level and high-level spatial features. Next, inspired by the recent transformer, we introduce a long-term temporal modeling module to model the global temporal relations based on the extracted spatial appearance features. Meanwhile, we design another short-term temporal modeling module to encode the motion characteristics between adjacent frame representations. After that, the final predictions can be obtained by feeding the embedded rich spatial-temporal features to a common frame-level class prototype matcher. We extensively validate the proposed SloshNet on four few-shot action recognition datasets, including Something-Something V2, Kinetics, UCF101, and HMDB51. It achieves favorable results against state-of-the-art methods in all datasets.
PB  - arXiv
PY  - 2023
ST  - Revisiting the Spatial and Temporal Modeling for Few-Shot Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v37i3.25403
ER  -


TY  - GEN
AU  - Tardelli, S.
AU  - Nizzoli, L.
AU  - Tesconi, M.
AU  - Da San Martino, G.
AU  - Cresci, S.
TI  - Temporal Dynamics of Coordinated Online Behavior: Stability, Archetypes, and Influence
AB  - Large-scale online campaigns, malicious or otherwise, require a significant degree of coordination among participants, which sparked interest in the study of coordinated online behavior. State-of-the-art methods for detecting coordinated behavior perform static analyses, disregarding the temporal dynamics of coordination. Here, we carry out the first dynamic analysis of coordinated behavior. To reach our goal we build a multiplex temporal network and we perform dynamic community detection to identify groups of users that exhibited coordinated behaviors in time. We find that: (i) coordinated communities feature variable degrees of temporal instability; (ii) dynamic analyses are needed to account for such instability, and results of static analyses can be unreliable and scarcely representative of unstable communities; (iii) some users exhibit distinct archetypal behaviors that have important practical implications; (iv) content and network characteristics contribute to explaining why users leave and join coordinated communities. Our results demonstrate the advantages of dynamic analyses and open up new directions of research on the unfolding of online debates, on the strategies of coordinated communities, and on the patterns of online influence.
PB  - arXiv
PY  - 2023
ST  - Temporal Dynamics of Coordinated Online Behavior
Y2  - 2025/05/05/21:54:31
DO  - 10.1073/pnas.2307038121
ER  -


TY  - GEN
AU  - Zahan, S.
AU  - Hassan, G.M.
AU  - Mian, A.
TI  - Learning Sparse Temporal Video Mapping for Action Quality Assessment in Floor Gymnastics
AB  - Athlete performance measurement in sports videos requires modeling long sequences since the entire spatio-temporal progression contributes dominantly to the performance. It is crucial to comprehend local discriminative spatial dependencies and global semantics for accurate evaluation. However, existing benchmark datasets mainly incorporate sports where the performance lasts only a few seconds. Consequently, state-of-the-art sports quality assessment methods specifically focus on spatial structure. Although they achieve high performance in short-term sports, they are unable to model prolonged video sequences and fail to achieve similar performance in long-term sports. To facilitate such analysis, we introduce a new dataset, coined AGF-Olympics, that incorporates artistic gymnastic floor routines. AFG-Olympics provides highly challenging scenarios with extensive background, viewpoint, and scale variations over an extended sample duration of up to 2 minutes. In addition, we propose a discriminative attention module to map the dense feature space into a sparse representation by disentangling complex associations. Extensive experiments indicate that our proposed module provides an effective way to embed long-range spatial and temporal correlation semantics.
PB  - arXiv
PY  - 2023
ST  - Learning Sparse Temporal Video Mapping for Action Quality Assessment in Floor Gymnastics
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tim.2024.3398072
ER  -


TY  - GEN
AU  - Peng, T.-Q.
AU  - Zhu, J.J.H.
TI  - Understanding Online Behaviors through a Temporal Lens
AB  - Timestamps in digital traces include significant detailed information on when human behaviors occur, which is universally available and standardized in all types of digital traces. Nevertheless, the concept of time is under-explicated in empirical studies of online behaviors. This paper discusses the (un)desirable properties of timestamps in digital traces and summarizes how timestamps in digital traces have been utilized in existing studies of online behaviors. The paper argues that time-in-behaviors perspective can provide a microscope with a renovated temporal lens to observe and understand online behaviors. Going beyond the traditional behaviors-in-time perspective, time-in-behaviors perspective enables empirical examination of online behaviors from multiple units of analysis (e.g., discrete behaviors, behavioral sessions, and behavioral trajectories) and from multiple dimensions (e.g., duration, order, transition, rhythm). The paper shows the potentials of the time-in-behaviors perspective with several empirical cases and proposes future directions in explicating the concept of time in computational social science.
PB  - arXiv
PY  - 2023
ST  - Understanding Online Behaviors through a Temporal Lens
Y2  - 2025/05/05/21:54:31
DO  - 10.4018/jcmam.2011070104
ER  -


TY  - GEN
AU  - O'Neill, G.C.
AU  - Mellor, S.
AU  - Seymour, R.A.
AU  - Maguire, E.A.
AU  - Barnes, G.R.
TI  - Is high-frequency activity evidence of an anterior temporal lobe network or micro-saccades?
AB  - There is renewed interest in electrical activity that extends beyond the typical electrophysiological 100 Hz bandwidth. This activity, often in the anterior temporal lobe, has been attributed to processes ranging from memory consolidation to epileptiform activity. Here, using an open-access resting state magnetoencephalography (MEG) dataset (n = 89), and a second task-based MEG dataset, we could reliably localise high-frequency power to the temporal lobes across multiple bands up to 300-400 Hz. A functional connectivity analysis of this activity revealed a robust resting state bilateral network between the temporal lobes. However, we also found robust coherence in the 100-200 and 200-300 Hz bands between source reconstructed MEG data and the electrooculography (EOG) localised to within the temporal poles. Additional denoising schemes applied to the data could reduce power localisation to the temporal poles but the topography of the functional network did not drastically alter. Whilst it is clear that this network is biological and robust to established denoising methods, we cannot definitively rule yet on whether this is of neural or myogenic origin.
PB  - bioRxiv
PY  - 2023
ST  - Is high-frequency activity evidence of an anterior temporal lobe network or micro-saccades?
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.01.09.523285
ER  -


TY  - GEN
AU  - Wang, X.
AU  - Zhang, S.
AU  - Qing, Z.
AU  - Jin, R.
AU  - Sang, N.
TI  - HyRSM++: Hybrid Relation Guided Temporal Set Matching for Few-shot Action Recognition
AB  - Few-shot action recognition is a challenging but practical problem aiming to learn a model that can be easily adapted to identify new action categories with only a few labeled samples. Recent attempts mainly focus on learning deep representations for each video individually under the episodic meta-learning regime and then performing temporal alignment to match query and support videos. However, they still suffer from two drawbacks: (i) learning individual features without considering the entire task may result in limited representation capability, and (ii) existing alignment strategies are sensitive to noises and misaligned instances. To handle the two limitations, we propose a novel Hybrid Relation guided temporal Set Matching (HyRSM++) approach for few-shot action recognition. The core idea of HyRSM++ is to integrate all videos within the task to learn discriminative representations and involve a robust matching technique. To be specific, HyRSM++ consists of two key components, a hybrid relation module and a temporal set matching metric. Given the basic representations from the feature extractor, the hybrid relation module is introduced to fully exploit associated relations within and cross videos in an episodic task and thus can learn task-specific embeddings. Subsequently, in the temporal set matching metric, we carry out the distance measure between query and support videos from a set matching perspective and design a bidirectional Mean Hausdorff Metric to improve the resilience to misaligned instances. In addition, we explicitly exploit the temporal coherence in videos to regularize the matching process. In this way, HyRSM++ facilitates informative correlation exchanged among videos and enables flexible predictions under the data-limited scenario. Furthermore, we extend the proposed HyRSM++ to deal with the more challenging semi-supervised few-shot action recognition and unsupervised few-shot action recognition tasks. Experimental results on multiple benchmarks demonstrate that our method consistently outperforms existing methods and achieves state-of-the-art performance under various few-shot settings. The source code is available at https://github.com/alibaba-mmai-research/HyRSMPlusPlus.
PB  - arXiv
PY  - 2023
ST  - HyRSM++
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.patcog.2023.110110
ER  -


TY  - GEN
AU  - Li, M.
AU  - Xu, X.
AU  - Fan, H.
AU  - Shou, M.Z.
AU  - Yan, S.
TI  - STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition
AB  - Existing methods of privacy-preserving action recognition (PPAR) mainly focus on frame-level (spatial) privacy removal through 2D CNNs. Unfortunately, they have two major drawbacks. First, they may compromise temporal dynamics in input videos, which are critical for accurate action recognition. Second, they are vulnerable to practical attacking scenarios where attackers probe for privacy from an entire video rather than individual frames. To address these issues, we propose a novel framework STPrivacy to perform video-level PPAR. For the first time, we introduce vision Transformers into PPAR by treating a video as a tubelet sequence, and accordingly design two complementary mechanisms, i.e., sparsification and anonymization, to remove privacy from a spatio-temporal perspective. In specific, our privacy sparsification mechanism applies adaptive token selection to abandon action-irrelevant tubelets. Then, our anonymization mechanism implicitly manipulates the remaining action-tubelets to erase privacy in the embedding space through adversarial learning. These mechanisms provide significant advantages in terms of privacy preservation for human eyes and action-privacy trade-off adjustment during deployment. We additionally contribute the first two large-scale PPAR benchmarks, VP-HMDB51 and VP-UCF101, to the community. Extensive evaluations on them, as well as two other tasks, validate the effectiveness and generalization capability of our framework.
PB  - arXiv
PY  - 2023
ST  - STPrivacy
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccv51070.2023.00471
ER  -


TY  - GEN
AU  - Mastrovito, D.
AU  - Hanson, C.
AU  - Hanson, S.
TI  - Temporal Dynamics of Activity in Default Mode Network Suggest a Role in Top-Down Processing for Trial Responses
AB  - The default mode network (DMN) is a collection of brain regions including midline frontal and parietal structures, medial and lateral temporal lobes, and lateral parietal cortex. Although there is evidence that the network can be subdivided into at least two subcomponents, the network reliably exhibits highly correlated activity both at rest and during task performance. Current understanding regarding the function of the DMN rests on a large body of research indicating that activity in the network decreases during task epochs of experimental paradigms relative to inter-trial intervals. A seeming contradiction arises when the experimental paradigm includes tasks involving autobiographical memory, thinking about one's self, planning for the future, or social cognition. In such cases, the DMN's activity increases and is correlated with attentional networks. Some have therefore concluded that the DMN supports advanced human cognitive abilities such as interoceptive processing and theory of mind. This conclusion may be called into question by evidence of correlated activity in homologous brain regions in other, even non-primate, species. Thus, there are contradictory findings related to the function of the DMN that have been difficult to integrate into a coherent theory regarding its function. Using data from the Human Connectome Project, we explore the temporal dynamics of activity in different regions of the DMN in relation to stimulus presentation. We show that generally the dorsal portion of the network exhibits only a transient initial decrease in activity at the start of trials that increases over trial duration. The ventral component often has more similarity in its time course to that of task-activated areas. We propose that task-associated ramping dynamics in the network are incompatible with a task-negative view of the DMN and propose the dorsal and ventral sub-components of network may rather work together to support bottom-up salience detection and subsequent top-down voluntary action. In this context, we re-interpret the body of anatomical and neurophysiological experimental evidence, arguing that this interpretation can accommodate the seeming contradictions regarding DMN function in the extant literature.
PB  - bioRxiv
PY  - 2023
ST  - Temporal Dynamics of Activity in Default Mode Network Suggest a Role in Top-Down Processing for Trial Responses
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.01.08.523152
ER  -


TY  - GEN
AU  - Khalil, K.
AU  - Lanza, V.
AU  - Manceau, D.
AU  - Aziz-Alaoui, M.A.
AU  - Provitolo, D.
TI  - ANALYSIS OF A SPATIO-TEMPORAL ADVECTION-DIFFUSION MODEL FOR HUMAN BEHAVIORS DURING A CATASTROPHIC EVENT
AB  - In this work, using the theory of first-order macroscopic crowd models, we introduce a compartmental advection-diffusion model, describing the spatio-temporal dynamics of a population in different human behaviors (alert, panic and control) during a catastrophic event. For this model, we prove the local existence, uniqueness and regularity of a solution, as well as the positivity and L1-boundedness of this solution. Then, in order to study the spatio-temporal propagation of these behavioral reactions within a population during a catastrophic event, we present several numerical simulations for different evacuation scenarios.
PB  - arXiv
PY  - 2023
ST  - ANALYSIS OF A SPATIO-TEMPORAL ADVECTION-DIFFUSION MODEL FOR HUMAN BEHAVIORS DURING A CATASTROPHIC EVENT
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4399373
ER  -


TY  - GEN
AU  - Whitmarsh, S.
AU  - Nguyen-Michel, V.-H.
AU  - Lehongre, K.
AU  - Frazzini, V.
AU  - Navarro, V.
TI  - Sleep increases firing rate modulation during interictal epileptic activities in mesial temporal structures
AB  - Epileptic seizures and interictal epileptiform discharges (IEDs) are strongly influenced by sleep and circadian rhythms. However, human data on the effect of sleep on neuronal behavior during interictal activity have been lacking. We analyzed EEG data from epileptic patients implanted with macro and micro electrodes targeting mesial temporal structures. Sleep staging was performed on concomitantly recorded polysomnography and video-EEG. Automated IED detection identified thousands of IEDs per patient. Both the rate and amplitude of IEDs were increased with deeper stages of NREM sleep. Single unit activity (SUA) and multi-unit activity (MUA) increased their firing during the IED spike, and strongly decreased during the subsequent slow wave. These time-locked firing rate modulations were shown to increase during deeper stages of NREM sleep. Finally, during resting behaviour, neuronal firing rate, bursting rate and firing regularity were all shown to progressively decrease with deeper stages of NREM sleep.
PB  - bioRxiv
PY  - 2022
ST  - Sleep increases firing rate modulation during interictal epileptic activities in mesial temporal structures
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.12.30.522096
ER  -


TY  - GEN
AU  - Shabaninia, E.
AU  - Nezamabadi-Pour, H.
AU  - Shafizadegan, F.
TI  - Transformers in Action Recognition: A Review on Temporal Modeling
AB  - In vision-based action recognition, spatio-temporal features from different modalities are used for recognizing activities. Temporal modeling is a long challenge of action recognition. However, there are limited methods such as pre-computed motion features, three-dimensional (3D) filters, and recurrent neural networks (RNN) for modeling motion information in deep-based approaches. Recently, transformers’ success in modeling long-range dependencies in natural language processing (NLP) tasks has gotten great attention from other domains; including speech, image, and video, to rely entirely on self-attention without using sequence-aligned RNNs or convolutions. Although the application of transformers to action recognition is relatively new, the amount of research proposed on this topic within the last few years is astounding. This paper especially reviews recent progress in deep learning methods for modeling temporal variations. It focuses on action recognition methods that use transformers for temporal modeling, discussing their main features, used modalities, and identifying opportunities and challenges for future research.
PB  - arXiv
PY  - 2022
ST  - Transformers in Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icip49359.2023.10222886
ER  -


TY  - GEN
AU  - Han, Z.
AU  - Liu, L.
AU  - Wang, X.
AU  - Tang, S.
AU  - Zheng, Z.
TI  - Probabilistic activity driven model of temporal simplicial networks and its application on higher-order dynamics
AB  - Network modeling characterizes the underlying principles of structural properties and is of vital significance for simulating dynamical processes in real world. However, bridging structure and dynamics is always challenging due to the multiple complexities in real systems. Here, through introducing the individual’s activity rate and the possibility of group interaction, we propose a probabilistic activity driven (PAD) model that could generate temporal higher-order networks with both power-law and high-clustering characteristics, which successfully links the two most critical structural features and a basic dynamical pattern in extensive complex systems. Surprisingly, the power-law exponents and the clustering coefficients of the aggregated PAD network could be tuned in a wide range by altering a set of model parameters. We further provide an approximation algorithm to select the proper parameters that can generate networks with given structural properties, the effectiveness of which is verified by fitting various real-world networks. Lastly, we explore the co-evolution of PAD model and higher-order contagion dynamics, and analytically derive the critical conditions for phase transition and bistable phenomenon. Our model provides a basic tool to reproduce complex structural properties and to study the widespread higher-order dynamics, which has great potential for applications across fields.
PB  - arXiv
PY  - 2022
ST  - Probabilistic activity driven model of temporal simplicial networks and its application on higher-order dynamics
Y2  - 2025/05/05/21:54:31
DO  - 10.1063/5.0167123
ER  -


TY  - GEN
AU  - Yao, X.
AU  - Li, H.
AU  - Qian, Z.
AU  - Gu, M.
AU  - Wang, P.
TI  - Strn: Brain-Inspired Spatiotemporal Relation Network for Video Action Recognition
AB  - Most existing models use independent frames as processing units to build model, which is difficult to understand the action based on the change of visual features due to the lack of consideration of spatiotemporal correlation, ignoring the role of object activity trajectories for action analysis. To solve this problem, a brain-inspired spatiotemporal relation network (STRN) is proposed considering the biological mechanisms of ventral visual stream. STRN presents bionic filters and snapshot neuron modules to simulate V1 and MT area cells in the form pathway, and accuracy is improved by enhancing spatial correlation and temporally correlation. We evaluate the effectiveness of our method on public benchmarks(something-something v2, jester, and moments dataset). Experimental results show that the STRN proposed in this paper outperforms general state-of-the-art methods and achieves better robustness and generalization performance.
PB  - SSRN
PY  - 2022
ST  - Strn
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4305462
ER  -


TY  - GEN
AU  - Rathod, V.
AU  - Seybold, B.
AU  - Vijayanarasimhan, S.
AU  - Birodkar, V.
AU  - Ross, D.A.
TI  - Open-Vocabulary Temporal Action Detection with Off-the-Shelf Image-Text Features
AB  - Detecting actions in untrimmed videos should not be limited to a small, closed set of classes. We present a simple, yet effective strategy for open-vocabulary temporal action detection utilizing pretrained image-text co-embeddings. Despite being trained on static images rather than videos, we show that image-text co-embeddings enable open-vocabulary performance competitive with fully-supervised models. We show that the performance can be further improved by ensembling the image-text features with features encoding local motion, like optical flow based features, or other modalities, like audio. In addition, we propose a more reasonable open-vocabulary evaluation setting for the ActivityNet data set, where the category splits are based on similarity rather than random assignment.
PB  - arXiv
PY  - 2022
ST  - Open-Vocabulary Temporal Action Detection with Off-the-Shelf Image-Text Features
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/fg59268.2024.10581896
ER  -


TY  - GEN
AU  - Singhania, D.
AU  - Rahaman, R.
AU  - Yao, A.
TI  - C2F-TCN: A Framework for Semi and Fully Supervised Temporal Action Segmentation
AB  - Temporal action segmentation tags action labels for every frame in an input untrimmed video containing multiple actions in a sequence. For the task of temporal action segmentation, we propose an encoder-decoder style architecture named C2F-TCN featuring a “coarse-to-fine” ensemble of decoder outputs. The C2F-TCN framework is enhanced with a novel model agnostic temporal feature augmentation strategy formed by the computationally inexpensive strategy of the stochastic max-pooling of segments. It produces more accurate and well-calibrated supervised results on three benchmark action segmentation datasets. We show that the architecture is flexible for both supervised and representation learning. In line with this, we present a novel unsupervised way to learn frame-wise representation from C2F-TCN. Our unsupervised learning approach hinges on the clustering capabilities of the input features and the formation of multi-resolution features from the decoder’s implicit structure. Further, we provide first semi-supervised temporal action segmentation results by merging representation learning with conventional supervised learning. Our semi-supervised learning scheme, called “Iterative-Contrastive-Classify (ICC)”, progressively improves in performance with more labeled data. The ICC semi-supervised learning in C2F-TCN, with 40% labeled videos, performs similar to fully supervised counterparts.
PB  - arXiv
PY  - 2022
ST  - C2F-TCN
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tpami.2023.3284080
ER  -


TY  - GEN
AU  - Linssen, H.
AU  - de Knegt, H.J.
AU  - Eikelboom, J.A.J.
TI  - Unveiling the intertwined roles of the spatial-temporal environment and behavioural modes in animal movement
AB  - 1. Animal movement arises from complex interactions between animals and their heterogeneous environment, making it challenging to capture the movement process in a single analysis. This has lead to a bias towards lower-dimensional representations of the process in such analyses. In order to better understand animal movement, its multiple components should be included and addressed simultaneously. 2. We present an analytic framework that integrates the behavioural, spatial, and temporal components of the movement process and their interactions, and allows for assessing the relative importance of these components. We propose a daily cyclic covariate to represent temporally cyclic movement patterns, for example diel variation in activity, and combine the three components in multi-modal Hidden Markov Models. We compare the statistical fits of models that include or exclude any of the behavioural, spatial and temporal components, and perform variance partitioning on the model that included all components to assess their relative importance to the movement process, both in isolation and in interaction. 3. We apply our framework to a case study on the movements of zebra, wildebeest and eland antelope in a South African reserve. Behavioural modes impacted movement the most, followed by diel rhythms and then the spatial environment (i.e. tree cover and terrain slope). Interactions between the components often explained more of the movement variation than the marginal effect of the spatial environment did on its own. Omitting components from the analysis led either to failure to detect relationships between input and response variables, resulting in overgeneralisations when drawing conclusions about the movement process, or to erroneously detecting spurious relationships, resulting in factually incorrect conclusions. 4. Our analytic framework can be used to study animal movement by integrating the different components of the movement process, thereby preventing incomplete or overly generic ecological interpretations. We demonstrate that understanding the drivers of animal movement, and ultimately the ecological phenomena that emerge from it, critically depends on considering the various components of the movement process, and especially the interactions between them.
PB  - bioRxiv
PY  - 2022
ST  - Unveiling the intertwined roles of the spatial-temporal environment and behavioural modes in animal movement
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.12.19.521042
ER  -


TY  - GEN
AU  - Ju, C.
AU  - Zheng, K.
AU  - Liu, J.
AU  - Wang, Y.
AU  - Tian, Q.
TI  - Distilling Vision-Language Pre-training to Collaborate with Weakly-Supervised Temporal Action Localization
AB  - Weakly-supervised temporal action localization (WTAL) learns to detect and classify action instances with only category labels. Most methods widely adopt the off-the-shelf Classification-Based Pre-training (CBP) to generate video features for action localization. However, the different optimization objectives between classification and localization, make temporally localized results suffer from the serious incomplete issue. To tackle this issue without additional annotations, this paper considers to distill free action knowledge from Vision-Language Pre-training (VLP), since we surprisingly observe that the localization results of vanilla VLP have an over-complete issue, which is just complementary to the CBP results. To fuse such complementarity, we propose a novel distillation-collaboration framework with two branches acting as CBP and VLP respectively. The framework is optimized through a dual-branch alternate training strategy. Specifically, during the B step, we distill the confident background pseudo-labels from the CBP branch; while during the F step, the confident foreground pseudo-labels are distilled from the VLP branch. And as a result, the dual-branch complementarity is effectively fused to promote a strong alliance. Extensive experiments and ablation studies on THUMOS14 and ActivityNet1.2 reveal that our method significantly outperforms state-of-the-art methods.
PB  - arXiv
PY  - 2022
ST  - Distilling Vision-Language Pre-training to Collaborate with Weakly-Supervised Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52729.2023.01417
ER  -


TY  - GEN
AU  - Koranda, M.
AU  - Rinnan, R.
AU  - Michelsen, A.
TI  - Plant functional types drive spatial and temporal variation in soil microbial community composition and extracellular enzyme activities in a tundra heath
AB  - Aims In this study we investigated divergent effects of two dominant plant functional types in tundra heath, dwarf shrubs and mosses, on microbial decomposition processes and soil carbon (C) and nutrient cycling. Methods We analysed samples of organic soil under three dwarf shrub species of distinct mycorrhizal association and life form (Betula nana, Empetrum hermaphroditum and Arctostaphylos alpinus) and under three moss species (Hylocomium splendens, Aulacomnium turgidum and Tomentypnum nitens) in early and late growing season. Results Our results revealed contrasting effects of shrubs and mosses on extracellular enzyme activities and soil C and nutrient pools which were linked with strong differences in soil microbial community structure. Specifically, moss soils were characterized by a bacterial-dominated microbial community associated with high soil nitrogen availability, while shrubs promoted a fungal-dominated microbial community and soil C accrual. The variation in soil microbial community composition under different plant species was explained by mycorrhizal association, root morphology, litter and soil organic matter quality and soil pH-value. Furthermore, we found that the seasonal variation in microbial biomass and enzyme activities, driven by plant belowground C allocation during the growing season, was most pronounced under the tallest shrub B. nana. Conclusion Our study demonstrates a close coupling of plant functional types with soil microbial communities, microbial decomposition processes and soil nutrient availability in tundra heath, which suggests potential strong impacts of global change-induced shifts in plant community composition on carbon and nutrient cycling in high-latitude ecosystems.
PB  - Research Square
PY  - 2022
ST  - Plant functional types drive spatial and temporal variation in soil microbial community composition and extracellular enzyme activities in a tundra heath
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-2379013/v1
ER  -


TY  - GEN
AU  - Krasowski, H.
AU  - Akella, P.
AU  - Ames, A.D.
AU  - Althoff, M.
TI  - Safe Reinforcement Learning with Probabilistic Guarantees Satisfying Temporal Logic Specifications in Continuous Action Spaces
AB  - Vanilla Reinforcement Learning (RL) can efficiently solve complex tasks but does not provide any guarantees on system behavior. To bridge this gap, we propose a three-step safe RL procedure for continuous action spaces that provides probabilistic guarantees with respect to temporal logic specifications. First, our approach probabilistically verifies a candidate controller with respect to a temporal logic specification while randomizing the control inputs to the system within a bounded set. Second, we improve the performance of this probabilistically verified controller by adding an RL agent that optimizes the verified controller for performance in the same bounded set around the control input. Third, we verify probabilistic safety guarantees with respect to temporal logic specifications for the learned agent. Our approach is efficiently implementable for continuous action and state spaces. The separation of safety verification and performance improvement into two distinct steps realizes both explicit probabilistic safety guarantees and a straightforward RL setup that focuses on performance. We evaluate our approach on an evasion task where a robot has to reach a goal while evading a dynamic obstacle with a specific maneuver. Our results show that our safe RL approach leads to efficient learning while maintaining its probabilistic safety specification.
PB  - arXiv
PY  - 2022
ST  - Safe Reinforcement Learning with Probabilistic Guarantees Satisfying Temporal Logic Specifications in Continuous Action Spaces
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cdc49753.2023.10383601
ER  -


TY  - GEN
AU  - Wang, B.
AU  - Song, Y.
AU  - Wang, F.
AU  - Shu, X.
AU  - Rui, Y.
TI  - Dilation-Erosion for Single-Frame Supervised Temporal Action Localization
AB  - To balance the annotation labor and the granularity of supervision, single-frame annotation has been introduced in temporal action localization. It provides a rough temporal location for an action but implicitly overstates the supervision from the annotated-frame during training, leading to the confusion between actions and backgrounds, i.e., action incompleteness and background false positives. To tackle the two challenges, in this work, we present the Snippet Classification model and the Dilation-Erosion module. In the Dilation-Erosion module, we expand the potential action segments with a loose criterion to alleviate the problem of action incompleteness and then remove the background from the potential action segments to alleviate the problem of action incompleteness. Relying on the single-frame annotation and the output of the snippet classification, the Dilation-Erosion module mines pseudo snippet-level ground-truth, hard backgrounds and evident backgrounds, which in turn further trains the Snippet Classification model. It forms a cyclic dependency. Furthermore, we propose a new embedding loss to aggregate the features of action instances with the same label and separate the features of actions from backgrounds. Experiments on THUMOS14 and ActivityNet 1.2 validate the effectiveness of the proposed method. Code has been made publicly available∗
PB  - arXiv
PY  - 2022
ST  - Dilation-Erosion for Single-Frame Supervised Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s11042-023-15196-1
ER  -


TY  - GEN
AU  - Xia, C.
AU  - Hu, Y.
AU  - Chen, J.
TI  - An activity-based spatial-temporal community electricity vulnerability assessment framework
AB  - The power system is among the most important critical infrastructures in urban cities and is getting increasingly essential in supporting people’s daily activities. However, it is also susceptible to most natural disasters such as tsunamis, floods, or earthquakes. Electricity vulnerability, therefore, forms a crucial basis for community resilience. This paper aims to present an assessment framework of spatial-temporal electricity vulnerability to support the building of community resilience against power outages. The framework includes vulnerability indexes in terms of occupant demographics, occupant activity patterns, and urban building characteristics. To integrate factors in these aspects, we also proposed a process as activity simulation-mapping-evaluation-visualization to apply the framework and visualize results. This framework can help planners make an effective first-time response by identifying the most vulnerable areas when a massive power outage happens during natural disasters. It can also be integrated into community resilience analysis models and potentially contributes to effective disaster risk management
PB  - arXiv
PY  - 2022
ST  - An activity-based spatial-temporal community electricity vulnerability assessment framework
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-981-19-9822-5_280
ER  -


TY  - GEN
AU  - Lee, J.
AU  - Lee, M.
AU  - Cho, S.
AU  - Jang, S.
AU  - Lee, S.
TI  - Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition
AB  - Skeleton-based action recognition has attracted considerable attention due to its compact representation of the human body’s skeletal sructure. Many recent methods have achieved remarkable performance using graph convolutional networks (GCNs) and convolutional neural networks (CNNs), which extract spatial and temporal features, respectively. Although spatial and temporal dependencies in the human skeleton have been explored separately, spatio-temporal dependency is rarely considered. In this paper, we propose the Spatio-Temporal Curve Network (STC-Net) to effectively leverage the spatio-temporal dependency of the human skeleton. Our proposed network consists of two novel elements: 1) The Spatio-Temporal Curve (STC) module; and 2) Dilated Kernels for Graph Convolution (DK-GC). The STC module dynamically adjusts the receptive field by identifying meaningful node connections between every adjacent frame and generating spatio-temporal curves based on the identified node connections, providing an adaptive spatio-temporal coverage. In addition, we propose DK-GC to consider long-range dependencies, which results in a large receptive field without any additional parameters by applying an extended kernel to the given adjacency matrices of the graph. Our STC-Net combines these two modules and achieves state-of-the-art performance on four skeleton-based action recognition benchmarks. Code is available at https://github.com/Jho-Yonsei/STC-Net.
PB  - arXiv
PY  - 2022
ST  - Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccv51070.2023.00941
ER  -


TY  - GEN
AU  - Xu, N.
AU  - Smith, D.M.
AU  - Jeno, G.
AU  - Schumacher, E.H.
AU  - Keilholz, S.D.
TI  - The interaction between random and systematic visual stimulation and infraslow quasiperiodic spatiotemporal patterns of whole brain activity
AB  - One prominent feature of the infraslow BOLD signal during rest or task is quasi-periodic spatiotemporal pattern (QPP) of signal changes that involves an alternation of activity in key functional networks and propagation of activity across brain areas, and that is known to tie to the infraslow neural activity involved in attention and arousal fluctuations. This ongoing whole-brain pattern of activity might potentially modify the response to incoming stimuli or be modified itself by the induced neural activity. To investigate this, we presented checkerboard sequences flashing at 6Hz to subjects. This is a salient visual stimulus that is known to produce a strong response in visual processing regions. Two different visual stimulation sequences were employed, a systematic stimulation sequence in which the visual stimulus appeared every 20.3 secs and a random stimulation sequence in which the visual stimulus occurred randomly every 14~62.3 secs. Three central observations emerged. First, the two different stimulation conditions affect the QPP waveform in different aspects, i.e., systematic stimulation has greater effects on its phase and random stimulation has greater effects on its magnitude. Second, the QPP was more frequent in the systematic condition with significantly shorter intervals between consecutive QPPs compared to the random condition. Third, the BOLD signal response to the visual stimulus across both conditions was swamped by the QPP at the stimulus onset. These results provide novel insights into the relationship between intrinsic patterns and stimulated brain activity.
PB  - bioRxiv
PY  - 2022
ST  - The interaction between random and systematic visual stimulation and infraslow quasiperiodic spatiotemporal patterns of whole brain activity
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.12.06.519337
ER  -


TY  - GEN
AU  - Yadav, S.K.
AU  - Luthra, A.
AU  - Pahwa, E.
AU  - Pandey, H.M.
AU  - Corcoran, P.
TI  - DroneAttention: Sparse Weighted Temporal Attention for Drone-Camera Based Activity Recognition
AB  - Human activity recognition (HAR) using drone-mounted cameras has attracted considerable interest from the computer vision research community in recent years. A robust and efficient HAR system has a pivotal role in fields like video surveillance, crowd behavior analysis, sports analysis, and human-computer interaction. What makes it challenging are the complex poses, understanding different viewpoints, and the environmental scenarios where the action is taking place. To address such complexities, in this paper, we propose a novel Sparse Weighted Temporal Attention (SWTA) module to utilize sparsely sampled video frames for obtaining global weighted temporal attention. The proposed SWTA is comprised of two parts. First, temporal segment network that sparsely samples a given set of frames. Second, weighted temporal attention, which incorporates a fusion of attention maps derived from optical flow, with raw RGB images. This is followed by a basenet network, which comprises a convolutional neural network (CNN) module along with fully connected layers that provide us with activity recognition. The SWTA network can be used as a plug-in module to the existing deep CNN architectures, for optimizing them to learn temporal information by eliminating the need for a separate temporal stream. It has been evaluated on three publicly available benchmark datasets, namely Okutama, MOD20, and Drone-Action. The proposed model has received an accuracy of 72.76%, 92.56%, and 78.86% on the respective datasets thereby surpassing the previous state-of-the-art performances by a margin of 25.26%, 18.56%, and 2.94%, respectively.
PB  - arXiv
PY  - 2022
ST  - DroneAttention
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.neunet.2022.12.005
ER  -


TY  - GEN
AU  - Betini, G.S.
AU  - Malaj, E.
AU  - Donkersteeg, C.
AU  - Morrissey, C.A.
AU  - Mahony, N.A.
TI  - Spatial-Temporal Variation in the Association between Agricultural Activities and Bird Communities in Canada
AB  - Agriculture is one the main drivers of bird decline in both Europe and North America. While it is clear that agricultural practices and changes in the rural landscape directly and indirectly affect bird communities, we still do not know the extent to which these impacts might change across broad spatial and temporal scales. To address this question, we combined information on agricultural activities with occurrence and abundance of 358 bird species across five time periods spanning 20 years in Canada. As a proxy for agricultural impact, we used a combined index that included different agricultural metrics, such as cropland and tillage area and area treated with pesticides. We found that agriculture impact was often negatively associated with bird diversity and evenness, but these associations varied by region and over time. We found strong support for an overall negative association between agriculture impact and bird diversity and evenness in 3 of the 4 agricultural regions of Canada – the Prairie, Eastern, and Atlantic regions. The impact of agriculture on bird diversity in the Prairies appeared to weaken over time, while in the Pacific region we found no effect of agriculture on diversity in the periods evaluated. These findings suggest that agricultural activities result in bird communities that are less diverse and disproportionately benefit certain species. The temporal and spatial variation in the impact of agriculture on bird diversity and evenness we observed is likely a result of regional differences in the native vegetation, the type of crops and commodities produced, the historical context of agriculture, as well as the native bird community and the extent of their association with open habitat. Thus, our work provides support for the idea that the agricultural impact on bird communities, while largely negative, is not uniform, and can vary in space and time.
PB  - SSRN
PY  - 2022
ST  - Spatial-Temporal Variation in the Association between Agricultural Activities and Bird Communities in Canada
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4280917
ER  -


TY  - GEN
AU  - Abdalaziz, M.
AU  - Redding, Z.V.
AU  - Fiebelkorn, I.C.
TI  - Rhythmic temporal coordination of neural activity prevents representational conflict during working memory
AB  - Selective attention is characterized by alternating states associated with either attentional sampling or attentional shifting, helping to avoid functional conflicts by isolating function-specific neural activity in time. We hypothesized that such rhythmic temporal coordination might also help to avoid representational conflicts during working memory. Multiple items can be simultaneously held in working memory, and these items can be represented by overlapping neural populations. Traditional theories propose that short-term storage of to-be-remembered items occurs through persistent neural activity, but when neurons are simultaneously representing multiple items, persistent activity creates a potential for representational conflicts. In comparison, more recent, ‘activity-silent’ theories of working memory propose that synaptic changes also contribute to the short-term storage of to-be-remembered items13-16. Transient bursts in neural activity17, rather than persistent activity, could serve to occasionally refresh these synaptic changes. Here, we used EEG and response times (RTs) to test whether rhythmic temporal coordination helps to isolate neural activity associated with different to-be-remembered items, which would help to avoid representational conflicts. Consistent with this hypothesis, we report that the relative strength of different item representations alternates over time as a function of frequency-specific phase. Although RTs were linked to theta (~6Hz) and beta (~25 Hz) phase during a memory delay, the relative strength of item representations only alternated as a function of beta phase. The present findings (i) are consistent with rhythmic temporal coordination being a general mechanism for avoiding either functional or representational conflicts during cognitive processes, and (ii) inform models describing the role of oscillatory dynamics in organizing working memory
PB  - bioRxiv
PY  - 2022
ST  - Rhythmic temporal coordination of neural activity prevents representational conflict during working memory
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.cub.2023.03.088
ER  -


TY  - GEN
AU  - Yang, J.
AU  - Shi, Y.
AU  - Zheng, Y.
AU  - Zhang, Z.
TI  - Spatial–Temporal Distribution Prediction Method of Urban Population Density through Behaviour–Environment Agent Model（BEM）
AB  - Based on the interrelationship between the built environment and spatial–temporal distribution of population density, this paper proposes a method to predict the spatial–temporal distribution of urban population density using the depth residual network model (ResNet) of neural network. This study used the time-sharing data of mobile phone users provided by the China Mobile Communications Corporation to predict the time–space sequence of the steady-state distribution of population density. Firstly, 40 prediction databases were constructed according to the characteristics of built environment and the spatial–temporal distribution of population density. Thereafter, the depth residual model ResNet was used as the basic framework to construct the behaviour–environment agent model (BEM) for model training and prediction. Finally, the average percentage error index was used to evaluate the prediction results. The results revealed that the accuracy rate of prediction results reached 76.92% in the central urban area of the verification case. The proposed method can be applied to prevent urban public safety incidents and alleviate pandemics. Moreover, this method can be practically applied to enable the construction of a “smart city” for improving the efficient allocation of urban resources and traffic mobility.
PB  - Research Square
PY  - 2022
ST  - Spatial–Temporal Distribution Prediction Method of Urban Population Density through Behaviour–Environment Agent Model（BEM）
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-2201266/v1
ER  -


TY  - GEN
AU  - Nag, S.
AU  - Xu, M.
AU  - Zhu, X.
AU  - Song, Y.-Z.
AU  - Xiang, T.
TI  - Multi-Modal Few-Shot Temporal Action Detection
AB  - Few-shot (FS) and zero-shot (ZS) learning are two approaches for scaling temporal action detection (TAD) to new classes. The former adapts a pretrained vision model to a new task represented by as few as a single video per class, whilst the latter requires no training examples by exploiting a semantic description of the new class. In this work, we introduce a new multi-modality few-shot (MMFS) TAD problem, as a marriage of FS-TAD and ZS-TAD by leveraging few-shot support videos and a new class name jointly. To tackle this problem, we further introduce a novel MUlti-modality PromPt mETa-learning (MUPPET) method. This is enabled by efficiently bridging pretrained vision and language models whilst maximally reusing already learned capacity. Concretely, we construct multimodal prompts by mapping support videos into the textual token space of a vision-language model using a meta-learned adapter-equipped visual semantics tokenizer. To tackle large intra-class variation, we further design a query feature regulation scheme. Extensive experiments on ActivityNetv1.3 and THUMOS14 demonstrate that our MUPPET outperforms state-of-the-art alternative methods, often by a large margin. MUPPET can be easily extended to few-shot object detection, achieving new state-of-the-art on MS-COCO. The code will be made available in https://github.com/sauradip/MUPPET
PB  - arXiv
PY  - 2022
ST  - Multi-Modal Few-Shot Temporal Action Detection
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4341961
ER  -


TY  - GEN
AU  - Nag, S.
AU  - Zhu, X.
AU  - Song, Y.-Z.
AU  - Xiang, T.
TI  - Post-Processing Temporal Action Detection
AB  - Existing Temporal Action Detection (TAD) methods typically take a pre-processing step in converting an input varying-length video into a fixed-length snippet representation sequence, before temporal boundary estimation and action classification. This pre-processing step would temporally downsample the video, reducing the inference resolution and hampering the detection performance in the original temporal resolution. In essence, this is due to a temporal quantization error introduced during the resolution downsampling and recovery. This could negatively impact the TAD performance, but is largely ignored by existing methods. To address this problem, in this work we introduce a novel model-agnostic post-processing method without model redesign and retraining. Specifically, we model the start and end points of action instances with a Gaussian distribution for enabling temporal boundary inference at a sub-snippet level. We further introduce an efficient Taylor-expansion based approximation, dubbed as Gaussian Approximated Post-processing (GAP). Extensive experiments demonstrate that our GAP can consistently improve a wide variety of pre-trained off-the-shelf TAD models on the challenging ActivityNet (+0.2%∼0.7% in average mAP) and THUMOS (+0.2%∼0.5% in average mAP) benchmarks. Such performance gains are already significant and highly comparable to those achieved by novel model designs. Also, GAP can be integrated with model training for further performance gain. Importantly, GAP enables lower temporal resolutions for more efficient inference, facilitating low-resource applications. The code will be available in https://github.com/sauradip/GAP
PB  - arXiv
PY  - 2022
ST  - Post-Processing Temporal Action Detection
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52729.2023.01806
ER  -


TY  - GEN
AU  - Zhao, C.
AU  - Liu, S.
AU  - Mangalam, K.
AU  - Ghanem, B.
TI  - Re^2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal Action Localization
AB  - Temporal action localization (TAL) requires long-form reasoning to predict actions of various durations and complex content. Given limited GPU memory, training TAL end to end (i.e., from videos to predictions) on long videos is a significant challenge. Most methods can only train on pre-extracted features without optimizing them for the localization problem, consequently limiting localization performance. In this work, to extend the potential in TAL networks, we propose a novel end-to-end method Re2TAL, which rewires pretrained video backbones for reversible TAL. Re2TAL builds a backbone with reversible modules, where the input can be recovered from the output such that the bulky intermediate activations can be cleared from memory during training. Instead of designing one single type of reversible module, we propose a network rewiring mechanism, to transform any module with a residual connection to a reversible module without changing any parameters. This provides two benefits: (1) a large variety of reversible networks are easily obtained from existing and even future model designs, and (2) the reversible models require much less training effort as they reuse the pre-trained parameters of their original non-reversible versions. Re2TAL, only using the RGB modality, reaches 37.01% average mAP on ActivityNet-v1.3, a new state-ofthe-art record, and mAP 64.9% at tIoU=0.5 on THUMOS-14, outperforming all other RGB-only methods. Code will be available at https://github.com/coolbay/Re2TAL.
PB  - arXiv
PY  - 2022
ST  - Re^2TAL
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52729.2023.01025
ER  -


TY  - GEN
AU  - Sun, W.
AU  - Su, R.
AU  - Yu, Q.
AU  - Xu, D.
TI  - Slow Motion Matters: A Slow Motion Enhanced Network for Weakly Supervised Temporal Action Localization
AB  - Weakly supervised temporal action localization (WTAL) aims to localize actions in untrimmed videos with only weak supervision information (e.g., video-level labels). Most existing models handle all input videos with a fixed temporal scale. However, such models are not sensitive to actions whose pace of the movements is different from the “normal” speed, especially slow-motion action instances, which complete the movements with a much slower speed than their counterparts with a “normal” speed. Here arises the slow-motion blurred issue: It is hard to explore salient slow-motion information from videos at normal speed. In this paper, we propose a novel framework termed Slow Motion Enhanced Network (SMEN) to improve the ability of a WTAL network by compensating its sensitivity on slow-motion action segments. The proposed SMEN comprises a Mining module and a Localization module. The mining module generates mask to mine slow-motion-related features by utilizing the relationships between the normal motion and slow motion; while the localization module leverages the mined slow-motion features as complementary information to improve the temporal action localization results. Our proposed framework can be easily adapted by existing WTAL networks and enable them be more sensitive to slow-motion actions. Extensive experiments on three benchmarks are conducted, which demonstrate the high performance of our proposed framework.
PB  - arXiv
PY  - 2022
ST  - Slow Motion Matters
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tcsvt.2022.3201540
ER  -


TY  - GEN
AU  - McIntosh, D.
AU  - Marques, T.P.
AU  - Albu, A.B.
AU  - Rountree, R.
AU  - De Leo, F.
TI  - TempNet: Temporal Attention Towards the Detection of Animal Behaviour in Videos
AB  - Recent advancements in cabled ocean observatories have increased the quality and prevalence of underwater videos; this data enables the extraction of high-level biologically relevant information such as species’ behaviours. Despite this increase in capability, most modern methods for the automatic interpretation of underwater videos focus only on the detection and counting organisms. We propose an efficient computer vision- and deep learning-based method for the detection of biological behaviours in videos. TempNet uses an encoder bridge and residual blocks to maintain model performance with a two-staged, spatial, then temporal, encoder. TempNet also presents temporal attention during spatial encoding as well as Wavelet Down-Sampling preprocessing to improve model accuracy. Although our system is designed for applications to diverse fish behaviours (i.e, is generic), we demonstrate its application to the detection of sablefish (Anoplopoma fimbria) startle events. We compare the proposed approach with a state-of-the-art end-to-end video detection method (ReMotENet) and a hybrid method previously offered exclusively for the detection of sablefish’s startle events in videos from an existing dataset. Results show that our novel method comfortably outperforms the comparison baselines in multiple metrics, reaching a per-clip accuracy and precision of 80% and 0.81, respectively. This represents a relative improvement of 31% in accuracy and 27% in precision over the compared methods using this dataset. Our computational pipeline is also highly efficient, as it can process each 4-second video clip in only 38ms. Furthermore, since it does not employ features specific to sablefish startle events, our system can be easily extended to other behaviours in future works.
PB  - arXiv
PY  - 2022
ST  - TempNet
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icpr56361.2022.9956609
ER  -


TY  - GEN
AU  - Li, Y.
AU  - Yin, R.
AU  - Park, H.
AU  - Kim, Y.
AU  - Panda, P.
TI  - Wearable-based Human Activity Recognition with Spatio-Temporal Spiking Neural Networks
AB  - We study the Human Activity Recognition (HAR) task, which predicts user daily activity based on time series data from wearable sensors. Recently, researchers use end-to-end Artificial Neural Networks (ANNs) to extract the features and perform classification in HAR. However, ANNs pose a huge computation burden on wearable devices and lack temporal feature extraction. In this work, we leverage Spiking Neural Networks (SNNs)—an architecture inspired by biological neurons—to HAR tasks. SNNs allow spatio-temporal extraction of features and enjoy low-power computation with binary spikes. We conduct extensive experiments on three HAR datasets with SNNs, demonstrating that SNNs are on par with ANNs in terms of accuracy while reducing up to 94% energy consumption. The code is publicly available in https://github.com/Intelligent-Computing-Lab-Yale/SNN_HAR
PB  - arXiv
PY  - 2022
ST  - Wearable-based Human Activity Recognition with Spatio-Temporal Spiking Neural Networks
Y2  - 2025/05/05/21:54:31
DO  - 10.3389/fnins.2023.1233037
ER  -


TY  - GEN
AU  - Kamra, A.
AU  - Das, S.
AU  - Bhatt, P.
AU  - Maity, T.
AU  - Rana, S.
TI  - A Dissipative Supramolecular Glue for Temporal Control of Amplified Enzyme Activity and Biocatalytic Cascades
AB  - Regulation of enzyme activity is key to the adaptation of cellular processes such as signal transduction and metabolism in response to varying external conditions. Synthetic molecular glues have provided effective systems for enzyme inhibition and regulation of protein-protein interactions. So far, all the molecular glue systems based on covalent interactions operated in equilibrium conditions. To emulate dynamic far-from-equilibrium biological processes, we introduce herein a transient supramolecular glue with controllable lifetime. The transient system uses multivalent supramolecular interactions between guanidium group-bearing surfactants and adenosine triphosphates (ATP), resulting in bilayer vesicle structures. Unlike the conventional fuels for non-equilibrium assemblies, ATP here plays the dual role of providing a structural component for the assembly as well as presenting active functional groups to “glue” enzymes on the surface. While gluing of the enzymes on the vesicles achieves augmented catalysis, oscillation of ATP concentration allows temporal control of the catalytic activities. We further demonstrate temporal activation and control of biocatalytic cascade networks on the vesicles, which represents an essential cellular component. Altogether, the temporal activation of biocatalytic cascades on the dissipative vesicular glue presents an adaptable and dynamic system emulating heterogeneous cellular processes, opening up avenues for effective protocell construction and therapeutic interventions.
PB  - ChemRxiv
PY  - 2022
ST  - A Dissipative Supramolecular Glue for Temporal Control of Amplified Enzyme Activity and Biocatalytic Cascades
Y2  - 2025/05/05/21:54:31
DO  - 10.26434/chemrxiv-2022-m9qz9
ER  -


TY  - GEN
AU  - Zhang, L.
AU  - Lieffers, J.
AU  - Pyarelal, A.
TI  - Using Features at Multiple Temporal and Spatial Resolutions to Predict Human Behavior in Real Time
AB  - When performing complex tasks, humans naturally reason at multiple temporal and spatial resolutions simultaneously. We contend that for an artificially intelligent agent to effectively model human teammates, i.e., demonstrate computational theory of mind (ToM), it should do the same. In this paper, we present an approach for integrating high and low-resolution spatial and temporal information to predict human behavior in real time and evaluate it on data collected from human subjects performing simulated urban search and rescue (USAR) missions in a Minecraft-based environment. Our model composes neural networks for high and low-resolution feature extraction with a neural network for behavior prediction, with all three networks trained simultaneously. The high-resolution extractor encodes dynamically changing goals robustly by taking as input the Manhattan distance difference between the humans’ Minecraft avatars and candidate goals in the environment for the latest few actions, computed from a high-resolution gridworld representation. In contrast, the low-resolution extractor encodes participants’ historical behavior using a historical state matrix computed from a low-resolution graph representation. Through supervised learning, our model acquires a robust prior for human behavior prediction, and can effectively deal with long-term observations. Our experimental results demonstrate that our method significantly improves prediction accuracy compared to approaches that only use high-resolution information.
PB  - arXiv
PY  - 2022
ST  - Using Features at Multiple Temporal and Spatial Resolutions to Predict Human Behavior in Real Time
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-21671-8_13
ER  -


TY  - GEN
AU  - Wilson-Aggarwal, J.
AU  - Gotts, N.
AU  - Arnold, K.
AU  - Nastouli, E.
AU  - Manley, E.
TI  - Assessing the spatial-temporal risks of SARS-CoV-2 infection for healthcare-workers in the hospital using behavioural indices from routine data
AB  - The COVID-19 pandemic has emphasised the need to rapidly assess infection risks for healthcare workers within the hospital environment. Using data from the first year of the pandemic, we investigated whether an individual’s COVID-19 test result was associated with behavioural markers derived from routinely collected hospital data two weeks prior to a test. The temporal and spatial context of behaviours were important, with the highest risks of infection during the first wave, for staff in contact with a greater number of patients and those with greater levels of activity on floors handling the majority of COVID-19 patients. Infection risks were higher for BAME staff and individuals working more shifts. Night shifts presented higher risks of infection between waves of COVID-19 patients. Our results demonstrate the epidemiological relevance of deriving markers of staff behaviour from electronic records, which extend beyond COVID-19 with applications for other communicable diseases and in supporting pandemic preparedness.
PB  - medRxiv
PY  - 2022
ST  - Assessing the spatial-temporal risks of SARS-CoV-2 infection for healthcare-workers in the hospital using behavioural indices from routine data
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.11.10.22282176
ER  -


TY  - GEN
AU  - Kang, H.
AU  - Kim, H.
AU  - An, J.
AU  - Cho, M.
AU  - Kim, S.J.
TI  - Soft-Landing Strategy for Alleviating the Task Discrepancy Problem in Temporal Action Localization Tasks
AB  - Temporal Action Localization (TAL) methods typically operate on top of feature sequences from a frozen snippet encoder that is pretrained with the Trimmed Action Classification (TAC) tasks, resulting in a task discrepancy problem. While existing TAL methods mitigate this issue either by retraining the encoder with a pretext task or by end-to-end finetuning, they commonly require an overload of high memory and computation. In this work, we introduce Soft-Landing (SoLa) strategy, an efficient yet effective framework to bridge the transferability gap between the pretrained encoder and the downstream tasks by incorporating a light-weight neural network, i.e., a SoLa module, on top of the frozen encoder. We also propose an unsupervised training scheme for the SoLa module; it learns with inter-frame Similarity Matching that uses the frame interval as its supervisory signal, eliminating the need for temporal annotations. Experimental evaluation on various benchmarks for downstream TAL tasks shows that our method effectively alleviates the task discrepancy problem with remarkable computational efficiency.
PB  - arXiv
PY  - 2022
ST  - Soft-Landing Strategy for Alleviating the Task Discrepancy Problem in Temporal Action Localization Tasks
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52729.2023.00630
ER  -


TY  - GEN
AU  - Yadav, S.K.
AU  - Pahwa, E.
AU  - Luthra, A.
AU  - Pandey, H.M.
AU  - Corcoran, P.
TI  - SWTF: Sparse Weighted Temporal Fusion for Drone-Based Activity Recognition
AB  - Drone-camera based human activity recognition (HAR) has received significant attention from the computer vision research community in the past few years. A robust and efficient HAR system has a pivotal role in fields like video surveillance, crowd behavior analysis, sports analysis, and human-computer interaction. What makes it challenging are the complex poses, understanding different viewpoints, and the environmental scenarios where the action is taking place. To address such complexities, in this paper, we propose a novel Sparse Weighted Temporal Fusion (SWTF) module to utilize sparsely sampled video frames for obtaining global weighted temporal fusion outcome. The proposed SWTF is divided into two components. First, a temporal segment network that sparsely samples a given set of frames. Second, weighted temporal fusion, that incorporates a fusion of feature maps derived from optical flow, with raw RGB images. This is followed by base-network, which comprises a convolutional neural network module along with fully connected layers that provide us with activity recognition. The SWTF network can be used as a plug-in module to the existing deep CNN architectures, for optimizing them to learn temporal information by eliminating the need for a separate temporal stream. It has been evaluated on three publicly available benchmark datasets, namely Okutama, MOD20, and Drone-Action. The proposed model has received an accuracy of 72.76%, 92.56%, and 78.86% on the respective datasets thereby surpassing the previous state-of-the-art performances by a significant margin.
PB  - arXiv
PY  - 2022
ST  - SWTF
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/ijcnn54540.2023.10191750
ER  -


TY  - GEN
AU  - Liu, Y.
AU  - Tang, Y.
AU  - Zhang, N.
AU  - Lin, R.-S.
AU  - Wang, H.
TI  - PRIOR-ENHANCED TEMPORAL ACTION LOCALIZATION USING SUBJECT-AWARE SPATIAL ATTENTION
AB  - Temporal action localization (TAL) aims to detect the boundary and identify the class of each action instance in a long untrimmed video. Current approaches treat video frames homogeneously, and tend to give background and key objects excessive attention. This limits their sensitivity to localize action boundaries. To this end, we propose a prior-enhanced temporal action localization method (PETAL), which only takes in RGB input and incorporates action subjects as priors. This proposal leverages action subjects' information with a plug-and-play subject-aware spatial attention module (SA-SAM) to generate an aggregated and subject-prioritized representation. Experimental results on THUMOS-14 and ActivityNet-1.3 datasets demonstrate that the proposed PETAL achieves competitive performance using only RGB features, e.g., boosting mAP by 2.41% or 0.25% over the state-of-the-art approach that uses RGB features or with additional optical flow features on the THUMOS-14 dataset.
PB  - arXiv
PY  - 2022
ST  - PRIOR-ENHANCED TEMPORAL ACTION LOCALIZATION USING SUBJECT-AWARE SPATIAL ATTENTION
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icassp49357.2023.10097050
ER  -


TY  - GEN
AU  - Gorpincenko, A.
AU  - Mackiewicz, M.
TI  - Extending Temporal Data Augmentation for Video Action Recognition
AB  - Pixel space augmentation has grown in popularity in many Deep Learning areas, due to its effectiveness, simplicity, and low computational cost. Data augmentation for videos, however, still remains an under-explored research topic, as most works have been treating inputs as stacks of static images rather than temporally linked series of data. Recently, it has been shown that involving the time dimension when designing augmentations can be superior to its spatial-only variants for video action recognition [34]. In this paper, we propose several novel enhancements to these techniques to strengthen the relationship between the spatial and temporal domains and achieve a deeper level of perturbations. The video action recognition results of our techniques outperform their respective variants in Top-1 and Top-5 settings on the UCF-101 [55] and the HMDB-51 [38] datasets.
PB  - arXiv
PY  - 2022
ST  - Extending Temporal Data Augmentation for Video Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-25825-1_8
ER  -


TY  - GEN
AU  - Tang, T.N.
AU  - Park, J.
AU  - Kim, K.
AU  - Sohn, K.
TI  - SimOn: A Simple Framework for Online Temporal Action Localization
AB  - Online Temporal Action Localization (On-TAL) aims to immediately provide action instances from untrimmed streaming videos. The model is not allowed to utilize future frames and any processing techniques to modify past predictions, making On-TAL much more challenging. In this paper, we propose a simple yet effective framework, termed SimOn, that learns to predict action instances using the popular Transformer architecture in an end-to-end manner. Specifically, the model takes the current frame feature as a query and a set of past context information as keys and values of the Transformer. Different from the prior work that uses a set of outputs of the model as past contexts, we leverage the past visual context and the learnable context embedding for the current query. Experimental results on the THUMOS14 and ActivityNet1.3 datasets show that our model remarkably outperforms the previous methods, achieving a new state-of-the-art On-TAL performance. In addition, the evaluation for Online Detection of Action Start (ODAS) demonstrates the effectiveness and robustness of our method in the online setting.
PB  - arXiv
PY  - 2022
ST  - SimOn
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.patcog.2022.108871
ER  -


TY  - GEN
AU  - Lafer-Sousa, R.
AU  - Wang, K.
AU  - Azadi, R.
AU  - Bohn, S.
AU  - Afraz, A.
TI  - Behavioral detectability of optogenetic stimulation of inferior temporal cortex varies with the size of concurrently viewed objects
AB  - We have previously demonstrated that macaque monkeys can behaviorally detect a subtle optogenetic impulse delivered to their inferior temporal (IT) cortex. We have also shown that the ability to detect the cortical stimulation impulse varies depending on some characteristics of the visual images viewed at the time of brain stimulation, revealing the visual nature of the perceptual events induced by stimulation of the IT cortex. Here we systematically studied the effect of the size of viewed objects on behavioral detectability of optogenetic stimulation of the central IT cortex. Surprisingly, we found that behavioral detection of the same optogenetic impulse highly varies with the size of the viewed object images. Reduction of the object size in four steps from 8 to 1 degree of visual angle significantly decreased detection performance. These results show that identical stimulation impulses delivered to the same neural population induce variable perceptual events depending on the mere size of the objects viewed at the time of brain stimulation.
PB  - bioRxiv
PY  - 2022
ST  - Behavioral detectability of optogenetic stimulation of inferior temporal cortex varies with the size of concurrently viewed objects
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.11.04.514791
ER  -


TY  - GEN
AU  - Biswas, S.
AU  - Rhodes, A.
AU  - Manuvinakurike, R.
AU  - Raffa, G.
AU  - Beckwith, R.
TI  - Distill and Collect for Semi-Supervised Temporal Action Segmentation
AB  - Recent temporal action segmentation approaches need frame annotations during training to be effective. These annotations are very expensive and time-consuming to obtain. This limits their performances when only limited annotated data is available. In contrast, we can easily collect a large corpus of in-domain unannotated videos by scavenging through the internet. Thus, this paper proposes an approach for the temporal action segmentation task that can simultaneously leverage knowledge from annotated and unannotated video sequences. Our approach uses multistream distillation that repeatedly refines and finally combines their frame predictions. Our model also predicts the action order, which is later used as a temporal constraint while estimating frames labels to counter the lack of supervision for unannotated videos. In the end, our evaluation of the proposed approach on two different datasets demonstrates its capability to achieve comparable performance to the full supervision despite limited annotation.
PB  - arXiv
PY  - 2022
ST  - Distill and Collect for Semi-Supervised Temporal Action Segmentation
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v36i2.20124
ER  -


TY  - GEN
AU  - Chioccioli, M.
AU  - Magruder, S.
AU  - McDonough, J.E.
AU  - Kaminski, N.
AU  - Sauler, M.
TI  - Spatiotemporal coordination of stem cell behavior following alveolar injury
AB  - Tissue repair requires a highly coordinated cellular response to injury. In the lung, alveolar type 2 (AT2) cells act as stem cells and can replace both themselves and alveolar type 1 cells (AT1); however, the complex orchestration of AT2 stem cell activity following lung injury is poorly understood owing to the inability of tracking individual stem cells and their dynamic behavior over time. Here, we apply live time lapse imaging to ex vivo mouse precision cut lung slice (PCLS) culture and in vivo mouse lung to track individual GFP-labeled AT2 cells following induction of alveolar injury by bleomycin. We observe highly dynamic movement of AT2 cells, including migration within and between alveoli. To map the dynamic evolution of AT2 cell behavior, we introduce Live Cell Encoder (LCE-PHATE), a novel method for converting static snapshots from time lapse imaging into single points representative of entire, dynamic cellular trajectories. Applying LCE-PHATE, we observe the emergence of at least three distinct morphokinetic AT2 cell states associated with AT2 stem cell injury response. Finally, small molecule-based inhibition of Rho-associated protein kinase (ROCK) pathway significantly reduced motility of AT2 stem cells following injury and reduced expression of Krt8, a marker of intermediate progenitor cells. Together, our results uncover motility of alveolar stem cells as a new injury response mechanism in the lung and reveal properties of stem cell motility at high cellular resolution.
PB  - bioRxiv
PY  - 2022
ST  - Spatiotemporal coordination of stem cell behavior following alveolar injury
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.10.28.514255
ER  -


TY  - GEN
AU  - Zhou, S.
AU  - Buonomano, D.V.
TI  - Unified control of temporal and spatial scales of sensorimotor behavior through neuromodulation of short-term synaptic plasticity
AB  - Neuromodulators such as dopamine have been shown to modulate short-term synaptic plasticity (STP). Here we propose that the neuromodulation of STP provides a general mechanism to scale neural dynamics and motor outputs in time and space. We trained RNNs that incorporated STP to produce complex motor trajectories—handwritten digits—with different temporal (speed) and spatial (size) scales. The learned dynamics underwent temporal and spatial scaling when higher synaptic release probabilities corresponded to higher speed/size. Neuromodulation of STP enhanced temporal or spatial generalization compared to weight modulation alone. The model accounted for the data of two experimental studies involving flexible sensorimotor timing. Our results address a long-standing debate regarding the role of dopamine in timing and predict novel mechanisms by which dopamine may slow down neural dynamics and thus slow “clock” speed.
PB  - bioRxiv
PY  - 2022
ST  - Unified control of temporal and spatial scales of sensorimotor behavior through neuromodulation of short-term synaptic plasticity
Y2  - 2025/05/05/21:54:31
DO  - 10.1126/sciadv.adk7257
ER  -


TY  - GEN
AU  - Wang, L.
AU  - Koniusz, P.
TI  - Temporal-Viewpoint Transportation Plan for Skeletal Few-shot Action Recognition
AB  - We propose a Few-shot Learning pipeline for 3D skeleton-based action recognition by Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE). To factor out misalignment between query and support sequences of 3D body joints, we propose an advanced variant of Dynamic Time Warping which jointly models each smooth path between the query and support frames to achieve simultaneously the best alignment in the temporal and simulated camera viewpoint spaces for end-to-end learning under the limited few-shot training data. Sequences are encoded with a temporal block encoder based on Simple Spectral Graph Convolution, a lightweight linear Graph Neural Network backbone. We also include a setting with a transformer. Finally, we propose a similarity-based loss which encourages the alignment of sequences of the same class while preventing the alignment of unrelated sequences. We show state-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D Multiview Activity II.
PB  - arXiv
PY  - 2022
ST  - Temporal-Viewpoint Transportation Plan for Skeletal Few-shot Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-26316-3_19
ER  -


TY  - GEN
AU  - Rangrej, S.B.
AU  - Liang, K.J.
AU  - Hassner, T.
AU  - Clark, J.J.
TI  - GliTr: Glimpse Transformers with Spatiotemporal Consistency for Online Action Prediction
AB  - Many online action prediction models observe complete frames to locate and attend to informative subregions in the frames called glimpses and recognize an ongoing action based on global and local information. However, in applications with constrained resources, an agent may not be able to observe the complete frame, yet must still locate useful glimpses to predict an incomplete action based on local information only. In this paper, we develop Glimpse Transformers (GliTr), which observe only narrow glimpses at all times, thus predicting an ongoing action and the following most informative glimpse location based on the partial spatiotemporal information collected so far. In the absence of a ground truth for the optimal glimpse locations for action recognition, we train GliTr using a novel spatiotemporal consistency objective: We require GliTr to attend to the glimpses with features similar to the corresponding complete frames (i.e. spatial consistency) and the resultant class logits at time t equivalent to the ones predicted using whole frames up to t (i.e. temporal consistency). Inclusion of our proposed consistency objective yields ∼ 10% higher accuracy on the Something-Something-v2 (SSv2) dataset than the baseline cross-entropy objective. Overall, despite observing only ∼ 33% of the total area per frame, GliTr achieves 53.02% and 93.91% accuracy on the SSv2 and Jester datasets, respectively.
PB  - arXiv
PY  - 2022
ST  - GliTr
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/wacv56688.2023.00341
ER  -


TY  - GEN
AU  - Tan, J.
AU  - Zhao, X.
AU  - Shi, X.
AU  - Kang, B.
AU  - Wang, L.
TI  - PointTAD: Multi-Label Temporal Action Detection with Learnable Query Points
AB  - Traditional temporal action detection (TAD) usually handles untrimmed videos with small number of action instances from a single label (e.g., ActivityNet, THUMOS). However, this setting might be unrealistic as different classes of actions often co-occur in practice. In this paper, we focus on the task of multi-label temporal action detection that aims to localize all action instances from a multi-label untrimmed video. Multi-label TAD is more challenging as it requires for finegrained class discrimination within a single video and precise localization of the co-occurring instances. To mitigate this issue, we extend the sparse query-based detection paradigm from the traditional TAD and propose the multi-label TAD framework of PointTAD. Specifically, our PointTAD introduces a small set of learnable query points to represent the important frames of each action instance. This point-based representation provides a flexible mechanism to localize the discriminative frames at boundaries and as well the important frames inside the action. Moreover, we perform the action decoding process with the Multi-level Interactive Module to capture both point-level and instance-level action semantics. Finally, our PointTAD employs an end-to-end trainable framework simply based on RGB input for easy deployment. We evaluate our proposed method on two popular benchmarks and introduce the new metric of detection-mAP for multi-label TAD. Our model outperforms all previous methods by a large margin under the detection-mAP metric, and also achieves promising results under the segmentation-mAP metric. Code is available at https://github.com/MCG-NJU/PointTAD.
PB  - arXiv
PY  - 2022
ST  - PointTAD
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52733.2024.01756
ER  -


TY  - GEN
AU  - Ding, G.
AU  - Sener, F.
AU  - Yao, A.
TI  - Temporal Action Segmentation: An Analysis of Modern Techniques
AB  - Temporal action segmentation (TAS) in videos aims at densely identifying video frames in minutes-long videos with multiple action classes. As a long-range video understanding task, researchers have developed an extended collection of methods and examined their performance using various benchmarks. Despite the rapid growth of TAS techniques in recent years, no systematic survey has been conducted in these sectors. This survey analyzes and summarizes the most significant contributions and trends. In particular, we first examine the task definition, common benchmarks, types of supervision, and prevalent evaluation measures. In addition, we systematically investigate two essential techniques of this topic, i.e., frame representation and temporal modeling, which have been studied extensively in the literature. We then conduct a thorough review of existing TAS works categorized by their levels of supervision and conclude our survey by identifying and emphasizing several research gaps. In addition, we have curated a list of TAS resources, which is available at https://github.com/nus-cvml/awesome-temporal-action-segmentation.
PB  - arXiv
PY  - 2022
ST  - Temporal Action Segmentation
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tpami.2023.3327284
ER  -


TY  - GEN
AU  - Xu, Y.
AU  - Wang, Z.
TI  - Leveraging Spatial Residual Attention and Temporal Markov Networks for Video Action Understanding
AB  - The effective use of temporal relationships while extracting fertile spatial features is the key to video action understanding. Video action understanding is a challenging visual task because it generally necessitates not only the features of individual key frames but also the contextual understanding of the entire video and the relationships among key frames. Temporal relationships pose a challenge to video action understanding. However, existing 3D convolutional neural network approaches are limited, with a great deal of redundant spatial and temporal information. In this paper, we present a novel two-stream approach that incorporates Spatial Residual Attention and Temporal Markov (SRATM) to learn complementary features to achieve stronger video action understanding performance. The spatial residual attention network captures effective spatial feature representation. Temporal Markov networks enhance the model by learning the temporal relationships via conducting probabilistic logic calculation among frames in a video. Experiment results on three realistic video datasets, namely, Something-Something-V1, Something-Something-V2, and Diving48, show that the proposed method achieves competitive results.
PB  - SSRN
PY  - 2022
ST  - Leveraging Spatial Residual Attention and Temporal Markov Networks for Video Action Understanding
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4251285
ER  -


TY  - GEN
AU  - Gao, Z.
AU  - Wang, Y.
AU  - Chen, J.
AU  - Liu, X.
AU  - Shi, Y.
TI  - MMTSA: Multi-Modal Temporal Segment Attention Network for Efficient Human Activity Recognition
AB  - Multimodal sensors provide complementary information to develop accurate machine-learning methods for human activity recognition (HAR), but introduce significantly higher computational load, which reduces efficiency. This paper proposes an efficient multimodal neural architecture for HAR using an RGB camera and inertial measurement units (IMUs) called Multimodal Temporal Segment Attention Network (MMTSA). MMTSA first transforms IMU sensor data into a temporal and structure-preserving gray-scale image using the Gramian Angular Field (GAF), representing the inherent properties of human activities. MMTSA then applies a multimodal sparse sampling method to reduce data redundancy. Lastly, MMTSA adopts an inter-segment attention module for efficient multimodal fusion. Using three well-established public datasets, we evaluated MMTSA’s effectiveness and efficiency in HAR. Results show that our method achieves superior performance improvements (11.13% of cross-subject F1-score on the MMAct dataset) than the previous state-of-the-art (SOTA) methods. The ablation study and analysis suggest that MMTSA’s effectiveness in fusing multimodal data for accurate HAR. The efficiency evaluation on an edge device showed that MMTSA achieved significantly better accuracy, lower computational load, and lower inference latency than SOTA methods.
PB  - arXiv
PY  - 2022
ST  - MMTSA
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icmnwc63764.2024.10872293
ER  -


TY  - GEN
AU  - Ahn, D.
AU  - Kim, S.
AU  - Hong, H.
AU  - Ko, B.C.
TI  - STAR-Transformer: A Spatio-temporal Cross Attention Transformer for Human Action Recognition
AB  - In action recognition, although the combination of spatio-temporal videos and skeleton features can improve the recognition performance, a separate model and balancing feature representation for cross-modal data are required. To solve these problems, we propose Spatio-TemporAl cRoss (STAR)-transformer, which can effectively represent two cross-modal features as a recognizable vector. First, from the input video and skeleton sequence, video frames are output as global grid tokens and skeletons are output as joint map tokens, respectively. These tokens are then aggregated into multi-class tokens and input into STAR-transformer. The STAR-transformer encoder consists of a full spatio-temporal attention (FAttn) module and a proposed zigzag spatio-temporal attention (ZAttn) module. Similarly, the continuous decoder consists of a FAttn module and a proposed binary spatio-temporal attention (BAttn) module. STAR-transformer learns an efficient multi-feature representation of the spatio-temporal features by properly arranging pairings of the FAttn, ZAttn, and BAttn modules. Experimental results on the Penn-Action, NTU-RGB+D 60, and 120 datasets show that the proposed method achieves a promising improvement in performance in comparison to previous state-of-the-art methods. MSC Codes 68T07
PB  - arXiv
PY  - 2022
ST  - STAR-Transformer
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/wacv56688.2023.00333
ER  -


TY  - GEN
AU  - Liu, Y.
AU  - Zhou, N.
AU  - Zhang, F.
AU  - Liu, K.
AU  - Liu, Z.
TI  - Apsl: Action-Positive Separation Learning for Unsupervised Temporal Action Localization
AB  - Unsupervised temporal action localization in untrimmed videos is a challenging and open issue. Existing works focus on the "clustering + localization" framework for unsupervised temporal action localization. However, it heavily relies on features used for clustering and localization, \textit{e.g.}, features implying potential background information would degrade the localization performance. To address this problem, we propose a novel Action-positive Separation Learning method (APSL). APSL follows a novel "feature separation + clustering + localization" iterative procedure. First, we introduce a novel feature separation learning (FSL) module. FSL employs separation learning to identify action and background features in a video, and then refines and removes potential action-negative and background-negative features (\textit{hard-to-locate}) from the identified features employing contrastive learning, thus obtaining action-positive features (\textit{easy-to-locate}). Then, in "clustering" step, we apply clustering to the separated action-positive features to obtain action pseudo-labels. In "localization" step, with action pseudo-labels and action-positive features, we employ a temporal action localization module to locate action instance regions, in turn, improving the performance of clustering and FSL. The three steps learn iteratively and reinforce each other during training. Comprehensive evaluations conducted on the THUMOS’14 and ActivityNet v1.2 datasets demonstrate that our method outperforms cutting-edge weakly-supervised and unsupervised methods, obtaining state-of-the-art performance.
PB  - SSRN
PY  - 2022
ST  - Apsl
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.ins.2023.02.047
ER  -


TY  - GEN
AU  - Duan, H.
AU  - Wang, J.
AU  - Chen, K.
AU  - Lin, D.
TI  - DG-STGCN: Dynamic Spatial-Temporal Modeling for Skeleton-based Action Recognition
AB  - Graph convolution networks (GCN) have been widely used in skeleton-based action recognition. We note that existing GCN-based approaches primarily rely on prescribed graphical structures (i.e., a manually defined topology of skeleton joints), which limits their flexibility to capture complicated correlations between joints. To move beyond this limitation, we propose a new framework for skeleton-based action recognition, namely Dynamic Group Spatio-Temporal GCN (DG-STGCN). It consists of two modules, DG-GCN and DG-TCN, respectively, for spatial and temporal modeling. In particular, DG-GCN uses learned affinity matrices to capture dynamic graphical structures instead of relying on a prescribed one, while DG-TCN performs group-wise temporal convolutions with varying receptive fields and incorporates a dynamic joint-skeleton fusion module for adaptive multi-level temporal modeling. On a wide range of benchmarks, including NTURGB+D, Kinetics-Skeleton, BABEL, and Toyota SmartHome, DG-STGCN consistently outperforms state-of-the-art methods, often by a notable margin.
PB  - arXiv
PY  - 2022
ST  - DG-STGCN
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-981-97-5594-3_21
ER  -


TY  - GEN
AU  - Maingi, V.
AU  - Zhang, Z.
AU  - Thachuk, C.
AU  - Chapman, E.R.
AU  - Rothemund, P.W.K.
TI  - Digital nanoreactors for control over absolute stoichiometry and spatiotemporal behavior of receptors within lipid bilayers
AB  - Interactions between membrane proteins are essential for cell survival and proper function, but the structural and mechanistic details of these interactions are often poorly understood. Even the biologically functional ratio of protein components within a multi-subunit membrane complex—the native stoichiometry—is difficult to establish. We have demonstrated digital nanoreactors that can control interactions between lipid-bound molecular receptors along three key dimensions: stoichiometric, spatial, and temporal. Each nanoreactor is based on a DNA origami ring, which both templates the synthesis of a liposome and provides tethering sites for DNA-based receptors. Receptors are released into the liposomal membrane using strand displacement and a DNA logic gate measures receptor heterodimer formation. High-efficiency tethering of receptors enables the kinetics of receptors in 1:1 and 2:2 absolute stoichiometries to be observed by bulk fluorescence in a plate reader which in principle is generalizable to any ratio. Similar ‘single molecule in bulk’ experiments using DNA-linked membrane proteins could determine native stoichiometry and the kinetics of membrane protein interactions for applications ranging from signalling research to drug discovery.
PB  - bioRxiv
PY  - 2022
ST  - Digital nanoreactors for control over absolute stoichiometry and spatiotemporal behavior of receptors within lipid bilayers
Y2  - 2025/05/05/21:54:31
ER  -


TY  - GEN
AU  - Khaertdinov, B.
AU  - Asteriadis, S.
TI  - Temporal Feature Alignment in Contrastive Self-Supervised Learning for Human Activity Recognition
AB  - Automated Human Activity Recognition has long been a problem of great interest in human-centered and ubiquitous computing. In the last years, a plethora of supervised learning algorithms based on deep neural networks has been suggested to address this problem using various modalities. While every modality has its own limitations, there is one common challenge. Namely, supervised learning requires vast amounts of annotated data which is practically hard to collect. In this paper, we benefit from the self-supervised learning paradigm (SSL) that is typically used to learn deep feature representations from unlabeled data. Moreover, we upgrade a contrastive SSL framework, namely SimCLR, widely used in various applications by introducing a temporal feature alignment procedure for Human Activity Recognition. Specifically, we propose integrating a dynamic time warping (DTW) algorithm in a latent space to force features to be aligned in a temporal dimension. Extensive experiments have been conducted for the unimodal scenario with inertial modality as well as in multimodal settings using inertial and skeleton data. According to the obtained results, the proposed approach has a great potential in learning robust feature representations compared to the recent SSL baselines, and clearly outperforms supervised models in semi-supervised learning.
PB  - arXiv
PY  - 2022
ST  - Temporal Feature Alignment in Contrastive Self-Supervised Learning for Human Activity Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/ijcb54206.2022.10007984
ER  -


TY  - GEN
AU  - Gao, Z.
AU  - Wang, P.
AU  - Lv, P.
AU  - Xu, M.
AU  - Li, W.
TI  - Focal and Global Spatial-Temporal Transformer for Skeleton-based Action Recognition
AB  - Despite great progress achieved by transformer in various vision tasks, it is still underexplored for skeleton-based action recognition with only a few attempts. Besides, these methods directly calculate the pair-wise global self-attention equally for all the joints in both the spatial and temporal dimensions, undervaluing the effect of discriminative local joints and the short-range temporal dynamics. In this work, we propose a novel Focal and Global Spatial-Temporal Transformer network (FG-STFormer), that is equipped with two key components: (1) FG-SFormer: focal joints and global parts coupling spatial transformer. It forces the network to focus on modelling correlations for both the learned discriminative spatial joints and human body parts respectively. The selective focal joints eliminate the negative effect of non-informative ones during accumulating the correlations. Meanwhile, the interactions between the focal joints and body parts are incorporated to enhance the spatial dependencies via mutual cross-attention. (2) FG-TFormer: focal and global temporal transformer. Dilated temporal convolution is integrated into the global self-attention mechanism to explicitly capture the local temporal motion patterns of joints or body parts, which is found to be vital important to make temporal transformer work. Extensive experimental results on three benchmarks, namely NTU-60, NTU-120 and NW-UCLA, show our FG-STFormer surpasses all existing transformer-based methods, and compares favourably with state-of-the-art GCN-based methods.
PB  - arXiv
PY  - 2022
ST  - Focal and Global Spatial-Temporal Transformer for Skeleton-based Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-26316-3_10
ER  -


TY  - GEN
AU  - Vo Ho Viet, K.
AU  - Truong, S.
AU  - Yamazaki, K.
AU  - Tran, M.-T.
AU  - Le, N.
TI  - AOE-Net: Entities Interactions Modeling with Adaptive Attention Mechanism for Temporal Action Proposals Generation
AB  - Temporal action proposal generation (TAPG) is a challenging task, which requires localizing action intervals in an untrimmed video. Intuitively, we as humans, perceive an action through the interactions between actors, relevant objects, and the surrounding environment. Despite the significant progress of TAPG, a vast majority of existing methods ignore the aforementioned principle of the human perceiving process by applying a backbone network into a given video as a black-box. In this paper, we propose to model these interactions with a multi-modal representation network, namely, Actors-Objects-Environment Interaction Network (AOE-Net). Our AOE-Net consists of two modules, i.e., perception-based multi-modal representation (PMR) and boundary-matching module (BMM). Additionally, we introduce adaptive attention mechanism (AAM) in PMR to focus only on main actors (or relevant objects) and model the relationships among them. PMR module represents each video snippet by a visual-linguistic feature, in which main actors and surrounding environment are represented by visual information, whereas relevant objects are depicted by linguistic features through an image-text model. BMM module processes the sequence of visual-linguistic features as its input and generates action proposals. Comprehensive experiments and extensive ablation studies on ActivityNet-1.3 and THUMOS-14 datasets show that our proposed AOE-Net outperforms previous state-of-the-art methods with remarkable performance and generalization for both TAPG and temporal action detection. To prove the robustness and effectiveness of AOE-Net, we further conduct an ablation study on egocentric videos, i.e. EPIC-KITCHENS 100 dataset. Our source code is publicly available at https://github.com/UARK-AICV/AOE-Net.
PB  - arXiv
PY  - 2022
ST  - AOE-Net
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s11263-022-01702-9
ER  -


TY  - GEN
AU  - Setogawa, S.
AU  - Kanda, R.
AU  - Tada, S.
AU  - Sekiguchi, H.
AU  - Ohkawa, N.
TI  - A novel micro-ECoG recording method for recording multisensory neural activity from the parietal to temporal cortices in mice
AB  - Characterization of inter-regional interactions in brain is essential for understanding the mechanism relevant to normal brain function and neurological disease. The recently developed flexible micro (μ)-electrocorticography (μECoG) device is one prominent method used to examine large-scale cortical activity across multiple regions. The sheet-shaped μECoG electrodes arrays can be placed on a relatively wide area of cortical surface beneath the skull by inserting the device into the space between skull and brain. Although rats and mice are useful tools for neuroscience, current μECoG recording methods in these animals are limited to the parietal region of cerebral cortex. Recording cortical activity from the temporal region of cortex in mice has proven difficult because of surgical barriers created by the skull and surrounding temporalis muscle anatomy. Here, we developed a sheet-shaped 64-channel μECoG device that allows access to the mouse temporal cortex, and we determined the factor determining the appropriate bending stiffness for the μECoG electrode array. We also established a surgical technique to implant the electrode arrays into the epidural space over a wide area of cerebral cortex covering from the barrel field to olfactory (piriform) cortex, which is the deepest region of the cerebral cortex. Using histology and computed tomography (CT) images, we confirmed that the tip of the μECoG device reached to the most ventral part of cerebral cortex without causing noticeable damage to the brain surface. Moreover, the device simultaneously recorded somatosensory and odor stimulus-evoked neural activity from dorsal and ventral parts of cerebral cortex in awake and anesthetized mice. These data indicate that our μECoG device and surgical techniques enable the recording of large-scale cortical activity from the parietal to temporal cortex in mice, including somatosensory and olfactory cortices. This system will provide more opportunities for the investigation of physiological functions from wider areas of the mouse cerebral cortex than those currently available with existing ECoG techniques.
PB  - bioRxiv
PY  - 2022
ST  - A novel micro-ECoG recording method for recording multisensory neural activity from the parietal to temporal cortices in mice
Y2  - 2025/05/05/21:54:31
DO  - 10.1186/s13041-023-01019-9
ER  -


TY  - GEN
AU  - Nwe, T.Z.
AU  - Maaroufi, N.I.
AU  - Allan, E.
AU  - Soliveres, S.
AU  - Kempel, A.
TI  - Plant attributes interact with fungal pathogens and nitrogen addition to drive soil enzymatic activities and their temporal variation
AB  - Nitrogen enrichment can alter soil communities and their functioning directly, via changes in nutrient availability and stoichiometry, or indirectly, by changing plant communities or higher trophic levels. In addition, soil biota and their associated functions may show strong temporal changes in their response to environmental changes, yet most current studies have only focused on one of these potential drivers or have measured soil functioning only once during the peak growing season. Therefore, we know little about the relative importance of the different mechanisms by which nitrogen enrichment affects soil communities, functioning and temporal stability. In a large grassland experiment manipulating nitrogen enrichment, plant species richness, functional composition and foliar pathogen presence, we measured activities of two enzymes, β-glucosidase and acid phosphatase, as indicators of soil functioning. We did so across different seasons and years to assess their temporal dynamics and how consistently they responded to multiple drivers. Nitrogen addition was the most important driver of β-glucosidase activity, and it increased β-glucosidase activity over time. However, interactions between plant attributes and fungicide application were the main drivers of acid phosphatase activity. The temporal stability of soil enzyme activity was differently affected by two facets of plant diversity (species richness [+] and functional diversity [-]), with nitrogen and fungicide addition dampening these effects. Synthesis: Fungicide effects, and their interactions with plant diversity, show the importance of foliar pathogens not only for above- but also for belowground processes, and highlight the possibility that these plant enemies are major modulators in the relationships between plant diversity and ecosystem functioning. We also show the need to consider temporal dynamics in belowground processes to better understand the responses of ecosystem functioning to environmental changes such as nutrient enrichment.
PB  - bioRxiv
PY  - 2022
ST  - Plant attributes interact with fungal pathogens and nitrogen addition to drive soil enzymatic activities and their temporal variation
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.09.29.510102
ER  -


TY  - GEN
AU  - Zhao, Y.
AU  - Li, Z.
AU  - Guo, X.
AU  - Lu, Y.
TI  - Alignment-guided Temporal Attention for Video Action Recognition
AB  - Temporal modeling is crucial for various video learning tasks. Most recent approaches employ either factorized (2D+1D) or joint (3D) spatial-temporal operations to extract temporal contexts from the input frames. While the former is more efficient in computation, the latter often obtains better performance. In this paper, we attribute this to a dilemma between the sufficiency and the efficiency of interactions among various positions in different frames. These interactions affect the extraction of task-relevant information shared among frames. To resolve this issue, we prove that frame-by-frame alignments have the potential to increase the mutual information between frame representations, thereby including more task-relevant information to boost effectiveness. Then we propose Alignment-guided Temporal Attention (ATA) to extend 1-dimensional temporal attention with parameter-free patch-level alignments between neighboring frames. It can act as a general plug-in for image backbones to conduct the action recognition task without any model-specific design. Extensive experiments on multiple benchmarks demonstrate the superiority and generality of our module.
PB  - arXiv
PY  - 2022
ST  - Alignment-guided Temporal Attention for Video Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccvw.2019.00189
ER  -


TY  - GEN
AU  - Cho, S.-H.
AU  - Kim, D.-K.
AU  - Park, S.H.
TI  - Modeling Daily Sequential Trip-Based Mode Choice Behaviors in an Activity-Based Framework Considering Spatiotemporal Constraints
AB  - Over the last four decades, researchers have attempted to build a demand-forecasting model using an activity-based model that considers individual travel behavior. This study aims to develop a sequential trip-based mode choice model using an activity-based framework that considers spatiotemporal constraints. The trip unit is defined to identify the study target, and the sequential trip pattern is analyzed to consider various combinations of travel modes. The adequacy of the model is reviewed through the verification of independence of irrelevant alternatives to estimate the sequential-trip-based mode choice behavior model using a multinomial logit model. Sequential trip patterns were analyzed using the Korean National Household Survey and passively generated data from the city of Seoul. In addition, this study validates the estimated model through the percent correctly predicted test using part of the Korean national household travel survey data. From the analysis, it was found that the previous trip mode has a significant effect on the current choice of travel mode, and the mode choice behaviors differ depending on the travel purpose. This study presents the improved demand forecasting model in a sequential trip-based mode choice framework considering the spatiotemporal constraints in urban areas where the public transport is developed.
PB  - SSRN
PY  - 2022
ST  - Modeling Daily Sequential Trip-Based Mode Choice Behaviors in an Activity-Based Framework Considering Spatiotemporal Constraints
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4231922
ER  -


TY  - GEN
AU  - Wen, W.
AU  - Li, Y.
AU  - Dong, Z.
AU  - Yang, W.
AU  - Liu, S.
TI  - Streaming Video Temporal Action Segmentation In Real Time
AB  - Temporal action segmentation (TAS) is a critical step toward long-term video understanding. Recent studies based on the pre-extracted video features instead of raw video image information. However, those models are trained complicatedly and limit application scenarios. It is hard for them to segment human actions of video in real time because they must work after the full video features are extracted. As the real-time action segmentation task is different from TAS task, we define it as streaming video real-time temporal action segmentation (SVTAS) task. In this paper, we propose a real-time end-to-end multi-modality model for SVTAS task. More specifically, we incrementally propose three frameworks to solve the SVTAS task and enhance the model performance step-by-step. To the best of our knowledge, it is the first multi-modality real-time temporal action segmentation model. Under the same evaluation criteria as full video TAS, our model segments human action in real time with less than 40% of state-of-the-art model computation and achieves 90% of the accuracy of the full video state-of-the-art model. Code is available at https://github.com/Thinksky5124/SVTAS.git.
PB  - arXiv
PY  - 2022
ST  - Streaming Video Temporal Action Segmentation In Real Time
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iske60036.2023.10481438
ER  -


TY  - GEN
AU  - Sili, D.
AU  - De Giorgi, C.
AU  - Pizzuti, A.
AU  - de Pasquale, F.
AU  - Betti, V.
TI  - The spatio-temporal architecture of everyday manual behavior
AB  - In everyday activities, humans use a finite number of postural hand configurations, but how do they flow into each other to create sophisticated manual behavior? We hypothesized that hand movement emerges through the temporal dynamics of a set of recurrent hand shapes characterized by specific transitions. Through a sensorized glove, we collected kinematics data from thirty-six participants preparing and having breakfast in naturalistic conditions. By means of a combined PCA/clustering-based approach, we identified a repertoire of hand states and their transitions over time. We found that manual behavior can be described in space through a complex organization of basic configurations. These, even in an unconstrained experiment, recurred across subjects. A specific temporal structure, highly consistent within the sample, seems to integrate such identified hand shapes to realize skilled movements. Our findings suggest that the simplification of the motor commands unravels in the temporal dimension more than in the spatial one.
PB  - bioRxiv
PY  - 2022
ST  - The spatio-temporal architecture of everyday manual behavior
Y2  - 2025/05/05/21:54:31
DO  - 10.1038/s41598-023-36280-4
ER  -


TY  - GEN
AU  - Wen, Y.
AU  - Pan, H.
AU  - Yang, L.
AU  - Komura, T.
AU  - Wang, W.
TI  - Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos
AB  - Understanding dynamic hand motions and actions from egocentric RGB videos is a fundamental yet challenging task due to self-occlusion and ambiguity. To address occlusion and ambiguity, we develop a transformer-based framework to exploit temporal information for robust estimation. Noticing the different temporal granularity of and the semantic correlation between hand pose estimation and action recognition, we build a network hierarchy with two cascaded transformer encoders, where the first one exploits the short-term temporal cue for hand pose estimation, and the latter aggregates per-frame pose and object information over a longer time span to recognize the action. Our approach achieves competitive results on two first-person hand action benchmarks, namely FPHA and H2O. Extensive ablation studies verify our design choices.
PB  - arXiv
PY  - 2022
ST  - Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52729.2023.02035
ER  -


TY  - GEN
AU  - Broholt, T.H.
AU  - Amato, V.
AU  - Christensen, L.R.L.
AU  - Kristensen, M.H.
AU  - Petersen, S.
TI  - Opportunities and Barriers for Temporal Demand Response as an Action to Challenges in District Heating
AB  - Recent studies assume that district heating (DH) operators can substitute traditional supply-side management initiatives with temporal demand response (DR) from building heating systems to eliminate and/or mitigate operational challenges. The study reported in this paper investigated whether DH practitioners are currently aligned with this assumption by mapping the current, prevailing, and expected future challenges of DH systems that temporal DR from building space heating systems could mitigate or substitute. An initial literature review identified a need for further investigations into the current attitude of DH practitioners towards temporal DR from building space heating systems as alternative actions to current challenges. Seven semi-structured interviews with employees of Danish DH companies were therefore conducted leading to insights that could help researchers target the development of temporal DR solutions that facilitates the practical transition to 4GDH and 5GDH. The attitude of the seven interviewees towards temporal DR as a solution to DH challenges was positive but not representative of the whole DH industry. It is therefore proposed that future work is to conduct a questionnaire survey to analyse the distribution, prevalence, and importance of the findings presented in this paper.
PB  - SSRN
PY  - 2022
ST  - Opportunities and Barriers for Temporal Demand Response as an Action to Challenges in District Heating
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4134863
ER  -


TY  - GEN
AU  - Gholami, E.
AU  - Motamedi, M.
AU  - Aravindakshan, A.
TI  - PARSRec: Explainable Personalized Attention-fused Recurrent Sequential Recommendation Using Session Partial Actions
AB  - The emerging meta- and multi-verse landscape is yet another step towards the more prevalent use of already ubiquitous online markets. In such markets, recommender systems play critical roles by offering items of interest to the users, thereby narrowing down a vast search space that comprises hundreds of thousands of products. Recommender systems are usually designed to learn common user behaviors and rely on them for inference. This approach, while effective, is oblivious to subtle idiosyncrasies that differentiate humans from each other. Focusing on this observation, we propose an architecture that relies on common patterns as well as individual behaviors to tailor its recommendations for each person. Simulations under a controlled environment show that our proposed model learns interpretable personalized user behaviors. Our empirical results on Nielsen Consumer Panel dataset indicate that the proposed approach achieves up to 27.9% performance improvement compared to the state-of-the-art.
PB  - arXiv
PY  - 2022
ST  - PARSRec
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/ijcnn52387.2021.9533533
ER  -


TY  - GEN
AU  - Aziere, N.
AU  - Todorovic, S.
TI  - Multistage Temporal Convolution Transformer for Action Segmentation
AB  - This paper addresses fully supervised action segmentation. Transformers have been shown to have large model capacity and powerful sequence modelling abilities, and hence seem quite suitable for capturing action grammar in videos. However, their performance in video understanding still lags behind that of temporal convolutional networks, or ConvNets for short. We hypothesize that this is because: (i) ConvNets tend to generalize better than Transformers, and (ii) Transformer’s large model capacity requires significantly larger training datasets than existing action segmentation benchmarks. We specify a new hybrid model, TCTr, that combines the strengths from both frameworks. TCTr seamlessly unifies depth-wise convolution and selfattention in a principled manner. Also, TCTr addresses the Transformer’s quadratic computational and memory complexity in the sequence length by learning how to adaptively estimate attention from local temporal neighborhoods, instead of all frames. Our experiments show that TCTr significantly outperforms the state of the art on the Breakfast, GTEA, and 50Salads datasets.
PB  - SSRN
PY  - 2022
ST  - Multistage Temporal Convolution Transformer for Action Segmentation
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.imavis.2022.104567
ER  -


TY  - GEN
AU  - Athanasiou, N.
AU  - Petrovich, M.
AU  - Black, M.J.
AU  - Varol, G.
TI  - TEACH: Temporal Action Composition for 3D Humans
AB  - Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for "TEmporal Action Compositions for Human motions", produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at teach.is.tue.mpg.de.
PB  - arXiv
PY  - 2022
ST  - TEACH
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/3dv57658.2022.00053
ER  -


TY  - GEN
AU  - Kim, S.
AU  - Lee, K.
AU  - Lee, M.
AU  - Ahn, T.
AU  - Lim, J.-T.
TI  - Spatiotemporal Interpretation of Three-Phase Saturation Behaviors in Gas Hydrate Formation and Dissociation Through Deep Learning Modeling
AB  - This study provides an interpretation of the three-phase saturation (water, gas, gas hydrate (GH); S W , S G , S GH ) in the GH cores during GH formation and depressurization experiments. The saturations are predicted in real-time based on X-ray computed tomography (CT) images using the verified deep learning model reported earlier (Kim et al., 2022), using the convolutional neural network (CNN) with data augmentation. The interpretation explains the saturation behaviors spatiotemporally: depressurization schedule from 20 MPa to 0 MPa, in terms of time and 64 CT images in the longitudinal direction of the core, in terms of space. Time-wise, there is a difference in pressure propagation from the inlet (1 st slice) to the outlet (64 th slice) within the core during the depressurization for GH dissociation. This transient status leads to increasing variations in S W and S GH near the dissociation pressure (16 MPa) and declining variations at the end of depressurization (0 MPa). Space-wise, before GH dissociation, GH is highly saturated near the outlet, while S GH is low near the inlet. This gradational trend in S GH within the core results from the spatial distribution of the initial S W in the preliminary stage; GH was formed in high amounts with high S W as long as sufficient amounts of methane were provided. This spatiotemporal analysis of the saturation behaviors is in accordance with the domain knowledge. Therefore, an integrated analysis including not only the proposed deep learning model but also a reasonable explanation for the saturation behaviors will help future research on GH-related.
PB  - SSRN
PY  - 2022
ST  - Spatiotemporal Interpretation of Three-Phase Saturation Behaviors in Gas Hydrate Formation and Dissociation Through Deep Learning Modeling
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.petrol.2022.111219
ER  -


TY  - GEN
AU  - Singh, G.
AU  - Choutas, V.
AU  - Saha, S.
AU  - Yu, F.
AU  - Van Gool, L.
TI  - Spatio-Temporal Action Detection Under Large Motion
AB  - Current methods for spatio-temporal action tube detection often extend a bounding box proposal at a given keyframe into a 3D temporal cuboid and pool features from nearby frames. However, such pooling fails to accumulate meaningful spatio-temporal features if the position or shape of the actor shows large 2D motion and variability through the frames, due to large camera motion, large actor shape deformation, fast actor action and so on. In this work, we aim to study the performance of cuboid-aware feature aggregation in action detection under large action. Further, we propose to enhance actor feature representation under large motion by tracking actors and performing temporal feature aggregation along the respective tracks. We define the actor motion with intersection-over-union (IoU) between the boxes of action tubes/tracks at various fixed time scales. The action having a large motion would result in lower IoU over time, and slower actions would maintain higher IoU. We find that track-aware feature aggregation consistently achieves a large improvement in action detection performance, especially for actions under large motion compared to cuboid-aware baseline. As a result, we also report state-of-the-art on the large-scale MultiSports dataset. The Code is available at https://github.com/gurkirt/ActionTrackDetectron.
PB  - arXiv
PY  - 2022
ST  - Spatio-Temporal Action Detection Under Large Motion
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/wacv56688.2023.00595
ER  -


TY  - GEN
AU  - Zou, F.
AU  - Guo, W.
AU  - Allen, E.J.
AU  - Hutchinson, J.B.
AU  - DuBrow, S.
TI  - Re-expression of CA1 and entorhinal activity patterns preserves temporal context memory at long timescales
AB  - Converging, cross-species evidence indicates that memory for time is supported by hippocampal area CA1 and entorhinal cortex. However, limited evidence characterizes how these regions preserve temporal memories over long timescales (e.g., months). At long timescales, memoranda may be encountered in multiple temporal contexts, potentially creating interference. Here, using 7T fMRI, we measured CA1 and entorhinal activity patterns as human participants viewed thousands of natural scene images distributed, and repeated, across many months. We show that memory for an image's original temporal context was predicted by the degree to which CA1/entorhinal activity patterns from the first encounter with an image were re-expressed during re-encounters occurring minutes to months later. Critically, temporal memory signals were dissociable from predictors of recognition confidence, which were carried by distinct medial temporal lobe expressions. These findings suggest that CA1 and entorhinal cortex preserve temporal memories across long timescales by coding for and reinstating temporal context information.
PB  - bioRxiv
PY  - 2022
ST  - Re-expression of CA1 and entorhinal activity patterns preserves temporal context memory at long timescales
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.08.31.506090
ER  -


TY  - GEN
AU  - Li, T.
AU  - Foo, L.G.
AU  - Ke, Q.
AU  - Wang, J.
AU  - Liu, J.
TI  - Dynamic Spatio-Temporal Specialization Learning for Fine-Grained Action Recognition
AB  - The goal of fine-grained action recognition is to successfully discriminate between action categories with subtle differences. To tackle this, we derive inspiration from the human visual system which contains specialized regions in the brain that are dedicated towards handling specific tasks. We design a novel Dynamic Spatio-Temporal Specialization (DSTS) module, which consists of specialized neurons that are only activated for a subset of samples that are highly similar. During training, the loss forces the specialized neurons to learn discriminative fine-grained differences to distinguish between these similar samples, improving fine-grained recognition. Moreover, a spatio-temporal specialization method further optimizes the architectures of the specialized neurons to capture either more spatial or temporal fine-grained information, to better tackle the large range of spatio-temporal variations in the videos. Lastly, we design an Upstream-Downstream Learning algorithm to optimize our model's dynamic decisions during training, improving the performance of our DSTS module. We obtain state-of-the-art performance on two widely-used fine-grained action recognition datasets.
PB  - arXiv
PY  - 2022
ST  - Dynamic Spatio-Temporal Specialization Learning for Fine-Grained Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-19772-7_23
ER  -


TY  - GEN
AU  - Meyer, J.
AU  - Yu, K.
AU  - Luna-Figueroa, E.
AU  - Deneen, B.
AU  - Noebels, J.
TI  - Glioblastoma disrupts cortical network activity at multiple spatial and temporal scales
AB  - The emergence of glioblastoma in cortical tissue initiates early and persistent neural hyperexcitability with signs ranging from mild cognitive impairment to convulsive seizures. The influence of peritumoral synaptic density, growth dynamics, and spatial contours of excess glutamate upon higher order neuronal network modularity is unknown. We combined cellular and widefield imaging of calcium and glutamate fluorescent reporters in two GBM mouse models with distinct synaptic microenvironments and growth profiles. Functional metrics of neural ensembles are dysregulated during tumor invasion depending on the stage of malignant progression and tumor cell proximity. Neural activity is significantly elevated during periods of accelerated tumor growth. Abnormal glutamate accumulation precedes and outpaces the spatial extent of baseline neuronal calcium signaling, indicating these processes are uncoupled in tumor cortex. Distinctive excitability homeostasis patterns and functional connectivity of local and remote neuronal populations support the promise of precision genetic diagnosis and management of this devastating brain disease.
PB  - bioRxiv
PY  - 2022
ST  - Glioblastoma disrupts cortical network activity at multiple spatial and temporal scales
Y2  - 2025/05/05/21:54:31
DO  - 10.1038/s41467-024-48757-5
ER  -


TY  - GEN
AU  - Radostova, D.
AU  - Kuncicka, D.
AU  - Krajcovic, B.
AU  - Stuchlik, A.
AU  - Brozka, H.
TI  - Incidental Temporal Binding in Rats: A Novel Behavioral Task Relevant to Episodic Memory
AB  - We designed a behavioral task called One-Trial Trace Escape Reaction (OTTER), in which rats incidentally associate two temporally discontinuous stimuli: a neutral acoustic cue (CS) with an aversive stimulus (US) which occurs two seconds later (CS-2s-US sequence). Rats are first habituated to two similar environmental contexts (A and B), each consisting of an interconnected dark and light chamber. Next, rats experience the CS-2s-US sequence in the dark chamber of one of the contexts (either A or B); the US is terminated immediately after a rat escapes into the light chamber. The CS-2s-US sequence is presented only once to ensure the incidental acquisition of the association. The recall is tested 24 h later when rats are presented with only the CS in the alternate context (B or A), and their behavioral response is observed. Our results show that 59 % of the rats responded to the CS by escaping to the light chamber, although they experienced only one CS-2s-US pairing. The OTTER task offers a flexible high throughput tool to study memory acquired incidentally after a single experience. Incidental acquisition of association between temporally discontinuous events is highly relevant to episodic memory formation.
PB  - bioRxiv
PY  - 2022
ST  - Incidental Temporal Binding in Rats
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2022.08.30.505895
ER  -


TY  - GEN
AU  - Lee, H.
AU  - Son, S.B.
AU  - Yun, W.J.
AU  - Jung, S.
AU  - Kim, D.H.
TI  - Spatio-Temporal Attack Course-of-Action (COA) Search Learning for Scalable and Time-Varying Networks
AB  - One of the key topics in network security research is the autonomous COA (Couse-of-Action) attack search method. Traditional COA attack search methods that passively search for attacks can be difficult, especially as the network gets bigger. To address these issues, new autonomous COA techniques are being developed, and among them, an intelligent spatial algorithm is designed in this paper for efficient operations in scalable networks. On top of the spatial search, a Monte-Carlo (MC)based temporal approach is additionally considered for taking care of time-varying network behaviors. Therefore, we propose a spatio-temporal attack COA search algorithm for scalable and time-varying networks.
PB  - arXiv
PY  - 2022
ST  - Spatio-Temporal Attack Course-of-Action (COA) Search Learning for Scalable and Time-Varying Networks
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/ictc55196.2022.9952999
ER  -


TY  - GEN
AU  - Wan, H.
AU  - Luo, M.
AU  - Li, Z.
AU  - Wang, Y.
TI  - Test: Temporal-Spatial Separated Transformer for Temporal Action Localization
AB  - Temporal action localization is a fundamental task in the video understanding field. Existing methods fall into three categories: anchor-based, actionness-guided, and anchor-free. Anchor-based models generate redundant proposals, and actionness-guided models calculate the actionness of each time step and enumerate every possible proposal. They need huge computation resources to process a large number of outputs and fine-tune the locations. As video resolution and the number of action instances increase, anchor-free models with fewer parameters become a more attractive option. However, while these models are advantageous in parameter efficiency, they typically struggle to achieve high performance due to the need to aggregate temporal-spatial features at every time step, which often fails to detect long-lasting action instances. To overcome this limitation, we design three efficient transformer-based architectures. They make spatial and temporal information of a video clip interact and obtain temporal features at each time step. Our designed architectures are adapted to any anchor-free frameworks. Based on the architectures, we propose a simple but effective framework named TeST. Compared to state-of-the-art baselines, TeST achieves 0.96% to 3.20% improvement of performance on two real-world datasets. Meanwhile, it improves time efficiency by 1.36 times and space efficiency by $1.08$ times. Further experiments prove the effectiveness of TeST's modules.
PB  - SSRN
PY  - 2024
ST  - Test
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4837237
ER  -


TY  - GEN
AU  - Cobos, M.E.
AU  - Winters, T.
AU  - Martinez, I.
AU  - Little, S.E.
AU  - Peterson, A.T.
TI  - Spatiotemporal dynamic models of Amblyomma americanum questing activity in the Central Great Plains
AB  - Ticks represent important vectors of a number of bacterial and viral disease agents, owing to their hematophagous nature and their questing behavior (the process in which they seek new hosts). Questing activity is notably seasonal with spatiotemporal dynamics that needs to be understood in detail as part of mediating and mitigating tick-borne disease risk. Models of the geography of tick questing activity developed to date, however, have ignored the temporal dimensions of that behavior; more fundamentally, they have often not considered the sampling underlying available occurrence data. Here, we have addressed these shortfalls for Amblyomma americanum, the most commonly encountered tick in the central Great Plains, via (1) detailed, longitudinal sampling to characterize the spatiotemporal dimensions of tick questing activity; (2) randomization tests to establish in which environmental dimensions a species is manifesting selective use; and (3) modeling methods that include both presence data and absence data, taking fullest advantage of the information available in the data resource. The outcome was a detailed picture of geographic and temporal variation in suitability for the species through the two-year course of this study. Such models that take full advantage of available information will be crucial in understanding the risk of tick-borne disease into the future.
PB  - bioRxiv
PY  - 2024
ST  - Spatiotemporal dynamic models of Amblyomma americanum questing activity in the Central Great Plains
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2024.05.14.594229
ER  -


TY  - GEN
AU  - Wu, T.
AU  - Ge, S.
AU  - Qin, J.
AU  - Wu, G.
AU  - Wang, L.
TI  - Open-Vocabulary Spatio-Temporal Action Detection
AB  - Spatio-temporal action detection (STAD) is an important fine-grained video understanding task. Current methods require box and label supervision for all action classes in advance. However, in real-world applications, it is very likely to come across new action classes not seen in training because the action category space is large and hard to enumerate. Also, the cost of data annotation and model training for new classes is extremely high for traditional methods, as we need to perform detailed box annotations and re-train the whole network from scratch. In this paper, we propose a new challenging setting by performing open-vocabulary STAD to better mimic the situation of action detection in an open world. Open-vocabulary spatio-temporal action detection (OV-STAD) requires training a model on a limited set of base classes with box and label supervision, which is expected to yield good generalization performance on novel action classes. For OV-STAD, we build two benchmarks based on the existing STAD datasets and propose a simple but effective method based on pretrained video-language models (VLM). To better adapt the holistic VLM for the fine-grained action detection task, we carefully fine-tune it on the localized video region-text pairs. This customized fine-tuning endows the VLM with better motion understanding, thus contributing to a more accurate alignment between video regions and texts. Local region feature and global video feature fusion before alignment is adopted to further improve the action detection performance by providing global context. Our method achieves a promising performance on novel classes.
PB  - arXiv
PY  - 2024
ST  - Open-Vocabulary Spatio-Temporal Action Detection
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/fg59268.2024.10581896
ER  -


TY  - GEN
AU  - Korban, M.
AU  - Youngs, P.
AU  - Acton, S.T.
TI  - A Semantic and Motion-Aware Spatiotemporal Transformer Network for Action Detection
AB  - This paper presents a novel spatiotemporal transformer network that introduces several original components to detect actions in untrimmed videos. First, the multi-feature selective semantic attention model calculates the correlations between spatial and motion features to model spatiotemporal interactions between different action semantics properly. Second, the motion-aware network encodes the locations of action semantics in video frames utilizing the motion-aware 2D positional encoding algorithm. Such a motion-aware mechanism memorizes the dynamic spatiotemporal variations in action frames that current methods cannot exploit. Third, the sequence-based temporal attention model captures the heterogeneous temporal dependencies in action frames. In contrast to standard temporal attention used in natural language processing, primarily aimed at finding similarities between linguistic words, the proposed sequence-based temporal attention is designed to determine both the differences and similarities between video frames that jointly define the meaning of actions. The proposed approach outperforms the state-of-the-art solutions on four spatiotemporal action datasets: AVA 2.2, AVA 2.1, UCF101-24, and EPIC-Kitchens.
PB  - arXiv
PY  - 2024
ST  - A Semantic and Motion-Aware Spatiotemporal Transformer Network for Action Detection
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tpami.2024.3377192
ER  -


TY  - GEN
AU  - Hsu, C.-Y.
AU  - Wang, I.H.
TI  - Tradeoffs among Action Taking Policies Matter in Active Sequential Multi-Hypothesis Testing: the Optimal Error Exponent Region
AB  - Reliability of sequential hypothesis testing can be greatly improved when decision maker is given the freedom to adaptively take an action that determines the distribution of the current collected sample. Such advantage of sampling adaptivity has been realized since Chernoff’s seminal paper in 1959 [1]. While a large body of works have explored and investigated the gain of adaptivity, in the general multiple-hypothesis setting, the fundamental limits of individual error probabilities have not been fully understood. In particular, in the asymptotic regime as the expected stopping time tends to infinity, the error exponents are only characterized in specific cases, such as that of the total error probability. In this paper, we consider a general setup of active sequential multiple-hypothesis testing where at each time slot, a temporally varying subset of data sources (out of a known set) emerges from which the decision maker can select to collect samples, subject to a family of expected selection budget constraints. The selection of sources, understood as the “action” at each time slot, is constrained in a predefined action space. At the end of each time slot, the decision maker either decides to make the inference on the M hypotheses, or continues to observe the data sources for the next time slot. The optimal tradeoffs among M(M - 1) types of error exponents are characterized. A companion asymptotically optimal test that strikes the balance between exploration and exploitation is proposed to achieve any target error exponents within the region. To the best of our knowledge, this is the first time in the literature to identify such tradeoffs among error exponents, and it uncovers the tension among different action taking policies even in the basic setting of Chernoff [1].
PB  - arXiv
PY  - 2024
ST  - Tradeoffs among Action Taking Policies Matter in Active Sequential Multi-Hypothesis Testing
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/isit57864.2024.10619686
ER  -


TY  - GEN
AU  - Xu, J.
AU  - Yin, S.
AU  - Zhao, G.
AU  - Wang, Z.
AU  - Peng, Y.
TI  - FineParser: A Fine-grained Spatio-temporal Action Parser for Human-centric Action Quality Assessment
AB  - Existing action quality assessment (AQA) methods mainly learn deep representations at the video level for scoring diverse actions. Due to the lack of a fine-grained understanding of actions in videos, they harshly suffer from low credibility and interpretability, thus insufficient for stringent applications, such as Olympic diving events. We argue that a fine-grained understanding of actions requires the model to perceive and parse actions in both time and space, which is also the key to the credibility and interpretability of the AQA technique. Based on this insight, we propose a new fine-grained spatial-temporal action parser named FineParser. It learns human-centric foreground action representations by focusing on target action regions within each frame and exploiting their fine-grained alignments in time and space to minimize the impact of invalid backgrounds during the assessment. In addition, we construct fine-grained annotations of human-centric foreground action masks for the FineDiving dataset, called FineDiving-HM. With refined annotations on diverse target action procedures, FineDiving-HM can promote the development of real-world AQA systems. Through extensive experiments, we demonstrate the effectiveness of FineParser, which outperforms state-of-the-art methods while supporting more tasks of fine-grained action understanding. Data and code are available at https://github.com/ PKU-ICST-MIPL/FineParser_CVPR2024.
PB  - arXiv
PY  - 2024
ST  - FineParser
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52733.2024.01386
ER  -


TY  - GEN
AU  - Yan, S.
AU  - Du, X.
AU  - Li, Z.
AU  - Jin, H.
AU  - Liu, M.
TI  - PROMPT WHEN THE ANIMAL IS: TEMPORAL ANIMAL BEHAVIOR GROUNDING WITH POSITIONAL RECOVERY TRAINING
AB  - Temporal grounding is crucial in multimodal learning, but it poses challenges when applied to animal behavior data due to the sparsity and uniform distribution of moments. To address these challenges, we propose a novel Positional Recovery Training framework (Port), which prompts the model with the start and end times of specific animal behaviors during training. Specifically, Port enhances the baseline model with a Recovering part to predict flipped label sequences and align distributions with a Dual-alignment method. This allows the model to focus on specific temporal regions prompted by ground-truth information. Extensive experiments on the Animal Kingdom dataset demonstrate the effectiveness of Port, achieving an IoU@0.3 of 38.52. It emerges as one of the top performers in the sub-track of MMVRAC in ICME 2024 Grand Challenges.
PB  - arXiv
PY  - 2024
ST  - PROMPT WHEN THE ANIMAL IS
Y2  - 2025/05/05/21:54:31
DO  - 10.1002/jeab.789
ER  -


TY  - GEN
AU  - Karthika, S.
AU  - Jane, N.Y.
AU  - Harichandran, K.N.
TI  - Spatio Temporal 3d Movenet Thunder Kinematic Skeleton Joint Point Classification Model for Human Activity Recognition
AB  - In computer vision, human activity recognition is essential for identifying the precise movement or action of an individual. However, recognizing human activities from video data remains challenging due to cluttered background, spatio-temporal dependencies, viewpoint variations, appearance changes and lighting conditions. The objective of this work is to develop a stacked ensemble 3D skeletal kinematic human activity recognition (SES-HAR) framework for effectively identifying human actions using video data. The kinematic joint points are generated using 3D MoveNet Thunder Pose Estimation Model using Gaussian Radial Basis Function Kernel. The proposed SES-HAR framework employs a stacking ensemble technique comprising two layers: the level-0 base-learners and the level-1 meta-learner. The base learners are constructed using three distinct models namely Convolutional Part-Aware Long Short-Term Memory Network (ConvP-LSTM), Spatial Bidirectional Gated Temporal Graph Convolutional Network (SBGTGCN) and Convolutional eXtreme Gradient Boosting (ConvXGB).These base learners provide individual outputs, which are then stacked and combined into a pooled dataset. In the level-1 layer, the meta-learner, which includes Logistic Regression (LR) utilize this pooled dataset to generate the final prediction.The proposed framework is experimented with 3D kinematic joint points extracted from Open Pose and MoveNet Thunder for NTU-RGB+D action recognition dataset. The experimental outcome shows that MoveNet Thunder with 17 3D skeletal kinematic joint points of SES-HAR framework provides better accuracy in terms of its cross-subject accuracy of 95.80% and cross-view accuracy of 97.87% when compared with open pose.
PB  - SSRN
PY  - 2024
ST  - Spatio Temporal 3d Movenet Thunder Kinematic Skeleton Joint Point Classification Model for Human Activity Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4821434
ER  -


TY  - GEN
AU  - Camacho, W.C.
AU  - Copetti, A.
AU  - Bertini, L.
AU  - Moreira, L.B.
AU  - Gomes, O.D.S.M.
TI  - Advancing Human Activity Recognition with 2d Cnn-Lstm and Recurrence Plot Transformations: An Approach to Sequential Image Representation and Processing of Inertial Sensor Data
AB  - The field of Human Activity Recognition (HAR) benefits significantly from deep learning, addressing the complexity of human behavior and the vast volume of data produced by sensors. This work adopts the strategy of converting inertial data (accelerometer and gyroscope) into 2D images, through recurrence plots. This approach facilitates the effective exploration of data input and neural network architectures. Utilizing the recent history of movements as input for the models, this work evaluates the impact of this methodology on HAR in an two way adapted architectures proposed: 2D CNN-LSTM and 2D CNN, whose performances are compared with other state-of-the-art deep learning models. The contributions of this study are threefold: the handling of input data, development of the two network architectures for HAR, and the high accuracy achieved between 97% and 98% on the public UCI-HAR dataset. These results highlight the benefit of incorporating temporal data to enhance accuracy in activity classification.
PB  - SSRN
PY  - 2024
ST  - Advancing Human Activity Recognition with 2d Cnn-Lstm and Recurrence Plot Transformations
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4818865
ER  -


TY  - GEN
AU  - Abdellaoui, S.
AU  - Dumitrescu, E.
AU  - Escudero, C.
AU  - Zamaï, E.
TI  - Temporal Assessment of Malicious Behaviors: Application to Turnout Field Data Monitoring
AB  - Monitored data collected from railway turnouts are vulnerable to cyberattacks: attackers may either conceal failures or trigger unnecessary maintenance actions. To address this issue, a cyberattack investigation method is proposed based on predictions made from the temporal evolution of the turnout behavior. These predictions are then compared to the field acquired data to detect any discrepancy. This method is illustrated on a collection of real-life data.
PB  - arXiv
PY  - 2024
ST  - Temporal Assessment of Malicious Behaviors
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccad60883.2024.10553981
ER  -


TY  - GEN
AU  - Feng, J.
AU  - Sun, Y.
AU  - Jin, W.
AU  - Liao, R.
TI  - Analytical and Experimental Studies on the Sequential Flaring-Buckling Behavior of Combined Bi-Tubes in Blind Bolts
AB  - Combined bi-tubes are innovatively applied in modern composite blind bolts to provide clamping force. In this study, the sequential flaring-buckling behavior of combined bi-tubes under axial compression on expanding dies has been experimentally and analytically investigated. First, axial compression tests were performed on three different dimensions of bi-tubes, and deformation modes and force-displacement curves were obtained to assess the specific energy absorption (SEA), clamping energy (ECL), and energy transfer ratio (ETR). The results showed that bi-tubes have superior energy-absorbing capacity and clamping efficiency. SEA could reach 35 kJ/kg, and the clamping energy (ECL) accounted for 50 ± 6% of the total energy dissipated. Afterward, a theoretical solution for flaring-buckling bi-tubes, including the flaring forming forces, frictions, and critical buckling limits, was derived based on constant-thickness circular tubes. Comparing forces and deformation modes from analytical and experimental approaches, it was observed that the analytical theory can evaluate the sequential flaring-buckling bi-tubes within acceptable proximity, with the maximum deviations of flaring forming forces and critical buckling limits being 3.3% and 6.6%, and it can effectively predict diverse deformation modes: single bell-shaped bulb on the clamped structure, upper bulb close to the platen, or double bulbs. This paper would provide guidance for the optimal design of the clamping structure on aircraft and automobiles.
PB  - SSRN
PY  - 2024
ST  - Analytical and Experimental Studies on the Sequential Flaring-Buckling Behavior of Combined Bi-Tubes in Blind Bolts
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4815113
ER  -


TY  - GEN
AU  - Peng, Z.
AU  - Hu, R.
AU  - Li, Z.
AU  - Zeng, K.
TI  - DSTANet: Learning a Dual-Stream Model for Anomaly Driving Action Detection Using Spatio-Temporal and Appearance Features
AB  - Driving action anomaly detection based on in-cab surveillance video has become the mainstream of current driving action research. However, there is a substantial redundancy of spatio-temporal information in the spatio-temporal action features extracted using only the 3D encoder, which will weaken the distinguishability between normal and anomaly videos, and there will be more difficult samples to be distinguished during the anomaly detection. To alleviate this problem, we propose a dual-stream model combining spatio-temporal and appearance features for anomaly driving action detection. Firstly, the model utilises 3D ResNet and 2D ResNet for feature extraction of spatio-temporal action and appearance information in the video. Next, to fully combine the respective advantages of the cross-dimension information and obtain fused features that can be more easily distinguished, a contrastive learning based alignment approach was used before fusion. In addition, cross-dimensional features are fused by introducing Transformer’s cross attention block. Moreover, the supervised contrastive learning objective function is employed to guides the model to distinguish between normal and anomaly driving action. Finally, calculating the feature similarity between the target sample and the normal memory center is utilized to obtain the anomaly score of the target sample. In the experiment, the proposed model DSTANet in this paper achieved the 96.81% AUC in the publicly available DAD dataset, which is higher than the popular models in recent years. Furthermore, feature distinguishability was significantly improved in the visualization experiments. The experiment code and data are publicly available at: https://github.com/CreatedTRYNA/DCC Driving Anomaly Detection-.
PB  - Research Square
PY  - 2024
ST  - DSTANet
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-4321265/v1
ER  -


TY  - GEN
AU  - Nguyen, T.T.
AU  - Kawanishi, Y.
AU  - Komamizu, T.
AU  - Ide, I.
TI  - One-Stage Open-Vocabulary Temporal Action Detection Leveraging Temporal Multi-scale and Action Label Features
AB  - Open-vocabulary Temporal Action Detection (Open-vocab TAD) is an advanced video analysis approach that expands Closed-vocabulary Temporal Action Detection (Closed-vocab TAD) capabilities. Closed-vocab TAD is typically confined to localizing and classifying actions based on a predefined set of categories. In contrast, Open-vocab TAD goes further and is not limited to these predefined categories. This is particularly useful in real-world scenarios where the variety of actions in videos can be vast and not always predictable. The prevalent methods in Open-vocab TAD typically employ a 2-stage approach, which involves generating action proposals and then identifying those actions. However, errors made during the first stage can adversely affect the subsequent action identification accuracy. Additionally, existing studies face challenges in handling actions of different durations owing to the use of fixed temporal processing methods. Therefore, we propose a 1-stage approach consisting of two primary modules: Multi-scale Video Analysis (MVA) and Video-Text Alignment (VTA). The MVA module captures actions at varying temporal resolutions, overcoming the challenge of detecting actions with diverse durations. The VTA module leverages the synergy between visual and textual modalities to precisely align video segments with corresponding action labels, a critical step for accurate action identification in Open-vocab scenarios. Evaluations on widely recognized datasets THUMOS14 and ActivityNet-1.3, showed that the proposed method achieved superior results compared to the other methods in both Open-vocab and Closed-vocab settings. This serves as a strong demonstration of the effectiveness of the proposed method in the TAD task.
PB  - arXiv
PY  - 2024
ST  - One-Stage Open-Vocabulary Temporal Action Detection Leveraging Temporal Multi-scale and Action Label Features
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/fg59268.2024.10581896
ER  -


TY  - GEN
AU  - Meyer-Baese, L.
AU  - Anumba, N.
AU  - Bolt, T.
AU  - Schumacher, E.
AU  - Keilholz, S.
TI  - Variation in the Distribution of Large-scale Spatiotemporal Patterns of Activity Across Brain States
AB  - A few large-scale spatiotemporal patterns of brain activity (quasiperiodic patterns or QPPs) account for most of the spatial structure observed in resting state functional magnetic resonance imaging (rs-fMRI). The QPPs capture well-known features such as the evolution of the global signal and the alternating dominance of the default mode and task positive networks. These widespread patterns of activity have plausible ties to neuromodulatory input that mediates changes in nonlocalized processes, including arousal and attention. To determine whether QPPs exhibit variations across brain conditions, the relative magnitude and distribution of the three strongest QPPs were examined in two scenarios. First, in data from the Human Connectome Project, the relative incidence and magnitude of the QPPs was examined over the course of the scan, under the hypothesis that increasing drowsiness would shift the expression of the QPPs over time. Second, using rs-fMRI in rats obtained with a novel approach that minimizes noise, the relative incidence and magnitude of the QPPs was examined under three different anesthetic conditions expected to create distinct types of brain activity. The results indicate that both the distribution of QPPs and their magnitude changes with brain state, evidence of the sensitivity of these large-scale patterns to widespread changes linked to alterations in brain conditions.
PB  - bioRxiv
PY  - 2024
ST  - Variation in the Distribution of Large-scale Spatiotemporal Patterns of Activity Across Brain States
Y2  - 2025/05/05/21:54:31
DO  - 10.3389/fnsys.2024.1425491
ER  -


TY  - GEN
AU  - Dautan, D.
AU  - Paslawski, W.
AU  - Montejo, S.G.
AU  - Dawson, T.M.
AU  - Svenningsson, P.
TI  - Gut-Initiated Alpha Synuclein Fibrils Drive Parkinson’s Disease Phenotypes: Temporal Mapping of non-Motor Symptoms and REM Sleep Behavior Disorder
AB  - Parkinson’s disease (PD) is characterized by progressive motor as well as less recognized non-motor symptoms that arise often years before motor manifestation, including sleep and gastrointestinal disturbances. Despite the heavy burden on the patient’s quality of life, these non-motor manifestations are poorly understood. To elucidate the temporal dynamics of the disease, we employed a mouse model involving injection of alpha-synuclein (αSyn) pre-formed fibrils (PFF) in the duodenum and antrum as a gut-brain model of Parkinsonism. Using anatomical mapping of αSyn-PFF propagation and behavioral and physiological characterizations, we unveil a correlation between post-injection time the temporal dynamics of αSyn propagation and non-motor/motor manifestations of the disease. We highlight the concurrent presence of αSyn aggregates in key brain regions, expressing acetylcholine or dopamine, involved in sleep duration, wakefulness, and particularly REM-associated atonia corresponding to REM behavioral disorder-like symptoms. This study presents a novel and in-depth exploration into the multifaceted nature of PD, unraveling the complex connections between α-synucleinopathies, gut-brain connectivity, and the emergence of non-motor phenotypes.
PB  - bioRxiv
PY  - 2024
ST  - Gut-Initiated Alpha Synuclein Fibrils Drive Parkinson’s Disease Phenotypes
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2024.04.22.590542
ER  -


TY  - GEN
AU  - Gosain, H.
AU  - Seebohm, G.
AU  - Holtmannspötter, M.
AU  - Kurre, R.
AU  - Busch, K.B.
TI  - Tagging of Exo70 at the N-terminus compromises its assembly with the exocyst complex and changes its spatiotemporal behavior at the plasma membrane
AB  - The vesicle-tethering exocyst complex is a key regulator of cell polarity. The subunit Exo70 is required for the targeting of the exocyst complex to the plasma membrane. While the N-terminus of Exo70 is important for its regulation by GTPases, the C-terminus binds to PI(4,5)P2 and Arp2/3. Here, we compare N- and C-terminal tagged Exo70 with respect to subcellular localization, dynamics and function in cell membrane expansion. Using high-resolution imaging, we determined the spatial distribution and dynamics in different sub-compartments of un-polarized and polarized cells. With lattice light-sheet microscopy, we show that HaloTag-Exo70, but not Exo70-HaloTag, promotes the outgrowth of filopodia-like structures from the axon of hippocampal neurons. Fluorescence lifetime imaging of sfGFP-Exo70 and molecular modeling results suggest that the assembly of sfGFP-Exo70 with the exocyst complex is reduced. This is supported by single particle tracking data showing higher mobility of N- than C-terminal tagged Exo70 at the plasma membrane. The distinct spatiotemporal properties of N-terminal tagged Exo70 were correlated with pronounced filopodia formation in unpolarized cells and neurons, a process that is less reliant on exocyst complex formation. We therefore propose that N-terminal tagging of Exo70 shifts its activity to processes that are less exocyst-dependent.
PB  - bioRxiv
PY  - 2024
ST  - Tagging of Exo70 at the N-terminus compromises its assembly with the exocyst complex and changes its spatiotemporal behavior at the plasma membrane
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2024.04.21.590474
ER  -


TY  - GEN
AU  - Wang, Y.
AU  - Zhou, S.
AU  - Xia, K.
AU  - Wang, L.
TI  - Learning Discriminative Spatio-temporal Representations for Semi-supervised Action Recognition
AB  - Semi-supervised action recognition aims to improve spatio-temporal reasoning ability with a few labeled data in conjunction with a large amount of unlabeled data. Albeit recent advancements, existing powerful methods are still prone to making ambiguous predictions under scarce labeled data, embodied as the limitation of distinguishing different actions with similar spatio-temporal information. In this paper, we approach this problem by empowering the model two aspects of capability, namely discriminative spatial modeling and temporal structure modeling for learning discriminative spatio-temporal representations. Specifically, we propose an Adaptive Contrastive Learning (ACL) strategy. It assesses the confidence of all unlabeled samples by the class prototypes of the labeled data, and adaptively selects positive-negative samples from a pseudo-labeled sample bank to construct contrastive learning. Additionally, we introduce a Multi-scale Temporal Learning (MTL) strategy. It could highlight informative semantics from long-term clips and integrate them into the short-term clip while suppressing noisy information. Afterwards, both of these two new techniques are integrated in a unified framework to encourage the model to make accurate predictions. Extensive experiments on UCF101, HMDB51 and Kinetics400 show the superiority of our method over prior state-of-the-art approaches. MSC Codes 68U10, 68T45
PB  - arXiv
PY  - 2024
ST  - Learning Discriminative Spatio-temporal Representations for Semi-supervised Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr46437.2021.01025
ER  -


TY  - GEN
AU  - Rani, D.R.
AU  - Prabhakar, C.J.
TI  - Combining Handcrafted Spatio-temporal and Deep Spatial Features for Effective Human Action Recognition
AB  - Human action recognition research has become increasingly sophisticated in recent years because it is used in many different applications. Many techniques have been introduced to solve the problem of action recognition, but a number of challenges still need to be resolved. To describe a video in action recognition, it is necessary to efficiently collect and aggregate the spatial-temporal information. In this paper, we address the problem of human action recognition by combining handcrafted spatio-temporal features with deep spatial features. This paper proposed a novel method for recognizing human actions in videos by concatenating handcrafted spatio-temporal texture features extracted by our proposed feature descriptor Volume Local Derivative Gradient Ternary Patterns (VLDGTP) and deep spatial features extracted from a modified Inception-v4 network. In order to reduce the dimension and to get equal sized feature vectors, we employed the PCA dimensionality reduction technique for both types of features. Then, the dimensionality reduced feature vectors are combined and passed to the SVM classifier for action recognition. Extensive experimentation is carried out on three standard datasets, such as KTH, UCF101 and HMDB-51 datasets. The experimental results and analysis show that our proposed method (VLDGTP+DEEP_FEATURES) yields better recognition accuracy for all three benchmark datasets compared to its competitors.
PB  - Research Square
PY  - 2024
ST  - Combining Handcrafted Spatio-temporal and Deep Spatial Features for Effective Human Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s44230-025-00095-5
ER  -


TY  - GEN
AU  - Liu, Y.
AU  - Liu, Z.
AU  - Zhai, Y.
AU  - Doerman, D.
AU  - Yuan, J.
TI  - STAT: Towards Generalizable Temporal Action Localization
AB  - Weakly-supervised temporal action localization (WTAL) aims to recognize and localize action instances with only video-level labels. Despite the significant progress, existing methods suffer from severe performance degradation when transferring to different distributions and thus may hardly adapt to real-world scenarios. To address this problem, we propose the Generalizable Temporal Action Localization task (GTAL), which focuses on improving the generalizability of action localization methods. We observed that the performance decline can be primarily attributed to the lack of generalizability to different action scales. To address this problem, we propose STAT (Self-supervised Temporal Adaptive Teacher), which leverages a teacher-student structure for iterative refinement. Our STAT features a refinement module and an alignment module. The former iteratively refines the model's output by leveraging contextual information and helps adapt to the target scale. The latter improves the refinement process by promoting a consensus between student and teacher models. We conduct extensive experiments on three datasets, THUMOS14, ActivityNet1.2, and HACS, and the results show that our method significantly improves the baseline methods under the cross-distribution evaluation setting, even approaching the same-distribution evaluation performance.
PB  - arXiv
PY  - 2024
ST  - STAT
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-78444-6_17
ER  -


TY  - GEN
AU  - Rahman, M.S.
AU  - Shihab, I.F.
AU  - Chu, L.
AU  - Sharma, A.
TI  - DeepLocalization: Using change point detection for Temporal Action Localization
AB  - In this study, we introduce DeepLocalization, an innovative framework devised for the real-time localization of actions tailored explicitly for monitoring driver behavior. Utilizing the power of advanced deep learning methodologies, our objective is to tackle the critical issue of distracted driving—a significant factor contributing to road accidents. Our strategy employs a dual approach: leveraging Graph-Based Change-Point Detection for pinpointing actions in time alongside a Video Large Language Model (Video-LLM) for precisely categorizing activities. Through careful prompt engineering, we customize the Video-LLM to adeptly handle driving activities’ nuances, ensuring its classification efficacy even with sparse data. Engineered to be lightweight, our framework is optimized for consumer-grade GPUs, making it vastly applicable in practical scenarios. We subjected our method to rigorous testing on the SynDD2 dataset, a complex benchmark for distracted driving behaviors, where it demonstrated commendable performance—achieving 57.5% accuracy in event classification and 51% in event detection. These outcomes underscore the substantial promise of DeepLocalization in accurately identifying diverse driver behaviors and their temporal occurrences, all within the bounds of limited computational resources.
PB  - arXiv
PY  - 2024
ST  - DeepLocalization
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvprw63382.2024.00721
ER  -


TY  - GEN
AU  - Lu, C.
AU  - Mak, M.-W.
TI  - Dita: Detr with Improved Queries for End-to-End Temporal Action Detection
AB  - The DEtection TRansformer (DETR), with its elegant architecture and set prediction, has revolutionized object detection. However, DETR-like models have yet to achieve comparable success in temporal action detection (TAD). To address this gap, we introduce a series of improvements to the original DETR, proposing a new DETR-based model for TAD that achieves competitive performance relative to conventional TAD methods. Specifically, we adapt advanced techniques from DETR variants used in object detection, including deformable attention, denoising training, and selective query recollection. Furthermore, we propose several new techniques aimed at enhancing detection precision and model convergence speed, such as geographic query grouping and learnable proposals. Leveraging these innovations, we introduce a new model called {\bf D}ETR with {\bf I}mproved queries for {\bf T}emporal {\bf A}ction Detection (DITA). DITA not only adheres to DETR’s elegant design philosophy but also competitive to state-of-the-art action detection models. Remarkably, it is the first TAD model to achieve an mAP over 70\% on THUMOS14, outperforming the previous best DETR variant by 13.5 percentage points.
PB  - SSRN
PY  - 2024
ST  - Dita
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.neucom.2024.127914
ER  -


TY  - GEN
AU  - Kim, M.
AU  - Han, D.
AU  - Kim, T.
AU  - Han, B.
TI  - Leveraging Temporal Contextualization for Video Action Recognition
AB  - We propose a novel framework for video understanding, called Temporally Contextualized CLIP (TC-CLIP), which leverages essential temporal information through global interactions in a spatio-temporal domain within a video. To be specific, we introduce Temporal Contextualization (TC), a layer-wise temporal information infusion mechanism for videos, which 1) extracts core information from each frame, 2) connects relevant information across frames for the summarization into context tokens, and 3) leverages the context tokens for feature encoding. Furthermore, the Video-conditional Prompting (VP) module processes context tokens to generate informative prompts in the text modality. Extensive experiments in zero-shot, few-shot, base-to-novel, and fully-supervised action recognition validate the effectiveness of our model. Ablation studies for TC and VP support our design choices. Our project page with the source code is available at https://github.com/naver-ai/tc-clip.
PB  - arXiv
PY  - 2024
ST  - Leveraging Temporal Contextualization for Video Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-72664-4_5
ER  -


TY  - GEN
AU  - Marcondes, D.S.
AU  - Domit, C.
AU  - Cantor, M.
TI  - Spatiotemporal Differences in the Acoustic Behavior of Guiana Dolphins Under Cumulative Noise Conditions
AB  - Anthropogenic noise disrupts animals’ acoustic behaviors, affecting their social and population dynamics. We investigate Guiana dolphins’ response to anthropogenic noise by analyzing their whistles and echolocation clicks in two areas of southern Brazil with varying noise levels—port and conservation area. Our findings reveal differences in whistle parameters between areas, with whistles overlapping more with noise at the port. Whistling rate decreased by up to 79% during high-noise years, suggesting fewer social sounds or masking under noisy conditions. However, echolocation emission rates remained consistent across areas and years, indicating maintained active foraging. This implies that Guiana dolphins may adapt to noise by adjusting whistling behavior while preserving foraging activities. Increased fundamental frequencies in whistles under high noise levels may serve to mitigate masking effects. These findings highlight concerns for dolphin conservation amidst cumulative anthropogenic impacts and provide essential baseline data for monitoring and mitigating acoustic impacts on Guiana dolphins.
PB  - SSRN
PY  - 2024
ST  - Spatiotemporal Differences in the Acoustic Behavior of Guiana Dolphins Under Cumulative Noise Conditions
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4795093
ER  -


TY  - GEN
AU  - Sun, L.
AU  - Lv, Y.
AU  - Bu, J.
AU  - Xiang, H.
AU  - Zhao, H.
TI  - A Study of Driver Personality Traits as Predictors of Fatigue Driving Behavior - a Perspective Based on the Temporal Self-Regulation Theory
AB  - Fatigue driving is one of the main causes of road traffic accidents around the world. Although each country has enacted relevant policies and regulations to prevent the emergence of driver fatigue driving behavior, the effect is not significant. In order to explore the decision-making factors of fatigue driving behavior from the perspective of social psychology, this study used the temporal self-regulation theory (TST), extended with personality traits, to explore the influencing factors of fatigue driving in Chinese drivers. The study also examined the correlation between drivers' personality traits and psychological factors. Through an online survey, 165 valid questionnaires were collected to measure five standardized TST variables, four selected personality traits, and demographic characteristics of drivers. Hierarchical multiple regression equation modeling was used to analyze the survey data. The results showed that TST significantly predicted fatigue driving behavior (R2 = 0.498, F(17, 147) = 8.576, p < 0.001). Personality traits did not explain significant differences. Environmental factors and self-control were significant predictors, while habit and personality did not predict behavior. No interaction effect was found in the intention-behavior relationship. These results provide theoretical and practical support for the management and intervention of driver fatigue driving behavior prevention.
PB  - SSRN
PY  - 2024
ST  - A Study of Driver Personality Traits as Predictors of Fatigue Driving Behavior - a Perspective Based on the Temporal Self-Regulation Theory
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4791003
ER  -


TY  - GEN
AU  - Lee, S.-H.
AU  - Kang, Y.-J.
AU  - Smith, B.N.
TI  - Activation of hypoactive parvalbumin-positive fast-spiking interneuron restores dentate inhibition to prevent epileptiform activity in the mouse intrahippocampal kainate model of temporal lobe epilepsy
AB  - Parvalbumin-positive (PV+) GABAergic interneurons in the dentate gyrus provide powerful perisomatic inhibition of dentate granule cells (DGCs) to prevent overexcitation and maintain the stability of dentate gyrus circuits. Most dentate PV+ interneurons survive status epilepticus, but surviving PV+ interneuron mediated inhibition is compromised in the dentate gyrus shortly after status epilepticus, contributing to epileptogenesis in temporal lobe epilepsy. It is uncertain whether the impaired activity of dentate PV+ interneurons recovers at later times or if it continues for months following status epilepticus. The development of compensatory modifications related to PV+ interneuron circuits in the months following status epilepticus is unknown, although reduced dentate GABAergic inhibition persists long after status epilepticus. We employed PV immunostaining and whole-cell patch-clamp recordings from dentate PV+ interneurons and DGCs in slices from male and female sham controls and intrahippocampal kainate (IHK) treated mice that developed spontaneous seizures months after status epilepticus to study epilepsy-associated changes in dentate PV+ interneuron circuits. We found that the number of dentate PV+ cells was reduced in IHK treated mice. Electrical recordings showed that: 1) Action potential firing rates of dentate PV+ interneurons were reduced in IHK treated mice up to four months after status epilepticus; 2) Spontaneous inhibitory postsynaptic currents (sIPSCs) in DGCs exhibited reduced frequency but increased amplitude in IHK treated mice; and 3) The amplitude of evoked IPSCs in DGCs by optogenetic activation of dentate PV+ cells was upregulated without changes in short-term plasticity. Video-EEG recordings revealed that IHK treated mice showed spontaneous epileptiform activity in the dentate gyrus and that chemogenetic activation of PV+ interneurons abolished the epileptiform activity. Our results suggest not only that the compensatory changes in PV+ interneuron circuits develop after IHK treatment, but also that increased PV+ interneuron mediated inhibition in the dentate gyrus may compensate for cell loss and reduced intrinsic excitability of dentate PV+ interneurons to stop seizures in temporal lobe epilepsy.
PB  - bioRxiv
PY  - 2024
ST  - Activation of hypoactive parvalbumin-positive fast-spiking interneuron restores dentate inhibition to prevent epileptiform activity in the mouse intrahippocampal kainate model of temporal lobe epilepsy
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2024.04.05.588316
ER  -


TY  - GEN
AU  - Liberatori, B.
AU  - Conti, A.
AU  - Rota, P.
AU  - Wang, Y.
AU  - Ricci, E.
TI  - Test-Time Zero-Shot Temporal Action Localization
AB  - Zero-Shot Temporal Action Localization (ZS-TAL) seeks to identify and locate actions in untrimmed videos unseen during training. Existing ZS-TAL methods involve finetuning a model on a large amount of annotated training data. While effective, training-based ZS-TAL approaches assume the availability of labeled data for supervised learning, which can be impractical in some applications. Furthermore, the training process naturally induces a domain bias into the learned model, which may adversely affect the model’s generalization ability to arbitrary videos. These considerations prompt us to approach the ZS-TAL problem from a radically novel perspective, relaxing the requirement for training data. To this aim, we introduce a novel method that performs Test-Time adaptation for Temporal Action Localization (T 3AL). In a nutshell, T 3AL adapts a pre-trained Vision and Language Model (VLM). T 3AL operates in three steps. First, a video-level pseudo-label of the action category is computed by aggregating information from the entire video. Then, action localization is performed adopting a novel procedure inspired by self-supervised learning. Finally, frame-level textual descriptions extracted with a state-of-the-art captioning model are employed for refining the action region proposals. We validate the effectiveness of T 3AL by conducting experiments on the THUMOS14 and the ActivityNet-v1.3 datasets. Our results demonstrate that T 3AL significantly outperforms zero-shot baselines based on state-of-the-art VLMs, confirming the benefit of a test-time adaptation approach.
PB  - arXiv
PY  - 2024
ST  - Test-Time Zero-Shot Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52733.2024.01771
ER  -


TY  - GEN
AU  - Zeng, Y.
AU  - Zhong, Y.
AU  - Feng, C.
AU  - Ma, L.
TI  - UniMD: Towards Unifying Moment Retrieval and Temporal Action Detection
AB  - Temporal Action Detection (TAD) focuses on detecting predefined actions, while Moment Retrieval (MR) aims to identify the events described by open-ended natural language within untrimmed videos. Despite that they focus on different events, we observe they have a significant connection. For instance, most descriptions in MR involve multiple actions from TAD. In this paper, we aim to investigate the potential synergy between TAD and MR. Firstly, we propose a unified architecture, termed Unified Moment Detection (UniMD), for both TAD and MR. It transforms the inputs of the two tasks, namely actions for TAD or events for MR, into a common embedding space, and utilizes two novel query-dependent decoders to generate a uniform output of classification score and temporal segments. Secondly, we explore the efficacy of two task fusion learning approaches, pre-training and co-training, in order to enhance the mutual benefits between TAD and MR. Extensive experiments demonstrate that the proposed task fusion learning scheme enables the two tasks to help each other and outperform the separately trained counterparts. Impressively, UniMD achieves state-of-the-art results on three paired datasets Ego4D, Charades-STA, and ActivityNet. Our code is available at https://github.com/yingsen1/UniMD.
PB  - arXiv
PY  - 2024
ST  - UniMD
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-72952-2_17
ER  -


TY  - GEN
AU  - Nakamura, I.
TI  - MULTI-SCALE SPATIAL-TEMPORAL SELF-ATTENTION GRAPH CONVOLUTIONAL NETWORKS FOR SKELETON-BASED ACTION RECOGNITION
AB  - MSC Codes I.5.1, I.2.10 Skeleton-based gesture recognition methods have achieved high success using Graph Convolutional Network (GCN). In addition, context-dependent adaptive topology as a neighborhood vertex information and attention mechanism leverages a model to better represent actions. In this paper, we propose self-attention GCN hybrid model, Multi-Scale Spatial-Temporal self-attention (MSST)-GCN to effectively improve modeling ability to achieve state-of-the-art results on several datasets. We utilize spatial self-attention module with adaptive topology to understand intra-frame interactions within a frame among different body parts, and temporal self-attention module to examine correlations between frames of a node. These two are followed by multi-scale convolution network with dilations, which not only captures the long-range temporal dependencies of joints but also the long-range spatial dependencies (i.e., long-distance dependencies) of node temporal behaviors. They are combined into high-level spatial-temporal representations and output the predicted action with the softmax classifier.
PB  - arXiv
PY  - 2024
ST  - MULTI-SCALE SPATIAL-TEMPORAL SELF-ATTENTION GRAPH CONVOLUTIONAL NETWORKS FOR SKELETON-BASED ACTION RECOGNITION
Y2  - 2025/05/05/21:54:31
DO  - 10.1117/1.jei.28.4.043032
ER  -


TY  - GEN
AU  - Kim, H.-J.
AU  - Hong, J.-H.
AU  - Kong, H.
AU  - Lee, S.-W.
TI  - TE-TAD: Towards Full End-to-End Temporal Action Detection via Time-Aligned Coordinate Expression
AB  - In this paper, we investigate that the normalized coordinate expression is a key factor as reliance on handcrafted components in query-based detectors for temporal action detection (TAD). Despite significant advancements towards an end-to-end framework in object detection, query-based detectors have been limited in achieving full end-to-end modeling in TAD. To address this issue, we propose TE-TAD, a full end-to-end temporal action detection transformer that integrates time-aligned coordinate expression. We reformulate coordinate expression utilizing actual timeline values, ensuring length-invariant representations from the extremely diverse video duration environment. Furthermore, our proposed adaptive query selection dynamically adjusts the number of queries based on video length, providing a suitable solution for varying video durations compared to a fixed query set. Our approach not only simplifies the TAD process by eliminating the need for hand-crafted components but also significantly improves the performance of query-based detectors. Our TE-TAD outperforms the previous query-based detectors and achieves competitive performance compared to state-of-the-art methods on popular benchmark datasets. Code is available at: https://github.com/Dotori-HJ/TE-TAD
PB  - arXiv
PY  - 2024
ST  - TE-TAD
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52733.2024.01782
ER  -


TY  - GEN
AU  - Bourgeais, Q.
AU  - Charrier, R.
AU  - Sanlaville, E.
AU  - Seifert, L.
TI  - A TEMPORAL GRAPH MODEL TO STUDY THE DYNAMICS OF COLLECTIVE BEHAVIOR AND PERFORMANCE IN TEAM SPORTS: AN APPLICATION TO BASKETBALL
AB  - In this study, a temporal graph model is designed to model the behavior of collective sports teams based on the networks of player interactions. The main motivation for the model is to integrate the temporal dimension into the analysis of players’ passing networks in order to gain deeper insights into the dynamics of system behavior, particularly how a system exploits the degeneracy property to self-regulate. First, the temporal graph model and the entropy measures used to assess the complexity of the dynamics of the network structure are introduced and illustrated. Second, an experiment using basketball data is conducted to investigate the relationship between the complexity level and team performance. This is accomplished by examining the correlations between the entropy measures in a team’s behavior and the team’s final performance, as well as the link between the relative score compared to that of the opponent and the entropy in the team’s behavior. Results indicate positive correlations between entropy measures and final team performance, and threshold values of relative score associated with changes in team behavior – thereby revealing common and unique team signatures. From a complexity science perspective, the model proves useful for identifying key performance factors in team sports and for studying the effects of given constraints on the exploitation of degeneracy to organize team behavior through various network structures. Future research can easily extend the model and apply it to other types of social networks.
PB  - arXiv
PY  - 2024
ST  - A TEMPORAL GRAPH MODEL TO STUDY THE DYNAMICS OF COLLECTIVE BEHAVIOR AND PERFORMANCE IN TEAM SPORTS
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s13278-024-01253-6
ER  -


TY  - GEN
AU  - Gupta, A.
AU  - Mittal, G.
AU  - Magooda, A.
AU  - Taylor, G.W.
AU  - Chen, M.
TI  - LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action Localization
AB  - Temporal Action Localization (TAL) involves localizing and classifying action snippets in an untrimmed video. The emergence of large video foundation models has led RGB-only video backbones to outperform previous methods needing both RGB and optical flow modalities. Leveraging these large models is often limited to training only the TAL head due to the prohibitively large GPU memory required to adapt the video backbone for TAL. To overcome this limitation, we introduce LoSA, the first memory-and-parameter-efficient backbone adapter designed specifically for TAL to handle untrimmed videos. LoSA specializes for TAL by introducing Long-Short-range Adapters that adapt the intermediate layers of the video backbone over different temporal ranges. These adapters run parallel to the video backbone to significantly reduce memory footprint. LoSA also includes Long-Short-range Gated Fusion that strategically combines the output of these adapters from the video backbone layers to enhance the video features provided to the TAL head. Experiments show that LoSA significantly outperforms all existing methods on standard TAL benchmarks, THUMOS-14 and ActivityNet-v1.3, by scaling end-to-end backbone adaptation to billion-parameter-plus models like VideoMAEv2 (ViT-g) and leveraging them beyond head-only transfer learning.
PB  - arXiv
PY  - 2024
ST  - LoSA
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/wacv61041.2025.00210
ER  -


TY  - GEN
AU  - Zhu, Y.
AU  - Zhang, G.
AU  - Tan, J.
AU  - Wu, G.
AU  - Wang, L.
TI  - Dual DETRs for Multi-Label Temporal Action Detection
AB  - Temporal Action Detection (TAD) aims to identify the action boundaries and the corresponding category within untrimmed videos. Inspired by the success of DETR in object detection, several methods have adapted the query-based framework to the TAD task. However, these approaches primarily followed DETR to predict actions at the instance level (i.e., identify each action by its center point), leading to sub-optimal boundary localization. To address this issue, we propose a new Dual-level query-based TAD framework, namely DualDETR, to detect actions from both instance-level and boundary-level. Decoding at different levels requires semantics of different granularity, therefore we introduce a two-branch decoding structure. This structure builds distinctive decoding processes for different levels, facilitating explicit capture of temporal cues and semantics at each level. On top of the two-branch design, we present a joint query initialization strategy to align queries from both levels. Specifically, we leverage encoder proposals to match queries from each level in a one-to-one manner. Then, the matched queries are initialized using position and content prior from the matched action proposal. The aligned dual-level queries can refine the matched proposal with complementary cues during subsequent decoding. We evaluate DualDETR on three challenging multi-label TAD benchmarks. The experimental results demonstrate the superior performance of DualDETR to the existing state-of-the-art methods, achieving a substantial improvement under det-mAP and delivering impressive results under seg-mAP.
PB  - arXiv
PY  - 2024
ST  - Dual DETRs for Multi-Label Temporal Action Detection
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52733.2024.01756
ER  -


TY  - GEN
AU  - Wang, K.
AU  - Wang, H.
AU  - Yan, Y.
AU  - Zhou, W.
AU  - Hong, B.
TI  - Replay of Interictal Sequential Activity Shapes the Epileptic Network Dynamics
AB  - Both the imbalance of neuronal excitation and inhibition, and the network disorganization may lead to hyperactivity in epilepsy. However, the insufficiency of seizure data poses the challenge of elucidating the network mechanisms behind the frequent and recurrent abnormal discharges. Our study of two extensive intracranial EEG datasets revealed that the seizure onset zone exhibits recurrent synchronous activation of interictal events. These synchronized discharges formed repetitive sequential patterns, indicative of a stable and intricate network structure within the seizure onset zone (SOZ). We hypothesized that the frequent replay of interictal sequential activity shapes the structure of the epileptic network, which in turn supports the occurrence of these discharges. The Hopfield-Kuramoto oscillator network model was employed to characterize the formation and evolution of the epileptic network, encoding the interictal sequential patterns into the network structure using the Hebbian rule. This model successfully replicated patient-specific interictal sequential activity. Dynamic change of the network connections was further introduced to build an adaptive Kuramoto model to simulate the interictal to ictal transition. The Kuramoto oscillator network with adaptive connections (KONWAC) model we proposed essentially combines two scales of Hebbian plasticity, shaping both the stereotyped propagation and the ictal transition in epileptic networks through the interplay of regularity and uncertainty in interictal discharges.
PB  - medRxiv
PY  - 2024
ST  - Replay of Interictal Sequential Activity Shapes the Epileptic Network Dynamics
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2024.03.28.24304879
ER  -


TY  - GEN
AU  - Shi, Y.
AU  - Yang, Y.
AU  - Song, L.
TI  - Understanding and Forecasting Consumer Sequential Multiscreen Viewing Behavior
AB  - Nowadays, consumers often sequentially use multiple screens to watch media content, so that firms advertise on TV, websites, and apps at the same time period to reach consumers widely. However, sequential multiscreeners may repeated exposure to the same advertisement and develop negative attitudes. Utilizing real-world data, this paper employs a zero-inflated negative binomial (ZINB) model to predict consumer sequential multiscreen viewing frequency. Moreover, we quantify the net impacts of significant predictors on the sequential multiscreen viewing frequency and find that media factors (such as number of viewing devices, PC screen size, cellphone ownership, and device concentration ratio) have equivalent impacts as audience factors (including user demographics and past viewing behaviors) and the new created variable device concentration ratio has the largest impact. The accurate prediction and quantified net impacts can guide firms allocate their advertising budget more efficiently across multiple screens and avoid consumer overexposure to the same ad content.
PB  - SSRN
PY  - 2024
ST  - Understanding and Forecasting Consumer Sequential Multiscreen Viewing Behavior
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4779067
ER  -


TY  - GEN
AU  - Zeng, R.
AU  - Chen, X.
AU  - Liang, J.
AU  - Cao, G.
AU  - Guo, Y.
TI  - Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions
AB  - Temporal action detection (TAD) aims to locate action positions and recognize action categories in long-term untrimmed videos. Although many methods have achieved promising results, their robustness has not been thoroughly studied. In practice, we observe that temporal information in videos can be occasionally corrupted, such as missing or blurred frames. Interestingly, existing methods often incur a significant performance drop even if only one frame is affected. To formally evaluate the robustness, we establish two temporal corruption robustness benchmarks, namely THUMOS14-C and ActivityNet-v1.3-C. In this paper, we extensively analyze the robustness of seven leading TAD methods and obtain some interesting findings: 1) Existing methods are particularly vulnerable to temporal corruptions, and end-to-end methods are often more susceptible than those with a pre-trained feature extractor; 2) Vulnerability mainly comes from localization error rather than classification error; 3) When corruptions occur in the middle of an action instance, TAD models tend to yield the largest performance drop. Besides building a benchmark, we further develop a simple but effective robust training method to defend against temporal corruptions, through the FrameDrop augmentation and Temporal-Robust Consistency loss. Remarkably, our approach not only improves robustness but also yields promising improvements on clean data. We believe that this study will serve as a benchmark for future research in robust video analysis. Source code and models are available at https://github.com/AlvinZeng/temporal-robustness-benchmark.
PB  - arXiv
PY  - 2024
ST  - Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52733.2024.01729
ER  -


TY  - GEN
AU  - Zhou, Y.
AU  - Yu, X.
AU  - López-Benítez, M.
AU  - Yu, L.
AU  - Yue, Y.
TI  - Radar-Based Swimming Activity Recognition with Temporal Dynamic Convolution and Spectral Data Augmentation
AB  - Radar-based human activity recognition (HAR) is a popular area of research. In this paper, we investigate methods to improve the generalization of micro-Doppler-based swimming activity recognition. We identify three main challenges to this task: a small dataset lacking motion diversity, inaccurate period estimation, and inefficient network design that does not take into account the unique characteristics of spectrograms. To address the limited motion diversity, we propose a spectral data augmentation tailored for micro-Doppler spectrograms, including positive augmentations that account for physical fidelity and negative augmentations that penalize the unrealistic examples. We also investigate self-supervised pre-training to effectively use these negative augmentations. To address inaccurate period estimation, we introduce a segmentation approach based on energy distribution to handle temporal period variation. To exploit the spreading pattern of limb motion in the Doppler dimension and the continuous properties of torso motion in the temporal dimension, we design a module consisting of both 2D convolution and 1D temporal dynamic convolution to serve as a feature extractor. Our evaluation on a self-collected swimming activity recognition dataset shows that our model achieves the best classification accuracy and robustness to corruptions, even compared to much larger models and multi-domain fusion models.
PB  - TechRxiv
PY  - 2024
ST  - Radar-Based Swimming Activity Recognition with Temporal Dynamic Convolution and Spectral Data Augmentation
Y2  - 2025/05/05/21:54:31
DO  - 10.36227/techrxiv.24591879.v1
ER  -


TY  - GEN
AU  - Silva, G.A.
TI  - Leveraging Quantum Superposition to Infer the Dynamic Behavior of a Spatial-Temporal Neural Network Signaling Model
AB  - The exploration of new problem classes for quantum computation is an active area of research. In this paper, we introduce and solve a novel problem class related to dynamics on large-scale networks relevant to neurobiology and machine learning. Specifically, we ask if a network can sustain inherent dynamic activity beyond some arbitrary observation time or if the activity ceases through quiescence or saturation via an’epileptic’-like state. We show that this class of problems can be formulated and structured to take advantage of quantum superposition and solved efficiently using the Deutsch–Jozsa and Grover quantum algorithms. To do so, we extend their functionality to address the unique requirements of how input (sub)sets into the algorithms must be mathematically structured while simultaneously constructing the inputs so that measurement outputs can be interpreted as meaningful properties of the network dynamics. This, in turn, allows us to answer the question we pose.
PB  - arXiv
PY  - 2024
ST  - Leveraging Quantum Superposition to Infer the Dynamic Behavior of a Spatial-Temporal Neural Network Signaling Model
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s00521-022-07285-3
ER  -


TY  - GEN
AU  - Fish, E.
AU  - Weinbren, J.
AU  - Gilbert, A.
TI  - PLOT-TAL - Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization
AB  - This paper introduces a novel approach to temporal action localization (TAL) in few-shot learning. Our work addresses the inherent limitations of conventional single-prompt learning methods that often lead to overfitting due to the inability to generalize across varying contexts in real-world videos. Recognizing the diversity of camera views, backgrounds, and objects in videos, we propose a multi-prompt learning framework enhanced with optimal transport. This design allows the model to learn a set of diverse prompts for each action, capturing general characteristics more effectively and distributing the representation to mitigate the risk of overfitting. Furthermore, by employing optimal transport theory, we efficiently align these prompts with action features, optimizing for a comprehensive representation that adapts to the multifaceted nature of video data. Our experiments demonstrate significant improvements in action localization accuracy and robustness in few-shot settings on the standard challenging datasets of THUMOS-14 and EpicKitchens100, highlighting the efficacy of our multi-prompt optimal transport approach in overcoming the challenges of conventional few-shot TAL methods.
PB  - arXiv
PY  - 2024
ST  - PLOT-TAL - Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4341961
ER  -


TY  - GEN
AU  - Han, Y.
AU  - Wang, H.
AU  - Wang, K.
AU  - Lian, D.
AU  - Chen, E.
TI  - END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential Recommendation
AB  - In recommendation systems, users frequently engage in multiple types of behaviors, such as clicking, adding to cart, and purchasing. Multi-behavior sequential recommendation aims to jointly consider multiple behaviors to improve the target behavior's performance. However, with diversified behavior data, user behavior sequences will become very long in the short term, which brings challenges to the efficiency of the sequence recommendation model. Meanwhile, some behavior data will also bring inevitable noise to the modeling of user interests. To address the aforementioned issues, firstly, we develop the Efficient Behavior Sequence Miner (EBM) that efficiently captures intricate patterns in user behavior while maintaining low time complexity and parameter count. Secondly, we design hard and soft denoising modules for different noise types and fully explore the relationship between behaviors and noise. Finally, we introduce a contrastive loss function along with a guided training strategy to contrast the valid information with the noisy signal in the data, and seamlessly integrate the two denoising processes to achieve a high degree of decoupling of the noisy signal. Sufficient experiments on real-world datasets demonstrate the effectiveness and efficiency of our approach in dealing with multi-behavior sequential recommendation.
PB  - arXiv
PY  - 2024
ST  - END4Rec
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3589334.3645380
ER  -


TY  - GEN
AU  - Tian, Z.
AU  - Zhao, W.X.
AU  - Zhang, C.
AU  - Ma, Z.
AU  - Wen, J.-R.
TI  - EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention
AB  - To capture user preference, transformer models have been widely applied to model sequential user behavior data. The core of transformer architecture lies in the self-attention mechanism, which computes the pairwise attention scores in a sequence. Due to the permutation-equivariant nature, positional encoding is used to enhance the attention between token representations. In this setting, the pairwise attention scores can be derived by both semantic difference and positional difference. However, prior studies often model the two kinds of difference measurements in different ways, which potentially limits the expressive capacity of sequence modeling. To address this issue, this paper proposes a novel transformer variant with complex vector attention, named EulerFormer, which provides a unified theoretical framework to formulate both semantic difference and positional difference. The EulerFormer involves two key technical improvements. First, it employs a new transformation function for efficiently transforming the sequence tokens into polar-form complex vectors using Euler’s formula, enabling the unified modeling of both semantic and positional information in a complex rotation form. Secondly, it develops a differential rotation mechanism, where the semantic rotation angles can be controlled by an adaptation function, enabling the adaptive integration of the semantic and positional information according to the semantic contexts. Furthermore, a phase contrastive learning task is proposed to improve the isotropy of contextual representations in EulerFormer. Our theoretical framework possesses a high degree of completeness and generality (e.g., RoPE can be instantiated as a special case). It is more robust to semantic variations and possesses more superior theoretical properties (e.g., long-term decay) in principle. Extensive experiments conducted on four public datasets demonstrate the effectiveness and efficiency of our approach. Our code is available at https://github.com/RUCAIBox/EulerFormer.
PB  - arXiv
PY  - 2024
ST  - EulerFormer
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3626772.3657805
ER  -


TY  - GEN
AU  - Pan, L.
AU  - Lu, J.
AU  - Tang, X.
TI  - Spatial-Temporal Graph Neural Ode Networks for Skeleton-Based Action Recognition
AB  - In the field of skeleton-based action recognition, accurate recognition of human actions is crucial for applications such as virtual reality and motion analysis. However, it faces challenges like intra-individual action differences and long-term time dependence. To address these challenges, we propose an innovative model called the Spatial-Temporal Graph Neural Ordinary Differential Equations (STG-NODE). First, in the data preprocessing stage, the dynamic time warping (DTW) algorithm is used to normalize and calculate the 3D skeleton data to facilitate the derivation of customized adjacency matrices to improve the influence of intra-individual action differences. Secondly, a custom ordinary differential equations (ODE) integrator is applied based on the initial conditions of the temporal features, which produces a solution function that simulates the dynamic evolution of the events. Finally, the outstanding ODE solver is used to numerically solve the time features based on the solution function to improve the influence of long term dependence on the recognition accuracy and form a more powerful time modeling ability. Through extensive experiments on various benchmarks on NTU RGB+D 60 and Kinetics Skeleton 400 datasets, we demonstrate the superior performance of STG-NODE in the domain of action recognition. The success of the STG-NODE model also provides new ideas and methods for the future development of the action recognition field.
PB  - SSRN
PY  - 2024
ST  - Spatial-Temporal Graph Neural Ode Networks for Skeleton-Based Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4772809
ER  -


TY  - GEN
AU  - Pouryafar, E.
TI  - Analyzing and Attribution of the Significant Large Scale Spatio-Temporal Variations in Vegetation Cover and Their Response to the Climatic Parameters, Direct Human Activities and Terrain Factors
AB  - The changes in the vegetation dynamics can lead to change in the basin streamflow. Vegetation changes could be due to both natural factors such as climate change and direct human activities. On the other hand, the topography characteristics could also alter the spatial distribution of vegetation coverage. In this paper, the spatio-temporal changes in the vegetation cover and their response to climate change, anthropogenic disturbances, and terrain factors were analyzed in the Zarrinehrood, Siminehrood, and Lilanchai river basin during 1982-2013. The case study is one of the main feeding sources of Lake Urmia which its streamflow variations are considered as one of the main reasons for the lake level falling in recent decades. In order to analyze the vegetation cover variations, the GIMMS/NDVI product for 4 months namely May, June, July, and August (during the growing season) was used between 1982-2013. Five climatic parameters namely minimum, maximum and mean temperature, precipitation, and potential evapotranspiration were applied to analyze their relationship with NDVI values. The results indicate that the annual NDVI has increased in most regions of the study area which this increasing trend is more significant around the western parts and basin outlet. These regions and some parts of the northern areas also have the lowest correlation between NDVI and climatic parameters. On the other hand, there is a significant relationship between NDVI and terrain factors such as elevation and slope. By increasing the average elevation up to 1450 meters and the average slope to around 4-6 degrees, the average annual NDVI has a significant downward trend and then increases gradually. To analyze these findings, some agricultural images and the Global Land Cover Map (GLCM) were used and finally found that the main reason is the agricultural land development, especially around the lowland and flat areas.
PB  - SSRN
PY  - 2024
ST  - Analyzing and Attribution of the Significant Large Scale Spatio-Temporal Variations in Vegetation Cover and Their Response to the Climatic Parameters, Direct Human Activities and Terrain Factors
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4770485
ER  -


TY  - GEN
AU  - Calvo, R.
AU  - Martorell, C.
AU  - Morales, G.B.
AU  - Di Santo, S.
AU  - Muñoz, M.A.
TI  - Frequency-dependent covariance reveals critical spatio-temporal patterns of synchronized activity in the human brain
AB  - Recent analyses combining advanced theoretical techniques and high-quality data from thousands of simultaneously recorded neurons provide strong support for the hypothesis that neural dynamics operate near the edge of instability across regions in the brain. However, these analyses, as well as related studies, often fail to capture the intricate temporal structure of brain activity as they primarily rely on time-integrated measurements across neurons. In this study, we present a novel framework designed to explore signatures of criticality across diverse frequency bands and construct a much more comprehensive description of brain activity. Additionally, we introduce a method for projecting brain activity onto a basis of spatio-temporal patterns, facilitating time-dependent dimensionality reduction. Applying this framework to a magnetoencephalography dataset, we observe significant differences in both criticality signatures and spatio-temporal activity patterns between healthy subjects and individuals with Parkinson’s disease.
PB  - arXiv
PY  - 2024
ST  - Frequency-dependent covariance reveals critical spatio-temporal patterns of synchronized activity in the human brain
Y2  - 2025/05/05/21:54:31
DO  - 10.1103/physrevlett.133.208401
ER  -


TY  - GEN
AU  - Yu, J.
AU  - Zhang, Z.
AU  - Wei, Z.
AU  - Zhu, J.
AU  - Zhu, W.
TI  - AUD-TGN: Advancing Action Unit Detection with Temporal Convolution and GPT-2 in Wild Audiovisual Contexts
AB  - Leveraging the synergy of both audio data and visual data is essential for understanding human emotions and behaviors, especially in in-the-wild setting. Traditional methods for integrating such multimodal information often stumble, leading to less-than-ideal outcomes in the task of facial action unit detection. To overcome these shortcomings, we propose a novel approach utilizing audio-visual multimodal data. This method enhances audio feature extraction by leveraging Mel Frequency Cepstral Coefficients (MFCC) and Log-Mel spectrogram features alongside a pre-trained VGGish network. Moreover, this paper adaptively captures fusion features across modalities by modeling the temporal relationships, and ultilizes a pre-trained GPT-2 model for sophisticated context-aware fusion of multimodal information. Our method notably improves the accuracy of AU detection by understanding the temporal and contextual nuances of the data, showcasing significant advancements in the comprehension of intricate scenarios. These findings underscore the potential of integrating temporal dynamics and contextual interpretation, paving the way for future research endeavors.
PB  - arXiv
PY  - 2024
ST  - AUD-TGN
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvprw63382.2024.00484
ER  -


TY  - GEN
AU  - Lee, S.
AU  - Wang, Y.
AU  - Woo, S.
AU  - Kim, C.
TI  - Spatio-Temporal Proximity-Aware Dual-Path Model for Panoramic Activity Recognition
AB  - Panoramic Activity Recognition (PAR) seeks to identify diverse human activities across different scales, from individual actions to social group and global activities in crowded panoramic scenes. PAR presents two major challenges: 1) recognizing the nuanced interactions among numerous individuals and 2) understanding multi-granular human activities. To address these, we propose Social Proximity-aware Dual-Path Network (SPDP-Net) based on two key design principles. First, while previous works often focus on spatial distance among individuals within an image, we argue to consider the spatio-temporal proximity. It is crucial for individual relation encoding to correctly understand social dynamics. Secondly, deviating from existing hierarchical approaches (individual-to-social-to-global activity), we introduce a dual-path architecture for multi-granular activity recognition. This architecture comprises individual-to-global and individual-to-social paths, mutually reinforcing each other’s task with global-local context through multiple layers. Through extensive experiments, we validate the effectiveness of the spatio-temporal proximity among individuals and the dual-path architecture in PAR. Furthermore, SPDP-Net achieves new state-of-the-art performance with 46.5% of overall F1 score on JRDB-PAR dataset.
PB  - arXiv
PY  - 2024
ST  - Spatio-Temporal Proximity-Aware Dual-Path Model for Panoramic Activity Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-73242-3_2
ER  -


TY  - GEN
AU  - Adibpour, P.
AU  - Nasser, H.
AU  - Pedoux, A.
AU  - Biran, V.
AU  - Dubois, J.
TI  - Characterizing the temporal dynamics and maturation of brain activity during sleep: an EEG microstate study in preterm and full-term infants.
AB  - By interfering with the normal sequence of mechanisms serving the brain maturation, premature birth and related stress can alter perinatal experiences, with potential long-term consequences on a child's neurodevelopment. The early characterization of brain functioning and maturational changes is thus of critical interest in premature infants who are at high risk of atypical outcomes and could benefit from early diagnosis and dedicated interventions. Using high-density electroencephalography (HD-EEG), we recorded brain activity in extreme and very preterm infants at the equivalent age of pregnancy term (n=43), and longitudinally 2-months later (n=33), compared with full-term born infants (n=14). We characterized the maturation of brain activity by using a dedicated microstate analysis to quantify the spatio-temporal dynamics of the spontaneous transient network activity while controlling for vigilance states. The comparison of premature and full-term infants first showed slower dynamics as well as altered spatio-temporal properties of brain activity in preterm infants. Maturation of functional networks between term-equivalent age and 2 months later in preterms was linked to the emergence of faster dynamics, manifested in part by shorter duration of microstates, as well as an evolution in the spatial organization of the dominant microstates. The inter-individual differences in the temporal dynamics of brain activity at term-equivalent age were further impacted by sex (with slower microstate dynamics in boys) and by gestational age at birth for some microstate dynamics but not by other considered risk factors. This study highlights the potential of the microstate approach to reveal maturational properties of the emerging brain network activity in premature infants.
PB  - bioRxiv
PY  - 2024
ST  - Characterizing the temporal dynamics and maturation of brain activity during sleep
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2024.03.19.585608
ER  -


TY  - GEN
AU  - Xia, K.
AU  - Wang, L.
AU  - Zhou, S.
AU  - Hua, G.
AU  - Tang, W.
TI  - Boosting Semi-Supervised Temporal Action Localization by Learning from Non-Target Classes
AB  - The crux of semi-supervised temporal action localization (SS-TAL) lies in excavating valuable information from abundant unlabeled videos. However, current approaches predominantly focus on building models that are robust to the error-prone target class (i.e., the predicted class with the highest confidence) while ignoring informative semantics within non-target classes. This paper approaches SS-TAL from a novel perspective by advocating for learning from non-target classes, transcending the conventional focus solely on the target class. The proposed approach involves partitioning the label space of the predicted class distribution into distinct subspaces: target class, positive classes, negative classes, and ambiguous classes, aiming to mine both positive and negative semantics that are absent in the target class, while excluding ambiguous classes. To this end, we first devise innovative strategies to adaptively select high-quality positive and negative classes from the label space, by modeling both the confidence and rank of a class in relation to those of the target class. Then, we introduce novel positive and negative losses designed to guide the learning process, pushing predictions closer to positive classes and away from negative classes. Finally, the positive and negative processes are integrated into a hybrid positive-negative learning framework, facilitating the utilization of non-target classes in both labeled and unlabeled videos. Experimental results on THUMOS14 and ActivityNet v1.3 demonstrate the superiority of the proposed method over prior state-of-the-art approaches.
PB  - arXiv
PY  - 2024
ST  - Boosting Semi-Supervised Temporal Action Localization by Learning from Non-Target Classes
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccv51070.2023.00932
ER  -


TY  - GEN
AU  - Liu, M.
AU  - Li, W.
AU  - Ge, F.
AU  - Gao, X.
TI  - Weakly-supervised Temporal Action Localization using Multi-branch Attention Weighting
AB  - Weakly-supervised temporal action localization trains an action localization model using only video-level labels for training. Owing to the lack of frame-level temporal annotations, most existing weakly-supervised temporal action localization methods are based on multiple instance learning mechanisms to predict the start and end times of all action instances in an untrimmed video and to classify them. Many localization approaches focus only on the most discriminative regions that contribute to the classification task, but ignore a large number of ambiguous background and context snippets in a video. So that results in poor localization performance. We believe that it is helpful to identify such snippets by modeling the background and context. Therefore, we propose a multi-branch attention weighting network (MAW-Net), which introduces an additional non-action class and integrates a multi-branch attention module to generate action and background attention, respectively. In addition, considering the correlation among context, action, and background, we use the difference of action and background attention to construct context attention. Finally, based on these three types of attention values, we obtain three new class activation sequences that distinguish action, background, and context. This enables our proposed model to effectively suppress the activation of context and background snippets and achieve better localization performance. Extensive experiments were performed on the THUMOS-14 and Activitynet1.3 datasets. The experimental results show that our method is superior to other state-of-the-art methods, and its performance is comparable to those of fully-supervised approaches.
PB  - Research Square
PY  - 2024
ST  - Weakly-supervised Temporal Action Localization using Multi-branch Attention Weighting
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-4079741/v1
ER  -


TY  - GEN
AU  - Do, J.
AU  - Kim, M.
TI  - SkateFormer: Skeletal-Temporal Transformer for Human Action Recognition
AB  - Skeleton-based action recognition, which classifies human actions based on the coordinates of joints and their connectivity within skeleton data, is widely utilized in various scenarios. While Graph Convolutional Networks (GCNs) have been proposed for skeleton data represented as graphs, they suffer from limited receptive fields constrained by joint connectivity. To address this limitation, recent advancements have introduced transformer-based methods. However, capturing correlations between all joints in all frames requires substantial memory resources. To alleviate this, we propose a novel approach called Skeletal-Temporal Transformer (SkateFormer) that partitions joints and frames based on different types of skeletal-temporal relation (Skate-Type) and performs skeletal-temporal self-attention (Skate-MSA) within each partition. We categorize the key skeletal-temporal relations for action recognition into a total of four distinct types. These types combine (i) two skeletal relation types based on physically neighboring and distant joints, and (ii) two temporal relation types based on neighboring and distant frames. Through this partition-specific attention strategy, our SkateFormer can selectively focus on key joints and frames crucial for action recognition in an action-adaptive manner with efficient computation. Extensive experiments on various benchmark datasets validate that our SkateFormer outperforms recent state-of-the-art methods.
PB  - arXiv
PY  - 2024
ST  - SkateFormer
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-72940-9_23
ER  -


TY  - GEN
AU  - Xiao, J.
AU  - Li, H.
AU  - Zhang, Y.
AU  - Shen, L.
AU  - Xu, J.
TI  - Temporal Spatial Transfer Characteristics and Cross-Scale Fusion Behavior Mechanism of Interface Temperature in Induction Welding of Plain Carbon Fiber Polyphenylene Sulfide
AB  - The intricate coupling of multiple physical fields and the nonlinear characteristics of materials in the induction welding of carbon fiber composites render the temperature transfer characteristics and the cross-scale fusion connection mechanism at the interface ambiguous. In this study, we have developed a transient three-dimensional finite element model to simulate the induction welding process of carbon fiber-reinforced polyphenylene sulfide (CF/PPS) composites. This model integrates the coupling mechanisms of electrical, magnetic, and thermal multiphysical fields. The primary objective is to accurately represent the temporal and spatial evolution of various characteristics throughout the induction welding process. Our investigation reveals that, due to electromagnetic influence, the edge effect at the induction welding interface extends beyond the limitation imposed solely by the low thermal conductivity of air. The combination of enhanced magnetic flux concentration and spatial constraints results in the concentration of eddy currents at the edge of interface, introducing an additional factor influencing the edge effect. Simultaneously, the temperature distribution at the induction welding interface exhibits gradient characteristics over time and space. In response to the fluctuating current heat source, the temperature difference across the welding interface demonstrates a substantial gradient distribution ranging from 225% to 680%. As a consequence, the shear strength of the interface experiences a reduction of 43.29%. Lastly, to gain a more comprehensive understanding of the impact of heat input on the fusion behavior of the bonding interface, we have conducted an analysis of the induction welding interface's fusion morphology using scanning electron microscopy (SEM). Furthermore, we have investigated the formation and fusion mechanisms of welding defects through elemental analysis using energy-dispersive spectroscopy (EDS). The results of our micro-macro cross-scale study reveal that as the heat input increases, the resin matrix at the welding interface undergoes decomposition, leading to the generation of a series of gases. As the welding material cools and solidifies, any bubbles that are unable to escape from the fusion region adhere to the surface of the thermally conductive wire mesh, resulting in the formation of pore defects. Studying the mechanism of cross-scale temperature space-time transfer and connection formation through the coupling of multiple physical fields can offer valuable theoretical insights for the practical engineering application of continuous welding in Carbon Fiber Reinforced Thermoplastic (CFRTP) materials.
PB  - SSRN
PY  - 2024
ST  - Temporal Spatial Transfer Characteristics and Cross-Scale Fusion Behavior Mechanism of Interface Temperature in Induction Welding of Plain Carbon Fiber Polyphenylene Sulfide
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.ijheatmasstransfer.2024.125846
ER  -


TY  - GEN
AU  - Ye, X.
AU  - Wang, K.I.-K.
TI  - Cross-user activity recognition via temporal relation optimal transport
AB  - Current research on human activity recognition (HAR) mainly assumes that training and testing data are drawn from the same distribution to achieve a generalised model, which means all the data are considered to be independent and identically distributed (i.i.d.). In many real-world applications, this assumption does not hold, and collected training and target testing datasets have non-uniform distribution, such as in the case of cross-user HAR. Domain adaptation is a promising approach for cross-user HAR tasks. Existing domain adaptation works based on the assumption that samples in each domain are i.i.d. and do not consider the knowledge of temporal relation hidden in time series data for aligning data distribution. This strong assumption of i.i.d. may not be suitable for time series-related domain adaptation methods because the samples formed by time series segmentation and feature extraction techniques are only coarse approximations to i.i.d. assumption in each domain. In this paper, we propose the temporal relation optimal transport (TROT) method to utilise temporal relation and relax the i.i.d. assumption for the samples in each domain for accurate and efficient knowledge transfer. We obtain the temporal relation representation and implement temporal relation alignment of activities via the Hidden Markov model (HMM) and optimal transport (OT) techniques. Besides, a new regularisation term that preserves temporal relation order information for an improved optimal transport mapping is proposed to enhance the domain adaptation performance. Comprehensive experiments are conducted on three public activity recognition datasets (i.e. OPPT, PAMAP2 and DSADS), demonstrating that TROT outperforms other state-of-the-art methods.
PB  - arXiv
PY  - 2024
ST  - Cross-user activity recognition via temporal relation optimal transport
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-63989-0_18
ER  -


TY  - GEN
AU  - Ye, X.
AU  - Wang, K.I.-K.
TI  - Deep Generative Domain Adaptation with Temporal Attention for Cross-User Activity Recognition
AB  - In Human Activity Recognition (HAR), a predominant assumption is that the data utilized for training and evaluation purposes are drawn from the same distribution. It is also assumed that all data samples are independent and identically distributed (i.i.d.). Contrarily, practical implementations often challenge this notion, manifesting data distribution discrepancies, especially in scenarios such as cross-user HAR. Domain adaptation is the promising approach to address these challenges inherent in cross-user HAR tasks. However, a clear gap in domain adaptation techniques is the neglect of the temporal relation embedded within time series data during the phase of aligning data distributions. Addressing this oversight, our research presents the Deep Generative Domain Adaptation with Temporal Attention (DGDATA) method. This novel method uniquely recognises and integrates temporal relations during the domain adaptation process. By synergizing the capabilities of generative models with the Temporal Relation Attention mechanism, our method improves the classification performance in cross-user HAR. A comprehensive evaluation has been conducted on three public sensor-based HAR datasets targeting different scenarios and applications to demonstrate the efficacy of the proposed DGDATA method.
PB  - arXiv
PY  - 2024
ST  - Deep Generative Domain Adaptation with Temporal Attention for Cross-User Activity Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.patcog.2024.110811
ER  -


TY  - GEN
AU  - Ye, X.
AU  - Abdulla, W.H.
AU  - Nair, N.
AU  - Wang, K.I.-K.
TI  - Cross-user activity recognition using deep domain adaptation with temporal relation information
AB  - Human Activity Recognition (HAR) is a cornerstone of ubiquitous computing, with promising applications in diverse fields such as health monitoring and ambient assisted living. Despite significant advancements, sensor-based HAR methods often operate under the assumption that training and testing data have identical distributions. However, in many real-world scenarios, particularly in sensor-based HAR, this assumption is invalidated by out-of-distribution (o.o.d.) challenges, including differences from heterogeneous sensors, change over time, and individual behavioural variability. This paper centres on the latter, exploring the cross-user HAR problem where behavioural variability across individuals results in differing data distributions. To address this challenge, we introduce the Deep Temporal State Domain Adaptation (DTSDA) model, an innovative approach tailored for time series domain adaptation in cross-user HAR. Contrary to the common assumption of sample independence in existing domain adaptation approaches, DTSDA recognizes and harnesses the inherent temporal relations in the data. Therefore, we introduce’Temporal State’, a concept that defined the different sub-activities within an activity, consistent across different users. We ensure these sub-activities follow a logical time sequence through’Temporal Consistency’ property and propose the’Pseudo Temporal State Labeling’ method to identify the user-invariant temporal relations. Moreover, the design principle of DTSDA integrates adversarial learning for better domain adaptation. Comprehensive evaluations on three HAR datasets demonstrate DTSDA’s superior performance in cross-user HAR applications by briding individual behavioral variability using temporal relations across sub-activities.
PB  - arXiv
PY  - 2024
ST  - Cross-user activity recognition using deep domain adaptation with temporal relation information
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tim.2025.3552453
ER  -


TY  - GEN
AU  - Ye, X.
AU  - Wang, K.I.-K.
TI  - Deep Generative Domain Adaptation with Temporal Relation Knowledge for Cross-User Activity Recognition
AB  - In human activity recognition (HAR), the assumption that training and testing data are independent and identically distributed (i.i.d.) often fails, particularly in cross-user scenarios where data distributions vary significantly. This discrepancy highlights the limitations of conventional domain adaptation methods in HAR, which typically overlook the inherent temporal relations in time-series data. To bridge this gap, our study introduces a Conditional Variational Autoencoder with Universal Sequence Mapping (CVAE-USM) approach, which addresses the unique challenges of time-series domain adaptation in HAR by relaxing the i.i.d. assumption and leveraging temporal relations to align data distributions effectively across different users. This method combines the strengths of Variational Autoencoder (VAE) and Universal Sequence Mapping (USM) to capture and utilize common temporal patterns between users for improved activity recognition. Our results, evaluated on two public HAR datasets (OPPT and PAMAP2), demonstrate that CVAE-USM outperforms existing state-of-the-art methods, offering a more accurate and generalizable solution for cross-user activity recognition.
PB  - arXiv
PY  - 2024
ST  - Deep Generative Domain Adaptation with Temporal Relation Knowledge for Cross-User Activity Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.patcog.2024.110811
ER  -


TY  - GEN
AU  - Fang, Q.
AU  - Tang, C.
AU  - Ma, S.
AU  - Yang, Y.
TI  - BID: Boundary-Interior Decoding for Unsupervised Temporal Action Localization Pre-Training
AB  - Skeleton-based motion representations are robust for action localization and understanding for their invariance to perspective, lighting, and occlusion, compared with images. Yet, they are often ambiguous and incomplete when taken out of context, even for human annotators. As infants discern gestures before associating them with words, actions can be conceptualized before being grounded with labels. Therefore, we propose the first unsupervised pre-training framework, Boundary-Interior Decoding (BID), that partitions a skeleton-based motion sequence into discovered semantically meaningful pre-action segments. By fine-tuning our pre-training network with a small number of annotated data, we show results out-performing SOTA methods by a large margin. MSC Codes 68T45
PB  - arXiv
PY  - 2024
ST  - BID
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52688.2022.01364
ER  -


TY  - GEN
AU  - Akdag, E.
AU  - Zhu, Z.
AU  - Bondarev, E.
AU  - de With, P.H.N.
TI  - Transformer-based Fusion of 2D-pose and Spatio-temporal Embeddings for Distracted Driver Action Recognition
AB  - Classification and localization of driving actions over time is important for advanced driver-assistance systems and naturalistic driving studies. Temporal localization is challenging because it requires robustness, reliability, and accuracy. In this study, we aim to improve the temporal localization and classification accuracy performance by adapting video action recognition and 2D human-pose estimation networks to one model. Therefore, we design a transformer-based fusion architecture to effectively combine 2D-pose features and spatio-temporal features. The model uses 2D-pose features as the positional embedding of the transformer architecture and spatio-temporal features as the main input to the encoder of the transformer. The proposed solution is generic and independent of the camera numbers and positions, giving frame-based class probabilities as output. Finally, the post-processing step combines information from different camera views to obtain final predictions and eliminate false positives. The model performs well on the A2 test set of the 2023 NVIDIA AI City Challenge for naturalistic driving action recognition, achieving the overlap score of the organizer-defined distracted driver behaviour metric of 0.5079.
PB  - arXiv
PY  - 2024
ST  - Transformer-based Fusion of 2D-pose and Spatio-temporal Embeddings for Distracted Driver Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvprw59228.2023.00576
ER  -


TY  - GEN
AU  - Alkanat, T.
AU  - Akdag, E.
AU  - Bondarev, E.
AU  - de With, P.H.N.
TI  - Density-Guided Label Smoothing for Temporal Localization of Driving Actions
AB  - Temporal localization of driving actions plays a crucial role in advanced driver-assistance systems and naturalistic driving studies. However, this is a challenging task due to strict requirements for robustness, reliability and accurate localization. In this work, we focus on improving the overall performance by efficiently utilizing video action recognition networks and adapting these to the problem of action localization. To this end, we first develop a density-guided label smoothing technique based on label probability distributions to facilitate better learning from boundary video-segments that typically include multiple labels. Second, we design a post-processing step to efficiently fuse information from video-segments and multiple camera views into scene-level predictions, which facilitates elimination of false positives. Our methodology yields a competitive performance on the A2 test set of the naturalistic driving action recognition track of the 2022 NVIDIA AI City Challenge with an F1 score of 0.271.
PB  - arXiv
PY  - 2024
ST  - Density-Guided Label Smoothing for Temporal Localization of Driving Actions
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvprw56347.2022.00358
ER  -


TY  - GEN
AU  - Ding, G.
AU  - Golong, H.
AU  - Yao, A.
TI  - Coherent Temporal Synthesis for Incremental Action Segmentation
AB  - Data replay is a successful incremental learning technique for images. It prevents catastrophic forgetting by keeping a reservoir of previous data, original or synthesized, to ensure the model retains past knowledge while adapting to novel concepts. However, its application in the video domain is rudimentary, as it simply stores frame exemplars for action recognition. This paper presents the first exploration of video data replay techniques for incremental action segmentation, focusing on action temporal modeling. We propose a Temporally Coherent Action (TCA) model, which represents actions using a generative model instead of storing individual frames. The integration of a conditioning variable that captures temporal coherence allows our model to understand the evolution of action features over time. Therefore, action segments generated by TCA for replay are diverse and temporally coherent. In a 10-task incremental setup on the Breakfast dataset, our approach achieves significant increases in accuracy for up to 22% compared to the baselines.
PB  - arXiv
PY  - 2024
ST  - Coherent Temporal Synthesis for Incremental Action Segmentation
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52733.2024.02691
ER  -


TY  - GEN
AU  - Ter, Y.T.
AU  - Westerman, E.L.
TI  - Asynchronous temporal variance in learning behaviour and neural gene expression in a butterfly
AB  - Mate preference learning, including imprinting-like learning, is pervasive across animal taxa, and can affect the selection and maintenance of certain phenotypes. However, not much is known about the temporal dynamics behind imprinting-like learning, or the genetic underpinnings underlying it. To uncover the temporal dynamics of imprinting-like learning, from both a behavioural and transcriptional perspective, we conducted learning and RNA-Seq time series using the butterfly Bicyclus anynana, a species where both sexes learn mate preferences. We exposed females to an unfamiliar, unpreferred male phenotype (4-spotted male) for five different exposure (training) periods, ranging from 30 minutes to three hours, and recorded their choice between the preferred, familiar male phenotype (2-spotted) and 4-spotted males in mate choice trials conducted two days after training. We also assessed differential gene expression of naïve females and females exposed to trainer males for these same five exposure periods, to identify temporal patterns in gene expression associated with learning. While we found that the longest exposure had the strongest effect on preference learning, we did not observe a linear effect of exposure time on learning. We also show that the highest peak of differentially expressed genes (DEGs) was after one hour of exposure. While a number of genes were uniquely DE at each time point, one gene, associated with transcription initiation, was differentially expressed during learning across all five time points. We observed a similar decreasing trend in both gene expression and learned response after 1.5 hours of exposure, offering new insights to possible attention and forgetting mechanisms. Therefore, our results indicate that both gene expression and learning are temporally dynamic, and not linear through time. They also highlight the role of transcription in mate preference learning and memory formation in Lepidoptera, and illustrate that imprinting-like learning may exhibit similar molecular temporal dynamics as associative learning.
PB  - bioRxiv
PY  - 2024
ST  - Asynchronous temporal variance in learning behaviour and neural gene expression in a butterfly
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2024.03.07.583937
ER  -


TY  - GEN
AU  - Golestani, A.
AU  - Rezaei, N.
AU  - Malekpour, M.-R.
AU  - Shahraz, S.
AU  - Farzadfar, F.
TI  - Predicting Errors in Accident Hotspots and Investigating Spatiotemporal, Weather, and Behavioral Factors Using Interpretable Machine Learning: an Analysis of Telematics Big Data
AB  - The utilization of explainable machine learning models has emerged as a key technique for predicting and interpreting various aspects of road traffic accidents (RTAs) in recent years. This study aimed to predict the occurrence of errors in road accident hotspots and interpret the most influential predictors using telematics data. Data from 1673 intercity buses across Iran in 2020, merged with weather data, formed a comprehensive dataset. After preprocessing, 619,988 records were used to build and compare six machine learning models. and the best model was selected for interpretation using SHAP (SHapley Additive exPlanation). Six models including logistic regression, K-nearest neighbors, random forest, Extreme Gradient Boosting (XGBoost), Naïve Bayes, and support vector machine were developed and XGBoost demonstrated the best performance with an area under the curve (AUC) of 91.70% (95% uncertainty interval: 91.33% − 92.09%). SHAP values identified spatial variables, especially province and road type, as the most critical features for error prediction in hotspots. Fatigue emerged as an important predictor, alongside certain weather variables like dew points. Temporal variables had a limited impact. Incorporating various spatiotemporal, behavioral, and weather-related variables collected by telematics, our analysis underscored the significance of spatial variables in predicting errors in accident hotspots in Iran. Policymakers are advised to prioritize decisions strengthening road infrastructures to mitigate the burden of RTAs.
PB  - Research Square
PY  - 2024
ST  - Predicting Errors in Accident Hotspots and Investigating Spatiotemporal, Weather, and Behavioral Factors Using Interpretable Machine Learning
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3979830/v1
ER  -


TY  - GEN
AU  - Moradian, M.
AU  - Dadlani, A.
AU  - Kairgeldin, R.
AU  - Khonsari, A.
TI  - Cost-Effective Activity Control of Asymptomatic Carriers in Layered Temporal Social Networks
AB  - The robustness of human social networks against epidemic propagation relies on the propensity for physical contact adaptation. During the early phase of infection, asymptomatic carriers exhibit the same activity level as susceptible individuals, which presents challenges for incorporating control measures in epidemic projection models. This paper focuses on modeling and cost-efficient activity control of susceptible and carrier individuals in the context of the susceptible-carrier-infected-removed (SCIR) epidemic model over a two-layer contact network. In this model, individuals switch from a static contact layer to create new links in a temporal layer based on state-dependent activation rates. We derive conditions for the infection to die out or persist in a homogeneous network. Considering the significant costs associated with reducing the activity of susceptible and carrier individuals, we formulate an optimization problem to minimize the disease decay rate while constrained by a limited budget. We propose the use of successive geometric programming (SGP) approximation for this optimization task. Through simulation experiments on Poisson random graphs, we assess the impact of different parameters on disease prevalence. The results demonstrate that our SGP framework achieves a cost reduction of nearly 33% compared to conventional methods based on degree and closeness centrality.
PB  - arXiv
PY  - 2024
ST  - Cost-Effective Activity Control of Asymptomatic Carriers in Layered Temporal Social Networks
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tcss.2024.3392715
ER  -


TY  - GEN
AU  - Ivanova, M.
AU  - Moss, E.G.
TI  - A temporal sequence of heterochronic gene activities promotes stage-specific developmental events in C. elegans
AB  - The heterochronic genes of the nematode Caenorhabditis elegans control the succession of postembryonic developmental events. The four core heterochronic genes lin-14, lin-28, hbl-1, and lin-41 act in a sequence to specify cell fates specific to each of the four larval stages. It was previously shown that lin-14 has two activities separated in time that promote L1 and L2 developmental events, respectively. Using the auxin-inducible degron system, we find that lin-28 and hbl-1 each have two activities that control L2 and L3 events which are also separated in time. Relative to events they control, both lin-28 and hbl-1 appear to act just prior to or concurrently with events of the L2. Relative to each other, lin-28 and hbl-1 appear to act simultaneously. By contrast, the lin-14 activity controlling L2 events precedes those of lin-28 and hbl-1 controlling the same events, suggesting lin-14’s regulation of lin-28 is responsible for the delay. Likewise, the activities of lin-28 and hbl-1 controlling L3 fates act well in advance of those fates, suggesting a similar regulatory gap. lin-41 acts early in the L3 to affect fates of the L4, although it was not possible to determine whether it too has two temporally separated activities. We also uncovered a feedback phenomenon that prevents the reactivation of heterochronic gene activity late in development after it has been downregulated. This study places the heterochronic gene activities into a timeline of postembryonic development relative to one another and to the developmental events whose timing they control.
PB  - bioRxiv
PY  - 2024
ST  - A temporal sequence of heterochronic gene activities promotes stage-specific developmental events in C. elegans
Y2  - 2025/05/05/21:54:31
DO  - 10.1093/g3journal/jkae130
ER  -


TY  - GEN
AU  - Azzam, F.
AU  - Ali, A.
AU  - Kayed, M.
AU  - Ali, H.
TI  - Temporal Dynamics of User Activities: Deep Learning Strategies and Mathematical Modeling for Long-Term and Short-Term Profiling
AB  - Profiling social media users is an analytical approach to generate an extensive blueprint of user’s personal characteristics, which can be useful for a diverse range of applications, such as targeted marketing and personalized recommendations. Although social user profiling has gained substantial attention in recent years, effectively constructing a collaborative model that could describe long and short-term profiles is still challenging. In this paper, we will discuss the profiling problem from two perspectives; how to mathematically model and track user’s behavior over short and long periods and how to enhance the classification of user’s activities. Using mathematical equations, our model can define periods in which the user's interests abruptly changed. A dataset consisting of 30,000 tweets was built and manually annotated into 10 topic categories. Bi-LSTM and GRU models are applied to classify the user’s activities representing his interests, which then are utilized to create and model the dynamic profile. In addition, the effect of word embedding techniques and pre-trained classification models on the accuracy of the classification process is explored in this research.
PB  - Research Square
PY  - 2024
ST  - Temporal Dynamics of User Activities
Y2  - 2025/05/05/21:54:31
DO  - 10.1038/s41598-024-64120-6
ER  -


TY  - GEN
AU  - Wu, Q.
AU  - Yang, X.
AU  - Wang, K.
AU  - Liang, J.
AU  - Han, Y.
TI  - 'PyTDL': A Versatile Toolkit of Temporal Difference Learning Algorithm to Simulate Behavior Process of Decision Making and Cognitive Learning
AB  - Humans and animals can learn complex tasks through reward-based feedback, dynamically adjusting their value expectations and choices based on past experience to optimize outcomes. Assessing the cognitive components behind such behaviors can be challenging, as they are often hidden behind observable actions. Neuroscientists use the temporal difference (TD) learning model, a reinforcement learning framework based on experiential learning and outcome prediction, to estimate cognitive elements like value representation and prediction error. While traditional TD algorithms provide a basic framework, they fall short in diverse and dynamic tasks due to fixed patterns. We present PyTDL, a Python-based open-source software for applying the Temporal Difference Learning algorithms to model cognitive process of decision making and learning. PyTDL offers a versatile and adaptive modular framework to implement customizable non-linear learning, value updating functions, and decision policies. We demonstrated the application of PyTDL by modeling decision processes of animals performing two types of cognitive tasks under uncertain conditions. The classic learning and decision algorithms, such as Q-learning, softmax and epsilon-greedy, failed to capture the highly dynamic decision adjustment process under the uncertain environment. Implementation of customized learning and decision function is necessary for applying TD learning model to mimic the empirical behavior data. PyTDL provide GUI and APIs to easily call both the classic TD learning algorithms and the customized functions and visualize the simulation results. This toolkit enables researchers to tailor models for specific tasks, align computational models with empirical data, and advance understanding of brain learning and decision-making in complex environments.
PB  - SSRN
PY  - 2024
ST  - 'PyTDL'
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4741097
ER  -


TY  - GEN
AU  - Zhai, J.
AU  - Liao, L.
AU  - Liu, X.
AU  - Lu, Y.
AU  - Shi, Y.
TI  - Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations
AB  - Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute. Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (“Generative Recommenders”), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data. HSTU outperforms baselines over synthetic and public datasets by up to 65.8% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion parameters, improve metrics in online A/B tests by 12.4% and have been deployed on multiple surfaces of a large internet platform with billions of users. More importantly, the model quality of Generative Recommenders empirically scales as a power-law of training compute across three orders of magnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for future model developments, and further paves the way for the first foundation models in recommendations.
PB  - arXiv
PY  - 2024
ST  - Actions Speak Louder than Words
Y2  - 2025/05/05/21:54:31
DO  - 10.1097/01253086-199721020-00008
ER  -


TY  - GEN
AU  - Seweryn, K.
AU  - Wróblewska, A.
AU  - Łukasik, S.
TI  - Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer - Current Trends and Research Perspectives
AB  - Analysing action scenes in soccer is a challenging task due to the complex and dynamic nature of the game, as well as the interactions between players. This article provides a comprehensive overview of this task divided into action recognition, spotting, and spatio-temporal action localization in soccer, with a particular emphasis on the modalities used and multimodal methods. We explore the publicly available data sources and metrics used to evaluate models' performance. The article reviews recent state-of-the-art methods that leverage deep learning techniques and traditional methods. We focus on multimodal methods, which integrate information from multiple sources, such as video and audio data, and also those that represent one source in various ways. The advantages and limitations of methods are discussed, along with their potential for improving the accuracy and robustness of models. Finally, the article highlights some of the open research questions and future directions in the field of soccer action analysis, including the potential for multimodal methods to advance this field. Overall, this survey provides a valuable resource for researchers interested in the field of analysing action scenes in soccer.
PB  - SSRN
PY  - 2024
ST  - Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer - Current Trends and Research Perspectives
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4736989
ER  -


TY  - GEN
AU  - An, Y.
AU  - Yi, Y.
AU  - Wu, L.
AU  - Li, Y.
AU  - Su, C.
TI  - Unsupervised Prototype Self-Calibration Spatio-Temporal Attention Network for Enhanced Few-Shot Action Recognition
AB  - The collection and annotation of large-scale video data pose significant challenges, prompting the exploration of few-shot learning models capable of recognizing unseen actions with limited training samples. However, many existing models rely on mean-of-class prototypes for classification, a method susceptible to outliers. To address this limitation, we propose the unsupervised prototype self-calibration spatio-temporal attention network (UPSSA), designed to refine prototypes through various strategies. Our network comprises three innovative components: a spatio-temporal attention network (STA), a dual adaptive triplet loss mechanism (DATL), and an unsupervised prototype self-calibration mechanism (UPSC). The STA employs self-attention and cross-attention sequentially, refining prototypes and query samples. This process reduces intra-class variations while enhancing inter-class variations. The DATL constructs prototype-centered and query-centered losses to simultaneously optimize prototypes, thereby improving the model's sensitivity to inter-class differences. Additionally, DATL incorporates adaptive margins to dynamically adjust the distances between positive and negative samples, mitigating the unreliability of fixed margins. For testing tasks, the UPSC minimizes data distribution shifts between the training and testing sets. Evaluation on the UCF-101 and HMDB-51 datasets demonstrates that our model achieves state-of-the-art results, underscoring the efficacy of our prototype enhancement methods for few-shot action recognition.
PB  - SSRN
PY  - 2024
ST  - Unsupervised Prototype Self-Calibration Spatio-Temporal Attention Network for Enhanced Few-Shot Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4736348
ER  -


TY  - GEN
AU  - Su, J.
AU  - Chen, C.
AU  - Lin, Z.
AU  - Liu, W.
AU  - Zheng, X.
TI  - Personalized Behavior-Aware Transformer for Multi-Behavior Sequential Recommendation
AB  - Sequential Recommendation (SR) captures users’ dynamic preferences by modeling how users transit among items. However, SR models that utilize only single type of behavior interaction data encounter performance degradation when the sequences are short. To tackle this problem, we focus on Multi-Behavior Sequential Recommendation (MBSR) in this paper, which aims to leverage time-evolving heterogeneous behavioral dependencies for better exploring users’ potential intents on the target behavior. Solving MBSR is challenging. On the one hand, users exhibit diverse multi-behavior patterns due to personal characteristics. On the other hand, there exists comprehensive co-influence between behavior correlations and item collaborations, the intensity of which is deeply affected by temporal factors. To tackle these challenges, we propose a Personalized Behavior-Aware Transformer framework (PBAT) for MBSR problem, which models personalized patterns and multifaceted sequential collaborations in a novel way to boost recommendation performance. First, PBAT develops a personalized behavior pattern generator in the representation layer, which extracts dynamic and discriminative behavior patterns for sequential learning. Second, PBAT reforms the self-attention layer with a behavior-aware collaboration extractor, which introduces a fused behavior-aware attention mechanism for incorporating both behavioral and temporal impacts into collaborative transitions. We conduct experiments on three benchmark datasets and the results demonstrate the effectiveness and interpretability of our framework. Our implementation code is released at https://github.com/TiliaceaeSU/PBAT.
PB  - arXiv
PY  - 2024
ST  - Personalized Behavior-Aware Transformer for Multi-Behavior Sequential Recommendation
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3581783.3611723
ER  -


TY  - GEN
AU  - Li, W.-X.
AU  - Wu, Y.-H.
AU  - Liu, Y.
AU  - Pan, W.-K.
AU  - Ming, Z.
TI  - BMLP: Behavior-aware MLP for Heterogeneous Sequential Recommendation
AB  - In real recommendation scenarios, users often have different types of behaviors, such as clicking and buying. Existing research methods show that it is possible to capture the heterogeneous interests of users through different types of behaviors. However, most multi-behavior approaches have limitations in learning the relationship between different behaviors. In this paper, we propose a novel multilayer perceptron (MLP)-based heterogeneous sequential recommendation method, namely behavior-aware multilayer perceptron (BMLP). Specifically, it has two main modules, including a heterogeneous interest perception (HIP) module, which models behaviors at multiple granularities through behavior types and transition relationships, and a purchase intent perception (PIP) module, which adaptively fuses subsequences of auxiliary behaviors to capture users’ purchase intent. Compared with mainstream sequence models, MLP is competitive in terms of accuracy and has unique advantages in simplicity and efficiency. Extensive experiments show that BMLP achieves significant improvement over state-of-the-art algorithms on four public datasets. In addition, its pure MLP architecture leads to a linear time complexity.
PB  - arXiv
PY  - 2024
ST  - BMLP
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s11704-023-2703-y
ER  -


TY  - GEN
AU  - Li, Y.
AU  - Chen, G.
AU  - Abramowitz, B.
AU  - Anzellotti, S.
AU  - Wei, D.
TI  - Learning Causal Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition
AB  - Few-shot action recognition aims at quickly adapting a pre-trained model to the novel data with a distribution shift using only a limited number of samples. Key challenges include how to identify and leverage the transferable knowledge learned by the pre-trained model. We therefore propose CDTD, or Causal Domain-Invariant Temporal Dynamics for knowledge transfer. To identify the temporally invariant and variant representations, we employ the causal representation learning methods for unsupervised pertaining, and then tune the classifier with supervisions in next stage. Specifically, we assume the domain information can be well estimated and the pre-trained image decoder and transition models can be well transferred. During adaptation, we fix the transferable temporal dynamics and update the image encoder and domain estimator. The efficacy of our approach is revealed by the superior accuracy of CDTD over leading alternatives across standard few-shot action recognition datasets.
PB  - arXiv
PY  - 2024
ST  - Learning Causal Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-030-58558-7_31
ER  -


TY  - GEN
AU  - Zheng, R.
AU  - Cheng, C.-A.
AU  - Daumé, H.
AU  - Huang, F.
AU  - Kolobov, A.
TI  - PRISE: LLM-Style Sequence Compression for Learning Temporal Action Abstractions in Control
AB  - Temporal action abstractions, along with belief state representations, are a powerful knowledge sharing mechanism for sequential decision making. In this work, we propose a novel view that treats inducing temporal action abstractions as a sequence compression problem. To do so, we bring a subtle but critical component of LLM training pipelines – input tokenization via byte pair encoding (BPE) – to bear on the seemingly distant task of learning skills of variable time span in continuous control domains. We introduce an approach called Primitive Sequence Encoding (PRISE) that combines continuous action quantization with BPE to learn powerful action abstractions. We empirically show that high-level skills discovered by PRISE from a multitask set of robotic manipulation demonstrations significantly boost the performance of Behavior Cloning on downstream tasks.
PB  - arXiv
PY  - 2024
ST  - PRISE
Y2  - 2025/05/05/21:54:31
DO  - 10.32470/ccn.2019.1258-0
ER  -


TY  - GEN
AU  - Jung, H.M.
TI  - Extension of Sequential Equilibrium to Games with in Nite Types and Actions
AB  - This paper extends the solution concept of Sequential Equilibrium (SE) to games with infinite types and actions, thereby introducing Global Sequential Equilibrium (GSE). While qualitatively similar to SE, GSE distinctively defines posteriors globally as whole probability distributions across all information sets in each period, rather than separately at each information set as in SE. The following properties of GSE are demonstrated: Its existence under the conditions of a compact set of strategy profiles (in the weak topology) and bounded and continuous utility functions; its equivalence to SE in nite games; its qualification as a Nash equilibrium and, in games with incomplete information, also as a subgame perfect Nash equilibrium; and its satisfaction of convex structural consistency under the condition of statistical independence of players types and states. These properties establish GSE as a robust and generalizable extension of SE, with important implications for the analysis of games with infinite types and actions.
PB  - SSRN
PY  - 2024
ST  - Extension of Sequential Equilibrium to Games with in Nite Types and Actions
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4727112
ER  -


TY  - GEN
AU  - Jung, H.M.
TI  - Extension of Sequential Equilibrium to Games with in Nite Types and Actions
AB  - This paper extends the solution concept of Sequential Equilibrium (SE) to games with infinite types and actions, thereby introducing Global Sequential Equilibrium (GSE). While qualitatively similar to SE, GSE distinctively defines posteriors globally as whole probability distributions across all information sets in each period, rather than separately at each information set as in SE. The following properties of GSE are demonstrated: Its existence under the conditions of a compact set of strategy profiles (in the weak topology) and bounded and continuous utility functions; its equivalence to SE in nite games; its qualification as a Nash equilibrium and, in games with incomplete information, also as a subgame perfect Nash equilibrium; and its satisfaction of convex structural consistency under the condition of statistical independence of players types and states. These properties establish GSE as a robust and generalizable extension of SE, with important implications for the analysis of games with infinite types and actions.
PB  - SSRN
PY  - 2024
ST  - Extension of Sequential Equilibrium to Games with in Nite Types and Actions
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4727112
ER  -


TY  - GEN
AU  - Mancastroppa, M.
AU  - Vezzani, A.
AU  - Colizza, V.
AU  - Burioni, R.
TI  - Preserving system activity while controlling epidemic spreading in adaptive temporal networks
AB  - Human behaviour strongly influences the spread of infectious diseases: understanding the interplay between epidemic dynamics and adaptive behaviours is essential to improve response strategies to epidemics, with the goal of containing the epidemic while preserving a sufficient level of operativeness in the population. Through activity-driven temporal networks, we formulate a general framework which models a wide range of adaptive behaviours and mitigation strategies, observed in real populations. We analytically derive the conditions for a widespread diffusion of epidemics in the presence of arbitrary adaptive behaviours, highlighting the crucial role of correlations between agents behaviour in the infected and in the susceptible state. We focus on the effects of sick-leave, comparing the effectiveness of different strategies in reducing the impact of the epidemic and preserving the system operativeness. We show the critical relevance of heterogeneity in individual behavior: in homogeneous networks, all sick-leave strategies are equivalent and poorly effective, while in heterogeneous networks, strategies targeting the most vulnerable nodes are able to effectively mitigate the epidemic, also avoiding a deterioration in system activity and maintaining a low level of absenteeism. Interestingly, with targeted strategies both the minimum of population activity and the maximum of absenteeism anticipate the infection peak, which is effectively flattened and delayed, so that full operativeness is almost restored when the infection peak arrives. We also provide realistic estimates of the model parameters for influenza-like illness, thereby suggesting strategies for managing epidemics and absenteeism in realistic populations.
PB  - arXiv
PY  - 2024
ST  - Preserving system activity while controlling epidemic spreading in adaptive temporal networks
Y2  - 2025/05/05/21:54:31
DO  - 10.1103/physrevresearch.6.033159
ER  -


TY  - GEN
AU  - Delgado, L.G.
AU  - Derome, A.
AU  - Longpré, S.
AU  - Saucier, C.
AU  - Denault, J.-B.
TI  - Spatiotemporal regulation of the hepatocyte growth factor receptor MET activity by sorting nexins 1/2 in HCT116 colorectal cancer cells
AB  - Cumulative research findings support the idea that endocytic trafficking is crucial in regulating receptor signaling and associated diseases. Specifically, strong evidence points to the involvement of sorting nexins (SNXs), particularly SNX1 and SNX2, in the signaling and trafficking of the receptor tyrosine kinase (RTK) MET in colorectal cancer (CRC). Activation of hepatocyte growth factor (HGF) receptor MET is a key driver of CRC progression. In this study, we utilized human HCT116 CRC cells with SNX1 and SNX2 genes knocked out to demonstrate that their absence leads to a delay in MET entering early endosomes. This delay results in increased phosphorylation of both MET and AKT upon HGF stimulation, while ERK1/2 (extracellular signal-regulated kinases 1 and 2) phosphorylation remains unaffected. Despite these changes, HGF-induced cell proliferation, scattering, and migration remain similar between the parental and the SNX1/2 knockout cells. However, in the absence of SNX1 and SNX2, these cells exhibit increased resistance to TRAIL-induced apoptosis. This research underscores the intricate relationship between intracellular trafficking, receptor signaling, and cellular responses and demonstrates for the first time that the modulation of MET trafficking by SNX1 and SNX2 is critical for receptor signaling that may exacerbate the disease.
PB  - bioRxiv
PY  - 2024
ST  - Spatiotemporal regulation of the hepatocyte growth factor receptor MET activity by sorting nexins 1/2 in HCT116 colorectal cancer cells
Y2  - 2025/05/05/21:54:31
DO  - 10.1042/bsr20240182
ER  -


TY  - GEN
AU  - Zheng, R.
AU  - Liang, Y.
AU  - Wang, X.
AU  - Basu, K.S.
AU  - Huang, F.
TI  - Premier-TACO is a Few-Shot Policy Learner: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss
AB  - We present Premier-TACO, a multitask feature representation learning approach designed to improve few-shot policy learning efficiency in sequential decision-making tasks. Premier-TACO leverages a subset of multitask offline datasets for pretraining a general feature representation, which captures critical environmental dynamics and is fine-tuned using minimal expert demonstrations. It advances the temporal action contrastive learning (TACO) objective, known for state-of-the-art results in visual control tasks, by incorporating a novel negative example sampling strategy. This strategy is crucial in significantly boosting TACO’s computational efficiency, making large-scale multitask offline pretraining feasible. Our extensive empirical evaluation in a diverse set of continuous control benchmarks including Deepmind Control Suite, MetaWorld, and LIBERO demonstrate Premier-TACO’s effectiveness in pretraining visual representations, significantly enhancing few-shot imitation learning of novel tasks. Our code, pretraining data, as well as pretrained model checkpoints will be released at https://github.com/PremierTACO/premier-taco.
PB  - arXiv
PY  - 2024
ST  - Premier-TACO is a Few-Shot Policy Learner
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icme55011.2023.00393
ER  -


TY  - GEN
AU  - Asha, S.
AU  - Shanmugapriya, D.
TI  - Identification of Deep Variation in Temporal Behavior Using Multi-Level Feature Engineering Technique for User Authentication Mechanism
AB  - At present, authentication mechanism is major focus domain for scientific and business organizations relying on robust security compared to traditional authentication. An exemplary solution to improvise the performance of user authentication system against authentication attack is user behavior analysis using Multi-level Feature Engineering technique (MFE). MFE recognizes insightful variations in sequence of user behavior and squeeze identical variations to recognize intrusive activity among user behavior. This paper incorporates a novel multi-tier user authentication framework that contains two layers such as MFE and core behavior identification. In MFE, a novel solution known as Clonal-Kernel-PCA (CKP) is incorporated to condense the amplified high-dimensional features of preeminent user key typing behavior. The three level of CKP-based MFE technique includes feature subset selection, feature mapping and dimensionality reduction. In feature subset selection, Clonal Selection Algorithm is used to select the best sequence of user behaviour. However, these sequences of behaviours are converted into high-dimensional data by incorporating Kernel Mean Embedding in feature mapping for scrutinizing in-depth variations in user typical behavior. Lessen the data dimension from previous level using PCA-based dimensionality reduction technique. In layer2, the resulting sequence of behavior is trained for core behavior identification using deep learning techniques such as Convolutional neural network, Recurrent Neural Network, Generative Adversarial Network, and Deep Belief Network (DBN) to extract core key typing behavior from well performing technique among the four. Subsequently, the combination of CKP-DBN outperforms other DL techniques after performing MFE by obtaining least EER of 0.5% along FAR, FRR and accuracy of 0.029%, 3.75%, and 99.46%. Thus, it is proposed as a novel multi-tier user solution for user authentication mechanism. The competent performance of CKP-DBN model is compared against multiple datasets and other existing solutions.
PB  - SSRN
PY  - 2024
ST  - Identification of Deep Variation in Temporal Behavior Using Multi-Level Feature Engineering Technique for User Authentication Mechanism
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4720723
ER  -


TY  - GEN
AU  - Tao, S.
AU  - Rowe, F.
AU  - Shan, H.
TI  - Unveiling the influence of behavioural, built environment and socio-economic features on the spatial and temporal variability of bus use using explainable machine learning
AB  - Understanding the variability of people’s travel patterns is key to transport planning and policy-making. However, to what extent daily transit use displays geographic and temporal variabilities, and what are the contributing factors have not been fully addressed. Drawing on smart card data in Beijing, China, this study seeks to address these deficits by adopting new indices to capture the spatial and temporal variability of bus use during peak hours and investigate their associations with relevant contextual features. Using explainable machine learning, our findings reveal non-linear interaction between spatial and temporal variability and trip frequency. Furthermore, greater distance to the urban centres (>10 kilometres) is associated with increased spatial variability of bus use, while greater separation of trip origins and destinations from the subcentres reduces both spatial and temporal variability. Higher availability of bus routes is linked to higher spatial variability but lower temporal variability. Meanwhile, both lower and higher road density is associated with higher spatial variability of bus use especially in morning times. These findings indicate that different built environment features moderate the flexibility of travel time and locations. Implications are derived to inform more responsive and reliable operation and planning of transit systems.
PB  - arXiv
PY  - 2024
ST  - Unveiling the influence of behavioural, built environment and socio-economic features on the spatial and temporal variability of bus use using explainable machine learning
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.5038140
ER  -


TY  - GEN
AU  - DeCuzzi, N.L.
AU  - Oberbauer, D.P.
AU  - Chmiel, K.J.
AU  - Zeki, A.A.
AU  - Albeck, J.G.
TI  - Spatiotemporal Clusters of ERK Activity Coordinate Cytokine-induced Inflammatory Responses in Human Airway Epithelial Cells
AB  - RATIONALE: Spatially coordinated ERK signaling events (“SPREADs”) transmit radially from a central point to adjacent cells via secreted ligands for EGFR and other receptors. SPREADs maintain homeostasis in non-pulmonary epithelia, but it is unknown whether they play a role in the airway epithelium or are dysregulated in inflammatory disease. OBJECTIVES: (1) To characterize spatiotemporal ERK activity in response to proinflammatory ligands, and (2) to assess pharmacological and metabolic regulation of cytokine-mediated SPREADs. METHODS: SPREADs were measured by live-cell ERK biosensors in human bronchial epithelial cell lines (HBE1 and 16HBE) and primary human bronchial epithelial (pHBE) cells, in both submerged and biphasic Air-Liquid Interface (ALI) culture conditions (i.e., differentiated cells). Cells were exposed to pro-inflammatory cytokines relevant to asthma and chronic obstructive pulmonary disease (COPD), and to pharmacological treatments (gefitinib, tocilizumab, hydrocortisone) and metabolic modulators (insulin, 2-deoxyglucose) to probe the airway epithelial mechanisms of SPREADs. Phospho-STAT3 immunofluorescence was used to measure localized inflammatory responses to IL-6. RESULTS: Pro-inflammatory cytokines significantly increased the frequency of SPREADs. Notably, differentiated pHBE cells display increased SPREAD frequency that coincides with airway epithelial barrier breakdown. SPREADs correlate with IL-6 peptide secretion and localized pSTAT3. Hydrocortisone, inhibitors of receptor signaling, and suppression of metabolic function decreased SPREAD occurrence. CONCLUSIONS: Pro-inflammatory cytokines modulate SPREADs in human airway epithelial cells via both secreted EGFR and IL6R ligands. SPREADs correlate with changes in epithelial barrier permeability, implying a role for spatiotemporal ERK signaling in barrier homeostasis and dysfunction during inflammation. The involvement of SPREADs in airway inflammation suggests a novel signaling mechanism that could be exploited clinically to supplement corticosteroid treatment for asthma and COPD.
PB  - bioRxiv
PY  - 2024
ST  - Spatiotemporal Clusters of ERK Activity Coordinate Cytokine-induced Inflammatory Responses in Human Airway Epithelial Cells
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2024.02.03.578773
ER  -


TY  - GEN
AU  - Monnens, S.Q.
AU  - Peters, C.
AU  - Hesselink, L.W.
AU  - Smeets, K.
AU  - Englitz, B.
TI  - The Recurrent Temporal Restricted Boltzmann Machine Captures Neural Assembly Dynamics in Whole-Brain Activity
AB  - Animal behaviour alternates between stochastic exploration and goal-directed actions, which are generated by the underlying neural dynamics. Previously, we demonstrated that the compositional Restricted Boltzmann Machine (cRBM) can decompose whole-brain activity of larval zebrafish data at the neural level into a small number (∼100-200) of assemblies that can account for the stochasticity of the neural activity (van der Plas et al., eLife, 2023). Here we advance this representation by extending to a combined stochastic-dynamical representation to account for both aspects using the Recurrent Temporal RBM (RTRBM) and transfer-learning based on the cRBM estimate. We demonstrate that the functional advantage of the RTRBM is captured in the temporal weights on the hidden units, representing neural assemblies, for both simulated and experimental data. Our results show that the temporal expansion outperforms the stochastic-only cRBM in terms of generalisation error and achieves a more accurate representation of the moments in time. Lastly, we demonstrate that we can identify the original time-scale of assembly dynamics by estimating multiple RTRBMs at different temporal resolutions. Together, we propose that RTRBMs are a valuable tool for capturing the combined stochastic and time-predictive dynamics of large-scale data sets.
PB  - bioRxiv
PY  - 2024
ST  - The Recurrent Temporal Restricted Boltzmann Machine Captures Neural Assembly Dynamics in Whole-Brain Activity
Y2  - 2025/05/05/21:54:31
DO  - 10.7554/elife.98489.3
ER  -


TY  - GEN
AU  - Modiba, R.V.
AU  - Pirk, C.W.W.
AU  - Yusuf, A.A.
TI  - Temporal dynamics of scout release behaviour of termitophagous ponerine ant, Megaponera analis
AB  - Megaponera analis is an obligate termitophagous species that is endemic to sub-Saharan Africa. The species forage by sending out scouts to search for termites, and once located, the scouts return to the nest to recruit nest mates. Scouts face unpredictable environmental problems, including sudden flooding, temperature changes, wind speed changes and predation by natural enemies. As a monotypic genus, M. analis has shown strong adaptive resilience to such environmental pressures; hence, its populations remain viable despite being obligate predators. We have observed 519 scouting trips from 18 different colonies separated by a large spatial area over three years to find out how long it takes for the colony to replace a scout that has likely met mercurial environmental perturbations. Study areas were at Maremani, Musina, Masebe and D’nyala Nature Reserves in the Limpopo Province of South Africa. The results suggested that colonies exhibited a tendency to deploy scouts in response to the level of starvation rather than solely relying on the absence of information from scouts already outside. However, when feedback was lacking, colonies delayed sending out scouts and dispatched a single scout after a prolonged period. The mean scouting turnover for all 519 trips was 9.36 ± 0.64 (SD) minutes. When turnover was divided into sessions, the first scouting session had a mean of 7.77 ± 12.17 (SD) minutes, while the second had 10.34 ± 18.32 (SD)minutes. The minimum turnover was zero minutes, and the maximum was 124 minutes.
PB  - Research Square
PY  - 2024
ST  - Temporal dynamics of scout release behaviour of termitophagous ponerine ant, Megaponera analis
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3913073/v1
ER  -


TY  - GEN
AU  - Hsu, Y.
AU  - Chen, S.
AU  - Bai, M.R.
TI  - Spatial-Temporal Activity-Informed Diarization and Separation
AB  - A robust multichannel speaker diarization and separation system is proposed by exploiting the spatio-temporal activity of the speakers. The system is realized in a hybrid architecture that combines the array signal processing units and the deep learning units. For speaker diarization, a spatial coherence matrix across time frames is computed based on the whitened relative transfer functions (wRTFs) of the microphone array. This serves as a robust feature for subsequent machine learning without the need for prior knowledge of the array configuration. A computationally efficient Spatial Activity-driven Speaker Diarization network (SASDnet) is constructed to estimate the speaker activity directly from the spatial coherence matrix. For speaker separation, we propose the Global and Local Activity-driven Speaker Extraction network (GLASEnet) to separate speaker signals via speaker-specific global and local spatial activity functions. The local spatial activity functions depend on the coherence between the wRTFs of each time-frequency bin and the target speaker-dominant bins. The global spatial activity functions are computed from the global spatial coherence functions based on frequency-averaged local spatial activity functions. Experimental results have demonstrated superior speaker, diarization, counting, and separation performance achieved by the proposed system with low computational complexity compared to the pre-selected baselines.
PB  - arXiv
PY  - 2024
ST  - Spatial-Temporal Activity-Informed Diarization and Separation
Y2  - 2025/05/05/21:54:31
DO  - 10.1121/10.0035830
ER  -


TY  - GEN
AU  - Mashiat, T.
AU  - DiChristofano, A.
AU  - Fowler, P.J.
AU  - Das, S.
TI  - BEYOND EVICTION PREDICTION: LEVERAGING LOCAL SPATIOTEMPORAL PUBLIC RECORDS TO INFORM ACTION
AB  - There has been considerable recent interest in scoring properties on the basis of eviction risk. The success of methods for eviction prediction is typically evaluated using different measures of predictive accuracy. However, the underlying goal of such prediction is to direct appropriate assistance to households that may be at greater risk so they remain stably housed. Thus, we must ask the question of how useful such predictions are in targeting outreach efforts – informing action. In this paper, we investigate this question using a novel dataset that matches information on properties, evictions, and owners. We perform an eviction prediction task to produce risk scores and then use these risk scores to plan targeted outreach policies. We show that the risk scores are, in fact, useful, enabling a theoretical team of caseworkers to reach more eviction-prone properties in the same amount of time, compared to outreach policies that are either neighborhood-based or focus on buildings with a recent history of evictions. We also discuss the importance of neighborhood and ownership features in both risk prediction and targeted outreach.
PB  - arXiv
PY  - 2024
ST  - BEYOND EVICTION PREDICTION
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3630106.3658978
ER  -


TY  - GEN
AU  - Ameen, M.S.
AU  - Jacobs, J.
AU  - Schabus, M.
AU  - Hoedlmoser, K.
AU  - Donoghue, T.
TI  - The Temporal Dynamics of Aperiodic Neural Activity Track Changes in Sleep Architecture
AB  - The aperiodic (1/f-like) component of electrophysiological data - whereby power systematically decreases with increasing frequency, as quantified by the aperiodic exponent - has been shown to differentiate sleep stages. Earlier work, however, has typically focused on measuring the aperiodic exponent across a narrow frequency range. In this work, we sought to further investigate aperiodic activity during sleep by extending these analyses across broader frequency ranges and considering alternate model definitions. This included measuring ‘knees’ in the aperiodic component, which reflect bends in the power spectrum, indicating a change in the exponent. We also sought to evaluate the temporal dynamics of aperiodic activity during sleep. To do so, we analyzed data from two sources: intracranial EEG (iEEG) from 106 epilepsy patients and high-density EEG from 17 healthy individuals, and measured aperiodic activity, explicitly comparing different frequency ranges and model forms. In doing so, we find that fitting broadband aperiodic models and incorporating a 'knee' feature effectively captures sleep-stage-dependent differences in aperiodic activity as well as temporal dynamics that relate to sleep stage transitions and responses to external stimuli. In particular, the knee parameter shows stage-specific variation, suggesting an interpretation of varying timescales across sleep stages. These results demonstrate that examining broader frequency ranges with the more complex aperiodic models reveals novel insights and interpretations for understanding aperiodic neural activity during sleep.
PB  - bioRxiv
PY  - 2024
ST  - The Temporal Dynamics of Aperiodic Neural Activity Track Changes in Sleep Architecture
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2024.01.25.577204
ER  -


TY  - GEN
AU  - Kamiya, K.
AU  - Tamaki, T.
TI  - Multi-model learning by sequential reading of untrimmed videos for action recognition
AB  - We propose a new method for learning videos by aggregating multiple models by sequentially extracting video clips from untrimmed video. The proposed method reduces the correlation between clips by feeding clips to multiple models in turn and synchronizes these models through federated learning. Experimental results show that the proposed method improves the performance compared to the no synchronization.
PB  - arXiv
PY  - 2024
ST  - Multi-model learning by sequential reading of untrimmed videos for action recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/wacv.2017.29
ER  -


TY  - GEN
AU  - Nepomoceno, E.B.
AU  - Rodrigues, S.
AU  - de Melo, K.S.
AU  - Freestone, D.
AU  - Caetano, M.S.
TI  - Insular and Prelimbic Cortices Control Behavioral Accuracy and Precision in a Temporal Decision-Making Task in Rats
AB  - The anterior insular cortex (AIC) comprises a region of sensory integration. It appears to detect salient events in order to guide goal-directed behavior, code tracking errors, and estimate the passage of time. Temporal processing in the AIC may be instantiated by the integration of representations of interoception. Projections between the AIC and the medial prefrontal cortex (mPFC) - found both in rats and humans - also suggest a possible role for these structures in the integration of autonomic responses during ongoing behavior. Few studies, however, have investigated the role of AIC and mPFC in decision-making and time estimation tasks. Moreover, their findings are not consistent, so the relationship between temporal decision-making and those areas remains unclear. The present study employed bilateral inactivations to explore the role of AIC and prelimbic cortex (PL) in rats during a temporal decision-making task. In this task, two levers are available simultaneously (but only one is active), one predicting reinforcement after a short, and the other after a long-fixed interval. Optimal performance requires a switch from the short to the long lever after the short-fixed interval elapsed and no reinforcement was delivered. Switch behavior from the short to the long lever was dependent on AIC and PL. During AIC inactivation, switch latencies became more variable, while during PL inactivation switch latencies became both more variable and less accurate. These findings point to a dissociation between AIC and PL in temporal decision-making, suggesting that the AIC is important for temporal precision, and PL is important for both temporal accuracy and precision.
PB  - SSRN
PY  - 2024
ST  - Insular and Prelimbic Cortices Control Behavioral Accuracy and Precision in a Temporal Decision-Making Task in Rats
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.bbr.2024.114961
ER  -


TY  - GEN
AU  - Li, C.
AU  - Li, S.
AU  - Gao, Y.
AU  - Li, J.
AU  - Li, W.
TI  - Unsupervised Spatial-Temporal Feature Enrichment and Fidelity Preservation Network for Skeleton based Action Recognition
AB  - Unsupervised skeleton based action recognition has achieved remarkable progress recently. Existing unsupervised learning methods suffer from severe overfitting problem, and thus small networks are used, significantly reducing the representation capability. To address this problem, the overfitting mechanism behind the unsupervised learning for skeleton based action recognition is first investigated. It is observed that the skeleton is already a relatively high-level and low-dimension feature, but not in the same manifold as the features for action recognition. Simply applying the existing unsupervised learning method may tend to produce features that discriminate the different samples instead of action classes, resulting in the overfitting problem. To solve this problem, this paper presents an Unsupervised spatial-temporal Feature Enrichment and Fidelity Preservation framework (U-FEFP) to generate rich distributed features that contain all the information of the skeleton sequence. A spatial-temporal feature transformation subnetwork is developed using spatial-temporal graph convolutional network and graph convolutional gate recurrent unit network as the basic feature extraction network. The unsupervised Bootstrap Your Own Latent based learning is used to generate rich distributed features and the unsupervised pretext task based learning is used to preserve the information of the skeleton sequence. The two unsupervised learning ways are collaborated as U-FEFP to produce robust and discriminative representations. Experimental results on three widely used benchmarks, namely NTU-RGB+D-60, NTU-RGB+D-120 and PKU-MMD dataset, demonstrate that the proposed U-FEFP achieves the best performance compared with the state-of-the-art unsupervised learning methods. t-SNE illustrations further validate that U-FEFP can learn more discriminative features for unsupervised skeleton based action recognition.
PB  - arXiv
PY  - 2024
ST  - Unsupervised Spatial-Temporal Feature Enrichment and Fidelity Preservation Network for Skeleton based Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tcsvt.2025.3540292
ER  -


TY  - GEN
AU  - Helmi, A.M.
AU  - Al-Qaness, M.A.A.
AU  - Dahou, A.
AU  - Trouba, N.T.
AU  - Elaziz, M.A.
TI  - Tcn-Inception: Temporal Convolutional Network and Inception Modules for Sensor-Based Human Activity Recognition
AB  - HighlightsTCN-InceptionTemporal Convolutional Network and Inception modules for Sensor-based Human Activity RecognitionMohammed A. A. Al-qaness,Abdelghani Dahou,Nafissa Toureche Trouba,Mohamed Abd Elaziz,Ahmed M. Helmi• Employing the Inception modules with Temporal Convolutional Network and using residual connections for capturing complex temporal patterns in HAR signals.• Extraction of multi-scale features and capturing of extended temporal dependencies using the novel model• Consideration of training efficiency and model depth through the strategic use of residual connections and batch normalization.• Studying four different complex datasets, with variant set of activities, and carrying out an ablation study for more detailed analysis of the model.
PB  - SSRN
PY  - 2024
ST  - Tcn-Inception
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4705410
ER  -


TY  - GEN
AU  - Lin, J.
AU  - Li, J.
AU  - Gao, J.
AU  - Ma, W.
AU  - Liu, Y.
TI  - Jointly Modeling Spatio-Temporal Features of Tactile Signals for Action Classification
AB  - Tactile signals collected by wearable electronics are essential in modeling and understanding human behavior. One of the main applications of tactile signals is action classification, especially in healthcare and robotics. However, existing tactile classification methods fail to capture the spatial and temporal features of tactile signals simultaneously, which results in sub-optimal performances. In this paper, we design Spatio-Temporal Aware tactility Transformer (STAT) to utilize continuous tactile signals for action classification. We propose spatial and temporal embeddings along with a new temporal pretraining task in our model, which aims to enhance the transformer in modeling the spatio-temporal features of tactile signals. Specially, the designed temporal pretraining task is to differentiate the time order of tubelet inputs to model the temporal properties explicitly. Experimental results on a public action classification dataset demonstrate that our model outperforms state-of-the-art methods in all metrics.
PB  - arXiv
PY  - 2024
ST  - Jointly Modeling Spatio-Temporal Features of Tactile Signals for Action Classification
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v38i12.29288
ER  -


TY  - GEN
AU  - Akyüz, N.
AU  - Marien, H.
AU  - Driessen, J.M.A.
AU  - Stok, M.M.
AU  - Aarts, H.
TI  - Choice Effects on Temporal Binding of Action and Outcomes: Examining the Role of Outcome Focus and Measures of Time Interval Estimation
AB  - The ability to make one's own choices is vital to the experience of intentional behavior. Such agency experiences are reflected in the perceptual compression of time between actions and resulting outcomes. Whereas some studies show that choice limitations weaken temporal binding, other studies do not find such an effect. Reviewing the literature, we noted two factors that may moderate choice limitation effects on temporal binding: (a) the level of action representation, according to which individuals focus on their motor actions or the consequences they produce; and (b) the response mode of the time interval estimation measurement where participants report numbers or use a slider to indicate time intervals. In two experiments we found clear effects of choice limitation on temporal binding but no clear moderator role of the two factors. Interestingly, overall analyses showed that the choice limitation effect gradually vanishes over time. We briefly discuss the findings in the context of everyday autonomy restrictions in shaping conscious experiences of intentional action.
PB  - SSRN
PY  - 2024
ST  - Choice Effects on Temporal Binding of Action and Outcomes
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.actpsy.2024.104434
ER  -


TY  - GEN
AU  - Ojijo, M.O.
AU  - Ramotsoela, D.
AU  - Oginga, R.A.
TI  - Slice Admission Control in 5g Wireless Communication with Multi-Dimensional State Space and Distributed Action Space: A Sequential Twin Actor-Critic Approach
AB  - Network slicing represents a paradigm shift in the way resources are allocated for different 5G network functions through network function virtualization. This innovation aims to facilitate logical resource allocation, accommodating the anticipated surge in network resource needs. This will harness automatic processing, scheduling, and orchestration for efficient management. To meet the challenge of managing network resources under heavy demand, slice providers need to leverage both artificial intelligence and slice admission control strategies. While 5G network resources can be allocated to maintain a slice, the logical allocation and real-time network evaluation must be continuously examined and adjusted if network resilience is to be maintained. The complex task of leveraging slice admission control to maintain 5G network resilience has not been fully investigated. To tackle this problem, we propose a machine learning approach for slice admission control and resource allocation optimization so as to maintain network resilience.Machine learning algorithms offer a powerful tool for making robust and autonomous decisions, which are crucial for effective slice admission control. By intelligently allocating resources based on real-time demand and network conditions, these algorithms can help ensure long-term network resilience and achieve key objectives. While various machine learning algorithms hold promise for 5G resource management and admission control, reinforcement learning (RL) has emerged as a particularly exciting solution. Its ability to mimic human learning processes makes it a versatile solution, well-suited to tackle the complex challenges of network control. To fill this gap, we propose a new technique known as sequential twin actor critic (STAC). Simulations show that the STAC improves network resilience through enhanced admission probability and overall utility
PB  - SSRN
PY  - 2024
ST  - Slice Admission Control in 5g Wireless Communication with Multi-Dimensional State Space and Distributed Action Space
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4696985
ER  -


TY  - GEN
AU  - Zhou, Y.
AU  - Garcke, J.
TI  - Learning Crowd Behaviors in Navigation with Attention-based Spatial-Temporal Graphs
AB  - Safe and efficient navigation in dynamic environments shared with humans remains an open and challenging task for mobile robots. Previous works have shown the efficacy of using reinforcement learning frameworks to train policies for efficient navigation. However, their performance deteriorates when crowd configurations change, i.e. become larger or more complex. Thus, it is crucial to fully understand the complex, dynamic, and sophisticated interactions of the crowd resulting in proactive and foresighted behaviors for robot navigation. In this paper, a novel deep graph learning architecture based on attention mechanisms is proposed, which leverages the spatial-temporal graph to enhance robot navigation. We employ spatial graphs to capture the current spatial interactions, and through the integration with RNN, the temporal graphs utilize past trajectory information to infer the future intentions of each agent. The spatial-temporal graph reasoning ability allows the robot to better understand and interpret the relationships between agents over time and space, thereby making more informed decisions. Compared to previous state-of-the-art methods, our method demonstrates superior robustness in terms of safety, efficiency, and generalization in various challenging scenarios.
PB  - arXiv
PY  - 2024
ST  - Learning Crowd Behaviors in Navigation with Attention-based Spatial-Temporal Graphs
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icra57147.2024.10610279
ER  -


TY  - GEN
AU  - Sheng, J.
AU  - Yu, J.
AU  - Li, Z.
AU  - Li, A.
AU  - Ge, Y.
TI  - Self-Supervised Temporal Adaptive Learning for Weakly-Supervised Temporal Action Localization
AB  - Weakly-supervised temporal action localization (WTAL) identifies and localizes actions in untrimmed videos with only video-level labels. Most methods prioritize discriminative snippets, often neglecting of hard action snippets while focusing on class-specific background. Although recent methods have tackled this issue through temporal modeling, they overlook the local temporal structure of actions. To capture this structure effectively, we propose a novel self-supervised temporal adaptive learning (STAL) network for WTAL, composed of two corresponding components: self-supervised temporal learning (STL) network and adaptive learning unit (ALU). Specifically, STL constructs a self-supervised task by performing an erasure and reconstruction process. This pseudo-label-based method relies on a classification task to perceive continuous temporal information for action localization task. Note that self-supervised task highly depends on the classification task, ALU designs two adaptive learning strategies to address this drawback from two perspectives. In detail, a task-adaptive learning strategy is used to train the proposed tasks to the best for more reliable pseudo labels. Meanwhile, a score-adaptive learning strategy is designed to balance class activation and attention scores. Experiments on two classical datasets, namely, THUMOS14 and ActivityNet datasets, verify the effectiveness of our method.
PB  - SSRN
PY  - 2024
ST  - Self-Supervised Temporal Adaptive Learning for Weakly-Supervised Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4690808
ER  -


TY  - GEN
AU  - Lachapelle, S.
AU  - López, P.R.
AU  - Sharma, Y.
AU  - Lacoste, A.
AU  - Lacoste-Julien, S.
TI  - Nonparametric Partial Disentanglement via Mechanism Sparsity: Sparse Actions, Interventions and Sparse Temporal Dependencies
AB  - This work introduces a novel principle for disentanglement we call mechanism sparsity regularization, which applies when the latent factors of interest depend sparsely on observed auxiliary variables and/or past latent factors. We propose a representation learning method that induces disentanglement by simultaneously learning the latent factors and the sparse causal graphical model that explains them. We develop a nonparametric identifiability theory that formalizes this principle and shows that the latent factors can be recovered by regularizing the learned causal graph to be sparse. More precisely, we show identifiablity up to a novel equivalence relation we call consistency, which allows some latent factors to remain entangled (hence the term partial disentanglement). To describe the structure of this entanglement, we introduce the notions of entanglement graphs and graph preserving functions. We further provide a graphical criterion which guarantees complete disentanglement, that is identifiability up to permutations and element-wise transformations. We demonstrate the scope of the mechanism sparsity principle as well as the assumptions it relies on with several worked out examples. For instance, the framework shows how one can leverage multi-node interventions with unknown targets on the latent factors to disentangle them. We further draw connections between our nonparametric results and the now popular exponential family assumption. Lastly, we propose an estimation procedure based on variational autoencoders and a sparsity constraint and demonstrate it on various synthetic datasets. This work is meant to be a significantly extended version of Lachapelle et al. (2022).
PB  - arXiv
PY  - 2024
ST  - Nonparametric Partial Disentanglement via Mechanism Sparsity
Y2  - 2025/05/05/21:54:31
DO  - 10.3182/20090706-3-fr-2004.00264
ER  -


TY  - GEN
AU  - Song, H.
AU  - Zhang, J.
AU  - Wang, Z.L.
TI  - Consistent Temporal Behaviors Over a Non-Newtonian/Newtonian Two-Phase Flow
AB  - The temporal behaviors of non-Newtonian/Newtonian two-phase flows are closely tied to flow pattern formation mechanisms, influenced significantly by the non-Newtonian index, and exhibiting nonlinear rheological characteristics. The rheological model parameters are model-dependent, resulting in poor predictability of their spatio-temporal characteristics. This limitation hinders the development of a unified analytical approach and consistent results, confining researches to case-by-case studies. In this study, sets of digital microfluidics experiments, along with continuous-discrete phase-interchanging schemes, yielded 72 datasets under various non-Newtonian fluid solution configurations, micro-channel structures, and multiple Carreau-related models. Among these datasets, we identified consistent temporal behaviors featuring non-Newtonian flow patterns in digital microfluidics. In the context of significant model dependency and widespread uncertainty in model parameters for non-Newtonian fluid characterization, this study demonstrates that the consistent representation of the non-Newtonian behavior exit, and may be essentially independent of specific data or models. Which finding holds positive implications for understanding the flow behavior and morphology of non-Newtonian fluids.
PB  - SSRN
PY  - 2024
ST  - Consistent Temporal Behaviors Over a Non-Newtonian/Newtonian Two-Phase Flow
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4687446
ER  -


TY  - GEN
AU  - Zhong, T.
AU  - Jin, X.
AU  - Long, Z.
AU  - Zhan, Y.
AU  - Xue, X.
TI  - A Solar-Powered Self-Clean Gas-Sensing Neural Modulator for Behavioral Intervention with Spatiotemporal Flexibility
AB  - The amalgamation of high-sensitivity gas sensors with neural electrical stimulation technology stands as a compelling solution for eliciting behavioral responses in the presence of low-level hazardous gases. The utilization of battery-powered or coil-based configurations introduces spatiotemporal limitations, encompassing constrained experimental durations due to battery life or the necessity to confine subjects to specific locales for wireless power transmission. In this study, we present an advanced wireless neural modulator, meticulously engineered for behavioral intervention in unrestricted environments. This device integrates a solar-powered unit, gas sensor, brain stimulation signal management system, and a neurostimulation electrode. The solar energy utilization eliminates the necessity for battery replacements and local range limitations. The surface of the solar cells is treated with a superhydrophobic coating, enhancing performance in outdoor settings by mitigating surface contamination issues inherent in traditional solar cells. This innovation guarantees continuous, long-term, on-demand operational capacity, ensuring uninterrupted power supply to the system. Experimentation revealed extended locomotor activity in mice, induced by stimulation of the periaqueductal gray (PAG) brain region, for up to thirty minutes. These results underscore the significant potential of our technology for spatially and temporally unrestricted neuromodulatory applications, offering a viable strategy to alleviate the detrimental effects of hazardous gas exposure on physiological functions.
PB  - SSRN
PY  - 2024
ST  - A Solar-Powered Self-Clean Gas-Sensing Neural Modulator for Behavioral Intervention with Spatiotemporal Flexibility
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4672699
ER  -


TY  - GEN
AU  - Wang, L.
AU  - Eddine, S.N.
AU  - Brothers, T.
AU  - Jensen, O.
AU  - Kuperberg, G.
TI  - Predictive Coding explains the dynamics of neural activity within the left ventromedial temporal lobe during reading comprehension
AB  - Predictive coding has been proposed as a computational theory of brain function. However, there is little conclusive evidence linking it to language comprehension. We measured brain activity with magnetoencephalography during reading comprehension and simulated this activity using a predictive coding model of lexico-semantic processing. Between 300-500ms, the left ventromedial temporal lobe produced a larger N400 to unexpected than expected inputs. Our simulations showed that this could be explained by the production of lexico-semantic prediction error. To distinguish predictive coding from other frameworks that can also account for the univariate N400 effect, we carried out two multivariate analyses. First, we showed that between 300-500ms, expected words produced neural patterns that matched those that were pre-activated before the bottom-up input appeared. Our simulations showed that this could be explained by a reinstatement of item-specific lexico-semantic predictions within state units. Second, expected inputs produced consistent patterns that were distinct from those produced by unexpected inputs. Our simulations showed that this emerged from the differential activation of functionally distinct state and error units during the predictive coding algorithm. Together, these findings provide strong evidence that the left ventromedial temporal lobe employs predictive coding to infer meaning from orthographic form during reading comprehension.
PB  - Research Square
PY  - 2024
ST  - Predictive Coding explains the dynamics of neural activity within the left ventromedial temporal lobe during reading comprehension
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3641593/v1
ER  -


TY  - GEN
AU  - Ghai, U.
AU  - Chachra, P.
AU  - Fanibunda, S.E.
AU  - Singh, V.
AU  - Vaidya, V.A.
TI  - Fluoxetine treatment during postnatal and juvenile temporal epochs evokes diametrically opposing changes in anxio-depressive behaviors, gene expression, mitochondrial function, and neuronal architecture in the medial prefrontal cortex
AB  - Background The selective serotonin reuptake inhibitor, fluoxetine is reported to evoke distinct effects on anxio-depressive behaviors based on the temporal window of administration. Here, we systematically addressed the influence of postnatal or juvenile fluoxetine treatment on anxio-depressive behavior, gene expression, mitochondrial biogenesis, and neuronal cytoarchitecture in adulthood. Methods Rat pups received postnatal fluoxetine (PNFlx) or juvenile fluoxetine (JFlx) treatment from postnatal day 2 (P2)-P21 or P28-48 respectively, and were assessed for changes in anxio-depressive behaviors, global gene expression, mitochondrial biogenesis/function, and dendritic cytoarchitecture in the medial prefrontal cortex (mPFC) in adulthood. Results PNFlx evoked long-lasting increases in anxio-depressive behaviors, whereas JFlx elicited persistent decreases in anxio-depressive behavior, accompanied by differential and minimally overlapping transcriptional changes in the mPFC in adulthood. We noted opposing changes in mitochondrial function and dendritic cytoarchitecture in the mPFC of PNFlx and JFlx animals, with a decline observed following PNFlx and an increase in response to JFlx treatment. Furthermore, the enhanced despair-like behavior in the PNFlx cohort was reversed by adult-onset treatment with nicotinamide, a precursor for NAD+ which enhances mitochondrial bioenergetics. Conclusions Fluoxetine treatment in early postnatal versus juvenile windows evokes opposing and persistent effects on anxio-depressive behavior in adult male rats, along with differential effects on gene expression, mitochondrial function, and dendritic morphology in the mPFC. Collectively, our findings highlight two distinct temporal windows in which fluoxetine exposure programs starkly differing outcomes in mood-related behavior, and posits a role for altered bioenergetics within the mPFC in contributing to these distinctive changes in emotionality.
PB  - bioRxiv
PY  - 2023
ST  - Fluoxetine treatment during postnatal and juvenile temporal epochs evokes diametrically opposing changes in anxio-depressive behaviors, gene expression, mitochondrial function, and neuronal architecture in the medial prefrontal cortex
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.12.29.573529
ER  -


TY  - GEN
AU  - McInnes, A.N.
AU  - Smithers, B.
AU  - Lipp, O.V.
AU  - Rothwell, J.C.
AU  - Marinovic, W.
TI  - From Inhibition to Excitation and Why: The Role of Temporal Urgency in Modulating Corticospinal Activity
AB  - Previous research on movement preparation identified a period of corticospinal suppression about 200 ms prior to movement initiation. This phenomenon has been observed for different types of motor tasks typically used to investigate movement preparation (e.g., reaction time, self-initiated, and anticipatory actions). However, we recently discovered that this phenomenon is not observed when actions must be initiated under time pressure. In the present study, we investigated urgency effects on corticospinal suppression throughout the time course of an anticipatory timing task. Participants were required to perform timing actions under two urgency scenarios, high and low, and we applied single-pulse transcranial magnetic stimulation at different times during the time course of preparation. We analysed the time course of excitability under high and low scenarios in relation to expected and actual movement onset times. Our results confirmed our earlier findings that corticospinal suppression is not observed when participants perform actions under high urgency scenarios. In addition, we found no evidence that this preparatory suppression could be shifted in time to occur later under high urgency scenarios. Moreover, we found evidence that responses prepared under high urgency are more likely to be disrupted by external events (e.g., TMS pulses). These results suggest that preparatory suppression might be a strategy employed by the central nervous system to shield motor actions from interference of external events (e.g., loud sounds) when time allows. Given these data, we propose conceptual models that could account for the absence of preparatory suppression under time pressure to act.
PB  - bioRxiv
PY  - 2023
ST  - From Inhibition to Excitation and Why
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.12.27.573452
ER  -


TY  - GEN
AU  - Miyasaka, A.
AU  - Kanda, T.
AU  - Nonaka, N.
AU  - Sakurai, K.
AU  - Liu, Q.
TI  - Sequential Transitions of Male Sexual Behaviors Driven by Dual Acetylcholine-Dopamine Dynamics
AB  - The neural mechanisms regulating sequential transitions of male sexual behaviors, such as mounting, intromission, and ejaculation, in the brain remain unclear. Here, we report that dopamine (DA) and acetylcholine (ACh) dynamics in the ventral shell of the nucleus accumbens (vsNAc) closely aligns with serial transitions of sexual behaviors in male mice. During intromission, the vsNAc exhibits dual ACh-DA rhythms generated by reciprocal regulation between ACh and DA signaling via nicotinic acetylcholine (nAChR) and dopamine D2 (D2R) receptors. Knockdown of choline acetyl transferase (ChAT) or D2R in the vsNAc diminished the likelihood of intromission and ejaculation. Optogenetic manipulations reveal that DA signaling sustains male sexual behaviors by suppressing activities of D2RvsNAc neurons. Moreover, ACh signaling promotes the initiation of mounting and intromission, but also induces the intromission-to-ejaculation transition by triggering a slowdown of DA rhythm. Therefore, dual ACh-DA dynamics harmonize in the vsNAc to drive sequential transitions of male mating behaviors.
PB  - bioRxiv
PY  - 2023
ST  - Sequential Transitions of Male Sexual Behaviors Driven by Dual Acetylcholine-Dopamine Dynamics
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.neuron.2025.01.032
ER  -


TY  - GEN
AU  - Zhang, S.
AU  - Yin, J.
AU  - Dang, Y.
TI  - Spatial-Temporal Decoupling Contrastive Learning for Skeleton-based Human Action Recognition
AB  - Skeleton-based action recognition is a central task in human-computer interaction. However, most previous methods suffer from two issues: (i) semantic ambiguity arising from spatial-temporal information mixture; and (ii) overlooking the explicit exploitation of the latent data distributions (i.e., the intra-class variations and inter-class relations), thereby leading to sub-optimum solutions of the skeleton encoders. To mitigate this, we propose a spatial-temporal decoupling contrastive learning (STD-CL) framework to obtain discriminative and semantically distinct representations from the sequences, which can be incorporated into various previous skeleton encoders and can be removed when testing. Specifically, we decouple the global features into spatial-specific and temporal-specific features to reduce the spatial-temporal coupling of features. Furthermore, to explicitly exploit the latent data distributions, we employ the attentive features to contrastive learning, which models the cross-sequence semantic relations by pulling together the features from the positive pairs and pushing away the negative pairs. Extensive experiments show that STD-CL with four various skeleton encoders (HCN, 2S-AGCN, CTR-GCN, and Hyperformer) achieves solid improvements on NTU60, NTU120, and NW-UCLA benchmarks. The code will be released at https://github.com/LibertyZsj/STD-CL soon.
PB  - arXiv
PY  - 2023
ST  - Spatial-Temporal Decoupling Contrastive Learning for Skeleton-based Human Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/atc63255.2024.10908313
ER  -


TY  - GEN
AU  - Liu, Q.
AU  - Wang, Z.
AU  - Rong, S.
AU  - Li, J.
AU  - Zhang, Y.
TI  - Revisiting Foreground and Background Separation in Weakly-supervised Temporal Action Localization: A Clustering-based Approach
AB  - Weakly-supervised temporal action localization aims to localize action instances in videos with only video-level action labels. Existing methods mainly embrace a localization-by-classification pipeline that optimizes the snippet-level prediction with a video classification loss. However, this formulation suffers from the discrepancy between classification and detection, resulting in inaccurate separation of foreground and background (F&B) snippets. To alleviate this problem, we propose to explore the underlying structure among the snippets by resorting to unsupervised snippet clustering, rather than heavily relying on the video classification loss. Specifically, we propose a novel clustering-based F&B separation algorithm. It comprises two core components: a snippet clustering component that groups the snippets into multiple latent clusters and a cluster classification component that further classifies the cluster as foreground or background. As there are no ground-truth labels to train these two components, we introduce a unified self-labeling mechanism based on optimal transport to produce high-quality pseudo-labels that match several plausible prior distributions. This ensures that the cluster assignments of the snippets can be accurately associated with their F&B labels, thereby boosting the F&B separation. We evaluate our method on three benchmarks: THUMOS14, ActivityNet v1.2 and v1.3. Our method achieves promising performance on all three benchmarks while being significantly more lightweight than previous methods. Code is available at https://github.com/Qinying-Liu/CASE
PB  - arXiv
PY  - 2023
ST  - Revisiting Foreground and Background Separation in Weakly-supervised Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccv51070.2023.00957
ER  -


TY  - GEN
AU  - Mourchid, Y.
AU  - Slama, R.
TI  - MR-STGN: Multi-Residual Spatio Temporal Graph Network using Attention Fusion for Patient Action Assessment
AB  - Accurate assessment of patient actions plays a crucial role in healthcare as it contributes significantly to disease progression monitoring and treatment effectiveness. However, traditional approaches to assess patient actions often rely on manual observation and scoring, which are subjective and time-consuming. In this paper, we propose an automated approach for patient action assessment using a Multi-Residual Spatio Temporal Graph Network (MR-STGN) that incorporates both angular and positional 3D skeletons. The MR-STGN is specifically designed to capture the spatio-temporal dynamics of patient actions. It achieves this by integrating information from multiple residual layers, with each layer extracting features at distinct levels of abstraction. Furthermore, we integrate an attention fusion mechanism into the network, which facilitates the adaptive weighting of various features. This empowers the model to concentrate on the most pertinent aspects of the patient’s movements, offering precise instructions regarding specific body parts or movements that require attention. Ablation studies are conducted to analyze the impact of individual components within the proposed model. We evaluate our model on the UI-PRMD dataset demonstrating its performance in accurately predicting real-time patient action scores, surpassing state-of-the-art methods.
PB  - arXiv
PY  - 2023
ST  - MR-STGN
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/mmsp59012.2023.10337711
ER  -


TY  - GEN
AU  - Pujol-Perich, D.
AU  - Clapés, A.
AU  - Escalera, S.
TI  - SADA: Semantic adversarial unsupervised domain adaptation for Temporal Action Localization
AB  - Temporal Action Localization (TAL) is a complex task that poses relevant challenges, particularly when attempting to generalize on new – unseen – domains in real-world applications. These scenarios, despite realistic, are often neglected in the literature, exposing these solutions to important performance degradation. In this work, we tackle this issue by introducing, for the first time, an approach for Unsupervised Domain Adaptation (UDA) in sparse TAL, which we refer to as Semantic Adversarial unsupervised Domain Adaptation (SADA). Our contributions are threefold: (1) we pioneer the development of a domain adaptation model that operates on realistic sparse action detection benchmarks; (2) we tackle the limitations of global-distribution alignment techniques by introducing a novel adversarial loss that is sensitive to local class distributions, ensuring finer-grained adaptation; and (3) we present a novel set of benchmarks based on EpicKitchens100 and CharadesEgo, that evaluate multiple domain shifts in a comprehensive manner. Our experiments indicate that SADA improves the adaptation across domains when compared to fully supervised state-of-the-art and alternative UDA methods, attaining a performance boost of up to 6.14% mAP. The code is publicly available at https://github.com/davidpujol/SADA.
PB  - arXiv
PY  - 2023
ST  - SADA
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/wacv61041.2025.00895
ER  -


TY  - GEN
AU  - Zhou, F.
AU  - Jiang, Z.
AU  - Zhou, H.
AU  - Li, X.
TI  - SMC-NCA: Semantic-guided Multi-level Contrast for Semi-supervised Temporal Action Segmentation
AB  - Semi-supervised temporal action segmentation (SS-TAS) aims to perform frame-wise classification in long untrimmed videos, where only a fraction of videos in the training set have labels. Recent studies have shown the potential of contrastive learning in unsupervised representation learning using unlabelled data. However, learning the representation of each frame by unsupervised contrastive learning for action segmentation remains an open and challenging problem. In this paper, we propose a novel Semantic-guided Multi-level Contrast scheme with a Neighbourhood-Consistency-Aware unit (SMC-NCA) to extract strong frame-wise representations for SS-TAS. Specifically, for representation learning, SMC is first used to explore intra- and inter-information variations in a unified and contrastive way, based on action-specific semantic information and temporal information highlighting relations between actions. Then, the NCA module, which is responsible for enforcing spatial consistency between neighbourhoods centered at different frames to alleviate over-segmentation issues, works alongside SMC for semi-supervised learning (SSL). Our SMC outperforms the other state-of-the-art methods on three benchmarks, offering improvements of up to 17.8% and 12.6% in terms of Edit distance and accuracy, respectively. Additionally, the NCA unit results in significantly better segmentation performance in the presence of only 5% labelled videos. We also demonstrate the generalizability and effectiveness of the proposed method on our Parkinson’s Disease Mouse Behaviour (PDMB) dataset. The code is publicly available at https://github.com/FeixiangZhou/SMC-NCA.
PB  - arXiv
PY  - 2023
ST  - SMC-NCA
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tmm.2024.3452980
ER  -


TY  - GEN
AU  - Hamoud, I.
AU  - Jamal, M.A.
AU  - Srivastav, V.
AU  - Padoy, N.
AU  - Mohareri, O.
TI  - ST(OR)2: Spatio-Temporal Object Level Reasoning for Activity Recognition in the Operating Room
AB  - Surgical robotics holds much promise for improving patient safety and clinician experience in the Operating Room (OR). However, it also comes with new challenges, requiring strong team coordination and effective OR management. Automatic detection of surgical activities is a key requirement for developing AI-based intelligent tools to tackle these challenges. The current state-of-the-art surgical activity recognition methods however operate on image-based representations and depend on large-scale labeled datasets whose collection is time-consuming and resource-expensive. This work proposes a new sample-efficient and object-based approach for surgical activity recognition in the OR. Our method focuses on the geometric arrangements between clinicians and surgical devices, thus utilizing the significant object interaction dynamics in the OR. We conduct experiments in a low-data regime study for long video activity recognition. We also benchmark our method against other object-centric approaches on clip-level action classification and show superior performance.
PB  - arXiv
PY  - 2023
ST  - ST(OR)2
Y2  - 2025/05/05/21:54:31
DO  - 10.12733/jics20103294
ER  -


TY  - GEN
AU  - Misalkar, H.D.
AU  - Harshavardhanan, P.
TI  - Tdbamla: Temporal and Dynamic Behavior Analysis in Android Malware Using Lstm and Attention Mechanisms
AB  - The increasing ubiquity of Android devices has precipitated a concomitant surge in sophisticated malware attacks, posing critical challenges to cybersecurity infrastructures worldwide. Existing models have achieved significant strides in malware detection but often suffer from high false-positive rates, lower recall, and computational delays, thus demanding a more efficient and accurate system. Current techniques primarily rely on static features and simplistic learning models, leading to inadequate handling of temporal aspects and dynamic behaviors exhibited by advanced malware. These limitations compromise the detection of modern, evasive malware, and impede real-time analysis. This paper introduces a novel framework for Android malware detection that incorporates Temporal and Dynamic Behavior Analysis using Long Short-Term Memory (LSTM) networks and Attention Mechanisms. We further propose development of an efficient Grey Wolf Optimized (GWO) Decision Trees to find the most salient API call patterns associated with malwares. An Iterative Fuzzy Logic (IFL) layer is also deployed before classification to assess the "trustworthiness" of app metadata samples. For Ongoing Learning, we propose use of Deep Q-Networks (DQNs), which helps the reinforcement learning model to adapt more quickly to changes in the threat landscapes. By focusing on crucial system calls and behavioral characteristics in real-time, our model captures the nuanced temporal patterns often exhibited by advanced malwares. Empirical evaluations demonstrate remarkable improvements across multiple performance metrics. Compared to existing models, our approach enhances the precision of malware identification by 8.5%, accuracy by 5.5%, and recall by 4.9%, while also achieving an 8.3% improvement in the Area Under the Receiver Operating Characteristic Curve (AUC), with higher specificity and a 4.5% reduction in identification delay. In malware pre-emption tasks, our model outperforms by improving precision by 4.3%, accuracy by 3.9%, recall by 4.9%, AUC by 3.5%, and increasing specificity by 2.9%. These gains make our framework highly applicable for real-time detection systems, cloud-based security solutions, and threat intelligence services, thereby contributing to a safer Android ecosystem.
PB  - SSRN
PY  - 2023
ST  - Tdbamla
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4665819
ER  -


TY  - GEN
AU  - Elsayed, S.
AU  - Rashed, A.
AU  - Schmidt-Thieme, L.
TI  - Context-Aware Sequential Model for Multi-Behaviour Recommendation
AB  - Sequential recommendation models have recently become a crucial component for next-item recommendation tasks in various online platforms due to their unrivaled ability to capture complex sequential patterns in historical user interactions. Nevertheless, many recent sequential models mainly focus on modeling a single behavior, representing the platform’s target relation, e.g., purchase. While on the other hand, other implicit user interactions, such as click information, and add-to-favorite, can provide deeper insights into the users’ sequential behavior and allows better modeling of the users’ profiles. Recent work in multi-behavioral models has been trying to partially address this problem by focusing on utilizing graph-based approaches for modeling multi-behavior data as heterogeneous graphs. However, many fail or neglect to capture the sequential patterns simultaneously. While few recent time-aware multi-behavioral methods try to address both aspects at the same time, they still consider auxiliary behaviors of the same importance to the learning process, which might not be the case in many scenarios. In this work, we propose a Context-Aware Sequential Model (CASM) for multi-behavioral recommendations that leverages the advantages of sequential models and can support an arbitrary number of behaviors seamlessly. Specifically, context-aware multi-head self-attention layers are employed to capture the multi-behavior dependencies between the heterogeneous historical interactions. Furthermore, we utilize a weighted binary cross-entropy loss to weigh the different behaviors differently through the learning process of the model to allow more precise control of their contributions based on the target recommendation scenario. Experimental results on four real-world datasets show that the proposed model significantly outperforms multiple multi-behavioral and sequential recommendation state-of-the-art approaches.
PB  - arXiv
PY  - 2023
ST  - Context-Aware Sequential Model for Multi-Behaviour Recommendation
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.ins.2019.09.007
ER  -


TY  - GEN
AU  - Tang, H.
AU  - Jiang, H.
AU  - Xu, M.
AU  - Zhu, J.
AU  - Nie, L.
TI  - Unsupervised Temporal Action Localization via Self-paced Incremental Learning
AB  - Recently, temporal action localization (TAL) has garnered significant interest in information retrieval community. However, existing supervised/weakly supervised methods are heavily dependent on extensive labeled temporal boundaries and action categories, which is labor-intensive and time-consuming. Although some unsupervised methods have utilized the “iteratively clustering and localization” paradigm for TAL, they still suffer from two pivotal impediments: 1) unsatisfactory video clustering confidence, and 2) unreliable video pseudolabels for model training. To address these limitations, we present a novel self-paced incremental learning model to enhance clustering and localization training simultaneously, thereby facilitating more effective unsupervised TAL. Concretely, we improve the clustering confidence through exploring the contextual feature-robust visual information. Thereafter, we design two (constant- and variable- speed) incremental instance learning strategies for easy-to-hard model training, thus ensuring the reliability of these video pseudolabels and further improving overall localization performance. Extensive experiments on two public datasets have substantiated the superiority of our model over several state-of-the-art competitors.
PB  - arXiv
PY  - 2023
ST  - Unsupervised Temporal Action Localization via Self-paced Incremental Learning
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.ins.2023.02.047
ER  -


TY  - GEN
AU  - Mamun, M.
AU  - Buffett, S.
TI  - TapTree: Process-Tree based Host Behavior Modeling and Threat Detection Framework via Sequential Pattern Mining
AB  - Host behaviour modelling is widely deployed in today’s corporate environments to aid in the detection and analysis of cyber attacks. Audit logs containing system-level events are frequently used for behavior modeling as they can provide detailed insight into cyber-threat occurrences. However, mapping low-level system events in audit logs to high-level behaviors has been a major challenge in identifying host contextual behavior for the purpose of detecting potential cyber threats. Relying on domain expert knowledge may limit its practical implementation. This paper presents TapTree, an automated process-tree based technique to extract host behavior by compiling system events’ semantic information. After extracting behaviors as system generated process trees, TapTree integrates event semantics as a representation of behaviors. To further reduce pattern matching workloads for the analyst, TapTree aggregates semantically equivalent patterns and optimizes representative behaviors. In our evaluation against a recent benchmark audit log dataset (DARPA OpTC), TapTree employs tree pattern queries and sequential pattern mining techniques to deduce the semantics of connected system events, achieving high accuracy for behavior abstraction and then Advanced Persistent Threat (APT) attack detection. Moreover, we illustrate how to update the baseline model gradually online, allowing it to adapt to new log patterns over time.
PB  - arXiv
PY  - 2023
ST  - TapTree
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-15777-6_30
ER  -


TY  - GEN
AU  - Li, Y.-H.
AU  - Li, Z.-Y.
AU  - Gao, S.
AU  - Hou, Q.
AU  - Cheng, M.-M.
TI  - A Decoupled Spatio-Temporal Framework for Skeleton-based Action Segmentation
AB  - —Effectively modeling discriminative spatio-temporal information is essential for segmenting activities in long action sequences. However, we observe that existing methods are limited in weak spatio-temporal modeling capability due to two forms of decoupled modeling: (i) cascaded interaction couples spatial and temporal modeling, which over-smooths motion modeling over the long sequence, and (ii) joint-shared temporal modeling adopts shared weights to model each joint, ignoring the distinct motion patterns of different joints. In this paper, we present a Decoupled Spatio-Temporal Framework (DeST) to address the above issues. Firstly, we decouple the cascaded spatio-temporal interaction to avoid stacking multiple spatio-temporal blocks, while achieving sufficient spatio-temporal interaction. Specifically, DeST performs once unified spatial modeling and divides the spatial features into different groups of sub-features, which then adaptively interact with temporal features from different layers. Since the different sub-features contain distinct spatial semantics, the model could learn better interaction patterns at each layer. Meanwhile, inspired by the fact that different joints move at different speeds, we propose joint-decoupled temporal modeling, which employs independent trainable weights to capture distinctive temporal features of each joint. On four large-scale benchmarks of different scenes, DeST significantly outperforms current state-of-the-art methods with less computational complexity.
PB  - arXiv
PY  - 2023
ST  - A Decoupled Spatio-Temporal Framework for Skeleton-based Action Segmentation
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s00371-023-03132-1
ER  -


TY  - GEN
AU  - Chen, C.
AU  - Chen, J.
AU  - Yang, W.
AU  - Ertl, T.
AU  - Liu, S.
TI  - Enhancing Single-Frame Supervision for Better Temporal Action Localization
AB  - Temporal action localization aims to identify the boundaries and categories of actions in videos, such as scoring a goal in a football match. Single-frame supervision has emerged as a labor-efficient way to train action localizers as it requires only one annotated frame per action. However, it often suffers from poor performance due to the lack of precise boundary annotations. To address this issue, we propose a visual analysis method that aligns similar actions and then propagates a few user-provided annotations (e.g., boundaries, category labels) to similar actions via the generated alignments. Our method models the alignment between actions as a heaviest path problem and the annotation propagation as a quadratic optimization problem. As the automatically generated alignments may not accurately match the associated actions and could produce inaccurate localization results, we develop a storyline visualization to explain the localization results of actions and their alignments. This visualization facilitates users in correcting wrong localization results and misalignments. The corrections are then used to improve the localization results of other actions. The effectiveness of our method in improving localization performance is demonstrated through quantitative evaluation and a case study.
PB  - arXiv
PY  - 2023
ST  - Enhancing Single-Frame Supervision for Better Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tvcg.2024.3388521
ER  -


TY  - GEN
AU  - Gong, D.
AU  - Lee, J.
AU  - Jung, D.
AU  - Kwak, S.
AU  - Cho, M.
TI  - Activity Grammars for Temporal Action Segmentation
AB  - Sequence prediction on temporal data requires the ability to understand compositional structures of multi-level semantics beyond individual and contextual properties. The task of temporal action segmentation, which aims at translating an untrimmed activity video into a sequence of action segments, remains challenging for this reason. This paper addresses the problem by introducing an effective activity grammar to guide neural predictions for temporal action segmentation. We propose a novel grammar induction algorithm that extracts a powerful context-free grammar from action sequence data. We also develop an efficient generalized parser that transforms frame-level probability distributions into a reliable sequence of actions according to the induced grammar with recursive rules. Our approach can be combined with any neural network for temporal action segmentation to enhance the sequence prediction and discover its compositional structure. Experimental results demonstrate that our method significantly improves temporal action segmentation in terms of both performance and interpretability on two standard benchmarks, Breakfast and 50 Salads.
PB  - arXiv
PY  - 2023
ST  - Activity Grammars for Temporal Action Segmentation
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tmm.2022.3231099
ER  -


TY  - GEN
AU  - Quattrocchi, C.
AU  - Furnari, A.
AU  - Mauro, D.D.
AU  - Giuffrida, M.V.
AU  - Farinella, G.M.
TI  - Synchronization is All You Need: Exocentric-to-Egocentric Transfer for Temporal Action Segmentation with Unlabeled Synchronized Video Pairs
AB  - We consider the problem of transferring a temporal action segmentation system initially designed for exocentric (fixed) cameras to an egocentric scenario, where wearable cameras capture video data. The conventional supervised approach requires the collection and labeling of a new set of egocentric videos to adapt the model, which is costly and time-consuming. Instead, we propose a novel methodology which performs the adaptation leveraging existing labeled exocentric videos and a new set of unlabeled, synchronized exocentric-egocentric video pairs, for which temporal action segmentation annotations do not need to be collected. We implement the proposed methodology with an approach based on knowledge distillation, which we investigate both at the feature and Temporal Action Segmentation model level. Experiments on Assembly101 and EgoExo4D demonstrate the effectiveness of the proposed method against classic unsupervised domain adaptation and temporal alignment approaches. Without bells and whistles, our best model performs on par with supervised approaches trained on labeled egocentric data, without ever seeing a single egocentric label, achieving a +15.99 improvement in the edit score (28.59 vs 12.60) on the Assembly101 dataset compared to a baseline model trained solely on exocentric data. In similar settings, our method also improves edit score by +3.32 on the challenging EgoExo4D benchmark. Code is available here: https://github.com/fpv-iplab/synchronization-is-all-you-need.
PB  - arXiv
PY  - 2023
ST  - Synchronization is All You Need
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-73220-1_15
ER  -


TY  - GEN
AU  - Pranjal, P.
AU  - Kumar, D.
AU  - Soni, A.
AU  - Chatterjee, R.S.
TI  - Investigating Spatio-temporal behaviour for Groundwater in NorthWest India: A Deep Learning Approach
AB  - Sustainability and conservation of natural resources require amalgamation of the novel approaches with the current usages and weather conditions. Groundwater resources is one of the natural resources which is varying annually and requires regular attention for the prediction. Our proposed deep learning (DL) approach, namely Convolution Neural Network-Long short term memory (ConvLSTM) has been implemented for the groundwater level (GWL) prediction model. The model is designed based on U-Net framework with up-sampling and down-sampling modules and also induces non-linearity using the ReLU activation function. Each module in the LSTM unit is responsible for pattern recognition based on the temporal information of GWL. The assessment of the groundwater in North-West India (NWI) has been carried out using several fundamental factors such as precipitation, soil moisture, evapotranspiration and satellite-based groundwater storage. In addition, in-situ groundwater has been used to get groundwater fluctuation scenarios (i.e., categorised into four cycles PrePre, PrePost, PostPre, and PostPost) w.r.t monsoon season to understand the difference (Δh) in GWL. The proposed model has been tested with other DL frameworks such as; Artificial neural network (ANN) and Convolution neural network (CNN). The model has been trained using the stochastic gradient method to optimise the internal parameter and validated using several geo-locations information of NWI, where ConvLSTM outperformed compared to the benchmark method. The proposed model has shown consistent least error in terms of root mean square root (RMSE) and mean square error (MAE) for the year 2014-17 with an overall score of 0.0957 and 0.0520, respectively.
PB  - Research Square
PY  - 2023
ST  - Investigating Spatio-temporal behaviour for Groundwater in NorthWest India
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3638857/v1
ER  -


TY  - GEN
AU  - Huu, B.L.N.
AU  - Matsui, T.
TI  - STEP CATFormer: Spatial-Temporal Effective Body-Part Cross Attention Transformer for Skeleton-based Action Recognition
AB  - Graph convolutional networks (GCNs) have been widely used and achieved remarkable results in skeleton-based action recognition. We think the key to skeleton-based action recognition is a skeleton hanging in frames, so we focus on how the Graph Convolutional Convolution networks learn different topologies and effectively aggregate joint features in the global temporal and local temporal. In this work, we propose three Channel-wise Tolopogy Graph Convolution based on Channel-wise Topology Refinement Graph Convolution (CTR-GCN). Combining CTR-GCN with two joint cross-attention modules can capture the upper-lower body part and hand-foot relationship skeleton features. After that, to capture features of human skeletons changing in frames we design the Temporal Attention Transformers to extract skeletons effectively. The Temporal Attention Transformers can learn the temporal features of human skeleton sequences. Finally, we fuse the temporal features output scale with MLP and classification. We develop a powerful graph convolutional network named Spatial Temporal Effective Body-part Cross Attention Transformer which notably high-performance on the NTU RGB+D, NTU RGB+D 120 datasets. Our code and models are available at https://github.com/maclong01/STEP-CATFormer
PB  - arXiv
PY  - 2023
ST  - STEP CATFormer
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3368402/v1
ER  -


TY  - GEN
AU  - Kilteni, K.
AU  - Ehrsson, H.H.
TI  - Dynamic changes in somatosensory and cerebellar activity mediate temporal recalibration of self-touch
AB  - An organism's ability to accurately anticipate the sensations caused by its own actions is crucial for a wide range of behavioral, perceptual, and cognitive functions. Notably, the sensorimotor expectations produced when touching one's own body attenuate such sensations, making them feel weaker and less ticklish and rendering them easily distinguishable from potentially harmful touches of external origin. How the brain learns and keeps these action-related sensory expectations updated is unclear. We employed psychophysics and functional magnetic resonance imaging to pinpoint the behavioral and neural substrates of dynamic recalibration of expected temporal delays in self-touch. Psychophysical results revealed that self-touches were less attenuated after systematic exposure to delayed self-generated touches, while responses in the contralateral somatosensory cortex that normally distinguish between delayed and nondelayed self-generated touches became indistinguishable. During the exposure, the ipsilateral anterior cerebellum showed increased activity, supporting its proposed role in recalibrating sensorimotor predictions. Moreover, responses in the cingulate areas gradually increased, suggesting that as delay adaptation progresses, the nondelayed self-touches trigger activity related to cognitive conflict. Together, our results show that sensorimotor predictions in the simplest act of touching one's own body are upheld by a sophisticated and flexible neural mechanism that maintains them accurate in time.
PB  - bioRxiv
PY  - 2023
ST  - Dynamic changes in somatosensory and cerebellar activity mediate temporal recalibration of self-touch
Y2  - 2025/05/05/21:54:31
DO  - 10.1038/s42003-024-06188-4
ER  -


TY  - GEN
AU  - Lee, Y.W.
AU  - Lee, J.
AU  - Yoon, C.W.
AU  - Chung, O.-S.
AU  - Lee, J.K.
TI  - Analysis of Activity, Temporal Partitioning, and Interactions among Mesopredators (Asian badgers, Raccoon dogs, and Wild Boars) in South Korea
AB  - Mesopredators like Asian badgers, Raccoon dogs, and Wild boars in South Korea, where apex predators are extinct, share habitats, offering a unique environment to study their interactions and activity patterns. This study deployed 130 motion-sensor cameras across 11 regions to examine their temporal activity. Asian badgers and Raccoon dogs displayed bimodal activity patterns; Asian badgers were active before midnight (21:00–24:00), while Raccoon dogs were active after midnight (02:00–05:00). Wild boars exhibited a unimodal pattern, mainly active before midnight (20:00–22:00). Seasonal variations affected the activity times of Raccoon dogs and Wild boars but not Asian badgers. The presence of Wild boars influenced the activity of Asian badgers and Raccoon dogs. Specifically, Raccoon dogs' activity varied significantly depending on whether Wild boars or Yellow-throated martens were present. Asian badgers' activity also changed in response to Wild boars. However, Wild boars appeared less influenced by the presence of other mesopredators. This study highlights the complex dynamics of predator interactions in shared habitats.
PB  - Research Square
PY  - 2023
ST  - Analysis of Activity, Temporal Partitioning, and Interactions among Mesopredators (Asian badgers, Raccoon dogs, and Wild Boars) in South Korea
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3651966/v1
ER  -


TY  - GEN
AU  - Pei, W.
AU  - Tan, Q.
AU  - Lu, G.
AU  - Tian, J.
TI  - D2ST-Adapter: Disentangled-and-Deformable Spatio-Temporal Adapter for Few-shot Action Recognition
AB  - Adapting large pre-trained image models to few-shot action recognition has proven to be an effective and efficient strategy for learning robust feature extractors, which is essential for few-shot learning. Typical fine-tuning based adaptation paradigm is prone to overfitting in the few-shot learning scenarios and offers little modeling flexibility for learning temporal features in video data. In this work we present the Disentangled-and-Deformable Spatio-Temporal Adapter (D2ST-Adapter), which is a novel adapter tuning framework well-suited for few-shot action recognition due to lightweight design and low parameter-learning overhead. It is designed in a dual-pathway architecture to encode spatial and temporal features in a disentangled manner. In particular, we devise the anisotropic Deformable Spatio-Temporal Attention module as the core component of D2ST-Adapter, which can be tailored with anisotropic sampling densities along spatial and temporal domains to learn spatial and temporal features specifically in corresponding pathways, allowing our D2ST-Adapter to encode features in a global view in 3D spatio-temporal space while maintaining a lightweight design. Extensive experiments with instantiations of our method on both pre-trained ResNet and ViT demonstrate the superiority of our method over state-of-the-art methods for few-shot action recognition. Our method is particularly well-suited to challenging scenarios where temporal dynamics are critical for action recognition.
PB  - arXiv
PY  - 2023
ST  - D2ST-Adapter
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tcsvt.2025.3541960
ER  -


TY  - GEN
AU  - Song, H.
AU  - Zhang, J.
AU  - Wang, Z.
TI  - Consistent temporal behaviors over a non-Newtonian/Newtonian two-phase flow
AB  - The temporal behaviors of non-Newtonian/Newtonian two-phase flows are closely tied to flow pattern formation mechanisms, influenced significantly by the non-Newtonian index, and exhibiting nonlinear rheological characteristics. The rheological model parameters are model-dependent, resulting in poor predictability of their spatio-temporal characteristics. This limitation hinders the development of a unified analytical approach and consistent results, confining researches to case-by-case studies. In this study, sets of digital microfluidics experiments, along with continuous-discrete phase-interchanging schemes, yielded 72 datasets under various non-Newtonian fluid solution configurations, microchannel structures, and multiple Carreau-related models. Among these datasets, we identified consistent temporal behaviors featuring non-Newtonian flow patterns in digital microfluidics. In the context of significant model dependency and widespread uncertainty in model parameters for non-Newtonian fluid characterization, this study demonstrates that the consistent representation of the non-Newtonian behavior exit, and may be essentially independent of specific data or models. Which finding holds positive implications for understanding the flow behavior and morphology of non-Newtonian fluids.
PB  - arXiv
PY  - 2023
ST  - Consistent temporal behaviors over a non-Newtonian/Newtonian two-phase flow
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4687446
ER  -


TY  - GEN
AU  - Marieshwari, B.N.
AU  - Keerthana, C.B.
AU  - Janarthanan, S.
TI  - Temporal variation in phenoloxidase enzyme activity in haemocyte lysate supernatant (HLS) of grubs of Oryctes rhinoceros
AB  - The phenoloxidase (PO) cascade is a significant component of the innate immune mechanism of insects, involved in melanization, sclerotization, wound healing and melanotic encapsulation. In this paper, we examined the nature of PO activity released from the haemocytes of the grub of Oryctes rhinoceros, by preparing haemocyte lysate supernatant (HLS) without any anticoagulant, as most of it contains EDTA which is reported as one of the inhibitors of the PO enzyme. The empirical analysis performed using different phenolic substrates in order to characterize the appropriate substrate of HLS PO revealed that phenoloxidase from HLS maximally oxidized an o-diphenolic substrate, dopamine followed by a monophenolic substrate tyramine, indicating that PO from HLS belongs to tyrosinase like phenoloxidase type as they possess the properties of catalyzing both monophenolic and diphenolic substrates. Furthermore, maximum PO activity was observed at 5 min of incubation for all the tested substrates at various concentrations. The ideal pH and temperature for HLS PO activity developed using 10 mM dopamine was observed to be 9 and 20°C, respectively. Among the various chemical inhibitors tested, HLS PO activity was significantly reduced in the case of PTU followed by EDTA. Further, PO activity in HLS was found to be increased in vitro by exogenous proteases viz., trypsin and a-chymotrypsin and detergents such as SDS and CPC. In addition, the selective response of HLS PO to activation by different types of bacterial LPS tested, suggested that haemocytes PO serve an important function in non-self recognition and host immune reactions.
PB  - Research Square
PY  - 2023
ST  - Temporal variation in phenoloxidase enzyme activity in haemocyte lysate supernatant (HLS) of grubs of Oryctes rhinoceros
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3649794/v1
ER  -


TY  - GEN
AU  - Wen, Y.
AU  - Pan, H.
AU  - Ohkawa, T.
AU  - Komura, T.
AU  - Wang, W.
TI  - Generative Hierarchical Temporal Transformer for Hand Pose and Action Modeling
AB  - We present a novel unified framework that concurrently tackles recognition and future prediction for human hand pose and action modeling. Previous works generally provide isolated solutions for either recognition or prediction, which not only increases the complexity of integration in practical applications, but more importantly, cannot exploit the synergy of both sides and suffer suboptimal performances in their respective domains. To address this problem, we propose a generative Transformer VAE architecture to model hand pose and action, where the encoder and decoder capture recognition and prediction respectively, and their connection through the VAE bottleneck mandates the learning of consistent hand motion from the past to the future and vice versa. Furthermore, to faithfully model the semantic dependency and different temporal granularity of hand pose and action, we decompose the framework into two cascaded VAE blocks: the first and latter blocks respectively model the short-span poses and long-span action, and are connected by a mid-level feature representing a sub-second series of hand poses. This decomposition into block cascades facilitates capturing both short-term and long-term temporal regularity in pose and action modeling, and enables training two blocks separately to fully utilize datasets with annotations of different temporal granularities. We train and evaluate our framework across multiple datasets; results show that our joint modeling of recognition and prediction improves over isolated solutions, and that our semantic and temporal hierarchy facilitates long-term pose and action modeling.
PB  - arXiv
PY  - 2023
ST  - Generative Hierarchical Temporal Transformer for Hand Pose and Action Modeling
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52729.2023.02035
ER  -


TY  - GEN
AU  - Zhang, M.
AU  - Shen, Q.
AU  - Zhao, Z.
AU  - Wang, S.
AU  - Huang, G.Q.
TI  - Risk-Averse Behavior and Incentive Policies: A New Perspective on Spatial-Temporal Traceability Supervision in Construction Logistics Supply Chains
AB  - Spatial-temporal decision support systems (STDS) serve as a crucial strategy for enhancing operational governance and mitigating risk within construction logistics. However, construction supply chains involve many opaque stakeholders, hindering whole industry compliance monitoring. This research formulates a tripartite evolutionary game model that scrutinizes the strategic interactions among government regulators, carriers, and contractors, thereby offering insights into the collaborative supervision outcomes shaped by these stakeholder engagements. Government regulators choose intelligent supervision incentives versus regular oversight. Carriers decide whether to invest in STDS or not. Contractors cooperate by enrolling or declining STDS. As key STDS investors, carriers’ decisions are investigated under varying risk preferences in an extension model. This examines how government incentives influence long-term intelligent supervision and risk aversion behavior emergence to improve safety and quality across construction supply chains. Our findings indicate that increased adverse events motivate government regulators to adopt STDS incentives for oversight, though carriers and contractors are not necessarily prompted to implement STDS themselves. Escalating risk aversion reduces carrier STDS adoption likelihood as they maintain basic services. Carriers perceiving contractor free riding as unfair competition also demotivates STDS rollout. Although larger subsidies initially raise STDS implementation probability, carriers become unwilling to adopt STDS over time even with greater subsidies. In summary, adverse events drive regulator but not necessarily carrier and contractor STDS adoption, while risk aversion, perceived fairness and changing subsidy effectiveness over time shape carrier decisions.
PB  - SSRN
PY  - 2023
ST  - Risk-Averse Behavior and Incentive Policies
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.cie.2024.110256
ER  -


TY  - GEN
AU  - Chang, J.-W.
AU  - Chen, M.-H.
AU  - Ma, H.-S.
AU  - Liu, H.-L.
TI  - Human Movement Science-Informed Multi-Task Spatio Temporal Graph Convolutional Networks for Fitness Action Recognition and Evaluation
AB  - In recent years, with the rise of health consciousness, people's demand for fitness has been steadily increasing. Improper exercise movements can easily lead to injuries while working out. Therefore, if we could utilize automated human action recognition to continuously monitor users' movements during exercise, it would effectively help prevent incorrect movements. Human action recognition has found extensive applications in multimedia computing, and we aim to leverage it to assess users' fitness conditions. Some existing action motion recognition models rely heavily on CNNs for image processing, which introduces unnecessary noise from the background and surroundings. To address this issue, some researches focus on the skeleton structure information for the action recognition and apply the graph convolutional network to enhance the performance. Generally, the proposed action human recognition method only recognize the type of actions. In this study, we are trying to design an automated fitness adviser system to improve the exercise experience in fitness gyms. The adviser system not only detect the correctness of fitness action but also evaluate the action quality simultaneously. To address this issue, we involve the human movement science such as Five Primary Kinetic Chains(5PKC) to provide the more accurate relations of human skeleton and muscles. Five Primary Kinetic Chains(5PKC) defines the primary physiological principle in human movement. The proposed model adopts Five Primary Kinetic Chains(5PKC) as a partitioning strategy to explore the skeleton partition and designs a multi-task objective function based on the Spatio Temporal Graph Convolutional Networks(ST-GCN) framework. For the experiments, a custom fitness dataset is collected from the gym and used to evaluate the performance of proposed model. In addition, we also compare with existing methods to confirm the effect of the proposed model.
PB  - SSRN
PY  - 2023
ST  - Human Movement Science-Informed Multi-Task Spatio Temporal Graph Convolutional Networks for Fitness Action Recognition and Evaluation
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4646398
ER  -


TY  - GEN
AU  - Zhang, Q.
AU  - Wu, R.
AU  - Nakata, M.
AU  - Ge, Q.-W.
TI  - Harnessing Time: A Skeleton Temporal Fusion Graph Convolutional Network for Action Recognition
AB  - Current methods in skeleton-based action recognition commonly prioritize spatial dimension features, specifically joint and bone representations. However, the significance of temporal dimension features remains relatively underexplored, despite their integral role in ensuring precise action classification. Moreover, while existing temporal graph convolution techniques effectively capture relationships between adjacent frames, they often fall short in modeling longer-range temporal dependencies crucial for efficiently harnessing the rich temporal dimension features. To tackle these two issues, we propose the Skeleton Temporal Fusion Graph Convolutional Network (STF-GCN). This approach introduces three encoding strategies to synergistically integrate temporal motion and angle features, optimizing the extraction of invaluable skeleton information for precise action identification. We further enhance the STF-GCN with a specially-designed Skeleton Temporal Fusion module, capable of aggregating both adjacent and longer-range temporal dimension features by creating varied-sized temporal receptive fields from the skeleton sequence. Evaluations on the extensive NTU-RGB+D 60 and 120 datasets not only validate the superior action classification accuracy of STF-GCN but also emphasize its enhanced effectiveness in distinguishing similar actions.
PB  - SSRN
PY  - 2023
ST  - Harnessing Time
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4646561
ER  -


TY  - GEN
AU  - Liu, S.
AU  - Zhang, C.-L.
AU  - Zhao, C.
AU  - Ghanem, B.
TI  - End-to-End Temporal Action Detection with 1B Parameters Across 1000 Frames
AB  - Recently, temporal action detection (TAD) has seen significant performance improvement with end-to-end training. However, due to the memory bottleneck, only models with limited scales and limited data volumes can afford end-to-end training, which inevitably restricts TAD performance. In this paper, we reduce the memory consumption for end-to-end training, and manage to scale up the TAD backbone to 1 billion parameters and the input video to 1,536 frames, leading to significant detection performance. The key to our approach lies in our proposed temporal-informative adapter (TIA), which is a novel lightweight module that reduces training memory. Using TIA, we free the humongous backbone from learning to adapt to the TAD task by only updating the parameters in TIA. TIA also leads to better TAD representation by temporally aggregating context from adjacent frames throughout the backbone. We evaluate our model across four representative datasets. Owing to our efficient design, we are able to train end-to-end on VideoMAEv2-giant and achieve 75.4% mAP on THUMOS14, being the first end-to-end model to outperform the best feature-based methods. Code is available at https://github.com/sming256/AdaTAD.
PB  - arXiv
PY  - 2023
ST  - End-to-End Temporal Action Detection with 1B Parameters Across 1000 Frames
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52733.2024.01759
ER  -


TY  - GEN
AU  - Bock, M.
AU  - Moeller, M.
AU  - van Laerhoven, K.
TI  - Temporal Action Localization for Inertial-based Human Activity Recognition
AB  - As of today, state-of-the-art activity recognition from wearable sensors relies on algorithms being trained to classify fixed windows of data. In contrast, video-based Human Activity Recognition, known as Temporal Action Localization (TAL), has followed a segment-based prediction approach, localizing activity segments in a timeline of arbitrary length. This paper is the first to systematically demonstrate the applicability of state-of-the-art TAL models for both offline and near-online Human Activity Recognition (HAR) using raw inertial data as well as pre-extracted latent features as input. Offline prediction results show that TAL models are able to outperform popular inertial models on a multitude of HAR benchmark datasets, with improvements reaching as much as 26% in F1-score. We show that by analyzing timelines as a whole, TAL models can produce more coherent segments and achieve higher NULL-class accuracy across all datasets. We demonstrate that TAL is less suited for the immediate classification of small-sized windows of data, yet offers an interesting perspective on inertial-based HAR – alleviating the need for fixed-size windows and enabling algorithms to recognize activities of arbitrary length. With design choices and training concepts yet to be explored, we argue that TAL architectures could be of significant value to the inertial-based HAR community. The code and data download to reproduce experiments is publicly available via github.com/mariusbock/tal_for_har.
PB  - arXiv
PY  - 2023
ST  - Temporal Action Localization for Inertial-based Human Activity Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3699770
ER  -


TY  - GEN
AU  - Vahdani, E.
AU  - Tian, Y.
TI  - ADM-Loc: Actionness Distribution Modeling for Point-supervised Temporal Action Localization
AB  - This paper addresses the challenge of point-supervised temporal action detection, in which only one frame per action instance is annotated in the training set. Self-training aims to provide supplementary supervision for the training process by generating pseudo-labels (action proposals) from a base model. However, most current methods generate action proposals by applying manually designed thresholds to action classification probabilities and treating adjacent snippets as independent entities. As a result, these methods struggle to generate complete action proposals, exhibit sensitivity to fluctuations in action classification scores, and generate redundant and overlapping action proposals. This paper proposes a novel framework termed ADM-Loc, which stands for Actionness Distribution Modeling for point-supervised action Localization. ADM-Loc generates action proposals by fitting a composite distribution, comprising both Gaussian and uniform distributions, to the action classification signals. This fitting process is tailored to each action class present in the video and is applied separately for each action instance, ensuring the distinctiveness of their distributions. ADM-Loc significantly enhances the alignment between the generated action proposals and ground-truth action instances and offers high-quality pseudo-labels for self-training. Moreover, to model action boundary snippets, it enforces consistency in action classification scores during training by employing Gaussian kernels, supervised with the proposed loss functions. ADM-Loc outperforms the state-of-the-art point-supervised methods on THUMOS’14 and ActivityNet-v1.2 datasets.
PB  - arXiv
PY  - 2023
ST  - ADM-Loc
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4943145
ER  -


TY  - GEN
AU  - Wang, H.
AU  - Mirmehdi, M.
AU  - Damen, D.
AU  - Perrett, T.
TI  - Centre Stage: Centricity-based Audio-Visual Temporal Action Detection
AB  - Previous one-stage action detection approaches have modelled temporal dependencies using only the visual modality. In this paper, we explore different strategies to incorporate the audio modality, using multi-scale cross-attention to fuse the two modalities. We also demonstrate the correlation between the distance from the timestep to the action centre and the accuracy of the predicted boundaries. Thus, we propose a novel network head to estimate the closeness of timesteps to the action centre, which we call the centricity score. This leads to increased confidence for proposals that exhibit more precise boundaries. Our method can be integrated with other one-stage anchor-free architectures and we demonstrate this on three recent baselines on the EPIC-Kitchens- 100 action detection benchmark where we achieve state-of-the-art performance. Detailed ablation studies showcase the benefits of fusing audio and our proposed centricity scores. Code and models for our proposed method are publicly available at https: //github.com/hanielwang/Audio-Visual-TAD.git.
PB  - arXiv
PY  - 2023
ST  - Centre Stage
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/icassp49357.2023.10095592
ER  -


TY  - GEN
AU  - Duan, Y.
AU  - Guan, Q.
AU  - Holme, P.
AU  - Yang, Y.
AU  - Guan, W.
TI  - Temporal link prediction methods based on behavioral synchrony
AB  - Link prediction—to identify potential missing or spurious links in temporal network data—has typically been based on local structures, ignoring long-term temporal effects. In this chapter, we propose link-prediction methods based on agents’ behavioral synchrony. Since synchronous behavior signals similarity and similar agents are known to have a tendency to connect in the future, behavioral synchrony could function as a precursor of contacts and, thus, as a basis for link prediction. We use four data sets of different sizes to test the algorithm’s accuracy. We compare the results with traditional link prediction models involving both static and temporal networks. Among our findings, we note that the proposed algorithm is superior to conventional methods, with the average accuracy improved by approximately 2%-5%. We identify different evolution patterns of four network topologies—a proximity network, a communication network, transportation data, and a collaboration network. We found that: (1) timescale similarity contributes more to the evolution of the human contact network and the human communication network; (2) such contribution is not observed through a transportation network whose evolution pattern is more dependent on network structure than on the behavior of regional agents; (3) both timescale similarity and local structural similarity contribute to the collaboration network.
PB  - arXiv
PY  - 2023
ST  - Temporal link prediction methods based on behavioral synchrony
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-30399-9_19
ER  -


TY  - GEN
AU  - Jadhav, P.R.
AU  - Aduri, R.
TI  - ChronoPscychosis: Temporal Segmentation and Its Impact on Schizophrenia Classification Using Motor Activity Data
AB  - Schizophrenia is a complicated mental illness characterized by a broad spectrum of symptoms affecting cognition, behavior, and emotion. The task of identifying reliable biomarkers to classify Schizophrenia accurately continues to be a challenge in the field of psychiatry. We investigate the temporal patterns within the motor activity data as a potential key to enhancing the categorization of individuals with Schizophrenia, using the dataset having motor activity recordings of 22 Schizophrenia patients and 32 control subjects. The dataset contains per-minute motor activity measurements collected for an average of 12.7 days in a row for each participant. We dissect each day into segments (Twelve, Eight, six, four, three, and two parts) and evaluate their impact on classification. We employ sixteen statistical features within these temporal segments and train them on Seven machine learning models to get deeper insights. LightGBM model outperforms the other six models. Our results indicate that the temporal segmentation significantly improves the classification, with AUC-ROC = 0.93, F1 score = 0.84(LightGBM- without any segmentation) and AUC-ROC = 0.98, F1 score = 0.93(LightGBM- with segmentation). Distinguishing between diurnal and nocturnal segments amplifies the differences between Schizophrenia patients and controls. However, further subdivisions into smaller time segments do not affect the AUC-ROC significantly. Morning, afternoon, evening, and night partitioning gives similar classification performance to day-night partitioning. These findings are valuable as they indicate that extensive temporal classification beyond distinguishing between day and night does not yield substantial results, offering an efficient approach for further classification, early diagnosis, and monitoring of Schizophrenia.
PB  - arXiv
PY  - 2023
ST  - ChronoPscychosis
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccas.2010.5670256
ER  -


TY  - GEN
AU  - Chen, Z.
AU  - Zhai, Y.
AU  - Zhang, J.
AU  - Wang, J.
TI  - Surgical Temporal Action-aware Network with Sequence Regularization for Phase Recognition
AB  - To assist surgeons in the operating theatre, surgical phase recognition is critical for developing computer-assisted surgical systems, which requires comprehensive understanding of surgical videos. Although existing studies made great progress, there are still two significant limitations worthy of improvement. First, due to the compromise of resource consumption, framewise visual features are extracted by 2D networks and disregard spatial and temporal knowledge of surgical actions, which hinders subsequent inter-frame modeling for phase prediction. Second, these works simply utilize ordinary classification loss with onehot phase labels to optimize the phase predictions, and cannot fully explore surgical videos under inadequate supervision. To overcome these two limitations, we propose a Surgical Temporal Action-aware Network with sequence Regularization, named STAR-Net, to recognize surgical phases more accurately from input videos. Specifically, we propose an efficient multi-scale surgical temporal action (MS-STA) module, which integrates visual features with spatial and temporal knowledge of surgical actions at the cost of 2D networks. Moreover, we devise the dualclassifier sequence regularization (DSR) to facilitate the training of STAR-Net by the sequence guidance of an auxiliary classifier with a smaller capacity. Our STAR-Net with MS-STA and DSR can exploit visual features of surgical actions with effective regularization, thereby leading to the superior performance of surgical phase recognition. Extensive experiments on a large-scale gastrectomy surgery dataset and the public Cholec80 benchmark prove that our STAR-Net significantly outperforms state-of-thearts of surgical phase recognition.
PB  - arXiv
PY  - 2023
ST  - Surgical Temporal Action-aware Network with Sequence Regularization for Phase Recognition
Y2  - 2025/05/05/21:54:31
ER  -


TY  - GEN
AU  - Mo, D.
AU  - Han, X.
AU  - Li, Y.
TI  - How Inclusive Leadership Affects Teachers' Innovative Behavior in Chinese Kindergartens: The Sequential Mediation Role of Teacher Efficacy and Psychological Safety
AB  - Although kindergarten teachers' innovative behavior is vital for developing young children's innovative literacy and advancing preschool education reform, research on the factors influencing this behavior is notably scarce.This research aims to uncover how inclusive leadership affects teachers' innovative behaviors, specifically examining the sequential mediating roles of teacher efficacy and psychological safety. It analyzes data from 1,020 teachers across 280 kindergartens in Guangxi, China, using Structural Equation Modeling and Bootstrap Analysis.The study reveals a significant positive correlation between principals' inclusive leadership and teachers' innovative behavior, with teacher efficacy and psychological safety acting as sequential mediators in this relationship. Additionally, it finds that teacher efficacy mediates the relationship between inclusive leadership and psychological safety, and psychological safety in turn mediates between teacher efficacy and innovative behavior. Overall, the study proposes a new model illustrating how principal inclusive leadership influences teacher innovative behavior, offering insights for enhancing kindergarten teachers' innovativeness. It emphasizes the importance of considering inclusive leadership, teacher efficacy, and psychological safety in designing programs to promote teacher innovation.
PB  - Research Square
PY  - 2023
ST  - How Inclusive Leadership Affects Teachers' Innovative Behavior in Chinese Kindergartens
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3596241/v1
ER  -


TY  - GEN
AU  - Rodrigues, F.S.
AU  - Monteiro, T.
AU  - Motiwala, A.
AU  - Paton, J.J.
TI  - The dorsolateral striatum encodes a temporal basis for the organization of behavior
AB  - To behave adaptively, the brain must register temporal structure in the environment and use it to organize behavior. The dorsolateral striatum (DLS) integrates sensorimotor input, and is necessary for accurate timing and structuring behavior in general. However, if DLS provides the basis for mapping temporal features in the environment to behavior, its activity should predict variation in that mapping. A reanalysis of DLS population activity in rats comparing the duration of two sequentially presented vibratory stimuli revealed a striking correspondence between neural activity and behavior. Varying vibration intensity of the second stimulus induced systematic biases in temporal judgments, and corresponding biases in multiple features of DLS activity during stimulus presentation, including population coding of time. In contrast, the same intensity manipulations applied to the first stimulus affected neither behavior nor neural activity. Furthermore, neuronal response profiles were best described as a continuum, arguing against hypotheses where categories of responses, e.g., ramping activity, selectively underpin temporal processing. These data represent important additional evidence that striatal population dynamics support the organization of behavior by mapping temporal information to action.
PB  - bioRxiv
PY  - 2023
ST  - The dorsolateral striatum encodes a temporal basis for the organization of behavior
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.11.13.566826
ER  -


TY  - GEN
AU  - Younes, K.
AU  - Smith, V.
AU  - Johns, E.
AU  - Young, C.B.
AU  - Mormino, E.C.
TI  - Temporal tau asymmetry spectrum influences divergent behavior and language patterns in Alzheimer`s disease
AB  - Understanding psychiatric symptoms in Alzheimer`s disease (AD) is crucial for advancing precision medicine and therapeutic strategies. The relationship between AD behavioral symptoms and asymmetry in spatial tau PET patterns is unknown. Braak tau progression implicates the temporal lobes early. However, the clinical and pathological implications of temporal tau laterality remain unexplored. This cross-sectional study investigated the correlation between temporal tau PET asymmetry and behavior assessed using the neuropsychiatric inventory, and composite scores for memory, executive function, and language; using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. In the entire cohort, continuous right and left temporal tau contributions to behavior and cognition were evaluated controlling for age, sex, education, and tau burden on the contralateral side. Additionally, a temporal tau laterality index was calculated to define “asymmetry-extreme” groups (individuals with laterality indices greater than two standard deviations from the mean). 858 individuals (age=73.9±7.7 years, 434(50%) females) were included, comprising 438 cognitively unimpaired (CU) (53.4%) and 420 impaired (CI) participants (48.9%). In the full cohort analysis, right temporal tau was associated with worse behavior (B(SE)=7.19 (2.9), p-value=0.01) and left temporal tau was associated with worse language (B(SE)=1.4(0.2), p-value<0.0001). Categorization into asymmetry-extreme groups revealed 20 right- and 27 left-asymmetric participants. Within these extreme groups, four patterns of tau PET uptake were observed: anterior temporal, typical AD, typical AD with frontal involvement, and posterior. Asymmetrical tau burden is associated with distinct behavioral and cognitive profiles. Behavioral and socioemotional measures are needed to understand right-sided asymmetry in AD.
PB  - medRxiv
PY  - 2023
ST  - Temporal tau asymmetry spectrum influences divergent behavior and language patterns in Alzheimer`s disease
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.bbi.2024.05.002
ER  -


TY  - GEN
AU  - Ding, W.
AU  - Ming, Z.
AU  - Wang, G.
AU  - Yan, Y.
TI  - System-of-Systems Approach to Spatio-Temporal Crowdsourcing Design Using Improved PPO Algorithm Based on an Invalid Action Masking
AB  - Spatio-temporal crowdsourcing (STC) is a typical case of complex system-of-systems (SoSs) design, wherein the primary objective is to allocate real-time tasks to suitable groups of workers. Over time, the STC allocation has gradually evolved into a dynamic matching involving three distinct entities: tasks, workers, and workplaces. Aiming at addressing the problems of poor convergence, slow response and sparse actions caused by the spatial complexity and time dynamics of the STC, this paper proposes an improved proximal policy optimization algorithm based on an invalid action masking (IAM-IPPO) for the SoSs design of the STC. Initially, the ternary dynamic matching (TDM) of tasks, workers and workplaces in the STC is described. Furthermore, the STC allocation is formulated as a Markov decision process, with the corresponding definition of state space, action space, and reward mechanism. On this basis, an invalid action masking (IAM) method is mainly introduced to update the policy-based network of proximal policy optimization (PPO), realizing sampling only from valid actions to masking invalid action selection. Subsequently, the algorithmic framework of IAM-IPPO is elaborated upon, and the model is trained to generate an effective allocation scheme. Comparative experiments are conducted on authentic datasets, aiming to assess performance indicators of the presented approach. The findings demonstrate a substantial enhancement in performance for the IAM-IPPO algorithm compared to other baselines, which is helpful in exploring excellent design schemes of the crowdsourcing SoSs, especially in dynamic large-scale cases.
PB  - SSRN
PY  - 2023
ST  - System-of-Systems Approach to Spatio-Temporal Crowdsourcing Design Using Improved PPO Algorithm Based on an Invalid Action Masking
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4623653
ER  -


TY  - GEN
AU  - Huang, Z.
AU  - Wolfson, J.
AU  - Fulkerson, J.A.
AU  - Demmer, R.
AU  - Chen, H.N.
TI  - A flexible framework for synthesizing human activity patterns with application to sequential categorical data
AB  - The ability to synthesize realistic data in a parametrizable way is valuable for a number of reasons, including privacy, missing data imputation, and evaluating the performance of statistical and computational methods. When the underlying data generating process is complex, data synthesis requires approaches that balance realism and simplicity. In this paper, we address the problem of synthesizing sequential categorical data of the type that is increasingly available from mobile applications and sensors that record participant status continuously over the course of multiple days and weeks. We propose the paired Markov Chain (paired-MC) method, a flexible framework that produces sequences that closely mimic real data while providing a straightforward mechanism for modifying characteristics of the synthesized sequences. We demonstrate the paired-MC method on two datasets, one reflecting daily human activity patterns collected via a smartphone application, and one encoding the intensities of physical activity measured by wearable accelerometers. In both settings, sequences synthesized by paired-MC better capture key characteristics of the real data than alternative approaches.
PB  - arXiv
PY  - 2023
ST  - A flexible framework for synthesizing human activity patterns with application to sequential categorical data
Y2  - 2025/05/05/21:54:31
DO  - 10.1080/10618600.2025.2450461
ER  -


TY  - GEN
AU  - Place, U.
AU  - Paubel, P.-V.
AU  - Capa, R.L.
TI  - Ego Depletion Effect with the Sequential Task Paradigm Involving Working Memory:A Cardiovascular and Behavioral Study
AB  - The current study explores the underlying mechanisms of the ego depletion effect based on two theoretical models: the strength model (Baumeister & Vohs, 2016) and the process model (Inzlicht & Schmeichel, 2012). Both predict the same results on the behavioral level, namely that exerting self-control reduces a person’s capacity on a subsequent task and leads to poorer performance. However, predictions differ on the amount of mental effort invested when performing the subsequent task. In this study, 48 healthy young adults were randomly allocated to one of three conditions (depleting vs. control vs. continuous) and performed a sequential task paradigm. Subsequently, we assessed working memory performance using the 2-back task and mental effort investment through cardiovascular reactivity. In the depleting condition (i.e., exhausting central task), the findings demonstrated a compensatory effort with more effort-related cardiovascular reactivity to slow down the decrease in performance. However, the continuous condition showed a similar performance-related decrease in mental effort investment. These results indicate that the sequential task paradigm favored compensatory effort through the re-engagement of motivation and attention. While challenging to contrast clearly, our findings suggest that both theoretical models might be involved in the ego depletion effect. This study carries important implications, as it underscores the significance of methodological precautions, cardiovascular measures, and time-on-task conditions (without motivational and attentional reengagement) in advancing research and theory on the ego depletion effect, particularly within the context of the replicability crisis.
PB  - SSRN
PY  - 2023
ST  - Ego Depletion Effect with the Sequential Task Paradigm Involving Working Memory
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4623457
ER  -


TY  - GEN
AU  - Kawahata, Y.
TI  - Detection Method for Social Subsets Consisting of Anti-Network Construction for Unilateral Preference Behavior on Directed Temporal Networks
AB  - In existing research, as an example of one-sided preference, a conversation structure in which a person who is assumed to be an adult mainly sends one-sided messages to a person who is assumed to be a minor was observed. If subgraphs composed based on such unilateral preferences could be automatically extracted from the network structure, it would be possible to automatically detect communication conducted based on specific motivations from a vast amount of conversation data. In this study, we construct a bottom-up method to detect subgraphs composed of unilateral preferences in Greedy, and discuss the subset of unilateral preferences detected in this simulation.
PB  - arXiv
PY  - 2023
ST  - Detection Method for Social Subsets Consisting of Anti-Network Construction for Unilateral Preference Behavior on Directed Temporal Networks
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-981-99-3925-1_8
ER  -


TY  - GEN
AU  - Zhang, H.
AU  - Cheng, M.
AU  - Liu, Q.
AU  - Jiang, J.
AU  - Chen, E.
TI  - AutoSAM: Towards Automatic Sampling of User Behaviors for Sequential Recommender Systems
AB  - Sequential recommender systems (SRS) have garnered immense popularity in recommendation due to their remarkable proficiency in capturing dynamic user preferences. In the current setup of SRS, a common configuration is to uniformly consider each historical behavior as a positive interaction. However, this setting has the potential to yield sub-optimal performance as each individual item often have a different impact on shaping the user’s interests. For instance, purchased items should hold more significance than merely clicked ones. Hence, in this paper, we propose a novel automatic sampling framework for sequential recommendation, named AutoSAM, to non-uniformly treat historical behaviors. Specifically, AutoSAM extends the conventional SRS framework by integrating an extra sampler to intelligently discern the skew distribution of the raw input, and then sample informative sub-sets to build more generalizable SRS. To tackle the challenges posed by non-differentiable sampling actions and to introduce multiple decision factors for sampling, we further design a novel reinforcement learning based method to guide the training of the sampler. We theoretically devise multi-objective sampling rewards including Future Prediction and Sequence Perplexity, and then optimize the whole framework in an end-to-end manner by combining the policy gradient. We conduct a broad range of tests on benchmark recommendation models and four real-world datasets from different online platforms. The experimental results demonstrate the effectiveness of the proposed approach. We also make our code publicly available
PB  - arXiv
PY  - 2023
ST  - AutoSAM
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3511808.3557476
ER  -


TY  - GEN
AU  - Phan, T.
AU  - Vo, K.
AU  - Le, D.
AU  - Adjeroh, D.
AU  - Le, N.
TI  - ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection
AB  - Temporal action detection (TAD) involves the localization and classification of action instances within untrimmed videos. While standard TAD follows fully supervised learning with closed-set setting on large training data, recent zero-shot TAD methods showcase the promising open-set setting by leveraging large-scale contrastive visual-language (ViL) pretrained models. However, existing zero-shot TAD methods have limitations on how to properly construct the strong relationship between two interdependent tasks of localization and classification and adapt ViL model to video understanding. In this work, we present ZEETAD, featuring two modules: dual-localization and zero-shot proposal classification. The former is a Transformer-based module that detects action events while selectively collecting crucial semantic embeddings for later recognition. The latter one, CLIP-based module, generates semantic embeddings from text and frame inputs for each temporal unit. Additionally, we enhance discriminative capability on unseen classes by minimally updating the frozen CLIP encoder with lightweight adapters. Extensive experiments on THUMOS14 and ActivityNet-1.3 datasets demonstrate our approach’s superior performance in zero-shot TAD and effective knowledge transfer from ViL models to unseen action categories. Code is available at https://github.com/UARK-AICV/ZEETAD.
PB  - arXiv
PY  - 2023
ST  - ZEETAD
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/wacv57701.2024.00689
ER  -


TY  - GEN
AU  - Jin, Z.
AU  - Shu, H.
AU  - Hu, T.
AU  - Wang, W.
AU  - Guo, L.
TI  - Behavior Classification and Spatiotemporal Analysis of Grazing Sheep Using Deep Learning
AB  - Monitoring multiple behavioral patterns and grazing trajectories of individual sheep canprovide valuable information for various aspects of livestock production. The emergence of low-cost, low-power miniature sensors, coupled with the continuous advancement of deep learning technologies, has ushered in a new generation of intelligent solutions for precision livestock farming. This study aims to explore the deployment methods of motion sensors, selection of data collection frequencies, and choice of deep learning algorithms to provide accurate classification of multiple behaviors in grazing sheep. Based on this, in conjunction with the acquired location information, the goal is to comprehend the spatiotemporal distribution of grazing sheep behaviors. Devices capable of collecting Inertial Measurement Units (IMU) and location data were attached to the jaw, neck, and hind leg of sheep. Four datasets were created using IMU data with a sampling frequency of 20Hz and a 5-second time window from different positions (neck, neck & leg, jaw, jaw & leg). Two deep learning models, Convolutional Neural Network (CNN) - Long Short Term Memory (LSTM) and Temporal Convolutional Network (TCN)-Transformer, were employed to classify six grazing sheep behaviors walking, standing, grazing, lying, standing-ruminating, and lying-ruminating. The results indicate that by fusing data from the neck- and leg-mounted device, utilizing the CNN-LSTM model, the highest accuracy for the classification of the six behaviors reached 98.4%. Furthermore, a comparison was made regarding the behavior classification accuracy of this fused data at different IMU data sampling frequencies (20, 10, 5, 2, 1Hz). It was found that even when the data frequency was reduced to 1Hz, the classification accuracy for the six sheep behaviors still exceeded 95%. Applying the trained model with the highest behavior classification accuracy to actual grazing processes of sheep allows for quantifying the total duration of each behavior. It also enables precise categorization of latitude and longitude coordinates into different behaviors, thereby generating spatiotemporal distribution maps of grazing sheep behaviors. This is of significant importance for gaining further insights into the physiological health status of grazing livestock, grazing choices, grassland utilization conditions, and enhancing grazing management practices.
PB  - SSRN
PY  - 2023
ST  - Behavior Classification and Spatiotemporal Analysis of Grazing Sheep Using Deep Learning
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.compag.2024.108894
ER  -


TY  - GEN
AU  - Kufer, K.
AU  - Schmitter, C.V.
AU  - Kircher, T.
AU  - Straube, B.
TI  - Temporal recalibration in response to delayed visual feedback of active versus passive actions: An fMRI study
AB  - The brain can adapt its expectations about the relative timing of actions and their sensory outcomes in a process known as temporal recalibration. This might occur as the recalibration of timing between the outcome and (1) the motor act (sensorimotor) or (2) tactile/proprioceptive information (inter-sensory). This fMRI recalibration study investigated sensorimotor contributions to temporal recalibration by comparing active and passive conditions. Subjects were repeatedly exposed to delayed (150ms) or undelayed visual stimuli, triggered by active or passive button presses. Recalibration effects were tested in delay detection tasks, including visual and auditory outcomes. We showed that both modalities were affected by visual recalibration. However, an active advantage was observed only in visual conditions. Recalibration was generally associated with the left cerebellum (lobules IV, V and vermis) while action related activation (active > passive) occurred in the right middle/superior frontal gyrus during adaptation and test phases. Recalibration transferred from vision to audition was related to action specific activations in the cingulate cortex, the angular gyrus and left inferior frontal gyrus. Our data provide new insights in sensorimotor contributions to temporal recalibration via the superior frontal gyrus and intersensory contributions mediated by the cerebellum.
PB  - Research Square
PY  - 2023
ST  - Temporal recalibration in response to delayed visual feedback of active versus passive actions
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3493865/v1
ER  -


TY  - GEN
AU  - Qashou, A.
AU  - Yousef, S.
TI  - Temporal Forecasting by Converting Stochastic Behaviour into a Stable Pattern in Electric Grid.
AB  - The malfunction variables of power stations are related to the areas of weather, physical structure, control, and load behavior. To predict temporal power failure is difficult due to their unpredictable characteristics. As high accuracy is normally required, the estimation of failures of short-term temporal prediction is highly difficult. This study presents a method for converting stochastic behavior into a stable pattern, which can subsequently be used in a short-term estimator. For this conversion, K-means clustering is employed, followed by Long-Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) algorithms are used to perform the Short-term estimation. The environment, the operation, and the generated signal factors are all simulated using mathematical models. Weather parameters and load samples have been collected as part of a dataset. Monte-Carlo simulation using MATLAB programming has been used to conduct experimental estimation of failures. The estimated failures of the experiment are then compared with the actual system temporal failures and found to be in good match. Therefore, to address the gap in knowledge for any future power grid estimated failures, the achieved results in this paper form good basis for a testbed to estimate any grid future failures.
PB  - Research Square
PY  - 2023
ST  - Temporal Forecasting by Converting Stochastic Behaviour into a Stable Pattern in Electric Grid.
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s13198-024-02454-0
ER  -


TY  - GEN
AU  - Lu, J.
AU  - Li, Z.
AU  - Li, X.
AU  - Yuan, S.
AU  - Duan, M.
TI  - Learning Subtle Variances on Temporal Sequence with Inception Attention for Skeleton-based Action Recognition
AB  - Graph convolutional network (GCN) has been employed in the task of skeleton-based action recognition with excellent performance. Studies applying GCN on the spatial-temporal graph paid more attention to feature transfer in the sparse spatial space and the capture of the long-term dependence of each spatial node in the time dimension. However, the sequential treatment of spatial and temporal information is inadequate to capture the intimate spatial-temporal dependencies. In addition, the high-resolution temporal information in the spatial-temporal graph has not been explored adequately. For the reasons, we propose to learn subtle variances on temporal sequence based on GCN with Inception Attention for skeleton-based action recognition. To learn the subtle variances of different actions, a self-supervised spatial-temporal graph representation learning is developed, where a spatial-temporal graph is divided into several sub-graphs on the temporal dimension with a random sub-graph shuffled, and the network is asked to predict the shuffled one to learn the subtle variances of different actions. An attention mechanism called Inception Attention is proposed to highlight the cubes of interest in the spatial-temporal graph, where the dependencies are captured along the spatial and temporal space simultaneously. Experiments on three large-scale public datasets, NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 verify the effectiveness of our method.
PB  - Research Square
PY  - 2023
ST  - Learning Subtle Variances on Temporal Sequence with Inception Attention for Skeleton-based Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3481493/v1
ER  -


TY  - GEN
AU  - Yang, F.
AU  - Wang, X.
AU  - Zhong, B.
AU  - Zhong, J.
TI  - Student Classroom Behavior Detection based on Spatio-Temporal Network and Multi-Model Fusion
AB  - Using deep learning methods to detect students’ classroom behavior automatically is a promising approach for analyzing their class performance and improving teaching effectiveness. However, the lack of publicly available spatio-temporal datasets on student behavior, as well as the high cost of manually labeling such datasets, pose significant challenges for researchers in this field. To address this issue, we proposed a method for extending the spatio-temporal behavior dataset in Student Classroom Scenarios (SCB-ST-Dataset4) through image dataset. Our SCB-ST-Dataset4 comprises 757265 images with 25810 labels, focusing on 3 behaviors: hand-raising, reading, writing. Our proposed method can rapidly generate spatiotemporal behavior datasets without requiring extra manual labeling. Furthermore, we proposed a Behavior Similarity Index (BSI) to explore the similarity of behaviors. We evaluated the dataset using the YOLOv5, YOLOv7, YOLOv8, and SlowFast algorithms, achieving a mean average precision (map) of up to 82.3%. Last, we fused multiple models to generate student behavior-related data from various perspectives. The experiment further demonstrates the effectiveness of our method. And SCB-ST-Dataset4 provides a robust foundation for future research in student behavior detection, potentially contributing to advancements in this field. The SCB-ST-Dataset4 is available for download at: https://github.com/Whiffe/SCB-dataset.
PB  - arXiv
PY  - 2023
ST  - Student Classroom Behavior Detection based on Spatio-Temporal Network and Multi-Model Fusion
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-46311-2_4
ER  -


TY  - GEN
AU  - Vahdani, E.
AU  - Tian, Y.
TI  - POTLoc: Pseudo-Label Oriented Transformer for Point-Supervised Temporal Action Localization
AB  - This paper tackles the challenge of point-supervised temporal action detection, wherein only a single frame is annotated for each action instance in the training set. Most of the current methods, hindered by the sparse nature of annotated points, struggle to effectively represent the continuous structure of actions or the inherent temporal and semantic dependencies within action instances. Consequently, these methods frequently learn merely the most distinctive segments of actions, leading to the creation of incomplete action proposals. This paper proposes POTLoc, a Pseudo-label Oriented Transformer for weakly-supervised Action Localization utilizing only point-level annotation. POTLoc is designed to identify and track continuous action structures via a self-training strategy. The base model begins by generating action proposals solely with point-level supervision. These proposals undergo refinement and regression to enhance the precision of the estimated action boundaries, which subsequently results in the production of ‘pseudo-labels’ to serve as supplementary supervisory signals. The architecture of the model integrates a transformer with a temporal feature pyramid to capture video snippet dependencies and model actions of varying duration. The pseudo-labels, providing information about the coarse locations and boundaries of actions, assist in guiding the transformer for enhanced learning of action dynamics. POTLoc outperforms the state-of-the-art point-supervised methods on THUMOS’14 and ActivityNet-v1.2 datasets. Our code is available at https://github.com/elahevahd/POTLoc.
PB  - arXiv
PY  - 2023
ST  - POTLoc
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.cviu.2024.104044
ER  -


TY  - GEN
AU  - Lopes-Dos-Santos, V.
AU  - Brizee, D.
AU  - Dupret, D.
TI  - Spatio-temporal organization of network activity patterns in the hippocampus
AB  - The hippocampus is a layered brain network, composed of diverse cell types arranged in multiple microcircuits with anatomically structured inputs, all working in concert to support memory. This intricate organization generates a myriad of electrophysiological signatures. While specific aspects of these activity patterns have been explored, a comprehensive understanding of the hippocampal layer-embedded dynamics remains elusive. Here, we developed a low-dimensional manifold to capture electrophysiological patterns, mapping their anatomical trajectory along the CA1-to-DG axis, and distinguishing layers based on sharp-wave and theta profiles. This profiling led to the characterization of selective theta-nested gamma signatures for each layer. It further revealed spike patterns associated with gamma rhythms, which highlight specific firing motifs between principal cells and interneurons, and differential firing properties within pyramidal sub-layers. These findings support a holistic understanding of the spatio-temporal activity patterns across hippocampal layers for unraveling the network operations that drive memory-guided behavior.
PB  - bioRxiv
PY  - 2023
ST  - Spatio-temporal organization of network activity patterns in the hippocampus
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.10.17.562689
ER  -


TY  - GEN
AU  - Liu, B.
AU  - Tian, X.
AU  - Peng, Y.
AU  - Huang, S.
AU  - Li, A.
TI  - Deciphering complex brain spatiotemporal dynamics shaping diverse human behavior
AB  - As a complex dynamic system, the brain remains notably active even at rest. Yet, the behavioral significance of such spontaneous activity is largely uncharted. We hypothesize that the variations in human behavior between individuals are embedded within the intricate patterns of intrinsic dynamics — an aspect previously underestimated but meriting integrative examination. Through resting-state functional MRI data, we extracted over 1 million spatiotemporal dynamic features, which encompass 7,700 temporal indices distributed across 271 brain regions for each individual. We introduced a comprehensive framework to thoroughly investigate the intricate spatiotemporal dynamics of the brain and their inherent associations with diverse human behaviors, integrating various analytical techniques. Our findings illuminate a novel set of dynamic properties that capture the distinctive 'fingerprint' of an individual and elucidate individual differences across cognitive and externalized behavioral dimensions. Interestingly, these dimensional brain signatures can be generalized and correlated with age-specific behaviors in an independent adolescent population. Our study sheds light on how intrinsic brain dynamics shape human behavior, offering a framework to understand behaviors and mental disorders through a complex systems lens.
PB  - Research Square
PY  - 2023
ST  - Deciphering complex brain spatiotemporal dynamics shaping diverse human behavior
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3344208/v1
ER  -


TY  - GEN
AU  - Zeng, F.
AU  - Tan, Z.
AU  - Zhou, Y.
TI  - TAP: Temporally-Aggregative Pretraining with Transformers for Temporal Action Detection
AB  - Given the long duration of untrimmed videos and the difficulties with end-to-end training, contemporary temporal action detection (TAD) methods heavily depend on pre-computed video feature sequences for subsequent analysis. However, video clip features extracted directly from video encoders trained for trimmed action classification commonly lack temporal sensitivity. To overcome this limitation, we propose an innovative framework named temporally-aggregative pretraining (TAP). TAP is rooted in the design principle of extracting TAD features of temporal sensitivity to improve discrimination between them and those from trimmed action classification. The proposed TAP consists of two fundamental modules at its core: a feature encoding module and a temporal aggregation module, which use both local and global features for pretraining. The feature encoding module employs a novel video encoder of a multiscale vision transformer, which ingeniously combines the essential concept of multiscale feature hierarchy with transformers to achieve effective feature extraction from video clips. The temporal aggregation module introduces a temporal pyramid pooling layer that effectively captures temporal-contextual semantic information from video feature sequences, enhancing more discriminative global video representations. Extensive experiments validate the significantly improved discriminative potency of our pretrained features for two commonly used datasets.
PB  - Research Square
PY  - 2023
ST  - TAP
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3420331/v1
ER  -


TY  - GEN
AU  - Fang, Z.
AU  - Yu, J.
AU  - Hong, R.
TI  - Boundary Discretization and Reliable Classification Network for Temporal Action Detection
AB  - Temporal action detection aims to recognize the action category and determine each action instance’s starting and ending time in untrimmed videos. The mixed methods have achieved remarkable performance by seamlessly merging anchor-based and anchor-free approaches. Nonetheless, there are still two crucial issues within the mixed framework: (1) Brute-force merging and handcrafted anchor design hinder the substantial potential and practicality of the mixed methods. (2) Within-category predictions show a significant abundance of false positives. In this paper, we propose a novel Boundary Discretization and Reliable Classification Network (BDRC-Net) that addresses the issues above by introducing boundary discretization and reliable classification modules. Specifically, the boundary discretization module (BDM) elegantly merges anchor-based and anchor-free approaches in the form of boundary discretization, eliminating the need for the traditional handcrafted anchor design. Furthermore, the reliable classification module (RCM) predicts reliable global action categories to reduce false positives. Extensive experiments conducted on different benchmarks demonstrate that our proposed method achieves competitive detection performance. Our source code is available at https://github.com/zhenyingfang/BDRC-Net.
PB  - arXiv
PY  - 2023
ST  - Boundary Discretization and Reliable Classification Network for Temporal Action Detection
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tmm.2025.3543108
ER  -


TY  - GEN
AU  - Andrao, M.
AU  - Treccani, B.
AU  - Zancanaro, M.
TI  - Language and Temporal Aspects: A Qualitative Study on Trigger Interpretation in Trigger-Action Rules
AB  - This paper presents a qualitative study that investigates the effects of some language choices in expressing the trigger part of a trigger-action rule on the users’ mental models. Specifically, we explored how 11 non-programmer participants articulated the definition of trigger-action rules in different contexts by choosing among alternative conjunctions, verbal structures, and order of primitives. Our study shed some new light on how lexical choices influence the users’ mental models in End-User Development tasks. Specifically, the conjunction “as soon as” clearly supports the idea of instantaneousness, and the conjunction “while” the idea of protractedness of an event; the most commonly used “if” and “when”, instead, are prone to create ambiguity in the mental representation of events. The order of rule elements helps participants to construct accurate mental models. Usually, individuals are facilitated in comprehension when the trigger is displayed at the beginning of the rule, even though sometimes the reverse order (with the action first) is preferred as it conveys the central element of the rule.
PB  - arXiv
PY  - 2023
ST  - Language and Temporal Aspects
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-34433-6_6
ER  -


TY  - GEN
AU  - Yin, Y.
AU  - Huang, Y.
AU  - Furuta, R.
AU  - Sato, Y.
TI  - Proposal-based Temporal Action Localization with Point-level Supervision
AB  - Point-level supervised temporal action localization (PTAL) aims at recognizing and localizing actions in untrimmed videos where only a single point (frame) within every action instance is annotated in training data. Without temporal annotations, most previous works adopt the multiple instance learning (MIL) framework, where the input video is segmented into non-overlapped short snippets, and action classification is performed independently on every short snippet. We argue that the MIL framework is suboptimal for PTAL because it operates on separated short snippets that contain limited temporal information. Therefore, the classifier only focuses on several easy-to-distinguish snippets instead of discovering the whole action instance without missing any relevant snippets. To alleviate this problem, we propose a novel method that localizes actions by generating and evaluating action proposals of flexible duration that involve more comprehensive temporal information. Moreover, we introduce an efficient clustering algorithm to efficiently generate dense pseudo labels that provide stronger supervision, and a fine-grained contrastive loss to further refine the quality of pseudo labels. Experiments show that our proposed method achieves competitive or superior performance to the state-of-the-art methods and some fully-supervised methods on four benchmarks: ActivityNet 1.3, THUMOS 14, GTEA, and BEOID datasets.
PB  - arXiv
PY  - 2023
ST  - Proposal-based Temporal Action Localization with Point-level Supervision
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/access.2023.3294572
ER  -


TY  - GEN
AU  - Zhang, J.
AU  - Feng, L.
AU  - He, Y.
AU  - Wu, Y.
AU  - Dong, Y.
TI  - Temporal Convolutional Explorer Helps Understand 1D-CNN’s Learning Behavior in Time Series Classification from Frequency Domain
AB  - While one-dimensional convolutional neural networks (1D-CNNs) have been empirically proven effective in time series classification tasks, we find that there remain undesirable outcomes that could arise in their application, motivating us to further investigate and understand their underlying mechanisms. In this work, we propose a Temporal Convolutional Explorer (TCE) to empirically explore the learning behavior of 1D-CNNs from the perspective of the frequency domain. Our TCE analysis highlights that deeper 1D-CNNs tend to distract the focus from the low-frequency components leading to the accuracy degradation phenomenon, and the disturbing convolution is the driving factor. Then, we leverage our findings to the practical application and propose a regulatory framework, which can easily be integrated into existing 1D-CNNs. It aims to rectify the suboptimal learning behavior by enabling the network to selectively bypass the specified disturbing convolutions. Finally, through comprehensive experiments on widely-used UCR, UEA, and UCI benchmarks, we demonstrate that 1) TCE’s insight into 1D-CNN’s learning behavior; 2) our regulatory framework enables state-of-the-art 1D-CNNs to get improved performances with less consumption of memory and computational overhead.
PB  - arXiv
PY  - 2023
ST  - Temporal Convolutional Explorer Helps Understand 1D-CNN’s Learning Behavior in Time Series Classification from Frequency Domain
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3583780.3615076
ER  -


TY  - GEN
AU  - Yang, F.
AU  - Wang, X.
AU  - Zhong, B.
AU  - Liu, D.
AU  - Zhong, J.
TI  - A Spatio-Temporal Attention-Based Method for Detecting Student Classroom Behaviors
AB  - Accurately detecting student behavior from classroom videos is beneficial for analyzing their classroom status and improving teaching efficiency. However, low accuracy in student classroom behavior detection is a prevalent issue. We propose a Spatio-Temporal Attention-Based Method for Detecting Student Classroom Behaviors (BDSTA) to address this issue. Firstly, the SlowFast network generates motion and environmental information feature maps from the video. Then, the spatio-temporal attention module is applied to the feature maps, including information aggregation, squeeze, and excitation processes. Subsequently, attention maps in the time, channel, and space dimensions are obtained, and multi-label behavior classification is performed based on these attention maps. To solve the long-tail data problem in student classroom behavior datasets, we use an improved focal loss function to assign more weight to the tail class data during training. Experimental results are conducted on a self-made student classroom behavior dataset named STSCB. Compared with the SlowFast model, the average accuracy of student behavior classification detection improves by 8.94% using BDSTA.
PB  - SSRN
PY  - 2023
ST  - A Spatio-Temporal Attention-Based Method for Detecting Student Classroom Behaviors
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4597485
ER  -


TY  - GEN
AU  - Ogunjo, S.
AU  - Akinsusi, J.O.
AU  - Rabiu, B.
AU  - Fuwape, I.A.
TI  - Dynamical Complexity and Multifractal Analysis of Geomagnetic Activities at High Temporal Scales Over Three Solar Cycles
AB  - Activities in geospace occur at different time scales. Understanding geomagnetic activity at high temporal scales will give insight into fast dynamics in geospace. This study aims to investigate dynamical complexities in geomagnetic activities at a high temporal scale across three solar cycles. Geomagnetic activities, as represented by 5-minute SYM-H data, were considered in this study over three solar cycles (22 - 24) from 1986 to 2019. Chaos analysis using sample entropy, Lyapunov exponent, and correlation dimension indicates that the geomagnetic activities are driven by intrinsic complex and chaotic processes. Positive Lyapunov exponent values between 0.13 - 0.18, 0.15 - 0.18, and 0.16 - 0.19 were obtained for solar cycles 22, 23, and 24 respectively. Furthermore, geomagnetic activities were also found to have multifractal structures driven by high fractal exponents with fine structures. A positive relationship was obtained between the annual mean values of SYM-H and the degree of complexity. It is concluded that geomagnetic activities have a short prediction horizon.
PB  - SSRN
PY  - 2023
ST  - Dynamical Complexity and Multifractal Analysis of Geomagnetic Activities at High Temporal Scales Over Three Solar Cycles
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.jastp.2024.106380
ER  -


TY  - GEN
AU  - Fish, E.
AU  - Weinbren, J.
AU  - Gilbert, A.
TI  - Multi-Resolution Audio-Visual Feature Fusion for Temporal Action Localization
AB  - Temporal Action Localization (TAL) aims to identify actions’ start, end, and class labels in untrimmed videos. While recent advancements using transformer networks and Feature Pyramid Networks (FPN) have enhanced visual feature recognition in TAL tasks, less progress has been made in the integration of audio features into such frameworks. This paper introduces the Multi-Resolution Audio-Visual Feature Fusion (MRAV-FF), an innovative method to merge audio-visual data across different temporal resolutions. Central to our approach is a hierarchical gated cross-attention mechanism, which discerningly weighs the importance of audio information at diverse temporal scales. Such a technique not only refines the precision of regression boundaries but also bolsters classification confidence. Importantly, MRAV-FF is versatile, making it compatible with existing FPN TAL architectures and offering a significant enhancement in performance when audio data is available.
PB  - arXiv
PY  - 2023
ST  - Multi-Resolution Audio-Visual Feature Fusion for Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccv.2019.00559
ER  -


TY  - GEN
AU  - Yang, F.
AU  - Wang, X.
AU  - Zhong, B.
AU  - Zhong, J.
AU  - Wang, T.
TI  - A Spatio-Temporal Attention-Based Method for Detecting Student Classroom Behaviors
AB  - Accurately detecting student behavior from classroom videos is beneficial for analyzing their classroom status and improving teaching efficiency. However, low accuracy in student classroom behavior detection is a prevalent issue. We propose a Spatio-Temporal Attention-Based Method for Detecting Student Classroom Behaviors (BDSTA) to address this issue. Firstly, the SlowFast network generates motion and environmental information feature maps from the video. Then, the spatio-temporal attention module is applied to the feature maps, including information aggregation, squeeze, and excitation processes. Subsequently, attention maps in the time, channel, and space dimensions are obtained, and multi-label behavior classification is performed based on these attention maps. To solve the long-tail data problem in student classroom behavior datasets, we use an improved focal loss function to assign more weight to the tail class data during training. Experimental results are conducted on a self-made student classroom behavior dataset named STSCB. Compared with the SlowFast model, the average accuracy of student behavior classification detection improves by 8.94% using BDSTA.
PB  - arXiv
PY  - 2023
ST  - A Spatio-Temporal Attention-Based Method for Detecting Student Classroom Behaviors
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4597485
ER  -


TY  - GEN
AU  - Boch, M.
AU  - Karl, S.
AU  - Wagner, I.C.
AU  - Huber, L.
AU  - Lamm, C.
TI  - Action observation reveals a network with divergent temporal and parietal lobe engagement in dogs compared to humans
AB  - Action observation is a fundamental pillar of social cognition. Neuroimaging research has revealed a human and primate action observation network (AON) encompassing fronto-temporo-parietal areas with links to a species’ imitation tendencies and relative lobe expansion. Dogs (Canis familiaris) have good action perception and imitation skills and a less expanded parietal than temporal lobe, but their AON remains unexplored. We conducted a functional MRI study with 28 dogs and 40 humans and found functionally analogous involvement of somatosensory and temporal brain areas of both species’ AONs and responses to transitive and intransitive action observation in line with their imitative skills. However, activation and task-based functional connectivity measures suggested significantly less parietal lobe involvement in dogs than in humans. These findings advance our understanding of the neural bases of action understanding and the convergent evolution of social cognition, with analogies and differences resulting from similar social environments and divergent brain expansion, respectively.
PB  - bioRxiv
PY  - 2023
ST  - Action observation reveals a network with divergent temporal and parietal lobe engagement in dogs compared to humans
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.10.02.560112
ER  -


TY  - GEN
AU  - Yan, K.
AU  - Dai, B.
AU  - Liu, H.G.
AU  - Wu, R.
AU  - Shen, W.
TI  - Deep Neural Network with Adaptive Dual-Modality Fusion for Temporal Aggressive Behavior Detection of Group-Housed Pigs
AB  - The frequent occurrence of pig aggressive behaviors in intensive group-housed environment seriously affects pig health, welfare and farms economy. Accurate detection of the occurring and temporal interval of aggressive behaviors is important for pig farming. The study aimed to develop an automatic temporal aggressive behavior detection method based on deep neural network. This network mainly consists of three modules, i.e., aggression feature extraction, adaptive dual-modality fusion and aggression temporal proposal generation. First, RGB data and optical flow data was used to extract the spatial and motion information of pig aggressive behaviors. Second, a modality attention and a temporal attention were specifically designed to adaptively fuse features of different modalities. Third, an anchor-free aggression temporal proposal generation strategy was applied to generate aggression proposals, which indicate the start and end times of aggressive behavior. To evaluate the proposed method, a behavior dataset containing 216 videos and 642 annotations was constructed. On the test set, this method achieves an AP value of 68.0%, an AR value of 77.8% in average number of proposals at 100 and an AUC value of 84.29% with 0.5 tIoU threshold. To test this method in practical application, our method was conducted on an additional 90 min untrimmed surveillance video and effectively predicted the real aggression instances. The results demonstrated that it can meet the practical needs of intelligent monitoring in pig farming and analysis in animal behavior research. We shared our temporal aggressive behavior detection dataset at https://github.com/IPCLab-NEAU/Temporal-Aggressive-Behavior-Detection for precision livestock farming research community.
PB  - SSRN
PY  - 2023
ST  - Deep Neural Network with Adaptive Dual-Modality Fusion for Temporal Aggressive Behavior Detection of Group-Housed Pigs
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.compag.2024.109243
ER  -


TY  - GEN
AU  - Zhang, J.
AU  - Wen, W.
AU  - Liu, S.
AU  - Feng, L.
AU  - Huang, G.
TI  - End-to-End Streaming Video Temporal Action Segmentation with Reinforcement Learning
AB  - The streaming temporal action segmentation (STAS) task, a supplementary task of temporal action segmentation (TAS), has not received adequate attention in the field of video understanding. Existing TAS methods are constrained to offline scenarios due to their heavy reliance on multimodal features and complete contextual information. The STAS task requires the model to classify each frame of the entire untrimmed video sequence clip by clip in time, thereby extending the applicability of TAS methods to online scenarios. However, directly applying existing TAS methods to SATS tasks results in significantly poor segmentation outcomes. In this paper, we thoroughly analyze the fundamental differences between STAS tasks and TAS tasks, attributing the severe performance degradation when transferring models to model bias and optimization dilemmas. We introduce an end-to-end streaming video temporal action segmentation model with reinforcement learning (SVTAS-RL). The end-to-end modeling method mitigates the modeling bias introduced by the change in task nature and enhances the feasibility of online solutions. Reinforcement learning is utilized to alleviate the optimization dilemma. Through extensive experiments, the SVTAS-RL model significantly outperforms existing STAS models and achieves competitive performance to the state-of-the-art TAS model on multiple datasets under the same evaluation criteria, demonstrating notable advantages on the ultra-long video dataset EGTEA. Code is available at https://github.com/Thinksky5124/SVTAS.
PB  - arXiv
PY  - 2023
ST  - End-to-End Streaming Video Temporal Action Segmentation with Reinforcement Learning
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tnnls.2025.3550910
ER  -


TY  - GEN
AU  - Wang, K.
AU  - Hou, P.
AU  - Xu, X.
AU  - Jia, G.
AU  - Hua, Y.
TI  - Automatically Identification of Pangolin's Behavior Using Deep Learning Based on Temporal Relative Attention Mechanism
AB  - With declining populations in the wild, captive rescue and breeding have become one of the most important ways to protect pangolins from extinction. At present, the success rate of artificial breeding is low, due to insufficient understanding of the breeding behavior characteristics of pangolins. The automatic recognition method based on machine vision not only monitors for 24 hours but also reduces the stress response of pangolins. This paper aimed to establish a temporal relation and attention mechanism network (Pangolin breeding attention & transfer network, PBATn) to monitor and recognize pangolin behaviors, including breeding and daily behavior. The PBATn was consisted of a backbone and a PBATn head. The backbone included the basic structure of Resnet-50, with added temporal relation, self-attention, and channel attention modules. The temporal relation, self-attention, and channel attention layers effectively solved the occlusion problem of breeding behavior. The GN and ReLU layers were added after each 7 × 7 convolutional layer and 1 × 1 convolutional layer in the ResNet-50 network. There were 11,476 videos including breeding behavior and daily behavior that were divided into training, validation, and test sets. For the training set and validation set, the PBATn network model had an accuracy of 98.95% and 96.11%, and a loss function value of 0.1531 and 0.1852. For the test set, the mAP, average accuracy, average recall, average specificity, and average F1 score were found to be higher than SLOWFAST, X3D, TANet, etc., with values of 97.50%, 99.17%, 97.55%, 99.53%, and 97.48%, respectively. The recognition accuracies of PBATn were 94.00% and 98.50% for the chasing and mounting breeding behaviors, respectively. The results showed PBATn outperformed the baseline methods in all aspects. This study provides a scientific basis for improving the breeding success of pangolins and also provides a new method for analyzing the behavior of other animals.
PB  - SSRN
PY  - 2023
ST  - Automatically Identification of Pangolin's Behavior Using Deep Learning Based on Temporal Relative Attention Mechanism
Y2  - 2025/05/05/21:54:31
ER  -


TY  - GEN
AU  - Liu, H.
AU  - Ding, J.
AU  - Zhu, Y.
AU  - Jiang, R.
AU  - Guo, Z.
TI  - Modeling Multi-aspect Preferences and Intents for Multi-behavioral Sequential Recommendation
AB  - Multi-behavioral sequential recommendation has recently attracted increasing attention. However, existing methods suffer from two major limitations. Firstly, user preferences and intents can be described in fine-grained detail from multiple perspectives; yet, these methods fail to capture their multi-aspect nature. Secondly, user behaviors may contain noises, and most existing methods could not effectively deal with noises. In this paper, we present an attentive recurrent model with multiple projections to capture Multi-Aspect preferences and INTents (MAINT in short). To extract multi-aspect preferences from target behaviors, we propose a multi-aspect projection mechanism for generating multiple preference representations from multiple aspects. To extract multi-aspect intents from multi-typed behaviors, we propose a behavior-enhanced LSTM and a multi-aspect refinement attention mechanism. The attention mechanism can filter out noises and generate multiple intent representations from different aspects. To adaptively fuse user preferences and intents, we propose a multi-aspect gated fusion mechanism. Extensive experiments conducted on real-world datasets have demonstrated the effectiveness of our model.
PB  - arXiv
PY  - 2023
ST  - Modeling Multi-aspect Preferences and Intents for Multi-behavioral Sequential Recommendation
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.knosys.2023.111013
ER  -


TY  - GEN
AU  - Lin, Z.
AU  - Gao, Y.
AU  - Li, D.
TI  - Cross-Attention Multi-Scale Spatial Temporal Transformer for Skeleton-based Action Recognition
AB  - In recent years, TransFormer has made remarkable achievements in a variety of tasks in computer vision. However, the Transformer-based methods have limitations in learning multi-scale features of skeleton data, while the multi-scale spatial temporal features contain potential both global and local information, which is crucial for skeleton-based action recognition. In this work, we explore the multi-scale feature representation of skeleton sequence in both the spatial and temporal dimensions, and propose an efficient cross-attention mechanism for cross-scale feature fusion. Moreover, we propose a Multi-scale Feature Extraction and Fusion Transformer (MFEF-Former), which can be divided into two types: (1) MFEF-SFormer for spatial modeling, which captures the inter-joint and inter-part correlations with self-attention, then performs multi-scale spatial feature fusion with cross-attention to model the correlations between joints and body parts. (2) MFEF-TFormer for temporal modeling, which captures the multi-scale temporal feature with self-attention and fuses the multi-scale feature with cross-attention. These two components are combined in a two-stream network, which is evaluated on two large-scale datasets, NTU RGB+D and NTU RGB+D 120. The experiments show that our proposed method outperforms other Transformer-based methods on skeleton-based action recognition and achieves state-of-the-art performance.
PB  - Research Square
PY  - 2023
ST  - Cross-Attention Multi-Scale Spatial Temporal Transformer for Skeleton-based Action Recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3368402/v1
ER  -


TY  - GEN
AU  - Xu, N.
AU  - Yousefi, B.
AU  - Anumba, N.
AU  - Zhang, X.
AU  - Keilholz, S.
TI  - QPPLab: A generally applicable software package for detecting, analyzing, and visualizing large-scale quasiperiodic spatiotemporal patterns (QPPs) of brain activity
AB  - Quasi-periodic patterns (QPPs) are prominent spatiotemporal brain dynamics observed in functional neuroimaging data, reflecting the alternation of high and low activity across brain regions and their propagation along cortical gradients. QPPs have been linked to neural processes such as attention, arousal fluctuations, and cognitive function. Despite their significance, existing QPP analysis tools are limited by study-specific parameters and complex workflows. To address these challenges, we present QPPLab, an open-source MATLAB-based toolbox for detecting, analyzing, and visualizing QPPs from fMRI time series. QPPLab integrates correlation-based iterative algorithms, supports customizable parameter settings, and features automated workflows to simplify analysis. Processing times vary depending on dataset size and the selected mode, with the fast detection mode completing analyses that can be 4–6 times faster than the robust detection mode. Results include spatiotemporal templates of QPPs, sliding correlation time courses, and functional connectivity maps. By reducing manual parameter adjustments and providing user-friendly tools, QPPLab enables researchers to efficiently study QPPs across diverse datasets and species, advancing our understanding of intrinsic brain dynamics.
PB  - bioRxiv
PY  - 2023
ST  - QPPLab
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.softx.2025.102067
ER  -


TY  - GEN
AU  - Zhang, H.
AU  - Feng, C.
AU  - Yang, J.
AU  - Li, Z.
AU  - Guo, C.
TI  - Boundary-Aware Proposal Generation Method for Temporal Action Localization
AB  - The goal of Temporal Action Localization (TAL) is to find the categories and temporal boundaries of actions in an untrimmed video. Most TAL methods rely heavily on action recognition models that are sensitive to action labels rather than temporal boundaries. More importantly, few works consider the background frames that are similar to action frames in pixels but dissimilar in semantics, which also leads to inaccurate temporal boundaries. To address the challenge above, we propose a Boundary-Aware Proposal Generation (BAPG) method with contrastive learning. Specifically, we define the above background frames as hard negative samples. Contrastive learning with hard negative mining is introduced to improve the discrimination of BAPG. BAPG is independent of the existing TAL network architecture, so it can be applied plug-and-play to mainstream TAL models. Extensive experimental results on THUMOS14 and ActivityNet-1.3 demonstrate that BAPG can significantly improve the performance of TAL.
PB  - arXiv
PY  - 2023
ST  - Boundary-Aware Proposal Generation Method for Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/vcip59821.2023.10402685
ER  -


TY  - GEN
AU  - El-Assal, M.
AU  - Tirilly, P.
AU  - Bilasco, I.M.
TI  - S3TC: Spiking Separated Spatial and Temporal Convolutions with Unsupervised STDP-based Learning for Action Recognition
AB  - Video analysis is a major computer vision task that has received a lot of attention in recent years. The current state-of-the-art performance for video analysis is achieved with Deep Neural Networks (DNNs) that have high computational costs and need large amounts of labeled data for training. Spiking Neural Networks (SNNs) have significantly lower computational costs (thousands of times) than regular non-spiking networks when implemented on neuromorphic hardware [1], [2]. They have been used for video analysis with methods like 3D Convolutional Spiking Neural Networks (3D CSNNs). However, these networks have a significantly larger number of parameters compared with spiking 2D CSNN. This, not only increases the computational costs, but also makes these networks more difficult to implement with neuromorphic hardware. In this work, we use CSNNs trained in an unsupervised manner with the Spike Timing-Dependent Plasticity (STDP) rule, and we introduce, for the first time, Spiking Separated Spatial and Temporal Convolutions (S3TCs) for the sake of reducing the number of parameters required for video analysis. This unsupervised learning has the advantage of not needing large amounts of labeled data for training. Factorizing a single spatio-temporal spiking convolution into a spatial and a temporal spiking convolution decreases the number of parameters of the network. We test our network with the KTH, Weizmann, and IXMAS datasets, and we show that S3TCs successfully extract spatio-temporal information from videos, while increasing the output spiking activity, and outperforming spiking 3D convolutions.
PB  - arXiv
PY  - 2023
ST  - S3TC
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-3-031-78395-1_20
ER  -


TY  - GEN
AU  - Liang, D.
AU  - Chen, Q.
AU  - Liu, Y.
TI  - SEQUENTIAL ACTION-INDUCED INVARIANT REPRESENTATION FOR REINFORCEMENT LEARNING
AB  - How to accurately learn task-relevant state representations from high-dimensional observations with visual distractions is a realistic and challenging problem in visual reinforcement learning. Recently, unsupervised representation learning methods based on bisimulation metrics, contrast, prediction, and reconstruction have shown the ability for task-relevant information extraction. However, due to the lack of appropriate mechanisms for the extraction of task information in the prediction, contrast, and reconstruction-related approaches and the limitations of bisimulation-related methods in domains with sparse rewards, it is still difficult for these methods to be effectively extended to environments with distractions. To alleviate these problems, in the paper, the action sequences, which contain task-intensive signals, are incorporated into representation learning. Specifically, we propose a Sequential Action–induced invariant Representation (SAR) method, in which the encoder is optimized by an auxiliary learner to only preserve the components that follow the control signals of sequential actions, so the agent can be induced to learn the robust representation against distractions. We conduct extensive experiments on the DeepMind Control suite tasks with distractions while achieving the best performance over strong baselines. We also demonstrate the effectiveness of our method at disregarding task-irrelevant information by deploying SAR to real-world CARLA-based autonomous driving with natural distractions. Finally, we provide the analysis results of generalization drawn from the generalization decay and t-SNE visualization. Code and demo videos are available at https://github.com/DMU-XMU/SAR.git.
PB  - arXiv
PY  - 2023
ST  - SEQUENTIAL ACTION-INDUCED INVARIANT REPRESENTATION FOR REINFORCEMENT LEARNING
Y2  - 2025/05/05/21:54:31
ER  -


TY  - GEN
AU  - Seweryn, K.
AU  - Wróblewska, A.
AU  - Łukasik, S.
TI  - SURVEY OF ACTION RECOGNITION, SPOTTING AND SPATIO-TEMPORAL LOCALIZATION IN SOCCER - CURRENT TRENDS AND RESEARCH PERSPECTIVES
AB  - Action scene understanding in soccer is a challenging task due to the complex and dynamic nature of the game, as well as the interactions between players. This article provides a comprehensive overview of this task divided into action recognition, spotting, and spatio-temporal action localization, with a particular emphasis on the modalities used and multimodal methods. We explore the publicly available data sources and metrics used to evaluate models' performance. The article reviews recent state-of-the-art methods that leverage deep learning techniques and traditional methods. We focus on multimodal methods, which integrate information from multiple sources, such as video and audio data, and also those that represent one source in various ways. The advantages and limitations of methods are discussed, along with their potential for improving the accuracy and robustness of models. Finally, the article highlights some of the open research questions and future directions in the field of soccer action recognition, including the potential for multimodal methods to advance this field. Overall, this survey provides a valuable resource for researchers interested in the field of action scene understanding in soccer.
PB  - arXiv
PY  - 2023
ST  - SURVEY OF ACTION RECOGNITION, SPOTTING AND SPATIO-TEMPORAL LOCALIZATION IN SOCCER - CURRENT TRENDS AND RESEARCH PERSPECTIVES
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4736989
ER  -


TY  - GEN
AU  - Pittaway, C.R.
AU  - Fielding, K.S.
AU  - Louis, W.R.
TI  - Pathways to Conventional and Radical Climate Action: The Role of Temporal Orientation, Environmental Cognitive Alternatives, and Climate Anxiety
AB  - Motivating climate action is challenging because the worst consequences of climate change are in the distant future, triggering a conflict between short- and long-term interests. Prior research suggests that only future-oriented individuals will act to protect the environment; however, we consider whether certain kinds of climate action are simply more appealing to present-oriented individuals than others. The present study tests this notion using structural equation modeling with a large sample from an Australian state (N = 967). We examine how two facets of temporal orientation – consideration of future and immediate consequences – predict intentions to engage in three forms of climate action at individual (private-sphere) and collective (public-sphere) levels: conventional private-sphere, conventional public-sphere, and radical public-sphere climate action. Consistent with past research, we find that intentions to engage in conventional forms of climate action are linked to higher consideration of future consequences and lower consideration of immediate consequences. In contrast, intentions to engage in radical climate action are related to higher consideration of immediate consequences. These findings challenge previous assumptions about the role of temporal orientation in environmental outcomes and suggest that present-oriented individuals can also be motivated to engage in environmentally protective behavior. We also explore two mechanisms by which temporal orientation is linked to environmental outcomes and find positive indirect relationships between temporal orientation and all three forms of climate action via access to environmental cognitive alternatives and climate anxiety.
PB  - SSRN
PY  - 2023
ST  - Pathways to Conventional and Radical Climate Action
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4569555
ER  -


TY  - GEN
AU  - Feng, X.
AU  - Xu, R.
AU  - He, Y.
AU  - Toyoura, M.
AU  - Peng, Y.
TI  - Boundary-Match U-Shaped Temporal Convolutional Network for Vulgar Action Segmentation
AB  - With the development of deep learning, many problems brought by the Internet have been solved. But efficiently localizing and recognizing vulgar action segments in videos is still a challenge, due to the blurred spatial features of vulgar actions making it difficult to distinguish them from general actions. In addition, the boundary ambiguity and over-segmentation issues also make vulgar action segmentation difficult. To solve the above problems, we propose Boundary-Match U-shaped Temporal Convolutional Network (BMUTCN) for vulgar action segmentation. Firstly, BMUTCN designs a U-shaped structure in the encoder-decoder temporal convolutional network to enhance the feature recognition ability of the model by incorporating video context. Secondly, we introduce a boundary-match map to integrate action boundary information with higher confidence for ambiguous boundary frames. Finally, we propose adaptive internal block suppression which can significantly reduce over-segmentation errors without losing accuracy. After experiments on multiple public datasets and the self-built vulgar dataset, our method achieves state-of-the-art results on the self-built vulgar dataset.
PB  - SSRN
PY  - 2023
ST  - Boundary-Match U-Shaped Temporal Convolutional Network for Vulgar Action Segmentation
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4576778
ER  -


TY  - GEN
AU  - Wirt, R.A.
AU  - Ricci, R.M.
AU  - Soluoku, T.K.
AU  - Seamans, J.K.
AU  - Hyman, J.M.
TI  - Representational Drift Enables the Temporal Integration of Experiences in ACC Ensemble Activity
AB  - Anterior cingulate cortex (ACC) activity is important for operations which require the ability to integrate multiple experiences over time, such as rule learning, cognitive flexibility, working memory, and long term memory recall. There is still little known about how time is processed in the brain, let alone by ACC neurons. To shed light on this, we conducted an analysis of neuronal activity while rats repeated the same behaviors during hour-long sessions. We aimed to investigate how this activity changed over time. We recorded large ensembles of neurons as rats performed a decision-free task with varying reward likelihoods at three different response ports (n=5). Neuronal state space analysis revealed that each repetition of the same behavior was distinctly encoded, and these codes evolved over the course of the session. More recent behaviors exhibited greater similarity than those further apart in time. ACC activity was dominated by a slow, gradual change in low-dimensional representations of neural state space, which aligned with the pace animal’s performed trials. Temporal progression, or drift, accounted for approximately 9.5% of the overall ensemble variance and was driven by the accumulation of experiences and not an internal clock. Notably, these signals were consistent across subjects, allowing us to accurately predict the trial number a rat was completing based on a model trained on data from a different animal. We observed that ramping firing rates over extended durations (tens of minutes) were responsible for driving the low-dimensional ensemble representations, primarily captured by the first principal component. ACC neurons exhibited various ramp lengths and combinations of neurons with shorter duration ramping activities created ensembles that tracked longer durations. Overall, we identified evidence of repeated experience-related ramping in 40% of ACC neurons. These findings provide valuable insights into how the ACC, at an ensemble level, conveys temporal information by reflecting the accumulation of experiences over extended periods.
PB  - SSRN
PY  - 2023
ST  - Representational Drift Enables the Temporal Integration of Experiences in ACC Ensemble Activity
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4568527
ER  -


TY  - GEN
AU  - Donneger, F.
AU  - Zanin, A.
AU  - Besson, J.
AU  - Chassoux, F.
AU  - Poncer, J.C.
TI  - Title: Enhancing KCC2 function reduces interictal activity and prevents seizures in mesial temporal lobe epilepsy
AB  - The neuronal K/Cl cotransporter KCC2 controls intraneuronal chloride and subsequently the efficiency of GABA signaling. In many neurological disorders, including mesial temporal lobe epilepsy (mTLE), reduced KCC2 expression or function may lead to depolarizing GABA signaling, which may then contribute to pathological activity and seizures. Compensating for the dysregulation of chloride transport in the pathology therefore appears to be a promising therapeutic strategy. Two small molecules - prochlorperazine (PCPZ) and CLP-257 - recently identified from library screening as candidate KCC2 enhancers, appear to improve symptoms in animal models of disorders associated with KCC2 extinction. However, their mode of action in cortical neurons and their therapeutic potential in epilepsy remain elusive and even controversial. Here, we show in rat hippocampal neurons that PCPZ and CLP-257 act by increasing KCC2 function and clustering while decreasing its membrane diffusion, independent of phosphorylation of canonical regulatory residues. Consistently, both compounds suppress spontaneous interictal-like discharges in postoperative tissue from patients with mTLE. Chronic administration of PCPZ also reduces seizure frequency and interictal activity in a mouse model of mTLE. These results unravel the mechanism of action of two KCC2 enhancers and validate their therapeutic potential in mesial temporal lobe epilepsy.
PB  - bioRxiv
PY  - 2023
ST  - Title
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.09.16.557753
ER  -


TY  - GEN
AU  - Li, Y.
AU  - Hou, Y.
AU  - Li, W.
TI  - Sub-action Prototype Learning for Point-level Weakly-supervised Temporal Action Localization
AB  - Point-level weakly-supervised temporal action localization (PWTAL) aims to localize actions with only a single timestamp annotation for each action instance. Existing methods tend to mine dense pseudo labels to alleviate the label sparsity, but overlook the potential sub-action temporal structures, resulting in inferior performance. To tackle this problem, we propose a novel sub-action prototype learning framework (SPL-Loc) which comprises Sub-action Prototype Clustering (SPC) and Ordered Prototype Alignment (OPA). SPC adaptively extracts representative sub-action prototypes which are capable to perceive the temporal scale and spatial content variation of action instances. OPA selects relevant prototypes to provide completeness clue for pseudo label generation by applying a temporal alignment loss. As a result, pseudo labels are derived from alignment results to improve action boundary prediction. Extensive experiments on three popular benchmarks demonstrate that the proposed SPL-Loc significantly outperforms existing SOTA PWTAL methods.
PB  - arXiv
PY  - 2023
ST  - Sub-action Prototype Learning for Point-level Weakly-supervised Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tip.2024.3431915
ER  -


TY  - GEN
AU  - Lehr, A.B.
AU  - Kumar, A.
AU  - Tetzlaff, C.
TI  - Sparse clustered inhibition projects sequential activity onto unique neural subspaces
AB  - Neural activity in the brain traces sequential trajectories on low dimensional subspaces. For flexible behavior, these neural subspaces must be manipulated and reoriented within tens of milliseconds. Using mathematical analysis and simulation of a recurrently connected neural circuit for sequence generation, we report that incorporating a subtype of interneurons that provides sparse but clustered inhibition enables the projection of sequential activity onto task- or context-specific neural subspaces. Depending on the sparsity of inhibitory projections, neural subspaces could be arbitrarily rotated with respect to the intrinsic subspace, without altering the key aspects of sequence generation. Thus, we propose a circuit motif and mechanism by which inhibitory interneurons can enable flexible switching between neural subspaces on a fast timescale of milliseconds, controlled by top down signals.
PB  - bioRxiv
PY  - 2023
ST  - Sparse clustered inhibition projects sequential activity onto unique neural subspaces
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.09.15.557865
ER  -


TY  - GEN
AU  - Padmasola, G.P.
AU  - Friscourt, F.
AU  - Schaller, K.
AU  - Sheybani, L.
AU  - Quairiaux, C.
TI  - Involvement of remote regions in sustained, but not transient, epileptic activities in the kainate mouse model of temporal lobe epilepsy
AB  - Animal and human studies have shown that the seizure-generating region is vastly dependent on distant neuronal hubs that can decrease duration and propagation of ongoing seizures. However, we still lack a comprehensive understanding of the impact of distant brain areas on specific interictal or ictal epileptic activities (e.g., isolated spikes, spike trains, seizures). Such knowledge is critically needed since all kinds of epileptic activities are not equivalent in terms of clinical expression and impact on the progression of the disease. We used surface, high-density EEG and multisite intracortical recordings, combined with pharmacological silencing of specific brain regions in the well-known kainate mouse model of temporal lobe epilepsy. We tested the impact of selective regional silencing on the generation of epileptic activities within a continuum ranging from very transient to more sustained and long-lasting discharges reminiscent of seizures. Silencing the contralateral hippocampus completely suppresses sustained ictal activities in the focus, as efficiently as silencing the focus itself, but while focus silencing abolishes all focal activities, contralateral silencing fails to control transient spikes. In parallel, we observed that sustained epileptic discharges in the focus are preceded by contralateral firing and more strongly phase locked to bi-hippocampal delta/theta oscillations than transient spiking activities, reinforcing the presumed dominant role of the contralateral hippocampus in promoting long-lasting, but not transient, epileptic activities. Altogether, our work provides suggestive evidence that the contralateral hippocampus is necessary for the interictal- to ictal-state transition and proposes that cross-talk between contralateral neuronal activity and ipsilateral delta/theta oscillation could be a candidate mechanism underlying the progression from short to long-lasting epileptic activities.
PB  - bioRxiv
PY  - 2023
ST  - Involvement of remote regions in sustained, but not transient, epileptic activities in the kainate mouse model of temporal lobe epilepsy
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.09.14.557684
ER  -


TY  - GEN
AU  - Neto, J.A.F.
AU  - Mendes, D.
AU  - Gonçalves, W.A.
AU  - Cintra, M.M.
AU  - Oliveira, J.F.
TI  - Temporal Evolution of Hurricane Activity: Insights from Decades of Category 1-5 Analysis
AB  - This study conducts an in-depth analysis of hurricane trajectories and their variabilities across categories 1 to 5 over several decades. Utilizing HURDAT2 data from 1961 to 2021, the analysis categorizes hurricanes based on the rate of pressure drop within a six-hour interval, revealing distinct patterns in intensification and weakening among different categories. The K-means clustering method synthesized hurricane trajectories into representative paths, illustrating significant variations among decades. The research indicates that Category 1 and 2 hurricanes predominantly originate from tropical depressions, with this trend slightly intensifying in Categories 3 and 4. In contrast, Category 5 displayed variation, revealing an increased frequency in subsequent decades. Moreover, the study analyzes the monthly distribution of hurricanes, finding September as the peak month across categories. The analysis further detects a significant interannual variability with a noticeable intensification in hurricane activity since the 1990s, albeit with some reductions in the early 2010s. The Accumulated Cyclone Energy (ACE) is used to summarize cyclonic activities, with results indicating a decrease from 1970–1995, followed by a consistent surge over the last 15 years. This is aligned with previous research suggesting an approximately 60% increase in ACE since the 1980s. Additionally, an analysis of North Atlantic basin data reflects a progressive increase in the frequency of named storms and hurricanes, particularly from 1991 onwards. In conclusion, the study highlights not only an escalating frequency of hurricanes but also an enhanced variability and unpredictability, which necessitates further research to comprehend the underlying causes and evaluate the potential socio-economic and environmental consequences.
PB  - Research Square
PY  - 2023
ST  - Temporal Evolution of Hurricane Activity
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s12665-024-11504-6
ER  -


TY  - GEN
AU  - Maktoubian, J.
AU  - Tran, S.N.
AU  - Shillabeer, A.
TI  - On Temporal Feature Engineering for Driver Behaviour Prediction
AB  - Driver behaviour modelling is a pivotal field of research that seeks to understand the complex and ever-changing driving behaviours on the road, leading to improvement of road safety, reduction in air pollution, and more effective vehicle performance. Thanks to advancements in sensor technology and machine learning (ML) algorithms, capturing and evaluating driver behaviour patterns become significantly more accessible. However, the accuracy of ML models relies heavily on data quality, so the role of feature extraction techniques in delivering high-quality inputs is crucial. The primary contribution of this research is to evaluate and compare various feature extraction techniques with the aim of enhancing data quality and ultimately improving the prediction of driver activities. We also designed, implemented, and evaluated two novel feature extraction approaches called Statistical Lag Feature (SLF) and Behaviour Feature (BF) methods to extract features from raw sensor data. The experimental findings unequivocally demonstrate that the SLF and BF techniques exhibit significantly improved prediction accuracy of ML models compared to Fast-Fourier Transform (FFT), Convolutional Neural Networks (CNN), Statistical, and Cross-Correlation methods.
PB  - SSRN
PY  - 2023
ST  - On Temporal Feature Engineering for Driver Behaviour Prediction
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4572165
ER  -


TY  - GEN
AU  - Ignat, O.
AU  - Castro, S.
AU  - Li, W.
AU  - Mihalcea, R.
TI  - Learning Human Action Representations from Temporal Context in Lifestyle Vlogs
AB  - We address the task of human action representation and show how the approach to generating word representations based on co-occurrence can be adapted to generate human action representations by analyzing their co-occurrence in videos. To this end, we formalize the new task of human action co-occurrence identification in online videos, i.e., determine whether two human actions are likely to co-occur in the same interval of time. We create and make publicly available the CO-ACT (Action Co-occurrence) dataset, consisting of a large graph of ∼12k co-occurring pairs of visual actions and their corresponding video clips. We describe graph link prediction models that leverage visual and textual information to automatically infer if two actions are co-occurring. We show that graphs are particularly well suited to capture relations between human actions, and the learned graph representations are effective for our task and capture novel and relevant information across different data domains. The CO-ACT dataset and the code introduced in this paper are publicly available at https://github.com/MichiganNLP/ vlog_action_co-occurrence.
PB  - arXiv
PY  - 2023
ST  - Learning Human Action Representations from Temporal Context in Lifestyle Vlogs
Y2  - 2025/05/05/21:54:31
DO  - 10.18653/v1/2024.textgraphs-1.1
ER  -


TY  - GEN
AU  - Li, Y.
AU  - Xue, Z.
AU  - Xu, H.
TI  - OTAS: Unsupervised Boundary Detection for Object-Centric Temporal Action Segmentation
AB  - Temporal action segmentation is typically achieved by discovering the dramatic variances in global visual descriptors. In this paper, we explore the merits of local features by proposing the unsupervised framework of Object-centric Temporal Action Segmentation (OTAS). Broadly speaking, OTAS consists of self-supervised global and local feature extraction modules as well as a boundary selection module that fuses the features and detects salient boundaries for action segmentation. As a second contribution, we discuss the pros and cons of existing frame-level and boundary-level evaluation metrics. Through extensive experiments, we find OTAS is superior to the previous state-of-the-art method by 41% on average in terms of our recommended F1 score. Surprisingly, OTAS even outperforms the ground-truth human annotations in the user study. Moreover, OTAS is efficient enough to allow real-time inference.
PB  - arXiv
PY  - 2023
ST  - OTAS
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/wacv57701.2024.00631
ER  -


TY  - GEN
AU  - Broholt, T.H.
AU  - Amato, V.
AU  - Christensen, L.R.L.
AU  - Kristensen, M.H.
AU  - Petersen, S.
TI  - Opportunities and Barriers for Temporal Demand Response as an Action to Challenges in District Heating
AB  - Recent studies assume that district heating operators can substitute traditional supply-side management initiatives with temporal demand response from building heating systems to eliminate and/or mitigate operational challenges. The study reported in this paper investigated whether district heating practitioners are currently aligned with this assumption by mapping the current, prevailing, and expected future challenges of district heating systems that temporal demand response from building space heating systems could mitigate or substitute. An initial literature review identified a need for further investigations into the current attitude of district heating practitioners towards temporal demand response from building space heating systems as alternative actions to current challenges. Seven semi-structured interviews with employees of Danish district heating companies were therefore conducted leading to insights that could help researchers target the development of temporal demand response solutions that facilitates the practical transition to 4th generation district heating and 5th generation district heating. The attitude of the seven interviewees towards temporal demand response as a solution to district heating challenges was positive but not representative of the whole district heating industry. It is therefore proposed that future work is to conduct a questionnaire survey to analyse the distribution, prevalence, and importance of the findings presented in this paper.
PB  - SSRN
PY  - 2023
ST  - Opportunities and Barriers for Temporal Demand Response as an Action to Challenges in District Heating
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4134863
ER  -


TY  - GEN
AU  - Shi, D.
AU  - Cao, Q.
AU  - Zhong, Y.
AU  - Zhu, H.
AU  - Tao, D.
TI  - Temporal Action Localization with Enhanced Instant Discriminability
AB  - Temporal action detection (TAD) aims to detect all action boundaries and their corresponding categories in an untrimmed video. The unclear boundaries of actions in videos often result in imprecise predictions of action boundaries by existing methods. To resolve this issue, we propose a one-stage framework named TriDet. First, we propose a Trident-head to model the action boundary via an estimated relative probability distribution around the boundary. Then, we analyze the rank-loss problem (i.e. instant discriminability deterioration) in transformer-based methods and propose an efficient scalable-granularity perception (SGP) layer to mitigate this issue. To further push the limit of instant discriminability in the video backbone, we leverage the strong representation capability of pretrained large models and investigate their performance on TAD. Last, considering the adequate spatial-temporal context for classification, we design a decoupled feature pyramid network with separate feature pyramids to incorporate rich spatial context from the large model for localization. Experimental results demonstrate the robustness of TriDet and its state-of-the-art performance on multiple TAD datasets, including hierarchical (multilabel) TAD datasets.
PB  - arXiv
PY  - 2023
ST  - Temporal Action Localization with Enhanced Instant Discriminability
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccv51070.2023.00609
ER  -


TY  - GEN
AU  - Wu, C.
AU  - Wu, X.-J.
AU  - Kittler, J.
AU  - Awais, M.
AU  - Feng, Z.
TI  - SCD-Net: Spatiotemporal Clues Disentanglement Network for Self-supervised Skeleton-based Action Recognition
AB  - Contrastive learning has achieved great success in skeleton-based action recognition. However, most existing approaches encode the skeleton sequences as entangled spatiotemporal representations and confine the contrasts to the same level of representation. Instead, this paper introduces a novel contrastive learning framework, namely Spatiotemporal Clues Disentanglement Network (SCD-Net). Specifically, we integrate the decoupling module with a feature extractor to derive explicit clues from spatial and temporal domains respectively. As for the training of SCD-Net, with a constructed global anchor, we encourage the interaction between the anchor and extracted clues. Further, we propose a new masking strategy with structural constraints to strengthen the contextual associations, leveraging the latest development from masked image modelling into the proposed SCD-Net. We conduct extensive evaluations on the NTU-RGB+D (60&120) and PKU-MMD (I&II) datasets, covering various downstream tasks such as action recognition, action retrieval, transfer learning, and semi-supervised learning. The experimental results demonstrate the effectiveness of our method, which outperforms the existing state-of-the-art (SOTA) approaches significantly.
PB  - arXiv
PY  - 2023
ST  - SCD-Net
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v38i6.28409
ER  -


TY  - GEN
AU  - Liu, P.
AU  - Wang, C.
AU  - Wang, C.
AU  - Qin, J.
AU  - Wang, J.
TI  - Foreground-Background Joint Modeling Based on Inter-Segment Action Feature Enhancement for Weakly-Supervised Temporal Action Localization
AB  - Weakly-supervised Temporal Action Localization (W-TAL) is a critical task in video understanding, and its objective is to classify the action category and locate the temporal boundary of action instances in untrimmed videos with only video-level labels. Given the lack of frame-level annotations, learning the spatiotemporal relationships among action snippets and accurately separating foreground and background are two necessary yet challenging problems. In this paper, we propose a novel Foreground-Background joint modeling based oninter-segment Action Feature Enhancement (FB-AFE) method to address these problems. Specifically, we construct a spatiotemporal cooperation enhancement scheme utilizing residual graph convolutional networks to capture the spatial and temporal dependencies between the current snippet and other snippets, generating snippet features with spatial and temporal correlations, thereby ensuring a complete feature representation for action localization. Furthermore, we propose a two-branch explicit foreground-background joint attention mechanism to guide foreground and background modeling, combined with an inverse enhancement strategy to enhance action attention weight for better foreground and background distinction, thereby improving the accuracy of action localization. Our method achieves accuracies of 66.7% and 38.9% on the THUMOS’14 and ActivityNet v1.3 datasets, respectively, and further compared with other approaches, our method gains its superiority.
PB  - SSRN
PY  - 2023
ST  - Foreground-Background Joint Modeling Based on Inter-Segment Action Feature Enhancement for Weakly-Supervised Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4567047
ER  -


TY  - GEN
AU  - Harrison, R.
AU  - van Reekum, C.M.
AU  - Adams, G.
AU  - Gandhi, W.
AU  - Salomons, T.V.
TI  - Temporal dynamics of hippocampal activity predict stable patterns of sensitization or habituation to noxious stimulation across sessions
AB  - Acute pain serves to warn an organism of potential damage. When nociceptive stimulation persists, two possible responses emerge: If no risk of harm is anticipated, habituation may occur. If harm is considered possible, pain sensitization is likely. An individual's adaptation to prolonged pain may provide insight into their ability to manage resources, and possibly their likelihood of developing chronic pain. Yet, little is known about the stability of these individual differences or their underlying neural mechanisms. Eighty-five participants undertook a repetitive noxious stimulation task and a resting-state scan in an MRI scanner, in a first session. They then completed the same task outside the scanner on three separate days. Pain adaptation was operationalized as the slope of change in pain ratings within session. Intraclass correlations were calculated between slopes across the four sessions, which demonstrated high stability and association with emotional disposition. Individuals who habituated to repeated stimuli showed increasing activity in the anterior hippocampus and amygdala, while individuals who sensitized showed increasing activity in the sensorimotor cortices. These clusters were then used as seeds in resting state analysis, with habituation associated with higher functional connectivity between hippocampus/amygdala and ventromedial prefrontal cortex(vmPFC), and higher connectivity between sensorimotor regions and the hippocampus, amygdala and insula cortex. Our findings suggest that pain adaptation is a stable phenotypic trait, which may have implications for the prediction of chronic pain. This study implicates neural sensory and appraisal systems in these stable responses, offering insight into the mechanisms underlying trait-like responses to prolonged nociceptive input.
PB  - bioRxiv
PY  - 2023
ST  - Temporal dynamics of hippocampal activity predict stable patterns of sensitization or habituation to noxious stimulation across sessions
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.09.06.555302
ER  -


TY  - GEN
AU  - Cutts, S.A.
AU  - Chumin, E.J.
AU  - Betzel, R.F.
AU  - Sporns, O.
TI  - Temporal Variability of Brain-Behavior Relationships in Fine-Scale Dynamics of Edge Time Series
AB  - Most work on functional connectivity (FC) in neuroimaging data prefers longer scan sessions or greater subject count to improve reliability of brain-behavior relationships or predictive models. Here, we investigate whether systematically isolating moments in time can improve brain-behavior relationships and outperform full scan data. We perform optimizations using a temporal filtering strategy to identify time points that improve brain-behavior relationships across 58 different behaviors. We analyzed functional brain networks from resting state fMRI data of 352 healthy subjects from the Human Connectome Project. Templates were created to select time points with similar patterns of brain activity. Optimizations were performed to produce templates for each behavior that maximize brain-behavior relationships from reconstructed functional networks. With 10% of scan data, optimized templates of select behavioral measures achieved greater strength of brain-behavior correlations and greater transfer between groups of subjects than full FC across multiple cross validation splits of the dataset. Therefore, selectively filtering time points may allow for development of more targeted FC analyses and increased understanding of how specific moments in time contribute to behavioral prediction.
PB  - bioRxiv
PY  - 2023
ST  - Temporal Variability of Brain-Behavior Relationships in Fine-Scale Dynamics of Edge Time Series
Y2  - 2025/05/05/21:54:31
DO  - 10.1162/imag_a_00443
ER  -


TY  - GEN
AU  - Seikai, T.
AU  - Yamanishi, T.
AU  - Takeshi, H.
AU  - Enomoto, A.
AU  - Kogo, M.
TI  - Glycine changes the sequential pattern of swallowing activity in the working heart–brainstem preparation in rats
AB  - Swallowing is a centrally programmed intricate activity consisting of both excitatory and inhibitory neural transmissions to ensure aspiration does not occur. However, the central mechanisms involved, in particular, the role of the inhibitory transmission underlying the establishment of sequential movements, are not well understood. Recently, some studies have demonstrated swallowing activity recorded from the working heart–brainstem preparation (WHBP), a useful model utilized to study the neuronal mechanisms involved in centrally controlled functions. However, such an activity has not yet been adequately confirmed. The aims of this study were first to confirm ability of WHBP to elicit swallowing activity, then study the roles of inhibitory neurotransmitter receptors in the sequential swallowing activity. Then, we found that application of a glycine receptor antagonist accelerate the timing of muscle activation of the middle pharyngeal constrictor muscle during the sequential activity of swallowing, while application of a GABAA receptor antagonist did not. Based on these results, we concluded that WHBP has a conserved neuronal network necessary for establishing swallowing activity, and that glycine receptors played a role in the orchestration of the sequential pattern.
PB  - Research Square
PY  - 2023
ST  - Glycine changes the sequential pattern of swallowing activity in the working heart–brainstem preparation in rats
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3302525/v1
ER  -


TY  - GEN
AU  - Dai, R.
AU  - Das, S.
AU  - Ryoo, M.S.
AU  - Brémond, F.
TI  - AAN: Attributes-Aware Network for Temporal Action Detection
AB  - The challenge of long-term video understanding remains constrained by the efficient extraction of object semantics and the modelling of their relationships for downstream tasks. Although OpenAI’s CLIP visual features exhibit discriminative properties for various vision tasks, particularly in object encoding, they are suboptimal for long-term video understanding. To address this issue, we present the Attributes-Aware Network (AAN), which consists of two key components: the Attributes Extractor and a Graph Reasoning block. These components facilitate the extraction of object-centric attributes and the modelling of their relationships within the video. By leveraging CLIP features, AAN outperforms state-of-the-art approaches on two popular action detection datasets: Charades and Toyota Smarthome Untrimmed datasets.
PB  - arXiv
PY  - 2023
ST  - AAN
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/978-981-97-5663-6_22
ER  -


TY  - GEN
AU  - Kim, E.S.
AU  - Yun, S.H.
AU  - Kim, J.Y.
AU  - Hyun, J.H.
AU  - Lee, D.K.
TI  - Sequential Action-Based Dynamic Decision-Support Model for Urban Ecological Planning
AB  - Decision-makers often encounter the challenge of efficient long-term ecological planning to consider economic and environmental trade-offs. However, tools to support stepwise mitigation strategies that account for dynamic ecological responses to continuing urban development are limited. Therefore, we developed a sequential action-based dynamic decision-making model to simulate stepwise mitigation measures in space over time using a multi-objective optimization algorithm. We focused on exploring the ecological corridors and habitat creation as mitigation measures through four decades to identify trade-offs and optimal plans that minimize landscape connectivity and edge density as ecological responses to urban development and minimize implementation costs per action. The results showed that adopting an initial low-investment strategy with sequential planning significantly enhanced potential ecological benefits beyond the 2000s compared with one-action planning. In contrast, the initial high-cost investment exhibited a gradual decline in the ecological benefits over time from 1995. The initial low-cost investment presented an opportunity to enhance ecological benefits by implementing early mitigation measures in locations capable of sustaining long-term ecological functions. The results suggest that sequential planning offers valuable insights into effective urban ecological planning by analyzing time-series data. This study enables step-by-step scenario testing, thereby supporting the facilitation of urban and regional sustainability planning.
PB  - SSRN
PY  - 2023
ST  - Sequential Action-Based Dynamic Decision-Support Model for Urban Ecological Planning
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4559025
ER  -


TY  - GEN
AU  - Zhu, Z.
AU  - Shao, W.
AU  - Jiao, D.
TI  - TLS-RWKV: Real-time Online Action Detection with Temporal Label Smoothing
AB  - Online action detection (OAD) is a challenging task that involves predicting the ongoing action class in real-time from streaming videos. In this article, we propose a novel approach for OAD based on the RWKV model and introducing temporal label smoothing. The RWKV model effectively captures temporal dependencies while maintaining computational efficiency, making it well-suited for real-time applications. The temporal label smoothing technique enhances the model’s robustness in handling ambiguous boundaries and feature similarities near action boundaries. We conducted experiments on two widely used datasets, THUMOS’14 and TVSeries, to evaluate the performance of our proposed approach. Our model achieves state-of-the-art performance with 71.8% mAP on THUMOS’14 and 89.7% cAP on TVSeries. Furthermore, our proposed approach demonstrates impressive efficiency, running at over 600 FPS and maintaining a competitive mAP of 59.9% on THUMOS’14 with RGB features alone. This high efficiency makes our model suitable for real-time deployment, even on resource-constrained devices. These results showcase the effectiveness and competitiveness of our proposed approach in OAD.
PB  - Research Square
PY  - 2023
ST  - TLS-RWKV
Y2  - 2025/05/05/21:54:31
DO  - 10.21203/rs.3.rs-3305376/v1
ER  -


TY  - GEN
AU  - Liu, Y.
AU  - Zhong, X.
AU  - Zhai, S.
AU  - Han, Y.
AU  - Qin, P.
TI  - Prompt-enhanced Hierarchical Transformer Elevating Cardiopulmonary Resuscitation Instruction via Temporal Action Segmentation
AB  - The vast majority of people who suffer unexpected cardiac arrest are performed cardiopulmonary resuscitation (CPR) by passersby in a desperate attempt to restore life, but endeavors turn out to be fruitless on account of disqualification. Fortunately, many pieces of research manifest that disciplined training will help to elevate the success rate of resuscitation, which constantly desires a seamless combination of novel techniques to yield further advancement. To this end, we collect a custom CPR video dataset in which trainees make efforts to behave resuscitation on mannequins independently in adherence to approved guidelines, thereby devising an auxiliary toolbox to assist supervision and rectification of intermediate potential issues via modern deep learning methodologies. Our research empirically views this problem as a temporal action segmentation (TAS) task in computer vision, which aims to segment an untrimmed video at a frame-wise level. Here, we propose a Prompt-enhanced hierarchical Transformer (PhiTrans) that integrates three indispensable modules, including a textual prompt-based Video Features Extractor (VFE), a transformer-based Action Segmentation Executor (ASE), and a regression-based Prediction Refinement Calibrator (PRC). The backbone of the model preferentially derives from applications in three approved public datasets (GTEA, 50Salads, and Breakfast) collected for TAS tasks, which accounts for the excavation of the segmentation pipeline on the CPR dataset. In general, we unprecedentedly probe into a feasible pipeline that genuinely elevates the CPR instruction qualification via action segmentation in conjunction with cutting-edge deep learning techniques. Associated experiments advocate our implementation with multiple metrics surpassing 91.0%.
PB  - arXiv
PY  - 2023
ST  - Prompt-enhanced Hierarchical Transformer Elevating Cardiopulmonary Resuscitation Instruction via Temporal Action Segmentation
Y2  - 2025/05/05/21:54:31
DO  - 10.1016/j.compbiomed.2023.107672
ER  -


TY  - GEN
AU  - Li, J.
AU  - Wang, J.
AU  - Wang, H.
AU  - Shi, F.
AU  - Liu, H.
TI  - Fragment and Integrate Network (FIN): A Novel Spatial-Temporal Modeling Based on Long Sequential Behavior for Online Food Ordering Click-Through Rate Prediction
AB  - Spatial-temporal information has been proven to be of great significance for click-through rate prediction tasks in online Location-Based Services (LBS), especially in mainstream food ordering platforms such as DoorDash, Uber Eats, Meituan, and Ele.me. Modeling user spatial-temporal preferences with sequential behavior data has become a hot topic in recommendation systems and online advertising. However, most of existing methods either lack the representation of rich spatial-temporal information or only handle user behaviors with limited length, e.g. 100. In this paper, we tackle these problems by designing a new spatial-temporal modeling paradigm named Fragment and Integrate Network (FIN). FIN consists of two networks: (i) Fragment Network (FN) extracts Multiple Sub-Sequences (MSS) from lifelong sequential behavior data, and captures the specific spatial-temporal representation by modeling each MSS respectively. Here both a simplified attention and a complicated attention are adopted to balance the performance gain and resource consumption. (ii) Integrate Network (IN) builds a new integrated sequence by utilizing spatial-temporal interaction on MSS and captures the comprehensive spatial-temporal representation by modeling the integrated sequence with a complicated attention. Both public datasets and production datasets have demonstrated the accuracy and scalability of FIN. Since 2022, FIN has been fully deployed in the recommendation advertising system of Ele.me, one of the most popular online food ordering platforms in China, obtaining 5.7% improvement on Click-Through Rate (CTR) and 7.3% increase on Revenue Per Mille (RPM).
PB  - arXiv
PY  - 2023
ST  - Fragment and Integrate Network (FIN)
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3583780.3615478
ER  -


TY  - GEN
AU  - Chen, X.
AU  - Li, Z.
AU  - Pan, W.
AU  - Ming, Z.
TI  - A Survey on Multi-Behavior Sequential Recommendation
AB  - People usually have the explicit or implicit desire to get the information they need and are most interested in from massive information, which has led to the creation of personalized recommender systems. Recommender systems are set up to address the issue of information overload in traditional information retrieval systems such as search engines, and have been a significant area of research focused on recommending information that is of most interest to users. There is a sequential nature to the behavior of a person interacting with a system, such as examining one item of clothing before examining others. The problem of taking this sequential nature into account in delivering recommendation is known as sequential recommendation (SR). The traditional sequential recommendation problem merely takes into account a single type of behavior of the users, while in real scenarios users tend to engage in multiple types of behaviors, such as examining and adding clothes to cart before purchasing them, leading to the proposal of multi-behavior sequential recommendation (MBSR). MBSR considers both sequentiality and heterogeneity of user behaviors, which can achieve state-of-the-art recommendation through suitable modeling. Hence, MBSR is a relatively new and worthy direction for in-depth research, for which some related works have been proposed. This survey aims to shed light on the MBSR problem. Firstly, we introduce MBSR in detail, including its problem definition, application scenarios and challenges faced. Secondly, we detail the classification of MBSR, including neighborhood-based methods, matrix factorization-based methods and deep learning-based methods, where we further classify the deep learning-based methods into different learning architectures based on RNN, GNN, Transformer, and generic architectures as well as architectures that integrate hybrid techniques. In each method, we present related works based on the data perspective and the modeling perspective, as well as analyze the strengths, weaknesses and features of these works. Finally, we discuss some promising future research directions to address the challenges and improve the current status of MBSR.
PB  - arXiv
PY  - 2023
ST  - A Survey on Multi-Behavior Sequential Recommendation
Y2  - 2025/05/05/21:54:31
DO  - 10.1145/3581783.3611723
ER  -


TY  - GEN
AU  - Chiba, K.
TI  - Spatiotemporal variations in seismic activity in and around the focal region of the 2021 M7.3 and 2022 M7.4 Fukushima-Oki earthquakes, Japan
AB  - The spatiotemporal evolution of seismic activity is presented for a broad region surrounding the focal areas of the 2021 M 7.3 and 2022 M 7.4 Fukushima-Oki earthquakes, which occurred within the subducting slab off the Pacific coast of Fukushima Prefecture, northeastern Japan. This study investigates the spatiotemporal variations in seismic activity during the periods before the 2021 M 7.3 earthquake, between the 2021 M 7.3 and 2022 M 7.4 earthquakes, and after the 2022 M 7.4 earthquake using the b -value of the Gutenberg–Richter relation, the aftershock decay rate (p -value), and changes in the seismicity rate (Z -value). The study area is also divided into two depth sections to investigate the depth variations in these seismicity parameters relative to the plate interface. The b -values in the deeper section (intraslab) are generally lower than those in the shallower section (around the plate interface) throughout the entire analysis period, including the hypocentral areas of the M 7.3 and M 7.4 earthquakes. The aftershock decay rates for the M 7.3 and M 7.4 earthquakes also show depth-dependent characteristics, with a slower decay rate (p < 1.0) at many grid nodes in the deeper section than in the shallower section. Furthermore, seismic quiescence was noted in the hypocentral area of the M 7.3 earthquake about two years before the occurrence of this mainshock. The locations of the M 7.3 and M 7.4 earthquakes around the down-dip edge of the slip area of the 2011 M 9.0 Tohoku earthquake suggests that the variations in seismic activity detected in this study mainly reflect stress increases due to the coseismic slip and postseismic deformation of this great earthquake. The present study suggests that the effect of viscoelastic relaxation is a dominant factor in the deeper section. Furthermore, the variations in seismicity may also reflect heterogeneous structures within the slab.
PB  - Research Square
PY  - 2023
ST  - Spatiotemporal variations in seismic activity in and around the focal region of the 2021 M7.3 and 2022 M7.4 Fukushima-Oki earthquakes, Japan
Y2  - 2025/05/05/21:54:31
DO  - 10.2139/ssrn.4463908
ER  -


TY  - GEN
AU  - Lu, Z.
AU  - Elhamifar, E.
TI  - BIT: Bi-Level Temporal Modeling for Efficient Supervised Action Segmentation
AB  - We address the task of supervised action segmentation which aims to partition a video into non-overlapping segments, each representing a different action. Recent works apply transformers to perform temporal modeling at the frame-level, which suffer from high computational cost and cannot well capture action dependencies over long temporal horizons. To address these issues, we propose an efficient BI-level Temporal modeling (BIT) framework that learns explicit action tokens to represent action segments, in parallel performs temporal modeling on frame and action levels, while maintaining a low computational cost. Our model contains (i) a frame branch that uses convolution to learn frame-level relationships, (ii) an action branch that uses transformer to learn action-level dependencies with a small set of action tokens and (iii) cross-attentions to allow communication between the two branches. We apply and extend a set-prediction objective to allow each action token to represent one or multiple action segments, thus can avoid learning a large number of tokens over long videos with many segments. Thanks to the design of our action branch, we can also seamlessly leverage textual transcripts of videos (when available) to help action segmentation by using them to initialize the action tokens. We evaluate our model on four video datasets (two egocentric and two third-person) for action segmentation with and without transcripts, showing that BIT significantly improves the state-of-the-art accuracy with much lower computational cost (30 times faster) compared to existing transformer-based methods.
PB  - arXiv
PY  - 2023
ST  - BIT
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/cvpr52733.2024.01721
ER  -


TY  - GEN
AU  - Yosry, S.
AU  - Elrefaei, L.
AU  - Ziedan, R.R.
TI  - Various frameworks for integrating image and video streams for spatiotemporal information learning employing 2D-3D Residual networks for human action recognition
AB  - Human action recognition has been identified as an important research topic in computer vision because it is an essential form of communication and interplay between computers and humans. To assist computers in automatically recognizing human behaviors and accurately comprehending human intentions. Inspired by some keyframe extraction and multifeatured fusion research, this paper improved the accuracy of action recognition by utilizing keyframe features and fusing them with video features. In this article, we suggest a novel multi-stream approach architecture made up of two distinct models fused using different fusion techniques. The first model combines convolutional neural networks in two dimensions (2D-CNN) with Long-Short Term Memory (LSTM) networks to glean long-term spatial and temporal features from video keyframe images for human action recognition. The second model is a 3-dimensional convolutional neural network (3D-CNN) that gathers quick spatial-temporal features from video clips. Next, we use Early and Late Fusion techniques for the two different models to recognize human action from video. The HMDB-51 and UCF-101 datasets, two important action recognition benchmarks, were used to test our method. When applied to the HMDB-51 dataset and the UCF-101 dataset, the Early-Fusion (EF) strategy had an accuracy of 70.2 % and 95.5 %, respectively, while the Late-Fusion (LF) strategy had an accuracy of 77.2 % and 97.5 %, respectively.
PB  - Research Square
PY  - 2023
ST  - Various frameworks for integrating image and video streams for spatiotemporal information learning employing 2D-3D Residual networks for human action recognition
Y2  - 2025/05/05/21:54:31
DO  - 10.1007/s42452-024-05774-9
ER  -


TY  - GEN
AU  - Cone, J.J.
AU  - Mitchell, A.O.
AU  - Parker, R.K.
AU  - Maunsell, J.H.R.
TI  - Temporal weighting of cortical and subcortical spikes reveals stimulus dependent differences in their contributions to behavior.
AB  - The primary visual cortex (V1) and the superior colliculus (SC) both occupy stations early in the processing of visual information. They have long been thought to perform distinct functions, with V1 supporting perception of visual features and the SC regulating orienting to visual inputs. However, growing evidence suggests that the SC supports perception of many of the same visual features traditionally associated with V1. To distinguish V1 and SC contributions to visual processing, it is critical to determine whether both areas causally contribute to perception of specific visual stimuli. Here, mice reported changes in visual contrast or luminance near perceptual threshold while we presented white noise patterns of optogenetic stimulation to V1 or SC inhibitory neurons. We then performed a reverse correlation analysis on the optogenetic stimuli to estimate a neuronal-behavioral kernel (NBK), a moment-to-moment estimate of the impact of V1 or SC inhibition on stimulus detection. We show that the earliest moments of stimulus-evoked activity in SC are critical for detection of both luminance or contrast changes. Strikingly, there was a robust stimulus-aligned modulation in the V1 contrast-detection NBK, but no sign of a comparable modulation for luminance detection. The data suggest that perception of visual contrast depends on both V1 and SC spiking, whereas mice preferentially use SC activity to detect changes in luminance. Electrophysiological recordings showed that neurons in both SC and V1 responded strongly to both visual stimulus types, while the reverse correlation analysis reveals when these neuronal signals actually contribute to visually-guided behaviors.
PB  - bioRxiv
PY  - 2023
ST  - Temporal weighting of cortical and subcortical spikes reveals stimulus dependent differences in their contributions to behavior.
Y2  - 2025/05/05/21:54:31
DO  - 10.1101/2023.08.23.554473
ER  -


TY  - GEN
AU  - Zhang, H.
AU  - Wang, X.
AU  - Xu, X.
AU  - Gao, C.
AU  - Sang, N.
TI  - HR-Pro: Point-supervised Temporal Action Localization via Hierarchical Reliability Propagation
AB  - Point-supervised Temporal Action Localization (PSTAL) is an emerging research direction for label-efficient learning. However, current methods mainly focus on optimizing the network either at the snippet-level or the instance-level, neglecting the inherent reliability of point annotations at both levels. In this paper, we propose a Hierarchical Reliability Propagation (HR-Pro) framework, which consists of two reliability-aware stages: Snippet-level Discrimination Learning and Instance-level Completeness Learning, both stages explore the efficient propagation of high-confidence cues in point annotations. For snippet-level learning, we introduce an online-updated memory to store reliable snippet prototypes for each class. We then employ a Reliability-aware Attention Block to capture both intra-video and inter-video dependencies of snippets, resulting in more discriminative and robust snippet representation. For instance-level learning, we propose a point-based proposal generation approach as a means of connecting snippets and instances, which produces high-confidence proposals for further optimization at the instance level. Through multi-level reliability-aware learning, we obtain more reliable confidence scores and more accurate temporal boundaries of predicted proposals. Our HR-Pro achieves state-of-the-art performance on multiple challenging benchmarks, including an impressive average mAP of 60.3% on THUMOS14. Notably, our HR-Pro largely surpasses all previous point-supervised methods, and even outperforms several competitive fully-supervised methods. Code will be available at https://github.com/pipixin321/HR-Pro.
PB  - arXiv
PY  - 2023
ST  - HR-Pro
Y2  - 2025/05/05/21:54:31
DO  - 10.1609/aaai.v38i7.28539
ER  -


TY  - GEN
AU  - Zhang, S.
AU  - Zhao, C.
TI  - Cross-Video Contextual Knowledge Exploration and Exploitation for Ambiguity Reduction in Weakly Supervised Temporal Action Localization
AB  - Weakly supervised temporal action localization (WSTAL) aims to localize actions in untrimmed videos using video-level labels. Despite recent advances, existing approaches mainly follow a localization-by-classification pipeline, generally processing each segment individually, thereby exploiting only limited contextual information. As a result, the model will lack a comprehensive understanding (e.g. appearance and temporal structure) of various action patterns, leading to ambiguity in classification learning and temporal localization. Our work addresses this from a novel perspective, by exploring and exploiting the cross-video contextual knowledge within the dataset to recover the dataset-level semantic structure of action instances via weak labels only, thereby indirectly improving the holistic understanding of fine-grained action patterns and alleviating the aforementioned ambiguities. Specifically, an end-to-end framework is proposed, including a Robust Memory-Guided Contrastive Learning (RMGCL) module and a Global Knowledge Summarization and Aggregation (GKSA) module. First, the RMGCL module explores the contrast and consistency of cross-video action features, assisting in learning more structured and compact embedding space, thus reducing ambiguity in classification learning. Further, the GKSA module is used to efficiently summarize and propagate the cross-video representative action knowledge in a learnable manner to promote holistic action patterns understanding, which in turn allows the generation of high-confidence pseudo-labels for self-learning, thus alleviating ambiguity in temporal localization. Extensive experiments on THUMOS14, ActivityNet1.3, and FineAction demonstrate that our method outperforms the state-of-the-art methods, and can be easily plugged into other WSTAL methods.
PB  - arXiv
PY  - 2023
ST  - Cross-Video Contextual Knowledge Exploration and Exploitation for Ambiguity Reduction in Weakly Supervised Temporal Action Localization
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/tcsvt.2023.3341881
ER  -


TY  - GEN
AU  - Warchocki, J.
AU  - Oprescu, T.
AU  - Wang, Y.
AU  - Strafforello, O.
AU  - van Gemert, J.
TI  - Benchmarking Data Efficiency and Computational Efficiency of Temporal Action Localization Models
AB  - In temporal action localization, given an input video, the goal is to predict which actions it contains, where they begin, and where they end. Training and testing current state-of-the-art deep learning models requires access to large amounts of data and computational power. However, gathering such data is challenging and computational resources might be limited. This work explores and measures how current deep temporal action localization models perform in settings constrained by the amount of data or computational power. We measure data efficiency by training each model on a subset of the training set. We find that TemporalMaxer outperforms other models in data-limited settings. Furthermore, we recommend TriDet when training time is limited. To test the efficiency of the models during inference, we pass videos of different lengths through each model. We find that TemporalMaxer requires the least computational resources, likely due to its simple architecture.
PB  - arXiv
PY  - 2023
ST  - Benchmarking Data Efficiency and Computational Efficiency of Temporal Action Localization Models
Y2  - 2025/05/05/21:54:31
DO  - 10.1109/iccvw60793.2023.00323
ER  -


TY  - GEN
AU  - Deng, W.
AU  - Lv, X.
AU  - Ni, Y.
AU  - Zhang, H.
AU  - Wen, X.
TI  - The Spatiotemporal Variation Trends of Vegetation in the Source Region of the Yellow River and Their Responses to Climate Change and Human Activities
AB  - The dynamic changes in vegetation significantly impact the safety and stability of the ecosystem in the source region of the Yellow River. However, the spatiotemporal patterns and driving factors of these changes remain unclear. Therefore, this study investigates the spatiotemporal characteristics of vegetation coverage and its relationship with climatic and anthropogenic drivers over the past 21 years (1998–2018) based on MODIS NDVI datasets, climatic data, and socio-economic data. The results indicate that: (1) From 1998 to 2018, vegetation in the source region of the Yellow River generally exhibited an increasing trend, with 92.72% of the area showing improvement in vegetation growth, while only 7.26% of the area experienced degradation. (2) Seasonal differences in vegetation trends were observed, with significant increases in spring, summer, and winter, but a non-significant decrease in autumn. Spatially, vegetation degradation in summer and autumn cannot be overlooked, affecting approximately one-quarter of the total area. (3) Strong correlations between vegetation dynamics and climatic variables, suggesting that temperature elevation and precipitation increase serve as primary drivers for vegetation improvement. (4) Anthropogenic factors, particularly overgrazing and rapid population growth (both human and livestock), were identified as major contributors to the degradation of low-altitude alpine grasslands during summer and autumn periods. These findings offer valuable insights for ecological conservation strategies and restoration practices in high-altitude alpine ecosystems.
PB  - SSRN
PY  - 2025
ST  - The Spatiotemporal Variation Trends of Vegetation in the Source Region of the Yellow River and Their Responses to Climate Change and Human Activities
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5210882
ER  -


TY  - GEN
AU  - Benmessabih, T.
AU  - Slama, R.
AU  - Havard, V.
AU  - Baudry, D.
TI  - Graph-Based Framework for Temporal Human Action Recognition and Segmentation in Industrial Context
AB  - Industry 5.0 places human operators at the center of industrial processes. In this context, analyzing human movements during work tasks has become crucial for a range of goals, from ensuring operator safety to improving productivity. More specifically, an accurate system for action recognition and segmentation is essential to identify and break down each action an operator performs. However, existing action segmentation systems face several challenges that interfere with their deployment in real industrial environments, such as the complexity of industrial tasks and the similarity of assembly gestures. To address this issue, this work presents a system for accurately segmenting and recognizing operator actions in an industrial environment. The proposed system is specifically designed for industrial assembly settings, leveraging graph embeddings to better represent operator posture and extract meaningful spatial and temporal features from 3D skeletons. It employs an encoder-decoder architecture, enhanced with an attention mechanism and an aggregation block, to effectively model spatio-temporal actions. Additionally, it integrates outputs from the decoder at multiple temporal resolutions, ensuring precise detection of action segment start and end frames. The proposed approach was validated using two industrial datasets, InHARD and HA4M, where it demonstrated strong performances. Furthermore, extensive experiments and ablation studies were conducted to provide deeper insights into its strengths. Visualizations are presented to illustrate how the proposed method addresses complex assembly video segmentation while also revealing specific exceptions that highlight areas for improvement. The code and pre-trained models will be made available upon acceptance in this repository.
PB  - SSRN
PY  - 2025
ST  - Graph-Based Framework for Temporal Human Action Recognition and Segmentation in Industrial Context
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5208198
ER  -


TY  - GEN
AU  - Cao, Y.
AU  - Dang, C.
TI  - A Characterization of Nash Equilibrium in Behavioral Strategies through an Extra Strategy Profile, Local Sequential Rationality, and Self-Independent Beliefs
AB  - The concept of Nash equilibrium in behavioral strategies (NashEBS) was formulated By Nash [19] for an extensive-form game through global rationality of nonconvex payoff functions. Kuhn’s payoff equivalence theorem resolves the nonconvexity issue, but it overlooks that one Nash equilibrium of the associated normal-form game can correspond to infinitely many NashEBSs of an extensive-form game. To remedy this multiplicity, the traditional approach as documented in Myerson [18] involves a two-step process: identifying a Nash equilibrium of the agent normal-form representation, followed by verifying whether the corresponding mixed strategy profile is a Nash equilibrium of the associated normal-form game, which often scales exponentially with the size of the extensive-form game tree. In response to these challenges, this paper develops a characterization of NashEBS through the incorporation of an extra behavioral strategy profile and beliefs, which meet local sequential rationality of linear payoff functions and self-independent consistency. This characterization allows one to analytically determine all NashEBSs for small extensive-form games. Building upon this characterization, we acquire a polynomial system serving as a necessary and sufficient condition for determining whether a behavioral strategy profile is a NashEBS. An application of the characterization yields differentiable path-following methods for computing such an equilibrium.
PB  - arXiv
PY  - 2025
ST  - A Characterization of Nash Equilibrium in Behavioral Strategies through an Extra Strategy Profile, Local Sequential Rationality, and Self-Independent Beliefs
Y2  - 2025/05/05/21:54:32
DO  - 10.1142/9789814525053_0007
ER  -


TY  - GEN
AU  - Augustine, F.
AU  - Doss, S.M.
AU  - Lee, R.M.
AU  - Singer, H.S.
TI  - YOLOv11-Based Quantification and Temporal Analysis of Repetitive Behaviors in Deer Mice
AB  - The quantification of animal behaviors from video recordings is often a labor-intensive process subject to inter-observer variability. This study presents an automated deep-learning model (YOLOv11) for identifying and quantifying several behaviors (Exploration, Grooming, Rearing, Wall-Rearing, and Jumping) in deer mice (Peromyscus maniculatus bairdii). The YOLOv11 system was trained to detect whole-animal behaviors using bounding box annotations, without requiring detailed body part positions. A computer-aided behavioral analysis approach also provided information on the temporal dynamics and sequential organization of behavior. This methodology includes the analysis of episode durations, transition probabilities, time-lagged cross-correlations, Granger causality, rhythmic patterns (using peak detection and Fast Fourier Transform analysis), Hidden Markov Models (HMMs), N-gram analyses, and sequence mining. HMMs identified underlying behavioral profiles, and sequence mining identified specific behaviors that could improve predictions of mouse behavior. In conclusion, the YOLOv11-based movement detection methodology and computerized analysis offer an efficient and reliable system for studying repetitive behaviors in deer mice.
PB  - SSRN
PY  - 2025
ST  - YOLOv11-Based Quantification and Temporal Analysis of Repetitive Behaviors in Deer Mice
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5181897
ER  -


TY  - GEN
AU  - Fenwick, J.
AU  - Chappell, K.
AU  - Wong, H.
TI  - A Spatiotemporal Analysis of Night Work Activity Patterns Using Journey Purpose Inferred from Longitudinal Smartcard Data
AB  - Automatic fare collection (AFC) systems using smartcards or contactless methods are now commonplace in public transport (PT) systems worldwide, facilitating revenue collection and generating rich spatiotemporal data. PT agencies can exploit this data to optimise service planning.This study proposes a generalised framework to infer home and work locations and journey purposes from AFC data, utilising an activity-centred heuristic method. We propose a novel approach to cluster proximate stops based on individual usage patterns and, critically, the model is agnostic to the day-of-week and time-of-day of journeys. This facilitates the inference of shift-working activities and reveals insights about the transport needs and patterns of different passenger groups.Applied to London's multi-modal PT system over a four-week period, we focus on night working activities, discerning the spatiotemporal patterns to highlight the different types of night work in the city, where research has often focused solely on night life and leisure activities. We identify short, medium and long night shifts, with longer shifts seen in areas close to hospitals and shorter shifts in hospitality and leisure zones, noting the variety in patterns across the city.The contributions of this study are twofold. First, it delivers a comprehensive framework that processes longitudinal AFC data and enhances journey purpose inference. Second, as an application, it identifies night workers and the spatiotemporal features of night shifts. By centring the detailed mobility patterns of less prominent users, this study facilitates more inclusive PT service provision and transport policymaking.
PB  - SSRN
PY  - 2025
ST  - A Spatiotemporal Analysis of Night Work Activity Patterns Using Journey Purpose Inferred from Longitudinal Smartcard Data
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5197714
ER  -


TY  - GEN
AU  - Pang, Z.
AU  - Sener, F.
AU  - Ramasubramanian, S.
AU  - Yao, A.
TI  - Cost-Sensitive Learning for Long-Tailed Temporal Action Segmentation
AB  - Temporal action segmentation in untrimmed procedural videos aims to densely label frames into action classes. These videos inherently exhibit long-tailed distributions, where actions vary widely in frequency and duration. In temporal action segmentation approaches, we identified a bi-level learning bias. This bias encompasses (1) a class-level bias, stemming from class imbalance favoring head classes, and (2) a transition-level bias arising from variations in transitions, prioritizing commonly observed transitions. As a remedy, we introduce a constrained optimization problem to alleviate both biases. We define learning states for action classes and their associated transitions and integrate them into the optimization process. We propose a novel cost-sensitive loss function formulated as a weighted cross-entropy loss, with weights adaptively adjusted based on the learning state of actions and their transitions. Experiments on three challenging temporal segmentation benchmarks and various frameworks demonstrate the effectiveness of our approach, resulting in significant improvements in both per-class frame-wise and segment-wise performance. Code is availabel at https://github.com/pangzhan27/CSL_LT-TAS.
PB  - arXiv
PY  - 2025
ST  - Cost-Sensitive Learning for Long-Tailed Temporal Action Segmentation
Y2  - 2025/05/05/21:54:32
DO  - 10.3390/rs14143295
ER  -


TY  - GEN
AU  - Chen, Z.
AU  - Wu, H.
AU  - Kung, C.-H.
AU  - Chen, Y.-T.
AU  - Peng, Y.-T.
TI  - ATARS: An Aerial Traffic Atomic Activity Recognition and Temporal Segmentation Dataset
AB  - Traffic Atomic Activity which describes traffic patterns for topological intersection dynamics is a crucial topic for the advancement of intelligent driving systems. However, existing atomic activity datasets are collected from an egocentric view, which cannot support the scenarios where traffic activities in an entire intersection are required. Moreover, existing datasets only provide video-level atomic activity annotations, which require exhausting efforts to manually trim the videos for recognition and limit their applications to untrimmed videos. To bridge this gap, we introduce the Aerial Traffic Atomic Activity Recognition and Segmentation (ATARS) dataset, the first aerial dataset designed for multi-label atomic activity analysis. We offer atomic activity labels for each frame, which accurately record the intervals for traffic activities. Moreover, we propose a novel task, Multi-label Temporal Atomic Activity Recognition, enabling the study of accurate temporal localization for atomic activity and easing the burden of manual video trimming for recognition. We conduct extensive experiments to evaluate existing state-of-the-art models on both atomic activity recognition and temporal atomic activity segmentation. The results highlight the unique challenges of our ATARS dataset, such as recognizing extremely small objects’ activities. We further provide comprehensive discussion analyzing these challenges and offer valuable insights for future direction to improve recognizing atomic activity in aerial view. Our source code and dataset are available at https://github.com/magecliff96/ATARS/.
PB  - arXiv
PY  - 2025
ST  - ATARS
Y2  - 2025/05/05/21:54:32
DO  - 10.1145/3369818
ER  -


TY  - GEN
AU  - Chen, X.
AU  - Guo, Y.
AU  - Liang, J.
AU  - Zeng, R.
AU  - Hu, X.
TI  - Temporal Action Detection Model Compression by Progressive Block Drop
AB  - Temporal action detection (TAD) aims to identify and localize action instances in untrimmed videos, which is essential for various video understanding tasks. However, recent improvements in model performance, driven by larger feature extractors and datasets, have led to increased computational demands. This presents a challenge for applications like autonomous driving and robotics, which rely on limited computational resources. While existing channel pruning methods can compress these models, reducing the number of channels often hinders the parallelization efficiency of GPU, due to the inefficient multiplication between small matrices. Instead of pruning channels, we propose a Progressive Block Drop method that reduces model depth while retaining layer width. In this way, we still use large matrices for computation but reduce the number of multiplications. Our approach iteratively removes redundant blocks in two steps: first, we drop blocks with minimal impact on model performance; and second, we employ a parameter-efficient cross-depth alignment technique, fine-tuning the pruned model to restore model accuracy. Our method achieves a 25% reduction in computational overhead on two TAD benchmarks (THUMOS14 and ActivityNet-1.3) to achieve lossless compression. More critically, we empirically show that our method is orthogonal to channel pruning methods and can be combined with it to yield further efficiency gains.
PB  - arXiv
PY  - 2025
ST  - Temporal Action Detection Model Compression by Progressive Block Drop
Y2  - 2025/05/05/21:54:32
DO  - 10.1609/aaai.v34i07.6829
ER  -


TY  - GEN
AU  - Seikavandi, M.J.
AU  - Fimland, J.
AU  - Barrett, M.J.
AU  - Burelli, P.
TI  - Exploring the Temporal Dynamics of Facial Mimicry in Emotion Processing Using Action Units
AB  - Facial mimicry—the automatic, unconscious imitation of others’ expressions—is vital for emotional understanding. This study investigates how mimicry differs across emotions using Face Action Units from videos and participants’ responses. Dynamic Time Warping quantified the temporal alignment between participants’ and stimuli’s facial expressions, revealing significant emotional variations. Post-hoc tests indicated greater mimicry for’Fear’ than’Happy’ and reduced mimicry for’Anger’ compared to’Fear’. The mimicry correlations with personality traits like Extraversion and Agreeableness were significant, showcasing subtle yet meaningful connections. These findings suggest specific emotions evoke stronger mimicry, with personality traits playing a secondary role in emotional alignment. Notably, our results highlight how personality-linked mimicry mechanisms extend beyond interpersonal communication to affective computing applications, such as remote human-human interactions and human-virtual-agent scenarios. Insights from temporal facial mimicry—e.g., designing digital agents that adaptively mirror user expressions—enable developers to create empathetic, personalized systems, enhancing emotional resonance and user engagement.
PB  - arXiv
PY  - 2025
ST  - Exploring the Temporal Dynamics of Facial Mimicry in Emotion Processing Using Action Units
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/icces48766.2020.9137919
ER  -


TY  - GEN
AU  - Yang, S.
AU  - Lu, S.
AU  - Wang, S.
AU  - Zheng, Z.
AU  - Kot, A.C.
TI  - Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition
AB  - This paper explores the promising interplay between spiking neural networks (SNNs) and event-based cameras for privacy-preserving human action recognition (HAR). The unique feature of event cameras in capturing only the outlines of motion, combined with SNNs’ proficiency in processing spatiotemporal data through spikes, establishes a highly synergistic compatibility for event-based HAR. Previous studies, however, have been limited by SNNs’ ability to process long-term temporal information, essential for precise HAR. In this paper, we introduce two novel frameworks to address this: temporal segment-based SNN (TS-SNN) and 3D convolutional SNN (3D-SNN). The TS-SNN extracts long-term temporal information by dividing actions into shorter segments, while the 3D-SNN replaces 2D spatial elements with 3D components to facilitate the transmission of temporal information. To promote further research in event-based HAR, we create a dataset, FallingDetection-CeleX, collected using the high-resolution CeleX-V event camera (1280 × 800), comprising 7 distinct actions. Extensive experimental results show that our proposed frameworks surpass state-of-the-art SNN methods on our newly collected dataset and three other neuromorphic datasets, showcasing their effectiveness in handling long-range temporal information for event-based HAR.
PB  - arXiv
PY  - 2025
ST  - Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition
Y2  - 2025/05/05/21:54:32
DO  - 10.24963/ijcai.2021/240
ER  -


TY  - GEN
AU  - Bae, K.
AU  - Kim, J.
AU  - Lee, S.
AU  - Lee, G.
AU  - Choi, J.
TI  - MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through Disentangled Spatial-Temporal Representations
AB  - In this work, we tackle action-scene hallucination in Video Large Language Models (Video-LLMs), where models incorrectly predict actions based on the scene context or scenes based on observed actions. We observe that existing Video-LLMs often suffer from action-scene hallucination due to two main factors. First, existing Video-LLMs intermingle spatial and temporal features by applying an attention operation across all tokens. Second, they use standard Rotary Position Embedding (RoPE), which causes the text tokens to overemphasize certain types of tokens depending on their sequential orders. To address these issues, we introduce MASH-VLM, Mitigating Action-Scene Hallucination in Video-LLMs through disentangled spatial-temporal representations. Our approach includes two key innovations: (1) DST-attention, a novel attention mechanism that disentangles spatial and temporal tokens within the LLM by using masked attention to restrict direct interactions between spatial and temporal tokens; (2) Harmonic-RoPE, which extends the dimensionality of the positional IDs, allowing spatial and temporal tokens to maintain balanced positions relative to the text tokens. To evaluate the action-scene hallucination in Video-LLMs, we introduce the UNSCENE benchmark with 1,320 videos and 4,078 QA pairs. MASH-VLM achieves state-of-the-art performance on the UNSCENE benchmark, as well as on existing video understanding benchmarks.
PB  - arXiv
PY  - 2025
ST  - MASH-VLM
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-031-73113-6_25
ER  -


TY  - GEN
AU  - Wang, J.-X.
AU  - Qin, K.-N.
AU  - Gan, X.
AU  - Zhang, S.-J.
AU  - Jin, Z.
TI  - The Longitudinal Relationship between Parental Conflict and Adolescent Antisocial Behavior: The Sequential Mediation Role of Hostile Cognition and Negative Emotion
AB  - Grounded in the ecological systems theory, this study examines the relationship between parental conflict and adolescent antisocial behavior, focusing on the underlying mechanisms. Using the Parental Conflict Scale, Hostile Cognition Scale, Negative emotion Scale, and Antisocial Behavior Scale, we conducted a six-month follow-up study with 443 adolescents (206 males, 46.5%; 237 females, 53.5%; mean age = 15.84 years). A sequential mediation model was employed to analyze the relationship between parental conflict and antisocial behavior, as well as the mediating roles of hostile cognition and negative emotion. The results indicate that parental conflict positively predicts antisocial behavior, with adolescents’ hostile cognition and negative emotion acting as significant mediators. In summary, higher levels of hostile cognition increase negative emotion, which indirectly lead to antisocial behaviors. Therefore, parents should take proactive steps to minimize conflicts and provide a supportive environment that fosters adolescent well-being, aiming to prevent or reduce negative emotion and antisocial behaviors.
PB  - SSRN
PY  - 2025
ST  - The Longitudinal Relationship between Parental Conflict and Adolescent Antisocial Behavior
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5183926
ER  -


TY  - GEN
AU  - Potter, M.
AU  - Everett, M.
AU  - Singh, A.
AU  - Imbiriba, T.
AU  - Goodwin, M.S.
TI  - Temporal Point Process Modeling of Aggressive Behavior Onset in Psychiatric Inpatient Youths with Autism
AB  - Aggressive behavior, including aggression towards others and self-injury, occurs in up to 80% of children and adolescents with autism, making it a leading cause of behavioral health referrals and a major driver of healthcare costs. Predicting when autistic youth will exhibit aggression is challenging due to their communication difficulties. Many are minimally verbal or have poor emotional insight. Recent advances in Machine Learning and wearable biosensing enable short-term aggression predictions within a limited future window (typically one to three minutes). However, existing models do not estimate aggression probability within longer future windows nor the expected number of aggression onsets over such a period. To address these limitations, we employ Temporal Point Processs (TPPs) to model the generative process of aggressive behavior onsets in inpatient youths with autism. We hypothesize that aggressive behavior onsets follow a self-exciting process driven by short-term history, making them well-suited for Hawkes Point Process modeling. We establish a benchmark and demonstrate through Goodness-of-Fit statistics and predictive metrics that TPPs perform well modeling aggressive behavior onsets in inpatient youths with autism. Additionally, we gain insights into the onset generative process, like the branching factor near criticality, and suggest TPPs may enhance future clinical decision-making and preemptive interventions.
PB  - arXiv
PY  - 2025
ST  - Temporal Point Process Modeling of Aggressive Behavior Onset in Psychiatric Inpatient Youths with Autism
Y2  - 2025/05/05/21:54:32
DO  - 10.1001/jamanetworkopen.2023.48898
ER  -


TY  - GEN
AU  - Omi, K.
AU  - Oshima, J.
AU  - Tamaki, T.
TI  - Action tube generation by person query matching for spatio-temporal action detection
AB  - This paper proposes a method for spatio-temporal action detection (STAD) that directly generates action tubes from the original video without relying on post-processing steps such as IoU-based linking and clip splitting. Our approach applies query-based detection (DETR) to each frame and matches DETR queries to link the same person across frames. We introduce the Query Matching Module (QMM), which uses metric learning to bring queries for the same person closer together across frames compared to queries for different people. Action classes are predicted using the sequence of queries obtained from QMM matching, allowing for variable-length inputs from videos longer than a single clip. Experimental results on JHMDB, UCF101-24, and AVA datasets demonstrate that our method performs well for large position changes of people while offering superior computational efficiency and lower resource requirements.
PB  - arXiv
PY  - 2025
ST  - Action tube generation by person query matching for spatio-temporal action detection
Y2  - 2025/05/05/21:54:32
DO  - 10.5220/0013089500003912
ER  -


TY  - GEN
AU  - Liang, Y.
AU  - Wang, S.
AU  - Yu, J.
AU  - Zhao, J.
AU  - Pentland, S.
TI  - Analyzing sequential activity and travel decisions with interpretable deep inverse reinforcement learning
AB  - Travel demand modeling has shifted from aggregated trip-based models to behavior-oriented activity-based models because daily trips are essentially driven by human activities. To analyze the sequential activity-travel decisions, deep inverse reinforcement learning (DIRL) has proven effective in learning the decision mechanisms by approximating a reward function to represent preferences and a policy function to replicate observed behavior using deep neural networks (DNNs). However, most existing research has focused on using DIRL to enhance only prediction accuracy, with limited exploration into interpreting the underlying decision mechanisms guiding sequential decision-making. To address this gap, we introduce an interpretable DIRL framework for analyzing activity-travel decision processes, bridging the gap between data-driven machine learning and theory-driven behavioral models. Our proposed framework adapts an adversarial IRL approach to infer the reward and policy functions of activity-travel behavior. The policy function is interpreted through a surrogate interpretable model based on choice probabilities from the policy function, while the reward function is interpreted by deriving both short-term rewards and long-term returns for various activity-travel patterns. Our analysis of real-world travel survey data reveals promising results in two key areas: (i) behavioral pattern insights from the policy function, highlighting critical factors in decision-making and variations among socio-demographic groups, and (ii) behavioral preference insights from the reward function, indicating the utility individuals gain from specific activity sequences.
PB  - arXiv
PY  - 2025
ST  - Analyzing sequential activity and travel decisions with interpretable deep inverse reinforcement learning
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/mdm.2013.28
ER  -


TY  - GEN
AU  - Aguilera, A.
AU  - Proulhac, L.
TI  - What Work Does to Mobility the Travel Behavior of Paris Region Residents in the Light of the Spatio-Temporal Patterns of Their Professional Activity
AB  - This article looks at the links between the spatio-temporal organization of work and the mobility practices of people working in the Paris region (France). It is based on data from the household travel survey conducted between 2018 and 2020 by transport authorities. Using an original typology that cross-references work locations and schedules during the survey day, a series of regressions analyzes differences in travel practices in terms of frequency, distance, schedule and use of transport modes. The results indicate that, compared to the configuration of working only at one's usual place and according to standard schedules, full-day teleworking reduces travel, distances (total and by car) and avoids rush hours, but does not necessarily reduce car use. Part-day teleworking reduces distances and rush-hour journeys, but not the number of trips made during the day. Having worked at several locations during the day increases travel frequency, distances and, in particular, distances covered by car, and is detrimental to active modes, public transport and multimodality practices based on these modes. Working staggered hours reduces private mobility and rush-hour travel. Finally, working from a usual location is more conducive to the use of alternatives to the car than any other spatio-temporal configuration of professional activity. Our results underline the importance of taking better account of the variety of spatial and temporal organization of work in transport and planning policies, in order to meet the objectives of decarbonizing mobility.
PB  - SSRN
PY  - 2025
ST  - What Work Does to Mobility the Travel Behavior of Paris Region Residents in the Light of the Spatio-Temporal Patterns of Their Professional Activity
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5179428
ER  -


TY  - GEN
AU  - Qi, F.
AU  - Hou, Z.
AU  - Lin, E.
AU  - Liang, J.
AU  - Zhou, X.
TI  - Pig behavior dataset and Spatial-temporal perception and enhancement networks based on the attention mechanism for pig behavior recognition
AB  - The recognition of pig behavior plays a crucial role in smart farming and welfare assurance for pigs. Currently, in the field of pig behavior recognition, the lack of publicly available behavioral datasets not only limits the development of innovative algorithms but also hampers model robustness and algorithm optimization.This paper proposes a dataset containing 13 pig behaviors that significantly impact welfare.Based on this dataset, this paper proposes a spatial-temporal perception and enhancement networks based on the attention mechanism to model the spatiotemporal features of pig behaviors and their associated interaction areas in video data. The network is composed of a spatiotemporal perception network and a spatiotemporal feature enhancement network. The spatiotemporal perception network is responsible for establishing connections between the pigs and the key regions of their behaviors in the video data. The spatiotemporal feature enhancement network further strengthens the important spatial features of individual pigs and captures the long-term dependencies of the spatiotemporal features of individual behaviors by remodeling these connections, thereby enhancing the model's perception of spatiotemporal changes in pig behaviors. Experimental results demonstrate that on the dataset established in this paper, our proposed model achieves a MAP score of 75.92%, which is an 8.17% improvement over the best-performing traditional model. This study not only improces the accuracy and generalizability of individual pig behavior recognition but also provides new technological tools for modern smart farming. The dataset and related code will be made publicly available alongside this paper.
PB  - arXiv
PY  - 2025
ST  - Pig behavior dataset and Spatial-temporal perception and enhancement networks based on the attention mechanism for pig behavior recognition
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5042825
ER  -


TY  - GEN
AU  - Qian, Z.
AU  - Zhang, C.
AU  - Huang, Y.
AU  - Wang, G.
AU  - Ying, J.
TI  - Joint Image-Instance Spatial-Temporal Attention for Few-shot Action Recognition
AB  - Few-shot Action Recognition (FSAR) constitutes a crucial challenge in computer vision, entailing the recognition of actions from a limited set of examples. Recent approaches mainly focus on employing image-level features to construct temporal dependencies and generate prototypes for each action category. However, a considerable number of these methods utilize mainly image-level features that incorporate background noise and focus insufficiently on real foreground (action-related instances), thereby compromising the recognition capability, particularly in the few-shot scenario. To tackle this issue, we propose a novel joint Image-Instance level Spatial-temporal attention approach (I2ST) for Few-shot Action Recognition. The core concept of I2ST is to perceive the action-related instances and integrate them with image features via spatial-temporal attention. Specifically, I2ST consists of two key components: Action-related Instance Perception and Joint Image-Instance Spatial-temporal Attention. Given the basic representations from the feature extractor, the Action-related Instance Perception is introduced to perceive action-related instances under the guidance of a text-guided segmentation model. Subsequently, the Joint Image-Instance Spatial-temporal Attention is used to construct the feature dependency between instances and images. To enhance the prototype representations of different categories of videos, a pair of spatial-temporal attention sub-modules is introduced to combine image features and instance embeddings across both temporal and spatial dimensions, and a global fusion sub-module is utilized to aggregate global contextual information, then robust action video prototypes can be formed. Finally, based on the video prototype, a Global-Local Prototype Matching is performed for reliable few-shot video matching. In this manner, our proposed I2ST can effectively exploit the foreground instance-level cues and model more accurate spatial-temporal relationships for the complex few-shot video recognition scenarios. Extensive experiments across standard few-shot benchmarks demonstrate that the proposed framework outperforms existing methods and achieves state-of-the-art performance under various few-shot settings.
PB  - arXiv
PY  - 2025
ST  - Joint Image-Instance Spatial-Temporal Attention for Few-shot Action Recognition
Y2  - 2025/05/05/21:54:32
DO  - 10.1016/j.cviu.2025.104322
ER  -


TY  - GEN
AU  - Li, J.
AU  - Feng, X.
AU  - Li, T.
AU  - Jing, W.
AU  - Cao, X.
TI  - Seasonal Dynamics of Hiking Activities in Mountainous Areas: A Topography-Aware Sequential Network Modeling Approach Using Crowdsourced Trajectories
AB  - The escalating popularity of hiking has intensified interactions between urban and wilderness landscapes, demanding spatially-explicit models to quantify trail usage dynamics and mitigate ecological pressures in mountainous regions. This study investigates spatiotemporal hiking patterns in China's Qinling Mountains (2020-2023) using crowdsourced GPS trajectories from outdoor sports Apps. We propose a novel topography-aware sequential network model that preserves spatial topological consistency across seasons, overcoming structural instability limitations in prior methods. By integrating terrain-adjusted mobility constraints and behavioral feature points, the model constructs dynamic networks to capture seasonal variations in hiker-environment interactions. Complex network analyses reveal hiking hotspots, core hubs and critical paths, reveal the popular routes in the network community, and reconstruct the community evolution process. Key findings include: (1) three elevation-specific activity zones (1,000m, 2,000m, 3,500m), (2) significant increase in short-distance hiking activities. (3) 62% of trails exhibiting single-season usage patterns, and (4) summer networks showing superior connectivity. The discussion highlights strategies to reconcile ecological conservation with hiking tourism development, including key path management, seasonal visitor flow regulation, and evidence-based transportation infrastructure planning. These findings offer actionable insights for sustainable tourism, contributing to GIS-driven frameworks that promote human-nature coexistence in mountain ecosystems.
PB  - SSRN
PY  - 2025
ST  - Seasonal Dynamics of Hiking Activities in Mountainous Areas
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5170551
ER  -


TY  - GEN
AU  - Lombardi, C.J.
AU  - Kosovichev, A.G.
AU  - Matsumoto, K.
TI  - Temporal Variations in Asteroseismic Frequencies of KIC 6106415: Insights into the Solar-Stellar Activity from GOLF and Kepler Observations
AB  - The Global Oscillations at Low Frequencies (GOLF) instrument aboard the Solar and Heliospheric Observatory (SOHO) mission has provided over two decades of continuous, high-precision data, enabling detailed measurements of the Sun’s oscillation frequencies. These oscillations, analyzed through Doppler velocity shifts, offer invaluable insights into the Sun’s internal structure and dynamics by applying methods of helioseismology. This methodology has been extended to the study of stars beyond the Sun, using data from various space missions. In particular, NASA’s Kepler mission, in operation from 2009 until 2018, observed over 500,000 stars, analyzing variations in brightness over time and creating a vast database of photometric data to study. This investigation focuses on the solar-type star KIC 6106415, comparing its oscillation frequencies with those derived from the GOLF data. By analyzing frequency patterns and mode lifetimes, we explore similarities and differences in internal structures, stellar evolution, and magnetic activity cycles between KIC 6106415 and the Sun. Our analysis reveals that KIC 6106415 exhibits starspot numbers similar to the Sun, peaking at an estimated 175, consistent with its faster rotation rate. The data suggest that KIC 6106415 may have shorter magnetic activity cycles than the Sun, reinforcing the link between stellar rotation and magnetic field generation in solar-type stars.
PB  - arXiv
PY  - 2025
ST  - Temporal Variations in Asteroseismic Frequencies of KIC 6106415
Y2  - 2025/05/05/21:54:32
DO  - 10.1051/0004-6361/201525968
ER  -


TY  - GEN
AU  - Chang, Q.
AU  - Dai, W.
AU  - Shuai, Z.
AU  - Yu, L.
AU  - Yue, Y.
TI  - Spatial-Temporal Perception with Causal Inference for Naturalistic Driving Action Recognition
AB  - Naturalistic driving action recognition is essential for vehicle cabin monitoring systems. However, the complexity of real-world backgrounds presents significant challenges for this task, and previous approaches have struggled with practical implementation due to their limited ability to observe subtle behavioral differences and effectively learn inter-frame features from video. In this paper, we propose a novel Spatial-Temporal Perception (STP) architecture that emphasizes both temporal information and spatial relationships between key objects, incorporating a causal decoder to perform behavior recognition and temporal action localization. Without requiring multimodal input, STP directly extracts temporal and spatial distance features from RGB video clips. Subsequently, these dual features are jointly encoded by maximizing the expected likelihood across all possible permutations of the factorization order. By integrating temporal and spatial features at different scales, STP can perceive subtle behavioral changes in challenging scenarios. Additionally, we introduce a causal-aware module to explore relationships between video frame features, significantly enhancing detection efficiency and performance. We validate the effectiveness of our approach using two publicly available driver distraction detection benchmarks. The results demonstrate that our framework achieves state-of-the-art performance.
PB  - arXiv
PY  - 2025
ST  - Spatial-Temporal Perception with Causal Inference for Naturalistic Driving Action Recognition
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/icassp49660.2025.10889969
ER  -


TY  - GEN
AU  - Landauer, M.
AU  - Alton, L.
AU  - Lindorfer, M.
AU  - Wurzenberger, M.
AU  - Hotwagner, W.
TI  - Trace of the Times: Rootkit Detection through Temporal Anomalies in Kernel Activity
AB  - Kernel rootkits provide adversaries with permanent high-privileged access to compromised systems and are often a key element of sophisticated attack chains. At the same time, they enable stealthy operation and are thus difficult to detect. Thereby, they inject code into kernel functions to appear invisible to users, for example, by manipulating file enumerations. Existing detection approaches are insufficient, because they rely on signatures that are unable to detect novel rootkits or require domain knowledge about the rootkits to be detected. To overcome this challenge, our approach leverages the fact that runtimes of kernel functions targeted by rootkits increase when additional code is executed. The framework outlined in this paper injects probes into the kernel to measure time stamps of functions within relevant system calls, computes distributions of function execution times, and uses statistical tests to detect time shifts. The evaluation of our open-source implementation on publicly available data sets indicates high detection accuracy with an F1 score of 98.7% across five scenarios with varying system states.
PB  - arXiv
PY  - 2025
ST  - Trace of the Times
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-642-05197-5_16
ER  -


TY  - GEN
AU  - Hu, Y.
AU  - Xu, J.
AU  - Gou, Z.
AU  - Cui, D.
TI  - Recognizing and Localizing Chicken Behaviors in Videos Based on Spatiotemporal Feature Learning
AB  - Timely acquisition of chicken behavioral information is crucial for assessing their health status and production performance. Video-based behavior recognition methods have emerged as a primary technique for obtaining such information due to their accuracy and robustness. Video-based models generally predicted a single behavior from a single video segment of a fixed duration. However, during periods of high activity in poultry, behavior transition may occur within the video segment, and existing models often failed to effectively capture such transition. The limitation indicated the insufficient temporal resolution of video-based behavior recognition models. This study proposed a chicken behavior recognition and localization model, CBLFormer, based on spatiotemporal feature learning. The model was designed to recognize behaviors occurring before and after transition in the video segment and to localize the corresponding time interval for each behavior. An improved transformer block, Cascade Encoder-Decoder Network (CEDNet), a transformer-based head, and the Weighted Distance Intersection over Union (WDIoU) loss were integrated into CBLFormer to enhance the model's ability to distinguish between different behavior categories and locate behavior boundaries. For the training and testing of CBLFormer, the dataset was created by collecting videos from 320 chickens across different ages and rearing densities. The results showed that the CBLFormer achieved an mAP@0.5:0.95 of 98.34% on the test set. The integration of CEDNet contributed the most to the performance improvement of the CBLFormer. Compared to other temporal behavior localization models, CBLFormer struck a favorable balance between model complexity and performance, with a computational complexity of 0.57 GFLOPs and a parameter size of 16.14 MB. Furthermore, visualization results confirmed that the model effectively captured the behavioral boundaries of chickens and correctly recognized behavior categories. The proposed method handles cases where poultry behavior transitions occur within the video segment and improves the temporal resolution of video-based behavior recognition models.
PB  - SSRN
PY  - 2025
ST  - Recognizing and Localizing Chicken Behaviors in Videos Based on Spatiotemporal Feature Learning
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5165056
ER  -


TY  - GEN
AU  - Liu, S.
AU  - Zhao, C.
AU  - Zohra, F.
AU  - Hinojosa, C.
AU  - Ghanem, B.
TI  - OpenTAD: A Unified Framework and Comprehensive Study of Temporal Action Detection
AB  - Temporal action detection (TAD) is a fundamental video understanding task that aims to identify human actions and localize their temporal boundaries in videos. Although this field has achieved remarkable progress in recent years, further progress and real-world applications are impeded by the absence of a standardized framework. Currently, different methods are compared under different implementation settings, evaluation protocols, etc., making it difficult to assess the real effectiveness of a specific technique. To address this issue, we propose OpenTAD, a unified TAD framework consolidating 16 different TAD methods and 9 standard datasets into a modular codebase. In OpenTAD, minimal effort is required to replace one module with a different design, train a feature-based TAD model in end-to-end mode, or switch between the two. OpenTAD also facilitates straightforward benchmarking across various datasets and enables fair and in-depth comparisons among different methods. With OpenTAD, we comprehensively study how innovations in different network components affect detection performance and identify the most effective design choices through extensive experiments. This study has led to a new state-of-the-art TAD method built upon existing techniques for each component. We have made our code and models available at https://github.com/sming256/OpenTAD.
PB  - arXiv
PY  - 2025
ST  - OpenTAD
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2025.02.10.637589
ER  -


TY  - GEN
AU  - Zeng, J.
AU  - Xu, H.
AU  - Gao, F.
AU  - Li, X.
AU  - Ai, W.
TI  - A Dual-Branch Selective Co-Temporal Attention Framework for Human Activity Recognition Using Multisource and Multimodal Sensor Data
AB  - With the gradual progression of sensor technology, sensor-based human activity recognition (HAR), and high privacy have embraced conspicuous prosperity in numerous applications. However, manifold urgent problems remain to be solved in current studies: the existing HAR frameworks have the insufficient capacity to extract comprehensive multiscale features. Meanwhile, there is a relative scarcity of research and application on HAR under multisource and multimodal sensing devices. It is difficult for current recognition techniques to model the spatiotemporal properties of multisource sensing data effectively. To address the above challenges, this paper proposes a human activity recognition (HAR) framework based on dual-branch selective co-temporal attention fusion (DS-CAF) for multisource and multimodal sensor scenarios, which proposes the aggregation transformation-based dual path module (ATDPM), co-temporal cross attention unit (C2AU), and selective fusion attention unit (SFAU) can extract human activity features from different dimensional spaces without increasing the model depth and enhance the expression of feature spatiotemporal consistency. Besides, the daily home activity (DHA) dataset is constructed based on the smart home scenarios. Extensive evaluation experiments using ten-fold cross validation are carried out on two publicly available benchmark datasets and the self-collected dataset, i.e., the physical activity monitoring for aging people (PAMAP2) dataset, the OPPORTUNITY dataset, and the DHA dataset. In these three datasets, DS-CAF's recognition accuracy achieves 98.99%, 98.11%, and 98.04%, respectively. The experimental results demonstrate that the proposed framework not only enhances performance but also effectively reduces computational overhead.
PB  - SSRN
PY  - 2025
ST  - A Dual-Branch Selective Co-Temporal Attention Framework for Human Activity Recognition Using Multisource and Multimodal Sensor Data
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5152077
ER  -


TY  - GEN
AU  - Vasylieva, I.
AU  - Smith, M.C.
AU  - Aravind, E.
AU  - Freyberg, Z.
AU  - Watson, A.M.
TI  - Brain-wide mapping reveals temporal and sexually dimorphic opioid actions
AB  - While the molecular and cellular effects of opioids have been extensively studied, the precise mechanisms by which these drugs target specific brain regions over tome remain unclear. Similarly, despite well-documented sex differences in opioid responses, the anatomical basis for this sexual dimorphism is not well characterized. To address these questoons, we developed an automated, scalable, and unbiased approach for whole-brain anatomical mapping of the neuronal actovity marker c-Fos in response to acute morphine exposure. Using ribbon scanning confocal microscopy, we imaged whole cleared brains from male and female wild-type mice at 1 hour and 4 hours post-morphine administratoon. Our whole-brain analysis of c-Fos expression revealed distonct patterns of morphine-induced regional brain actovatoon across tome and sex. Notably, we observed a greater number of structures with significant actovity differences at 4 hours compared to 1 hour. In male mice, significant changes were primarily localized to regions within the dopamine system, whereas in female mice, they were concentrated in cortocal regions. By combining high-throughput imaging with whole-brain expression analysis, partocularly in the context of opioid actoons, our approach provides a more comprehensive understanding of how drugs of abuse affect the brain.
PB  - bioRxiv
PY  - 2025
ST  - Brain-wide mapping reveals temporal and sexually dimorphic opioid actions
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2025.02.19.638902
ER  -


TY  - GEN
AU  - Ochin, J.
AU  - Devineau, G.
AU  - Stanciulescu, B.
AU  - Manitsaris, S.
TI  - Game State and Spatio-temporal Action Detection in Soccer using Graph Neural Networks and 3D Convolutional Networks
AB  - Soccer analytics rely on two data sources: the player positions on the pitch and the sequences of events they perform. With around 2000 ball events per game, their precise and exhaustive annotation based on a monocular video stream remains a tedious and costly manual task. While state-of-the-art spatio-temporal action detection methods show promise for automating this task, they lack contextual understanding of the game. Assuming professional players’ behaviors are interdependent, we hypothesize that incorporating surrounding players’ information such as positions, velocity and team membership can enhance purely visual predictions. We propose a spatio-temporal action detection approach that combines visual and game state information via Graph Neural Networks trained end-to-end with state-of-the-art 3D CNNs, demonstrating improved metrics through game state integration.
PB  - arXiv
PY  - 2025
ST  - Game State and Spatio-temporal Action Detection in Soccer using Graph Neural Networks and 3D Convolutional Networks
Y2  - 2025/05/05/21:54:32
DO  - 10.5220/0013161100003905
ER  -


TY  - GEN
AU  - Ashraf, S.
AU  - Kumar, P.
AU  - Dwivedi, P.
AU  - Pillai, D.
AU  - Mangal, R.
TI  - Emergence of Order in Chemically Active Droplets: Temporal Dynamics and Collective Behavior
AB  - Collective behaviors such as swarming, chemical signaling, and clustering are fundamental to biological microorganisms, enabling hierarchical colony formation, coordinated motion, and enhanced nutrient accessibility—crucial for their survival. Over the past few decades, extensive research has been dedicated to unraveling the mechanisms underlying these diverse collective patterns through experimental model systems. Among these, active droplets have emerged as valuable synthetic analogs, effectively replicating key biological attributes and serving as ideal platforms for investigating collective phenomena. This research explores the collective behavior of 4-Cyano-4’-pentyl-biphenyl (5CB) oil droplets across varying Péclet (Pe) numbers. At high Pe, droplets exhibit a pusher mode of propulsion and form dynamic chain-like patterns. Decreasing Pe enhances repulsive interactions among droplets, resulting in the inhibition of clustering. In the low Pe regime, their repulsive interactions predominated by chemical field lead to the emergence of an ordered structure. Furthermore, we illustrate how active droplets efficiently navigate within a soft structured environment. These findings contribute to our comprehension of self-organized phenomena in active matter systems and provide insights for designing strategies for controlled locomotion in intricate fluidic environments.
PB  - arXiv
PY  - 2025
ST  - Emergence of Order in Chemically Active Droplets
Y2  - 2025/05/05/21:54:32
DO  - 10.1103/physrevlett.130.248201
ER  -


TY  - GEN
AU  - Ansari, M.
AU  - Fazl-Ersi, E.
TI  - Semantic Smoothness Optimization via Graph-CutEnergy Minimization for Temporal Action Segmentation
AB  - Temporal action segmentation, crucial for understanding human activities in video content, remains a challenging task due to the complexity and variability of human actions. Existing approaches, such as temporal convolutional networks (TCNs) and transformer-based architectures, often fail to adequately model the intricate dependencies and semantic relationships between sequential actions. In this paper, we propose a novel framework formulated as an energy minimization problem to improve temporal action segmentation. Our approach incorporates data and smoothness costs, utilizing a graph-cut algorithm to achieve energy minimization. The data cost quantifies the likelihood of assigning appropriate semantic labels to frames based on visual features, while the smoothness cost ensures temporal consistency between neighboring frames by modeling semantic transitions. Extensive experiments on the GTEA, 50Salads, and Breakfast datasets demonstrate that our framework outperforms state-of-the-art methods, providing more accurate and temporally consistent action segmentation. By explicitly modeling semantic relationships and ensuring smooth action transitions, our approach contributes to more robust and reliable action recognition in untrimmed video sequences, with potential applications in robotics, video surveillance, and human-computer interaction. The code and datasets supporting the results of this study are publicly available at the project’s GitHub repository.
PB  - Research Square
PY  - 2025
ST  - Semantic Smoothness Optimization via Graph-CutEnergy Minimization for Temporal Action Segmentation
Y2  - 2025/05/05/21:54:32
DO  - 10.21203/rs.3.rs-5946071/v1
ER  -


TY  - GEN
AU  - Dass, S.D.S.
AU  - Barua, H.B.
AU  - Krishnasamy, G.
AU  - Paramesran, R.
AU  - Phan, R.C.-W.
TI  - MD-BERT: Action Recognition in Dark Videos via Dynamic Multi-Stream Fusion and Temporal Modeling
AB  - Action recognition in dark, low-light (underexposed) or noisy videos is a challenging task due to visibility degradation, which can hinder critical spatiotemporal details. This paper proposes MD-BERT, a novel multi-stream approach that integrates complementary pre-processing techniques such as gamma correction and histogram equalization alongside raw dark frames to address these challenges. We introduce the Dynamic Feature Fusion (DFF) module, extending existing attentional fusion methods to a three-stream setting, thereby capturing fine-grained and global contextual information across different brightness and contrast enhancements. The fused spatiotemporal features are then processed by a BERT-based temporal model, which leverages its bidirectional self-attention to effectively capture long-range dependencies and contextual relationships across frames. Extensive experiments on the ARID V1.0 and ARID V1.5 dark video datasets show that MD-BERT outperforms existing methods, establishing a new state-of-the-art performance. Ablation studies further highlight the individual contributions of each input stream and the effectiveness of the proposed DFF and BERT modules. The official website of this work is available at: https://github.com/HrishavBakulBarua/DarkBERT MSC Codes Artificial intelligence, Computer vision, Machine learning, Deep learning, Human-computer Interaction
PB  - arXiv
PY  - 2025
ST  - MD-BERT
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-319-73603-7_29
ER  -


TY  - GEN
AU  - Mohammadi, N.
AU  - Danielson, J.
AU  - Espinosa-Leal, L.
TI  - Temporal patterns and variations of depression related health information seeking behaviour – a longitudinal analysis of Google search data
AB  - The current research work represents the result of analyzing online health information-seeking behavior regarding depression over ten years 2013–2022 in Finland, finding the rhythms and fluctuations on different time-scales. Our paper analyzes diurnal, weekly, and seasonal patterns in six Google-suggested depression-related search queries by relying on data provided by Google Trends and the Google Health Trends Application Programming Interface-GHT-API. Our results reveal that search volumes follows temporal patterns on daily, weekly, and seasonal levels. We observe a periodical robust diurnal pattern with a significant nightly peak. Weekly patterns highlight increased search activity during weekends, with the exception of the term "Masennus"(eng. depression) which exhibits unique dynamics during weekdays. Seasonal patterns take the shape of bimodal curves for "masennus" and "masennustesti" with a peak in search interest during spring and autumn and deep during summer. These findings bear implications for healthcare providers and policy makers in pursuit of the best mental health care. Driving forces for such temporal patterns may provide the basis for further research, thus giving more depth to our understanding of online information seeking in mental health contexts.
PB  - Research Square
PY  - 2025
ST  - Temporal patterns and variations of depression related health information seeking behaviour – a longitudinal analysis of Google search data
Y2  - 2025/05/05/21:54:32
DO  - 10.21203/rs.3.rs-5919096/v1
ER  -


TY  - GEN
AU  - Lu, D.
AU  - Wu, S.
AU  - Zhang, H.
AU  - Xu, G.
AU  - Han, Q.
TI  - Causal Cascading Convolution Networks for Multi-Behavior Sequential Recommendation
AB  - Exploring multi-behavioral sequence recommendation has emerged as a crucial topic in recent years. User interactions on online platforms revolve around singular actions. These interactions may also involve diverse behaviors. Each of these varied behaviors reflects different facets of user preferences in their interaction sequences with items.Existing methods primarily construct user preferences and interests based on correlations among users, items, and behaviors. However, in real-world scenarios, causality often drives users to make their next decision rather than merely relying on correlation. Unfortunately, this causal relationship is frequently overlooked by most multi-behavior models. To address this gap, we propose a Causality-based Multi-behavior Sequential Recommendation (CMSR) framework to capture the underlying causal relationships among user behaviors and predict future actions. Specifically, CMSR first independently encodes each behavioral sequence to capture user preferences across different behaviors. It then aggregates inter-item behavioral relationships through hypergraph convolution. We also employ cascaded neural networks to capture directional dependencies in multi-behavior sequences within the behavior chain. Finally, CMSR transfers the influences of causal relationships among behaviors by utilizing a causal graph construction approach. We conduct extensive experiments on realworld datasets to evaluate the performance of the proposed CMSR.
PB  - SSRN
PY  - 2025
ST  - Causal Cascading Convolution Networks for Multi-Behavior Sequential Recommendation
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5124767
ER  -


TY  - GEN
AU  - de Chaumont, F.
AU  - Yvenou, G.
AU  - Villalba, A.P.
AU  - Bourgeron, T.
AU  - Ey, E.
TI  - Female C57BL/6J mice perform distinctive urination behaviour accompanied by ultrasonic vocalisations sequences with a stereotypic temporal organisation.
AB  - Ultrasonic vocalisations (USVs) are largely studied in mice as a marker of social communication. These USVs are usually recorded during short social encounters in unfamiliar test cages. In the present study, we explored how freely interacting pairs of C57BL/6J adult female mice spontaneously use USVs over long-term monitoring. In this situation, we discovered that these mice display a previously undescribed behaviour: they emit specific USV sequences while depositing a large quantity of urine in a corner of the cage. The most striking feature of USVs accompanying this vocalised urination behaviour was the stereotyped duration of the inter-USV intervals. The variability in the frequency of occurrence of this behaviour was important between pairs. Interestingly, when accompanied by the specific USV sequence, urination was correlated with a significant increase in locomotor activity in both the emitter and the cage mate, in contrast with urination without USVs. Altogether, the observation and description of this vocalised urination behaviour argue for exploring mouse vocalisations at the sequence level to understand the USV-behaviours interactions.
PB  - bioRxiv
PY  - 2025
ST  - Female C57BL/6J mice perform distinctive urination behaviour accompanied by ultrasonic vocalisations sequences with a stereotypic temporal organisation.
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2025.02.04.635841
ER  -


TY  - GEN
AU  - Girmaji, R.
AU  - Jain, S.
AU  - Beri, B.
AU  - Bansal, S.
AU  - Gandhi, V.
TI  - Minimalistic Video Saliency Prediction via Efficient Decoder & Spatio Temporal Action Cues
AB  - This paper introduces ViNet-S, a 36MB model based on the ViNet architecture with a U-Net design, featuring a lightweight decoder that significantly reduces model size and parameters without compromising performance. Additionally, ViNet-A (148MB) incorporates spatio-temporal action localization (STAL) features, differing from traditional video saliency models that use action classification backbones. Our studies show that an ensemble of ViNet-S and ViNet-A, by averaging predicted saliency maps, achieves state-of-the-art performance on three visual-only and six audio-visual saliency datasets, outperforming transformer-based models in both parameter efficiency and real-time performance, with ViNet-S reaching over 1000fps.
PB  - arXiv
PY  - 2025
ST  - Minimalistic Video Saliency Prediction via Efficient Decoder & Spatio Temporal Action Cues
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/icassp49660.2025.10888852
ER  -


TY  - GEN
AU  - Li, Y.
AU  - Sun, J.
TI  - Spatiotemporal Dynamics of a Delayed Diffusive Predator-Prey Model with Hunting Cooperation in Predator and Anti-Predator Behaviors in Prey
AB  - Spatial memory means the ability of an organism to encode, store, and recall spatial information about its surrounding environment, which has recently been considered in models. We introduce spatial memory into a predator-prey model with hunting cooperation in predator and anti-predator behaviors in prey. We take the memory-based diffusion coefficient α as a bifurcation parameter and consider the effect of memory delay on the stability and bifurcation of the positive constant solution. For the model without delay, we obtain that when the positive number α is small enough, the positive constant solution becomes unstable and steady state bifurcation emerges. For the delayed model, under some conditions, we observe that Hopf bifurcation exists and stability switches may occur. Moreover, we get a minimum critical value of time delay when α is large enough. The positive constant equilibrium is stable under the critical value while the positive constant solution is unstable beyond the critical value. We take numerical simulations to demonstrate our theories in the end.
PB  - SSRN
PY  - 2025
ST  - Spatiotemporal Dynamics of a Delayed Diffusive Predator-Prey Model with Hunting Cooperation in Predator and Anti-Predator Behaviors in Prey
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5118119
ER  -


TY  - GEN
AU  - Rollere, I.
AU  - Hartsfield, C.
AU  - Courtenay, S.
AU  - Fenwick, L.
AU  - Grunwald, A.
TI  - Algorithmic Segmentation and Behavioral Profiling for Ransomware Detection Using Temporal-Correlation Graphs
AB  - The rapid evolution of cyber threats has outpaced traditional detection methodologies, necessitating innovative approaches capable of addressing the adaptive and complex behaviors of modern adversaries. A novel framework was introduced, leveraging Temporal-Correlation Graphs to model the intricate relationships and temporal patterns inherent in malicious operations. The approach dynamically captured behavioral anomalies, offering a robust mechanism for distinguishing between benign and malicious activities in real-time scenarios. Extensive experiments demonstrated the framework’s effectiveness across diverse ransomware families, with consistently high precision, recall, and overall detection accuracy. Comparative evaluations highlighted its better performance over traditional signature-based and heuristic methods, particularly in handling polymorphic and previously unseen ransomware variants. The architecture was designed with scalability and modularity in mind, ensuring compatibility with enterprise-scale environments while maintaining resource efficiency. Analysis of encryption speeds, anomaly patterns, and temporal correlations provided deeper insights into the operational strategies of ransomware, validating the framework’s adaptability to evolving threats. The research contributes to advancing cybersecurity technologies by integrating dynamic graph analytics and machine learning for future innovations in threat detection. Results from this study underline the potential for transforming the way organizations detect and mitigate complex cyberattacks.
PB  - arXiv
PY  - 2025
ST  - Algorithmic Segmentation and Behavioral Profiling for Ransomware Detection Using Temporal-Correlation Graphs
Y2  - 2025/05/05/21:54:32
DO  - 10.22541/au.173204044.48061626/v1
ER  -


TY  - GEN
AU  - Duong, A.-K.
AU  - Gomez-Krämer, P.
TI  - Action Recognition Using Temporal Shift Module and Ensemble Learning
AB  - This paper presents the first-rank solution for the Multi-Modal Action Recognition Challenge, part of the Multi-Modal Visual Pattern Recognition Workshop at the International Conference on Pattern Recognition (ICPR) 2024. The competition aimed to recognize human actions using a diverse dataset of 20 action classes, collected from multi-modal sources. The proposed approach is built upon the Temporal Shift Module (TSM), a technique aimed at efficiently capturing temporal dynamics in video data, incorporating multiple data input types. Our strategy included transfer learning to leverage pre-trained models, followed by meticulous fine-tuning on the challenge’s specific dataset to optimize performance for the 20 action classes. We carefully selected a backbone network to balance computational efficiency and recognition accuracy and further refined the model using an ensemble technique that integrates outputs from different modalities. This ensemble approach proved crucial in boosting the overall performance. Our solution achieved a perfect top-1 accuracy on the test set, demonstrating the effectiveness of the proposed approach in recognizing human actions across 20 classes. Our code is available online 3
PB  - arXiv
PY  - 2025
ST  - Action Recognition Using Temporal Shift Module and Ensemble Learning
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/ur57808.2023.10202338
ER  -


TY  - GEN
AU  - Maire, T.
AU  - Wan, Z.
AU  - Lambrechts, L.
AU  - Hol, F.J.H.
TI  - BuzzWatch: uncovering multi-scale temporal patterns in mosquito behavior through continuous long-term monitoring
AB  - Understanding the temporal dynamics of mosquito behavior is essential for developing effective interventions against pathogen transmission. However, limited knowledge exists about the environmental, physiological, and genetic factors influencing mosquito activity patterns. This knowledge gap is partly due to a lack of tools to accurately quantify the behavior of free-flying mosquitoes over extended periods. Here, we introduce BuzzWatch, an open-source, low-cost platform designed to continuously monitor mosquito flight behavior over several weeks with high temporal resolution. BuzzWatch records videos of mosquitoes freely flying in a transparent cage and automates the extraction, analysis, and visualization of behavioral data, including flight trajectories and population-level flight and sugar-feeding statistics. Using BuzzWatch, we quantified the daily rhythms of 10 Aedes aegypti populations from various geographic origins. Globally invasive Ae. aegypti showed increased sugar feeding and flight activity during midday compared to native African populations. Our platform further revealed subtle, long-lasting effects of blood feeding on activity patterns and a complex response to extended daylight periods. By integrating a host-seeking module in BuzzWatch to deliver CO2 and heat pulses, we observed a twofold increase in Ae. aegypti’s response to host-associated cues during the daytime compared to nighttime. Combined, these results demonstrate BuzzWatch’s potential to investigate responses to host cues over seconds, natural variability in daily rhythms over hours, and phenotypic plasticity over days. BuzzWatch offers a novel perspective on mosquito behavior over multiple timescales, paving the way for advanced ecological and epidemiological studies that can inform targeted and effective vector control strategies.
PB  - bioRxiv
PY  - 2025
ST  - BuzzWatch
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2025.01.24.634688
ER  -


TY  - GEN
AU  - Han, C.
AU  - Wang, H.
AU  - Kuang, J.
AU  - Zhang, L.
AU  - Gui, J.
TI  - Training-Free Zero-Shot Temporal Action Detection with Vision-Language Models
AB  - Existing zero-shot temporal action detection (ZSTAD) methods predominantly use fully supervised or unsupervised strategies to recognize unseen activities. However, these training-based methods are prone to domain shifts and require high computational costs, which hinder their practical applicability in real-world scenarios. In this paper, unlike previous works, we propose a training-Free Zero-shot temporal Action Detection (FreeZAD) method, leveraging existing vision-language (ViL) models to directly classify and localize unseen activities within untrimmed videos without any additional fine-tuning or adaptation. We mitigate the need for explicit temporal modeling and reliance on pseudo-label quality by designing the LOGarithmic decay weighted Outer-Inner-Contrastive Score (LogOIC) and frequency-based Actionness Calibration. Furthermore, we introduce a test-time adaptation (TTA) strategy using Prototype-Centric Sampling (PCS) to expand FreeZAD, enabling ViL models to adapt more effectively for ZSTAD. Extensive experiments on the THUMOS14 and ActivityNet-1.3 datasets demonstrate that our training-free method outperforms state-of-the-art unsupervised methods while requiring only 1/13 of the runtime. When equipped with TTA, the enhanced method further narrows the gap with fully supervised methods.
PB  - arXiv
PY  - 2025
ST  - Training-Free Zero-Shot Temporal Action Detection with Vision-Language Models
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-031-20062-5_39
ER  -


TY  - GEN
AU  - Shan, R.
AU  - Zhu, J.
AU  - Lin, J.
AU  - Yu, Y.
AU  - Zhang, W.
TI  - Full-Stack Optimized Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation
AB  - As large language models (LLMs) achieves remarkable success in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and are being actively explored currently. In this paper, we focus on adapting and enhancing large language models for recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation realms, i.e., LLMs fail to effectively extract useful information from a pure textual context of long user behavior sequence, even if the length of context is well below the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models Plus (ReLLaX), which provides full-stack optimization from three perspectives, i.e., data, prompt and parameter. For data-level enhancement, we design semantic user behavior retrieval (SUBR) to reduce the heterogeneity of the behavior sequence, thus lowering the difficulty for LLMs to extract the essential information from user behavior sequences. Although SUBR can improve the data quality, further increase of the sequence length will still raise its heterogeneity to a level where LLMs can no longer comprehend it. Hence, we further propose to perform prompt-level and parameter-level enhancement, with the integration of conventional recommendation models (CRMs). As for prompt-level enhancement, we apply soft prompt augmentation (SPA) to explicitly inject collaborative knowledge from CRMs into the prompt. The item representations of LLMs are thus more aligned with recommendation, helping LLMs better explore the item relationships in the sequence and facilitating comprehension. Finally for parameter-level enhancement, we propose component fully-interactive LoRA (CFLoRA). By enabling sufficient interaction between the LoRA atom components, the expressive ability of LoRA is extended, making the parameters effectively capture more sequence information. Moreover, we present new perspectives to compare current LoRA-based LLM4Rec methods, i.e. from both a composite and a decomposed view. We theoretically demonstrate that the ways they employ LoRA for recommendation are degraded versions of our CFLoRA, with different constraints on atom component interactions. Extensive experiments are conducted on three real-world public datasets to demonstrate the superiority of ReLLaX compared with existing baseline models, as well as its capability to alleviating lifelong sequential behavior incomprehension. Our code is available1.
PB  - arXiv
PY  - 2025
ST  - Full-Stack Optimized Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation
Y2  - 2025/05/05/21:54:32
DO  - 10.1145/3589334.3645467
ER  -


TY  - GEN
AU  - Mei, M.
AU  - Cai, Y.
AU  - Liu, C.
AU  - Wang, Z.
TI  - Asymptotic Behavior of Solutions to the Cauchy Problem For 1d $P$-System with Spatiotemporal Damping: Case 1. $V_+=V_-$
AB  - This paper investigates the Cauchy problem for the $p$-system with spatiotemporal damping, modeling one-dimensional compressible flow through porous media in Lagrangian coordinates. We focus on the large-time asymptotic behavior of the system's solutions when the state constants for the specific volume are the same: $v_+=v_-$, but the state constants for the velocity are different: $u_+\not= u_-$. We show the convergence of the solutions to their diffusion waves with the different algebraic time decay rates according to different exponent of time-damping: $0\le \lambda<\frac{3}{5}$, $\lambda=\frac{3}{5}$ and $\frac{3}{5}<\lambda<1$, respectively. Our analysis employs an energy method to establish a series of a priori estimates, offering new insights and theoretical support for understanding the long-time dynamics of compressible flows in porous media with spatially heterogeneous damping.
PB  - SSRN
PY  - 2025
ST  - Asymptotic Behavior of Solutions to the Cauchy Problem For 1d $P$-System with Spatiotemporal Damping
Y2  - 2025/05/05/21:54:32
DO  - 10.1016/j.jde.2025.113347
ER  -


TY  - GEN
AU  - Zhang, Q.
AU  - Qi, Y.
AU  - Tang, X.
AU  - Zhang, K.
AU  - Yuan, C.
TI  - Rethinking Pseudo-Label Guided Learning for Weakly Supervised Temporal Action Localization from the Perspective of Noise Correction
AB  - Pseudo-label learning methods have been widely applied in weakly-supervised temporal action localization. Existing works directly utilize weakly-supervised base model to generate instance-level pseudo-labels for training the fully-supervised detection head. We argue that the noise in pseudo-labels would interfere with the learning of fully-supervised detection head, leading to significant performance leakage. Issues with noisy labels include:(1) inaccurate boundary localization; (2) undetected short action clips; (3) multiple adjacent segments incorrectly detected as one segment. To target these issues, we introduce a two-stage noisy label learning strategy to harness every potential useful signal in noisy labels. First, we propose a frame-level pseudo-label generation model with a context-aware denoising algorithm to refine the boundaries. Second, we introduce an online-revised teacher-student framework with a missing instance compensation module and an ambiguous instance correction module to solve the short-action-missing and many-to-one problems. Besides, we apply a high-quality pseudo-label mining loss in our online-revised teacher-student framework to add different weights to the noisy labels to train more effectively. Our model outperforms the previous state-of-the-art method in detection accuracy and inference speed greatly upon the THUMOS14 and ActivityNet v1.2 benchmarks.
PB  - arXiv
PY  - 2025
ST  - Rethinking Pseudo-Label Guided Learning for Weakly Supervised Temporal Action Localization from the Perspective of Noise Correction
Y2  - 2025/05/05/21:54:32
DO  - 10.1609/aaai.v39i10.33094
ER  -


TY  - GEN
AU  - Xie, H.
AU  - Li, H.
AU  - Zheng, C.
AU  - Liao, J.
AU  - Liu, L.
TI  - Decomposing and Fusing Intra- and Inter-Sensor Spatio-Temporal Signal for Multi-Sensor Wearable Human Activity Recognition
AB  - Wearable Human Activity Recognition (WHAR) is a prominent research area within ubiquitous computing. Multi-sensor synchronous measurement has proven to be more effective for WHAR than using a single sensor. However, existing WHAR methods use shared convolutional kernels for indiscriminate temporal feature extraction across each sensor variable, which fails to effectively capture spatio-temporal relationships of intra-sensor and inter-sensor variables. We propose the DecomposeWHAR model consisting of a decomposition phase and a fusion phase to better model the relationships between modality variables. The decomposition creates high-dimensional representations of each intra-sensor variable through the improved Depth Separable Convolution to capture local temporal features while preserving their unique characteristics. The fusion phase begins by capturing relationships between intra-sensor variables and fusing their features at both the channel and variable levels. Long-range temporal dependencies are modeled using the State Space Model (SSM), and later cross-sensor interactions are dynamically captured through a self-attention mechanism, highlighting inter-sensor spatial correlations. Our model demonstrates superior performance on three widely used WHAR datasets, significantly outperforming state-of-the-art models while maintaining acceptable computational efficiency. Our codes and supplementary materials are available at https://github.com/Anakin2555/DecomposeWHAR.
PB  - arXiv
PY  - 2025
ST  - Decomposing and Fusing Intra- and Inter-Sensor Spatio-Temporal Signal for Multi-Sensor Wearable Human Activity Recognition
Y2  - 2025/05/05/21:54:32
DO  - 10.1609/aaai.v39i13.33582
ER  -


TY  - GEN
AU  - Karki, P.
AU  - Sharma, S.
AU  - Biswasharma, R.
AU  - Gyawali, M.
AU  - Poudyal, K.N.
TI  - Spatiotemporal Variation of Lightning Activity Over Nepal's Complex Terrain: Links to Meteorological Factors
AB  - This study investigates the diurnal and seasonal variations of lightning activity over the complex terrain of Nepal with a large altitudinal variation, ranging from as low as 59 m to 8848 m above the mean sea level. Although, lightning is a major natural disaster in Nepal which claims over 100 lives each year, and that there have been a few studies pertaining to the lighting mapping, distribution of lightning flashes along the altitude has not been considered. In this study, we divided Nepal topographically into four regions to investigate the altitudinal variation of lightning activities from southern Indo-Gangetic plains to the northern Himalayas. We utilized lightning stroke data from the GLD360 network, alongside meteorological data from the ERA5 reanalysis dataset, to examine the influence of factors such as Convective Available Potential Energy (CAPE), temperature, and humidity on lightning activity. A distinct altitudinal gradient in lightning flash density, with the highest activity occurring in the foothill regions and a marked decrease at higher altitudes is revealed. The temporal scale analysis reveals that lightning activity exhibits two surges, one in the afternoon and the other around midnight. In general, lightning activity peaks in the early afternoon over the northern most region, mostly encompassing Himalayas, with a slight delayed peak over the hilly regions (lesser Himalayas), whereas the activity peaks by the mid-night over the southern plains The observed diurnal patterns, with peaks in the afternoon and during the night can be attributed to the orographic lifting stemming from local baroclinicity induced by daytime insolation or nocturnal radiated cooling over the complex terrain This study lays a foundation for spatial and temporal variation of thunderstorm activity over Nepal’s complex terrain, further investigation is needed taking other meteorological factors into account.
PB  - SSRN
PY  - 2025
ST  - Spatiotemporal Variation of Lightning Activity Over Nepal's Complex Terrain
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5102159
ER  -


TY  - GEN
AU  - Ohno, S.
AU  - Nomoto, M.
AU  - Inokuchi, K.
TI  - The medial prefrontal cortex encodes procedural rules as sequential neuronal activity dynamics
AB  - The prefrontal cortex plays a crucial role in procedural rule learning; however, the specific neuronal mechanism through which it represents rules is unknown. We hypothesized that sequential neuronal activities in the prefrontal cortex encode these rules. To investigate this, we recorded neuronal activities in the medial prefrontal cortex of mice during rule learning using Ca2+ imaging. We utilized a method based on convolutional negative matrix factorization, iSeq, to automatically detect temporal neuronal sequences in the recorded data. As rule learning advanced, these neuronal sequences began to encode critical information for rule execution. In mice that had mastered the rule, the dynamics of neuronal sequences could predict success and failure of reward acquisition. Furthermore, the composition of cell populations within the neuronal sequences was rearranged throughout the learning process. These findings suggest that as animals learn a rule, the medial prefrontal cortex continually updates its neuronal sequences to assign significance to behavioral actions crucial for reward acquisition.
PB  - bioRxiv
PY  - 2025
ST  - The medial prefrontal cortex encodes procedural rules as sequential neuronal activity dynamics
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2025.01.16.633469
ER  -


TY  - GEN
AU  - Huang, J.
TI  - The quantitative spatiotemporal relationship of whole brain activity of human brains revealed by fMRI
AB  - Human brain consists of many functional systems from the essential sensory, motor, attention and memory systems to higher order cognitive functions such as reasoning and language. Performing even a simple task may evoke multiple systems and cognitive functions, resulting in a whole brain activity across the entire brain. Despite the importance of studying task-evoked brain activated networks, investigating this whole brain activity may be crucial for understanding the neural bases of individual behavioral and clinical traits. BOLD-fMRI measures the four-dimensional (3 spatial and 1 temporal) neural activity across the entire brain at large-scale systems level. All local activities across the entire brain constitute the whole brain activity and each local activity is a part of that whole brain activity. Unlike a local activity that is characterized by its temporal neural activity, the whole brain activity is characterized by its spatial variation across the entire brain. We present a novel data-driven method to analyze the whole brain activity when performing tasks. The method enabled us to analyze the whole brain activity for each task trial and each individual subject with no requirement of a priori knowledge of task-evoked BOLD response. Our study revealed a quantitative spatiotemporal relationship of the whole brain activity with the local activities. The whole brain activity demonstrated a remarkable dynamic activity that varied from trial to trial when performing the same task repeatedly, showing the importance of analyzing the whole brain activity for investigating the neural bases of personal traits.
PB  - bioRxiv
PY  - 2025
ST  - The quantitative spatiotemporal relationship of whole brain activity of human brains revealed by fMRI
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2025.01.16.633389
ER  -


TY  - GEN
AU  - Charoenpitaks, K.
AU  - Nguyen, V.-Q.
AU  - Suganuma, M.
AU  - Ino, H.
AU  - Okatani, T.
TI  - TB-Bench: Training and Testing Multi-Modal AI for Understanding Spatio-Temporal Traffic Behaviors from Dashcam Images/Videos
AB  - The application of Multi-modal Large Language Models (MLLMs) in Autonomous Driving (AD) faces significant challenges due to their limited training on traffic-specific data and the absence of dedicated benchmarks for spatiotemporal understanding. This study addresses these issues by proposing TB-Bench, a comprehensive benchmark designed to evaluate MLLMs on understanding traffic behaviors across eight perception tasks from ego-centric views. We also introduce vision-language instruction tuning datasets, TB-100k and TB-250k, along with simple yet effective baselines for the tasks. Through extensive experiments, we show that existing MLLMs underperform in these tasks, with even a powerful model like GPT-4o achieving less than 35% accuracy on average. In contrast, when fine-tuned with TB-100k or TB-250k, our baseline models achieve average accuracy up to 85%, significantly enhancing performance on the tasks. Additionally, we demonstrate performance transfer by co-training TB-100k with another traffic dataset, leading to improved performance on the latter. Overall, this study represents a step forward by introducing a comprehensive benchmark, high-quality datasets, and baselines, thus supporting the gradual integration of MLLMs into the perception, prediction, and planning stages of AD.
PB  - arXiv
PY  - 2025
ST  - TB-Bench
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/ictc55196.2022.9952473
ER  -


TY  - GEN
AU  - Sinha, A.
AU  - Raj, M.S.
AU  - Wang, P.
AU  - Helmy, A.
AU  - Das, S.
TI  - MS-Temba: Multi-Scale Temporal Mamba for Efficient Temporal Action Detection
AB  - Temporal Action Detection (TAD) in untrimmed videos requires models that can efficiently (1) process long-duration videos, (2) capture temporal variations within action classes, and (3) handle dense, overlapping actions, all while remaining suitable for resource-constrained edge deployment. While Transformer-based methods achieve high accuracy, their quadratic complexity hinders deployment in such scenarios. Given the recent popularity of linear complexity Mamba-based models, leveraging them for TAD is a natural choice. However, naïvely adapting Mamba from language or vision tasks fails to provide an optimal solution and does not address the challenges of long, untrimmed videos. Therefore, we propose Multi-Scale Temporal Mamba (MS-Temba), the first Mamba-based architecture specifically designed for densely labeled TAD tasks. MS-Temba features Temporal Mamba Blocks (Temba Blocks), consisting of Temporal Convolutional Module (TCM) and Dilated SSM (D-SSM). TCM captures short-term dependencies using dilated convolutions, while D-SSM introduces a novel dilated state-space mechanism to model long-range temporal relationships effectively at each temporal scale. These multi-scale representations are aggregated by Scale-Aware State Fuser, which learns a unified representation for detecting densely overlapping actions. Experiments show that MS-Temba achieves state-of-the-art performance on long-duration videos, remains competitive on shorter segments, and reduces model complexity by 88%. Its efficiency and effectiveness make MS-Temba well-suited for real-world edge deployment. 1
PB  - arXiv
PY  - 2025
ST  - MS-Temba
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/cvpr52688.2022.01941
ER  -


TY  - GEN
AU  - Ong, Q.
AU  - Lim, C.J.Y.
AU  - Liao, Y.
AU  - Yang, X.
AU  - Han, W.
TI  - Optogenetic control of Protein Kinase C-epsilon activity reveals its intrinsic signaling properties with spatiotemporal resolution
AB  - The regulation of PKC epsilon (PKCε) and its downstream effects is still not fully understood, making it challenging to develop targeted therapies or interventions. A more precise tool that enables spatiotemporal control of PKCε activity is thus required. Here, we describe a photo-activatable optogenetic PKCε probe (Opto-PKCε) consisting of an engineered PKCε catalytic domain and a blue-light inducible dimerization domain. Molecular dynamics and AlphaFold simulations enable rationalization of the dark-light activity of the optogenetic probe. We first characterize the binding partners of Opto-PKCε, which are similar to those of PKCε. Subsequent validation of the Opto-PKCε tool is performed with phosphoproteome analysis, which reveals that only PKCε substrates are phosphorylated upon light activation. Opto-PKCε could be engineered for recruitment to specific subcellular locations. Activation of Opto-PKCε in isolated hepatocytes reveals its sustained activation at the plasma membrane is required for its phosphorylation of the insulin receptor at Thr1160. In addition, Opto-PKCε recruitment to the mitochondria results in its lowering of the spare respiratory capacity through phosphorylation of complex I NDUFS4. These results demonstrate that Opto-PKCε may have broad applications for the studies of PKCε signaling with high specificity and spatiotemporal resolution.
PB  - bioRxiv
PY  - 2025
ST  - Optogenetic control of Protein Kinase C-epsilon activity reveals its intrinsic signaling properties with spatiotemporal resolution
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2025.01.06.631444
ER  -


TY  - GEN
AU  - Ghosh, I.
AU  - Chugh, G.
AU  - Faridee, A.Z.M.D.
AU  - Roy, N.
TI  - Unsupervised Domain Adaptation Via Temporal Ensembling and Conditional Representation Learning for Cross-User Action Recognition
AB  - Recent advancements in deep learning-based wearable human action recognition (wHAR) have improved the capture and classification of complex motions, but adoption remains limited due to the lack of expert annotations and domain discrepancies from user variations. Limited annotations hinder the model’s ability to generalize to out-of-distribution samples. While data augmentation can improve generalizability, unsupervised augmentation techniques must be applied carefully to avoid introducing noise. Unsupervised domain adaptation (UDA) addresses domain discrepancies by aligning conditional distributions with labeled target samples, but vanilla pseudo-labeling can lead to error propagation. To address these challenges, we propose μDAR, a novel joint optimization architecture comprised of three functions: (i) consistency regularizer between augmented samples to improve model classification generalizability, (ii) temporal ensemble for robust pseudo-label generation and (iii) conditional distribution alignment to improve domain generalizability. The temporal ensemble works by aggregating predictions from past epochs to smooth out noisy pseudo-label predictions, which are then used in the conditional distribution alignment module to minimize kernel-based class-wise conditional maximum mean discrepancy (kCMMD) between the source and target feature space to learn a domain invariant embedding. The consistency-regularized augmentations ensure that multiple augmentations of the same sample share the same labels; this results in (a) strong generalization with limited source domain samples and (b) consistent pseudo-label generation in target samples. The novel integration of these three modules in μDAR results in a range of ≈ 4-12% average macro-F1 score improvement over six state-of-the-art UDA methods in four benchmark wHAR datasets.
PB  - SSRN
PY  - 2025
ST  - Unsupervised Domain Adaptation Via Temporal Ensembling and Conditional Representation Learning for Cross-User Action Recognition
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5081688
ER  -


TY  - GEN
AU  - Huang, Y.
AU  - Chen, H.
AU  - Xu, Z.
AU  - Sun, H.
AU  - Shao, D.
TI  - SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization
AB  - Human action understanding is crucial for the advancement of multimodal systems. While recent developments, driven by powerful large language models (LLMs), aim to be general enough to cover a wide range of categories, they often overlook the need for more specific capabilities. In this work, we address the more challenging task of Fine-grained Action Recognition (FAR), which focuses on detailed semantic labels within shorter temporal duration (e.g., "salto backward tucked with 1 turn"). Given the high costs of annotating finegrained labels and the substantial data needed for fine-tuning LLMs, we propose to adopt semi-supervised learning (SSL). Our framework, SeFAR, incorporates several innovative designs to tackle these challenges. Specifically, to capture sufficient visual details, we construct Dual-level temporal elements as more effective representations, based on which we design a new strong augmentation strategy for the Teacher- Student learning paradigm through involving moderate temporal perturbation. Furthermore, to handle the high uncertainty within the teacher model's predictions for FAR, we propose the Adaptive Regulation to stabilize the learning process. Experiments show that SeFAR achieves state-of-the-art performance on two FAR datasets, FineGym and FineDiving, across various data scopes. It also outperforms other semisupervised methods on two classical coarse-grained datasets, UCF101 and HMDB51. Further analysis and ablation studies validate the effectiveness of our designs. Additionally, we show that the features extracted by our SeFAR could largely promote the ability of multimodal foundation models to understand fine-grained and domain-specific semantics. Code & Datasets: https://github.com/KyleHuang9/SeFAR.
PB  - arXiv
PY  - 2025
ST  - SeFAR
Y2  - 2025/05/05/21:54:32
DO  - 10.1609/aaai.v39i4.32400
ER  -


TY  - GEN
AU  - Jiang, Z.
AU  - Chen, W.
AU  - Zhang, W.
AU  - Lin, Y.
AU  - Wan, H.
TI  - MSTBC: X Bot Detection with Multiple Social-Temporal Behavior Contrast
AB  - X bot detection aims to automatically identify malicious X bots on the X platform, playing a crucial role in protecting information and maintaining platform stability. Recently, mixture-based methods primarily simultaneously consider investigating various social features (e.g. user metadata, tweets, and social relationships) of users to differentiate humans and bots, which hold excellent performance. However, two major challenges have not been adequately addressed in current mixture-based methods: (1) Humans and bots exhibit different temporal behavior patterns, which has not been fully explored. (2) Existing mixture-based methods promote the detection by fusing diverse features but overlook the noise accumulation that arises during the fusion process. In this paper, we propose a novel X bot detection method with Multiple Social-Temporal Behavior Contrast (MSTBC), which integrates users’ multiple social-temporal behaviors, including the static behavior (description content), social behavior (social structure) and temporal behavior (temporal behavior patterns). Specifically, the fine-grained temporal behaviors of users are represented as four different prompts. A temporal behavior PLM with temporal behavior prompts in MSTBC serves as the encoder to understand temporal behavior patterns. In addition, we employ multi-behavior contrast to minimize the differences of various features of users, alleviating the noise accumulation that arises during the fusion of diverse features. Experimental results demonstrate that MSTBC outperforms state-of-the-art models on four datasets. The code is available at https://anonymous.4open.science/r/MSTBC-C659.
PB  - Research Square
PY  - 2025
ST  - MSTBC
Y2  - 2025/05/05/21:54:32
DO  - 10.21203/rs.3.rs-5699605/v1
ER  -


TY  - GEN
AU  - Yoshida, S.M.
AU  - Shibata, T.
AU  - Terao, M.
AU  - Okatani, T.
AU  - Sugiyama, M.
TI  - Action-Agnostic Point-Level Supervision for Temporal Action Detection
AB  - We propose action-agnostic point-level (AAPL) supervision for temporal action detection to achieve accurate action instance detection with a lightly annotated dataset. In the proposed scheme, a small portion of video frames is sampled in an unsupervised manner and presented to human annotators, who then label the frames with action categories. Unlike point-level supervision, which requires annotators to search for every action instance in an untrimmed video, frames to annotate are selected without human intervention in AAPL supervision. We also propose a detection model and learning method to effectively utilize the AAPL labels. Extensive experiments on the variety of datasets (THUMOS’14, FineAction, GTEA, BEOID, and ActivityNet 1.3) demonstrate that the proposed approach is competitive with or outperforms prior methods for video-level and point-level supervision in terms of the trade-off between the annotation cost and detection performance.1
PB  - arXiv
PY  - 2024
ST  - Action-Agnostic Point-Level Supervision for Temporal Action Detection
Y2  - 2025/05/05/21:54:32
DO  - 10.1609/aaai.v39i9.33037
ER  -


TY  - GEN
AU  - Zahoor, A.
AU  - Gillani, I.A.
AU  - Bashir, J.U.
TI  - Influence Maximization in Temporal Networks with Persistent and Reactive Behaviors
AB  - Influence maximization in temporal social networks presents unique challenges due to the dynamic interactions that evolve over time. Traditional diffusion models often fall short in capturing the real-world complexities of active-inactive transitions among nodes, obscuring the true behavior of influence spread. In dynamic networks, nodes do not simply transition to an active state once; rather, they can oscillate between active and inactive states, with the potential for reactivation and reinforcement over time. This reactivation allows previously influenced nodes to regain influence potency, enhancing their ability to spread influence to others and amplifying the overall diffusion process. Ignoring these transitions can thus conceal the cumulative impact of influence, making it essential to account for them in any effective diffusion model. To address these challenges, we introduce the Continuous Persistent Susceptible-Infected Model with Reinforcement and Re-activation (cpSI-R), which explicitly incorporates active-inactive transitions, capturing the progressive reinforcement that makes nodes more potent spreaders upon reactivation. This model naturally leads to a submodular and monotone objective function, which supports efficient optimization for seed selection in influence maximization tasks. Alongside cpSI-R, we propose an efficient temporal snapshot sampling method, simplifying the analysis of evolving networks. We then adapt the prior algorithms of seed selection to our model and sampling strategy, resulting in reduced computational costs and enhanced seed selection efficiency. Experimental evaluations on diverse datasets demonstrate substantial improvements in performance over baseline methods, underscoring the effectiveness of cpSI-R for real-world temporal networks.
PB  - arXiv
PY  - 2024
ST  - Influence Maximization in Temporal Networks with Persistent and Reactive Behaviors
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5223111
ER  -


TY  - GEN
AU  - Fuse, S.
AU  - Sugiyama, Y.
AU  - Dhingra, R.R.
AU  - Okada, Y.
AU  - Oku, Y.
TI  - Spatio-temporal segregation between sensory relay and swallowing pre-motor population activities by optical imaging in the rat nucleus of the solitary tract.
AB  - The nucleus tractus solitarius (NTS) contains neurons that relay sensory swallowing commands information from the oropharyngeal cavity and swallowing premotor neurons of the dorsal swallowing group (DSG). However, the spatio-temporal dynamics of the interplay between the sensory relay and the DSG is not well understood. Here we employed fluorescence imaging after microinjection of the calcium indicator into the NTS in an arterially perfused brainstem preparation of rat (n = 8) to investigate neuronal population activity in the NTS in response to superior laryngeal nerve (SLN) stimulation. Respiratory and swallowing motor activities were determined by simultaneous recordings of phrenic and vagal nerve activity (PNA, VNA). Analysis of SLN stimulation near the threshold triggering a swallowing allowed us to analyze Ca2+ signals related to the sensory relay and the DSG. We show that activation of sensory relay neurons triggers spatially confined Ca2+ signals exclusively unilateral to the stimulated SLN at short latencies (114.3 ± 94.4 ms). However, SLN-evoked swallowing triggered Ca2+ signals bilaterally at longer latencies (200 ± 145.2 ms) and engaged anatomically distributed DSG activity across the dorsal medulla oblongata. The Ca2+ signals originating from the DSG preceded evoked VNA swallow motor bursts, thus the swallowing premotor neurons that drive laryngeal motor pools are located outside the DSG. In conclusion, the study illuminates the spatial-temporal features of sensory-motor integration of swallowing in the NTS and further supports the hypothesis that the NTS harbors swallowing pre-motor neurons that may generate the swallowing motor activity while first order pre-motor pools are located outside the DSG.
PB  - Research Square
PY  - 2024
ST  - Spatio-temporal segregation between sensory relay and swallowing pre-motor population activities by optical imaging in the rat nucleus of the solitary tract.
Y2  - 2025/05/05/21:54:32
DO  - 10.21203/rs.3.rs-5104317/v1
ER  -


TY  - GEN
AU  - Charvátová, K.
AU  - Malý, P.
TI  - Spectro-temporal symmetry in action-detected optical spectroscopy: highlighting excited-state dynamics in large systems
AB  - Multidimensional optical spectroscopy observes transient excitation dynamics through the time evolution of spectral correlations. Its action-detected variants offer several advantages over the coherent detection and are thus becoming increasingly widespread. Nevertheless, a drawback of action-detected spectra is the presence of a large stationary background of so-called incoherent mixing of excitations from independent states that resembles a product of ground-state absorption spectra and obscures the excited-state signal. This issue is especially problematic in fluorescence-detected two-dimensional electronic spectroscopy (F-2DES) and fluorescence-detected pump–probe spectroscopy (FPP) of extended systems, where large incoherent mixing arises from efficient exciton–exciton annihilation. In this work, we demonstrate on the example of F-2DES and F-PP an inherent spectro-temporal symmetry of action-detected spectra, which allows general, system-independent subtraction of any stationary signals including incoherent mixing. We derive the expressions for spectra with normal and reversed time ordering of the pulses, relating these to the symmetry of the system response. As we demonstrate both analytically and numerically, the difference signal constructed from spectra with normal and reversed pulse ordering is free of incoherent mixing and highlights the excitation dynamics. We further verify the approach on the experimental F-PP spectra of a molecular squaraine heterodimer and the F-2DES spectra of the photosynthetic antenna LH2 of purple bacteria. The approach is generally applicable to action-detected 2DES and pump–probe spectroscopy without experimental modifications and independent of the studied system, enabling their application to large systems such as molecular complexes.
PB  - arXiv
PY  - 2024
ST  - Spectro-temporal symmetry in action-detected optical spectroscopy
Y2  - 2025/05/05/21:54:32
DO  - 10.1063/5.0255316
ER  -


TY  - GEN
AU  - Adachi, R.
AU  - Kojima, H.
AU  - Ikegami, T.
TI  - Spatiotemporal characterization of emergent behavior of self-propelled oil droplet
AB  - To further understand the complex behavior of swimming microorganisms, the spontaneous motion of nonliving matter provides essential insights. While substantial research has focused on quantitatively analyzing complex behavioral patterns, characterizing these dynamics aiming for inclusive comparison to the behavior of living systems remains challenging. In this study, we experimentally and numerically investigated the’life-like’ behavior of an oil droplet in an aqueous surfactant solution by identifying behavioral modes of spontaneous motion patterns in response to varying physical parameters, such as volume and oil concentration. Leveraging data-driven nonparametric dynamical systems analysis, we discovered the low dimensionality and nonlinearity of the underlying dynamical system governing oil droplet motion. Notably, our simulations demonstrate that the two-dimensional Langevin equations effectively reproduce the overall behavior experimentally observed while retaining the rational correspondence with physical parameter interpretations. These findings not only elucidate the fundamental dynamics governing the spontaneous motion of oil droplet systems but also suggest potential pathways for developing more descriptive models that bridge the gap between nonliving and living behaviors.
PB  - arXiv
PY  - 2024
ST  - Spatiotemporal characterization of emergent behavior of self-propelled oil droplet
Y2  - 2025/05/05/21:54:32
DO  - 10.1063/pt.3.4508
ER  -


TY  - GEN
AU  - Liu, Z.
AU  - Zhang, L.
AU  - Li, B.
AU  - Chen, Z.
AU  - Zhu, C.
TI  - WiFi CSI Based Temporal Activity Detection via Dual Pyramid Network
AB  - We address the challenge of WiFi-based temporal activity detection and propose an efficient Dual Pyramid Network that integrates Temporal Signal Semantic Encoders and Local Sensitive Response Encoders. The Temporal Signal Semantic Encoder splits feature learning into high and low-frequency components, using a novel Signed Mask-Attention mechanism to emphasize important areas and downplay unimportant ones, with the features fused using ContraNorm. The Local Sensitive Response Encoder captures fluctuations without learning. These feature pyramids are then combined using a new cross-attention fusion mechanism. We also introduce a dataset with over 2,114 activity segments across 553 WiFi CSI samples, each lasting around 85 seconds. Extensive experiments show our method outperforms challenging baselines.
PB  - arXiv
PY  - 2024
ST  - WiFi CSI Based Temporal Activity Detection via Dual Pyramid Network
Y2  - 2025/05/05/21:54:32
DO  - 10.1609/aaai.v39i1.32035
ER  -


TY  - GEN
AU  - Yu, Y.
AU  - Cao, C.
AU  - Zhang, Y.
AU  - Min, L.
AU  - Zhang, Y.
TI  - Building a Multi-modal Spatiotemporal Expert for Zero-shot Action Recognition with CLIP
AB  - Zero-shot action recognition (ZSAR) requires collaborative multi-modal spatiotemporal understanding. However, finetuning CLIP directly for ZSAR yields suboptimal performance, given its inherent constraints in capturing essential temporal dynamics from both vision and text perspectives, especially when encountering novel actions with fine-grained spatiotemporal discrepancies. In this work, we propose Spatiotemporal Dynamic Duo (STDD), a novel CLIP-based framework to comprehend multi-modal spatiotemporal dynamics synergistically. For the vision side, we propose an efficient Space-time Cross Attention, which captures spatiotemporal dynamics flexibly with simple yet effective operations applied before and after spatial attention, without adding additional parameters or increasing computational complexity. For the semantic side, we conduct spatiotemporal text augmentation by comprehensively constructing an Action Semantic Knowledge Graph (ASKG) to derive nuanced text prompts. The ASKG elaborates on static and dynamic concepts and their interrelations, based on the idea of decomposing actions into spatial appearances and temporal motions. During the training phase, the frame-level video representations are meticulously aligned with prompt-level nuanced text representations, which are concurrently regulated by the video representations from the frozen CLIP to enhance generalizability. Extensive experiments validate the effectiveness of our approach, which consistently surpasses state-of-the-art approaches on popular video benchmarks (i.e., Kinetics-600, UCF101, and HMDB51) under challenging ZSAR settings.
PB  - arXiv
PY  - 2024
ST  - Building a Multi-modal Spatiotemporal Expert for Zero-shot Action Recognition with CLIP
Y2  - 2025/05/05/21:54:32
DO  - 10.1609/aaai.v39i9.33050
ER  -


TY  - GEN
AU  - Li, Q.
AU  - Liu, D.
AU  - Kong, J.
AU  - Xu, H.
AU  - Wang, J.
TI  - Temporal Action Localization with Cross Layer Task Decoupling and Refinement
AB  - Temporal action localization (TAL) involves dual tasks to classify and localize actions within untrimmed videos. However, the two tasks often have conflicting requirements for features. Existing methods typically employ separate heads for classification and localization tasks but share the same input feature, leading to suboptimal performance. To address this issue, we propose a novel TAL method with Cross Layer Task Decoupling and Refinement (CLTDR). Based on the feature pyramid of video, CLTDR strategy integrates semantically strong features from higher pyramid layers and detailed boundary-aware boundary features from lower pyramid layers to effectively disentangle the action classification and localization tasks. Moreover, the multiple features from cross layers are also employed to refine and align the disentangled classification and regression results. At last, a lightweight Gated Multi-Granularity (GMG) module is proposed to comprehensively extract and aggregate video features at instant, local, and global temporal granularities. Benefiting from the CLTDR and GMG modules, our method achieves state-of-the-art performance on five challenging benchmarks: THUMOS14, MultiTHUMOS, EPIC-KITCHENS-100, ActivityNet-1.3, and HACS. Our code and pre-trained models are publicly available at: https://github.com/LiQiang0307/CLTDR-GMG.
PB  - arXiv
PY  - 2024
ST  - Temporal Action Localization with Cross Layer Task Decoupling and Refinement
Y2  - 2025/05/05/21:54:32
DO  - 10.1609/aaai.v39i5.32516
ER  -


TY  - GEN
AU  - Li, K.
AU  - Peng, X.
AU  - Guo, D.
AU  - Yang, X.
AU  - Wang, M.
TI  - Repetitive Action Counting with Hybrid Temporal Relation Modeling
AB  - Repetitive Action Counting (RAC) aims to count the number of repetitive actions occurring in videos. In the real world, repetitive actions have great diversity and bring numerous challenges (e.g., viewpoint changes, non-uniform periods, and action interruptions). Existing methods based on the temporal self-similarity matrix (TSSM) for RAC are trapped in the bottleneck of insufficient capturing action periods when applied to complicated daily videos. To tackle this issue, we propose a novel method named Hybrid Temporal Relation Modeling Network (HTRM-Net) to build diverse TSSM for RAC. The HTRM-Net mainly consists of three key components: bimodal temporal self-similarity matrix modeling, random matrix dropping, and local temporal context modeling. Specifically, we construct temporal self-similarity matrices by bi-modal (self-attention and dual-softmax) operations, yielding diverse matrix representations from the combination of row-wise and column-wise correlations. To further enhance matrix representations, we propose incorporating a random matrix dropping module to guide channel-wise learning of the matrix explicitly. After that, we inject the local temporal context of video frames and the learned matrix into temporal correlation modeling, which can make the model robust enough to cope with error-prone situations, such as action interruption. Finally, a multi-scale matrix fusion module is designed to aggregate temporal correlations adaptively in multi-scale matrices. Extensive experiments across intra- and cross-datasets demonstrate that the proposed method not only outperforms current state-of-the-art methods and but also exhibits robust capabilities in accurately counting repetitive actions in unseen action categories. Notably, our method surpasses the classical TransRAC method by 20.04% in MAE and 22.76% in OBO.
PB  - arXiv
PY  - 2024
ST  - Repetitive Action Counting with Hybrid Temporal Relation Modeling
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/tmm.2025.3535385
ER  -


TY  - GEN
AU  - Niederkofler, T.
AU  - Sitonio, C.
AU  - Lechner, C.
TI  - Let's Have a Party! - Temporal Landmarks and Firm Behaviour: How Corporate Anniversaries Influence Managerial Decision
AB  - This study investigates how symbolic temporal landmarks, like corporate anniversaries, affect firm performance. Temporal landmarks may act as reference points for managerial action. Specifically, we examine whether managers leverage these anniversaries financially, potentially using earnings management to boost firm performance in key anniversary years, while minimizing detection by stakeholders. We argue that the motivation to present stronger results impacts managerial decision-making.
PB  - SSRN
PY  - 2024
ST  - Let's Have a Party! - Temporal Landmarks and Firm Behaviour
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5044357
ER  -


TY  - GEN
AU  - Niederkofler, T.
AU  - Sitonio, C.
AU  - Lechner, C.
TI  - Let's Have a Party! - Temporal Landmarks and Firm Behaviour: How Corporate Anniversaries Influence Managerial Decision
AB  - This study investigates how symbolic temporal landmarks, like corporate anniversaries, affect firm performance. Temporal landmarks may act as reference points for managerial action. Specifically, we examine whether managers leverage these anniversaries financially, potentially using earnings management to boost firm performance in key anniversary years, while minimizing detection by stakeholders. We argue that the motivation to present stronger results impacts managerial decision-making.
PB  - SSRN
PY  - 2024
ST  - Let's Have a Party! - Temporal Landmarks and Firm Behaviour
Y2  - 2025/05/05/21:54:32
ER  -


TY  - GEN
AU  - Qi, F.
AU  - Hou, Z.
AU  - Lin, E.
AU  - Liang, J.
AU  - Zhou, X.
TI  - Pig Behavior Dataset and Spatial-Temporal Perception and Enhancement Networks Based on the Attention Mechanism for Pig Behavior Recognition
AB  - The recognition of pig behavior plays a crucial role in smart farming and welfare assurance for pigs.Currently,in the field of pig behavior recognition,the lack of publicly available behavioral datasets not only limits the development of innovative algorithms but also hampers model robustness and algorithm optimization.This paper proposes and publicly releases a dataset containing 13 pig behaviors that significantly impact welfare.In multi-target behavior recognition tasks,prediction accuracy depends on sensitive features and their dynamic changes related to the target of recognition over time.Based on this dataset,this paper proposes a spatial-temporal perception and enhancement networks based on the attention mechanism for classifying 13 behaviors of 8 pigs in video data.The network consists of a spatiotemporal perception network and feature enhancement network. The spatiotemporal perception network is responsible for capturing essential spatial features and subtle changes at key positions during movement to remove redundant information unrelated to individual pigs.The spatiotemporal feature enhancement network leverages the channel-level spatial attention mechanism and the Motion Feature Enhancement Module to further strengthen important spatial features of individual pigs and capture the long-term dependencies of spatiotemporal features in individual behaviors.This enhances the model’s sensitivity to spatiotemporal changes.Experimental results show our model achieves a mAP of 75.92% for the 13 behaviors and 87.63% for the top 5 behaviors by sample count, improving by 14.94% and 6.31%, respectively, over traditional models.This study improves the accuracy of individual pig behavior and offers a novel technical approach for modern smart farming. The dataset and relevant code will be publicly available with this paper.
PB  - SSRN
PY  - 2024
ST  - Pig Behavior Dataset and Spatial-Temporal Perception and Enhancement Networks Based on the Attention Mechanism for Pig Behavior Recognition
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5042825
ER  -


TY  - GEN
AU  - Ma, L.
AU  - Padmanabhan, A.
AU  - Ganesh, A.
AU  - Kumar, S.
AU  - Achan, K.
TI  - Improving Sequential Recommender Systems with Online and In-store User Behavior
AB  - —Online e-commerce platforms have been extending in-store shopping, which allows users to keep the canonical online browsing and checkout experience while exploring in-store shopping. However, the growing transition between online and in-store becomes a challenge to online sequential recommender systems for future online interaction prediction due to the lack of holistic modeling of hybrid user behaviors (online & in-store). The challenges are two-fold. First, combining online & in-store user behavior data into a single data schema and supporting multiple stages in the model life cycle (pre-training, training, inference, etc.) organically needs a new data pipeline design. Second, online recommender systems, which solely relies on online user behavior sequences, must be redesigned to support online and in-store user data as input under the sequential modeling setting. To overcome the first challenge, we propose a hybrid, omnichannel data pipeline to compile online & in-store user behavior data by caching information from diverse data sources. Later, we introduce a model-agnostic encoder module to the sequential recommender system to interpret the user in-store transaction and augment the modeling capacity for better online interaction prediction given the hybrid user behavior.
PB  - arXiv
PY  - 2024
ST  - Improving Sequential Recommender Systems with Online and In-store User Behavior
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/bigdata62323.2024.10825717
ER  -


TY  - GEN
AU  - Kaneko, R.
AU  - Simpson, E.H.
AU  - Balsam, P.D.
TI  - IMPACT OF TEMPORAL UNCERTAINTY ON SIGN-TRACKING BEHAVIOR
AB  - Sign-tracking behavior, also known as “autoshaping”, is defined as approach and interaction with reward-predictive cues. It is associated with addiction-related phenotypes and compulsive behavior. Several previous studies have demonstrated that when there is uncertainty about reward properties (e.g. probability, size), sign-tracking is increased. However, the effect of cue-uncertainty on sign-tracking behavior is not known. Here, using a Pavlovian conditioning paradigm, we manipulated the temporal uncertainty about the appearance of cues by implementing either fixed or variable inter-trial intervals (ITIs) of different durations across groups of mice. We found that temporal uncertainty during acquisition significantly enhances sign-tracking, which persists during extinction, even when ITI variability was different in the extinction session than in the acquisition session. This suggests that the effects of temporal uncertainty are learned and retained, rather than performance-based. Our results demonstrate that sign-tracking behavior is not only modified by the characteristic of the reward, but it can also be modified by the uncertainty regarding cues. These findings highlight how temporal predictability shapes cue-directed behaviors and has implications for understanding reward-related behavioral responses including sign-tracking behaviors.
PB  - bioRxiv
PY  - 2024
ST  - IMPACT OF TEMPORAL UNCERTAINTY ON SIGN-TRACKING BEHAVIOR
Y2  - 2025/05/05/21:54:32
DO  - 10.1037/xan0000394
ER  -


TY  - GEN
AU  - Yuan, Z.
AU  - Liu, K.
AU  - Dan, Z.
AU  - Mima, C.
AU  - Lu, C.
TI  - The Stream Runoff Spatiotemproal Evolution and Influence Factors of Nianchu River Basin in Southwestern Tibet Driven by Climate Change and Human Activity
AB  - Study RegionNianchu River Basin (NRB), a key agricultural area on the Tibetan Plateau in China.Study focusRunoff is recognized as the most important outcome of water resources management, playing a crucial role in both ecology and hydrology. This study analyzes the spatiotemporal variability of runoff in the NRB over the past 48 years and further quantify the impacts of climate conditions, vegetation cover, and irrigation water withdrawal (IWW) on runoff changes. This study aims to improve our understanding of the mechanisms driving runoff changes in alpine mountains.New hydrological insight for the regionThe results indicate that NRB's runoff has significantly declined at a rate of -0.021 mm·a⁻¹. The effects of the various influences on runoff are non-stationary at the spatiotemporal scales, with precipitation (Pre) and temperature (Tmp) as the main factors, and Pre consistently contributing the most to changes in runoff until 2002. However, over time, the contribution of Pre has significantly declined. After 2002, the contribution of Tmp gradually became comparable to that of Pre and eventually surpassed it. IWW extraction generally exerts adverse effects, and in areas with poorer vegetation in NRB, NDVI changes significantly impact runoff. Under the context of global warming and increasing human activities, Tmp and IWW have shown a growing adverse impact on basin runoff, offering new insights for water resource management.
PB  - SSRN
PY  - 2024
ST  - The Stream Runoff Spatiotemproal Evolution and Influence Factors of Nianchu River Basin in Southwestern Tibet Driven by Climate Change and Human Activity
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5038153
ER  -


TY  - GEN
AU  - Wang, Y.
AU  - Gao, Z.
AU  - Wang, Q.
AU  - Li, P.
AU  - Hu, Q.
TI  - TAMT: Temporal-Aware Model Tuning for Cross-Domain Few-Shot Action Recognition
AB  - Going beyond few-shot action recognition (FSAR), cross-domain FSAR (CDFSAR) has attracted recent research interests by solving the domain gap lying in source-to-target transfer learning. Existing CDFSAR methods mainly focus on joint training of source and target data to mitigate the side effect of domain gap. However, such kind of methods suffer from two limitations: First, pair-wise joint training requires retraining deep models in case of one source data and multiple target ones, which incurs heavy computation cost, especially for large source and small target data. Second, pre-trained models after joint training are adopted to target domain in a straightforward manner, hardly taking full potential of pre-trained models and then limiting recognition performance. To overcome above limitations, this paper proposes a simple yet effective baseline, namely Temporal-Aware Model Tuning (TAMT) for CDFSAR. Specifically, our TAMT involves a decoupled paradigm by performing pre-training on source data and fine-tuning target data, which avoids retraining for multiple target data with single source. To effectively and efficiently explore the potential of pre-trained models in transferring to target domain, our TAMT proposes a Hierarchical Temporal Tuning Network (HTTN), whose core involves local temporal-aware adapters (TAA) and a global temporal-aware moment tuning (GTMT). Particularly, TAA learns few parameters to recalibrate the intermediate features of frozen pre-trained models, enabling efficient adaptation to target domains. Furthermore, GTMT helps to generate powerful video representations, improving match performance on the target domain. Experiments on several widely used video benchmarks show our TAMT outperforms the recently proposed counterparts by 13%∼31%, achieving new state-of-the-art CDFSAR results.
PB  - arXiv
PY  - 2024
ST  - TAMT
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/cvprw53098.2021.00313
ER  -


TY  - GEN
AU  - Rockoirina, P.
AU  - Fairweather, S.
AU  - Abercrombie, E.
AU  - Cumberbatch, E.
AU  - MacIntyre, G.
TI  - Autonomous Ransomware Detection Using Temporal Behavior Chain Analysis
AB  - Effective detection of sophisticated cyber threats requires innovative frameworks capable of analyzing the complex and evolving behaviors of malicious software. The Temporal Behavior Chain Analysis (TBCA) framework introduces a novel methodology for identifying ransomware activity through the examination of temporally dependent system interactions. By constructing directed acyclic graphs to represent behavioral sequences, the framework captures subtle anomalies that traditional detection methods fail to identify. Machine learning techniques, including Hidden Markov Models and neural networks, enhance the framework’s ability to discern malicious patterns from benign behaviors with high accuracy and efficiency. Evaluations demonstrated superior detection rates across multiple ransomware families, including LockBit, BlackCat, and Hive, even when advanced obfuscation techniques were employed. The modular design of TBCA ensures seamless integration into existing infrastructures while maintaining scalability for deployment in diverse operational environments. Resource efficiency was validated through extensive scalability testing, which confirmed the framework’s capability to process high volumes of data with minimal computational overhead. Comparative analyses revealed significant improvements over traditional detection approaches, particularly in handling emerging and previously unseen ransomware variants. Temporal modeling of sequential dependencies enables early detection, reducing potential system damage and facilitating real-time mitigation strategies. The integration of adaptive decision-making mechanisms minimizes false positives, enhancing reliability in high-stakes environments. Cross-platform evaluations further highlighted the robustness of the framework, demonstrating its effectiveness on Windows, Linux, and macOS systems. The TBCA framework establishes a comprehensive foundation for advancing proactive cybersecurity defenses against increasingly complex and dynamic threats.
PB  - TechRxiv
PY  - 2024
ST  - Autonomous Ransomware Detection Using Temporal Behavior Chain Analysis
Y2  - 2025/05/05/21:54:32
DO  - 10.36227/techrxiv.173337230.07088213/v1
ER  -


TY  - GEN
AU  - Blaas, N.
AU  - Winterbourne, J.
AU  - Beauregarde, W.
AU  - Heathcote, E.
TI  - Ransomware Detection Through Contextual Behavior Mapping and Sequential Dependency Analysis
AB  - The escalating sophistication of cyber threats necessitates innovative detection methodologies to safeguard digital infrastructures. This research introduces a novel framework that integrates contextual behavior mapping with sequential dependency analysis, aiming to enhance the identification of both known and emerging ransomware variants. By employing probabilistic modeling and graph-based techniques, the proposed system effectively captures complex operational patterns inherent in ransomware activities. Extensive experiments conducted within controlled environments demonstrate the framework’s high detection accuracy and low false positive rates, even when confronted with advanced evasion strategies. The scalability assessment reveals its capability to manage concurrent ransomware instances without significant performance degradation, showing its applicability in large-scale network infrastructures. Furthermore, the resource efficiency analysis indicates minimal computational overhead, facilitating seamless integration into existing security architectures. The robustness evaluation against polymorphic and metamorphic ransomware families highlights the framework’s resilience, emphasizing the importance of adaptive detection mechanisms in contemporary cybersecurity landscapes. Collectively, these findings validate the proposed approach as a practical and efficient solution for ransomware detection, addressing critical challenges in modern security environments through innovative analytical methodologies and scalable design.
PB  - Research Square
PY  - 2024
ST  - Ransomware Detection Through Contextual Behavior Mapping and Sequential Dependency Analysis
Y2  - 2025/05/05/21:54:32
DO  - 10.21203/rs.3.rs-5527159/v1
ER  -


TY  - GEN
AU  - Bansal, N.
AU  - Bansal, A.
AU  - Gupta, M.
TI  - Enhanced Action Recognition through Deep Spatiotemporal Learning Using 3D CNN and GRU
AB  - The issues revolve around efficiently analyzing large video data streams while minimizing computer complexity and performing processing in real-time. On the other hand, it becomes more difficult to quickly react to unusual actions because of this. Also, smart homes, security systems, assisted living facilities, and health monitoring might all benefit from the ability to recognize events from video sequences. The techniques used to analyse data are still under constant scrutiny, even if sensing technology has advanced, especially with respect to 3D video. By combining 3D Convolutional Neural Networks (CNN) with gated recurrent units (GRU), we have created a new method for learning spatiotemporal features in movies. We found that 3D convolutional neural networks (CNNs) acquire spatiotemporal information better than 2D CNNs using the UCF50 dataset. Using smaller 3x3x3 convolution kernels in a uniform design also improves performance. Furthermore, we found that 3D CNN with GRU integrated yields better accuracy than 3D CNN alone. The results show that GRU outperforms LSTM in terms of accuracy (89.89%) and calculation time (less than LSTM) when compared.
PB  - Research Square
PY  - 2024
ST  - Enhanced Action Recognition through Deep Spatiotemporal Learning Using 3D CNN and GRU
Y2  - 2025/05/05/21:54:32
DO  - 10.62441/nano-ntp.vi.4306
ER  -


TY  - GEN
AU  - Das, S.
AU  - Adhikary, R.
AU  - Bagchi, S.
AU  - Chakraborty, A.
AU  - Das, A.
TI  - Correlating molecular structure and self-assembly mechanism via Temporal Analysis of Multidimensional Chemical Interactions Space: Understanding the difference between assembly behaviors of isomeric peptides in water
AB  - Computational modelling of self-assembly mechanisms is a promising way to establish chemically meaningful relationships between molecular structures of the building blocks and the final outcomes of the spontaneous assemblies. However, such connections are not immediately apparent, due to the presence of complex interplay involving a multitude of intermolecular interactions with complicated temporal variations. In this paper, we propose a method, called Temporal Analysis of Multidimensional Chemical Interaction Space (TAMCIS), which looks at important combinations of interactions, rather than analyzing them one at a time. Each molecule was assigned a vector order parameter, with components representing appropriately chosen chemical interactions.
PB  - ChemRxiv
PY  - 2024
ST  - Correlating molecular structure and self-assembly mechanism via Temporal Analysis of Multidimensional Chemical Interactions Space
Y2  - 2025/05/05/21:54:32
DO  - 10.26434/chemrxiv-2024-0cgwn
ER  -


TY  - GEN
AU  - Oloketuyi, J.
AU  - Liu, Y.
AU  - Deng, L.
AU  - Sha, F.
AU  - Liu, Q.
TI  - Investigating the Behavior and Spatiotemporal Variations of Green-line Emission in the Solar Corona
AB  - Understanding coronal structure and dynamics can be facilitated by analyzing green-line emission, which enables the investigation of diverse coronal structures such as coronal loops, streamers, coronal holes, and various eruptions in the solar atmosphere. In this study, we investigated the spatiotemporal behaviors of green-line emissions in both low and high latitudes across nine solar cycles, ranging from Solar Cycle 17 to the current Solar Cycle 25, using the modified homogeneous data set. We employed methodologies such as cross correlation, power spectral density, and wavelet transform techniques for this analysis. We found distinct behaviors in green-line energy across various latitudinal distributions in the solar atmosphere. The trends observed at higher latitudes differ from those at lower latitudes. The emission behaviors show a close association with other solar phenomena like solar flares, sunspots, and coronal mass ejections throughout the solar cycles. The observed variations exhibit harmonic periods. The emission activity is significantly higher in the low latitudes, accounting for over 70% of the emissions, while the higher latitudes contribute less than 30%. The emissions exhibit asymmetric behavior between the northern and southern hemispheres, leading to a 44 yr cycle of solar hemispheric dominance shifts. Various factors, such as Alfvén waves, solar magnetic fields, sunspots, differential rotation, and reconnection events, influence the observed differences in behavior between lower and higher latitudes, suggesting the existence of potential underlying phenomena contributing to deviations in properties, intensity, temporal dynamics, and spatiotemporal lifetime.
PB  - arXiv
PY  - 2024
ST  - Investigating the Behavior and Spatiotemporal Variations of Green-line Emission in the Solar Corona
Y2  - 2025/05/05/21:54:32
DO  - 10.3847/1538-4365/ad746a
ER  -


TY  - GEN
AU  - Chen, L.
AU  - Li, Z.
AU  - Zhang, C.
AU  - Zhou, M.
AU  - Peng, J.
TI  - Spatiotemporal Changes of Vegetation in the Northern Foothills of Qinling Mountains Based on Kndvi Considering Climate Time-Lag Effects and Human Activities
AB  - Vegetation is fundamental to regulating the climate system and ensuring carbon balance. Recognizing the effects of climate change (CC) and human activities (HA) is vital for understanding shifts in vegetation. However, climate time-lag effects are rarely measured, resulting in an inadequate assessment of CC's effects on vegetation dynamics. In this study, firstly, based on the Landsat image dataset, the spatiotemporal variations of the kernel Normalized Difference Vegetation Index (kNDVI) in the northern foothills of the Qinling Mountains (NQLM) from 1986 to 2022 were analyzed. Then, the multiple regression residuals method, accounting for time-lag effects, was employed to determine the effects of CC and HA on kNDVI change. Finally, six patterns of kNDVI change were obtained based on the kNDVI trend and the changes of CC and HA to kNDVI. Our research found: (1) During the past 37 years, the kNDVI increased at a rate of 0.0061/a, the annual kNDVI increased in 84.82% areas of the NQLM, and the kNDVI decreased in 0.86% areas. (2) The kNDVI exhibited a positive correlation with both precipitation and temperature, kNDVI response to precipitation with 1-month time lag and 0-month for temperature. (3) The contribution of CC to kNDVI change was 84%, temperature and precipitation drive kNDVI change rates with 0.0012/a and 0.0039/a, respectively. The contribution of HA to kNDVI change was only 16%. (4) Among the six patterns of kNDVI change, CC and HA collectively contributed to kNDVI change, and the effect of CC alone was more significant than that of HA. These findings will assist the critical need for informed, sustainable vegetation restoration strategies in the NQLM.
PB  - SSRN
PY  - 2024
ST  - Spatiotemporal Changes of Vegetation in the Northern Foothills of Qinling Mountains Based on Kndvi Considering Climate Time-Lag Effects and Human Activities
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5031411
ER  -


TY  - GEN
AU  - Chen, H.
AU  - Wang, L.
AU  - Chen, Y.
AU  - Gedeon, T.
AU  - Koniusz, P.
TI  - When Spatial meets Temporal in Action Recognition
AB  - Video action recognition has made significant strides, but challenges remain in effectively using both spatial and temporal information. While existing methods often focus on either spatial features (e.g., object appearance) or temporal dynamics (e.g., motion), they rarely address the need for a comprehensive integration of both. Capturing the rich temporal evolution of video frames, while preserving their spatial details, is crucial for improving accuracy. In this paper, we introduce the Temporal Integration and Motion Enhancement (TIME) layer, a novel preprocessing technique designed to incorporate temporal information. The TIME layer generates new video frames by rearranging the original sequence, preserving temporal order while embedding N2 temporally evolving frames into a single spatial grid of size N × N. This transformation creates new frames that balance both spatial and temporal information, making them compatible with existing video models. When N = 1, the layer captures rich spatial details, similar to existing methods. As N increases (N ≥ 2), temporal information becomes more prominent, while the spatial information decreases to ensure compatibility with model inputs. We demonstrate the effectiveness of the TIME layer by integrating it into popular action recognition models, such as ResNet-50, Vision Transformer, and Video Masked Autoencoders, for both RGB and depth video data. Our experiments show that the TIME layer enhances recognition accuracy, offering valuable insights for video processing tasks.
PB  - arXiv
PY  - 2024
ST  - When Spatial meets Temporal in Action Recognition
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-030-00776-8_78
ER  -


TY  - GEN
AU  - Smagghe, W.
AU  - Persyn, F.
AU  - Mertens, T.
AU  - De Jaeger, G.
AU  - Van Leene, J.
TI  - Spatiotemporal visualization of SnRK1 activity unveils its multifaceted role during plant growth and development
AB  - Plants are able to endure fluctuating environments through complex signaling networks, meticulously balancing growth decisions based on internal and external cues. Central to these networks, Sucrose non-fermenting-1 related kinase 1 (SnRK1) acts as a molecular fuel gauge that promotes survival by restraining growth and favoring catabolism under restrictive conditions. However, the detailed spatiotemporal dynamics of SnRK1's regulation of plant growth remain poorly understood given the lack of adequate tools that can capture these dynamics at cellular resolution. In this study, we employed a separation of phase-based activity reporter of kinase (SPARK)-based sensor to monitor SnRK1 activity at cellular resolution during the plant life cycle. Our findings unveiled a dual role for SnRK1: a constitutive one, tightly linked to meristematic and vascular tissues, and a dynamic one, steering growth according to energy and nutrient availability. Real-time visualization of a growing Arabidopsis root corroborated this dual role, showing SnRK1’s essential role in maintaining the apical root meristem, while also dynamically steering circadian root growth. Applying CRISPR-based tissue-specific knockout (CRISPR-TSKO) of SnRK1, confirmed SnRK1’s pivotal function in root growth and development. Our results highlight the power of ASP-SPARK for real-time, in vivo analysis of SnRK1 activity, advancing our understanding of this key metabolic regulator and paving the way for detailed insights into its relationship with plant growth and stress responses.
PB  - Research Square
PY  - 2024
ST  - Spatiotemporal visualization of SnRK1 activity unveils its multifaceted role during plant growth and development
Y2  - 2025/05/05/21:54:32
DO  - 10.21203/rs.3.rs-5290460/v1
ER  -


TY  - GEN
AU  - Yamasaki, K.
AU  - Fujisawa, A.
AU  - Nagashima, Y.
AU  - Kasuya, N.
AU  - Yamada, T.
TI  - Advanced Analysis of Spatiotemporal Behaviors of Modal Structures and Couplings for Plasma Tomography
AB  - Advanced methods, based on the Fourier-Rectangular Function (FRF) series expansion (K. Yamasaki et al., J.Appl. Phys. 126 043304 (2019)), are proposed to analyze plasma images obtained with tomography. The method is applied to images of the entire cross-section of a cylindrical plasma that is dominated by an oscillatory state. The oscillations are characterized by an azimuthal mode of m=4 (mother mode), whose spatial pattern is modulated by producing child modes (m = 3 and m = 5) through the nonlinear coupling with m=1 mode (farther mode). The proposed methods identify the spatiotemporal properties of the modes and their nonlinear coupling propagation along the amplitude trajectory of the father mode. Here, we report the newly developed analysis methods for tomography images and their capabilities to uncover underlying processes in the oscillatory state of turbulent plasmas.
PB  - Research Square
PY  - 2024
ST  - Advanced Analysis of Spatiotemporal Behaviors of Modal Structures and Couplings for Plasma Tomography
Y2  - 2025/05/05/21:54:32
DO  - 10.21203/rs.3.rs-5325781/v1
ER  -


TY  - GEN
AU  - He, C.
AU  - Liu, Y.
AU  - Li, Q.
AU  - Hong, C.
AU  - Yao, X.
TI  - Multi-Grained Preference Enhanced Transformer for Multi-Behavior Sequential Recommendation
AB  - Sequential recommendation (SR) aims to predict the next purchasing item according to users’ dynamic preference learned from their historical user-item interactions. To improve the performance of recommendation, learning dynamic heterogeneous cross-type behavior dependencies is indispensable for recommender system. However, there still exists some challenges in Multi-Behavior Sequential Recommendation (MBSR). On the one hand, existing methods only model heterogeneous multi-behavior dependencies at behavior-level or item-level, and modeling interaction-level dependencies is still a challenge. On the other hand, the dynamic multi-grained behavior-aware preference is hard to capture in interaction sequences, which reflects interaction-aware sequential pattern. To tackle these challenges, we propose a Multi-Grained Preference enhanced Transformer framework (M-GPT). First, M-GPT constructs an interaction-level graph of historical cross-typed interactions in a sequence. Then graph convolution is performed to derive interaction-level multi-behavior dependency representation repeatedly, in which the complex correlation between historical cross-typed interactions at specific orders can be well learned. Secondly, a novel multifaceted transformer architecture equipped with multi-grained user preference extraction is proposed to encode the interaction-aware sequential pattern enhanced by capturing temporal behavior-aware multi-grained preference . Experiments on the real-world datasets indicate that our method M-GPT consistently outperforms various state-of-the-art recommendation methods. Our code is available at: https://anonymous.4open.science/r/MGPTDF31.
PB  - arXiv
PY  - 2024
ST  - Multi-Grained Preference Enhanced Transformer for Multi-Behavior Sequential Recommendation
Y2  - 2025/05/05/21:54:32
DO  - 10.1145/3534678.3539342
ER  -


TY  - GEN
AU  - Zhang, Q.
AU  - Qi, Y.
TI  - Can MLLMs Guide Weakly-Supervised Temporal Action Localization Tasks?
AB  - Recent breakthroughs in Multimodal Large Language Models (MLLMs) have gained significant recognition within the deep learning community, where the fusion of the Video Foundation Models (VFMs) and Large Language Models(LLMs) has proven instrumental in constructing robust video understanding systems, effectively surmounting constraints associated with predefined visual tasks. These sophisticated MLLMs exhibit remarkable proficiency in comprehending videos, swiftly attaining unprecedented performance levels across diverse benchmarks. However, their operation demands substantial memory and computational resources, underscoring the continued importance of traditional models in video comprehension tasks. In this paper, we introduce a novel learning paradigm termed MLLM4WTAL. This paradigm harnesses the potential of MLLM to offer temporal action key semantics and complete semantic priors for conventional Weakly-supervised Temporal Action Localization (WTAL) methods. MLLM4WTAL facilitates the enhancement of WTAL by leveraging MLLM guidance. It achieves this by integrating two distinct modules: Key Semantic Matching (KSM) and Complete Semantic Reconstruction (CSR). These modules work in tandem to effectively address prevalent issues like incomplete and over-complete outcomes common in WTAL methods. Rigorous experiments are conducted to validate the efficacy of our proposed approach in augmenting the performance of various heterogeneous WTAL models.
PB  - arXiv
PY  - 2024
ST  - Can MLLMs Guide Weakly-Supervised Temporal Action Localization Tasks?
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/s00521-022-07102-x
ER  -


TY  - GEN
AU  - Mehmood, F.
AU  - Guo, X.
AU  - Chen, E.
AU  - Khan, A.A.
AU  - Ullah, S.
TI  - Extended Multi-stream Temporal-attention Module for Skeleton-based Human Action Recognition (HAR)
AB  - Graph convolutional networks (GCNs) are an effective skeleton-based human action recognition (HAR) technique. GCNs enable the specification of CNNs to a non-Euclidean frame that is more flexible. The previous GCN-based models still have a lot of issues: (I) The graph structure is the same for all model layers and input data. GCN model's hierarchical structure and human action recognition input diversity make this a problematic approach; (II) Bone length and orientation are understudied due to their significance and variance in HAR. For this purpose, we introduce an Extended Multi-stream Temporal-attention Adaptive GCN (EMS-TAGCN). By training the network topology of the proposed model either consistently or independently according to the input data, this data-based technique makes graphs more flexible and faster to adapt to a new dataset. A spatial, temporal, and channel attention module helps the adaptive graph convolutional layer focus on joints, frames, and features. Hence, a multi-stream framework representing bones, joints, and their motion enhances recognition accuracy. Our proposed model outperforms the NTU RGBD for CS and CV by 0.6% and 1.4%, respectively, while Kinetics-skeleton Top-1 and Top-5 are 1.4% improved, UCF-101 has improved 2.34% accuracy and HMDB-51 dataset has significantly improved 1.8% accuracy. According to the results, our model has performed better than the other models. Our model consistently outperformed other models, and the results were statistically significant that demonstrating the superiority of our model for the task of HAR and its ability to provide the most reliable and accurate results.
PB  - arXiv
PY  - 2024
ST  - Extended Multi-stream Temporal-attention Module for Skeleton-based Human Action Recognition (HAR)
Y2  - 2025/05/05/21:54:32
DO  - 10.1016/j.chb.2024.108482
ER  -


TY  - GEN
AU  - Agrawal, T.
AU  - Ali, A.
AU  - Dantcheva, A.
AU  - Bremond, F.
TI  - AM Flow: Adapters for Temporal Processing in Action Recognition
AB  - Deep learning models, in particular image models, have recently gained generalisability and robustness. In this work, we propose to exploit such advances in the realm of video classification. Video foundation models suffer from the requirement of extensive pretraining and a large training time. Towards mitigating such limitations, we propose”Attention Map (AM) Flow” for image models, a method for identifying pixels relevant to motion in each input video frame. In this context, we propose two methods to compute AM flow, depending on camera motion. AM flow allows the separation of spatial and temporal processing, while providing improved results over combined spatio-temporal processing (as in video models). Adapters, one of the popular techniques in parameter efficient transfer learning, facilitate the incorporation of AM flow into pretrained image models, mitigating the need for full-finetuning. We extend adapters to”temporal processing adapters” by incorporating a temporal processing unit into the adapters. Our work achieves faster convergence, therefore reducing the number of epochs needed for training. Moreover, we endow an image model with the ability to achieve state-of-the-art results on popular action recognition datasets. This reduces training time and simplifies pretraining. We present experiments on Kinetics-400, Something-Something v2, and Toyota Smarthome datasets, showcasing state-of-the-art or comparable results. Our code will be made available on github.
PB  - arXiv
PY  - 2024
ST  - AM Flow
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/cvpr.2007.383512
ER  -


TY  - GEN
AU  - Shen, W.
AU  - Zhou, B.
AU  - Jiang, R.
AU  - Shen, S.
TI  - Sequential Charging Station Location Optimization under Uncertain Charging Behavior and User Growth
AB  - Charging station availability is crucial for a thriving electric vehicle market. Due to budget constraints, locating these stations usually proceeds in phases, which calls for careful consideration of the (random) charging demand growth throughout the planning horizon. This paper integrates user choice behavior into two-stage and multi-stage stochastic programming models for intracity charging station planning under demand uncertainty. We derive a second-order conic representation for the nonlinear, nonconvex formulation by taking advantage of the binary nature of location variables and propose subgradient inequalities to accelerate computation. Numerical results demonstrate the value of employing multi-stage models, particularly in scenarios of high demand fluctuations, increased demand dispersion, and high user sensitivity to the distance-to-recharge.
PB  - arXiv
PY  - 2024
ST  - Sequential Charging Station Location Optimization under Uncertain Charging Behavior and User Growth
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/cdc56724.2024.10885967
ER  -


TY  - GEN
AU  - Zhong, Q.
AU  - Ding, G.
AU  - Yao, A.
TI  - OnlineTAS: An Online Baseline for Temporal Action Segmentation
AB  - Temporal context plays a significant role in temporal action segmentation. In an offline setting, the context is typically captured by the segmentation network after observing the entire sequence. However, capturing and using such context information in an online setting remains an under-explored problem. This work presents the an online framework for temporal action segmentation. At the core of the framework is an adaptive memory designed to accommodate dynamic changes in context over time, alongside a feature augmentation module that enhances the frames with the memory. In addition, we propose a post-processing approach to mitigate the severe over-segmentation in the online setting. On three common segmentation benchmarks, our approach achieves state-of-the-art performance.
PB  - arXiv
PY  - 2024
ST  - OnlineTAS
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-981-99-8537-1_23
ER  -


TY  - GEN
AU  - Mehmood, F.
AU  - Chen, E.
AU  - Abbas, T.
AU  - Alzanin, S.M.
TI  - Human Action Recognition (HAR) Using Skeleton-based Spatial Temporal Relative Transformer Network: ST-RTR
AB  - Human Action Recognition (HAR) is an interesting research area in human-computer interaction used to monitor the activities of elderly and disabled individuals affected by physical and mental health. In the recent era, skeleton-based HAR has received much attention because skeleton data has shown that it can handle changes in striking, body size, camera views, and complex backgrounds. One key characteristic of ST-GCN is automatically learning spatial and temporal patterns from skeleton sequences. It has some limitations, as this method only works for short-range correlation due to its limited receptive field. Consequently, understanding human action requires long-range interconnection. To address this issue, we developed a spatial-temporal relative transformer ST-RTR model. The ST-RTR includes joint and relay nodes, which allow efficient communication and data transmission within the network. These nodes help to break the inherent spatial and temporal skeleton topologies, which enables the model to understand long-range human action better. Furthermore, we combine ST-RTR with a fusion model for further performance improvements. To assess the performance of the ST-RTR method, we conducted experiments on three skeleton-based HAR benchmarks: NTU RGB+D 60, NTU RGB+D 120, and UAV-Human. It boosted CS and CV by 2.11 % and 1.45% on NTU RGB+D 60, 1.25% and 1.05% on NTU RGB+D 120. On UAV-Human datasets, accuracy improved by 2.54%. The experimental outcomes explain that the proposed ST-RTR model significantly improves action recognition associated with the standard ST-GCN method.
PB  - arXiv
PY  - 2024
ST  - Human Action Recognition (HAR) Using Skeleton-based Spatial Temporal Relative Transformer Network
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-030-68796-0_50
ER  -


TY  - GEN
AU  - Chen, S.
AU  - Li, W.
AU  - Gu, J.
AU  - Chen, C.
AU  - Guo, Y.
TI  - Technical Report for ActivityNet Challenge 2022 - Temporal Action Localization
AB  - In the task of temporal action localization of ActivityNet-1.3 datasets, wepropose to locate the temporal boundaries of each action and predict actionclass in untrimmed videos. We first apply VideoSwinTransformer as featureextractor to extract different features. Then we apply a unified networkfollowing Faster-TAD to simultaneously obtain proposals and semantic labels.Last, we ensemble the results of different temporal action detection modelswhich complement each other. Faster-TAD simplifies the pipeline of TAD and getsremarkable performance, obtaining comparable results as those of multi-stepapproaches.
PB  - arXiv
PY  - 2024
ST  - Technical Report for ActivityNet Challenge 2022 - Temporal Action Localization
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-031-71626-3_9
ER  -


TY  - GEN
AU  - Li, W.
AU  - Bai, Y.
AU  - Chen, Z.
AU  - Lou, S.
AU  - Liao, Y.
TI  - Spatiotemporal Analysis of Wildfires in Alberta, Canada Over the Past Sixty Years: Increased Wildfire Frequency by Human Activities
AB  - Wildfires are a major socio-ecological issue in Alberta. The region’s extensive forests and grasslands provide abundant natural fuel. Since the onset of the 21st century, climate change and human activities have led to an increase in wildfire frequency. While some studies have focused on specific wildfire events and periods in Alberta, there remains a lack of comprehensive analysis addressing long-term dynamics and the multifactorial influences. This study aims to provide a thorough analysis of the historical dynamics of wildfires in Alberta, Canada (1961-2020), to examine long-term trends. Using historical datasets provided by the Alberta government, we analyzed the trends in wildfire frequency and burned areas over the past 60 years. The results reveal that although the total burned areas have declined over the last 60 years, the frequency of wildfires has significantly increased since the 21st century, with an average annual increase of approximately 0.0541 events. Spatial autocorrelation analysis shows significant clustering of human-caused (recreational and residential) fire events, while lightning-caused fires are more widely dispersed across the province. Hotspot analysis further identified specific areas with high fire activity. Finally, we incorporated 12 variables—including meteorological, human, and topographical factors (maximum temperature, precipitation, wind speed, soil moisture, snow water equivalent, lightning density, elevation, slope, aspect, population density, distance from population to roads, and land cover)—into a geographically weighted logistic regression (GWLR) model. The GWLR results highlighted the spatial variability in the influence of these factors on wildfire distribution. Population density emerged as the most significant factor affecting burned areas, underscoring the critical role of human activities in wildfire occurrences. The findings will help optimize the spatial planning of future wildfire prevention and control measures.
PB  - SSRN
PY  - 2024
ST  - Spatiotemporal Analysis of Wildfires in Alberta, Canada Over the Past Sixty Years
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5005510
ER  -


TY  - GEN
AU  - Aguila, C.A.
AU  - Lucas, A.
AU  - Lavelle, S.
AU  - Litt, B.
AU  - Conrad, E.C.
TI  - Mesial-to-lateral patterns of epileptiform activity identify the seizure onset zone in mesial temporal lobe epilepsy.
AB  - Mesial temporal lobe epilepsy (mTLE) is a common localization of drug-resistant epilepsy in adults. Patients often undergo intracranial EEG (iEEG) monitoring to confirm localization and determine candidacy for focal ablation or resection. Clinicians primarily base surgical decision-making on seizure onset patterns, with imaging abnormalities and information from interictal epileptiform discharge (spikes) used as ancillary data. How the morphology and timing of spikes within multi-electrode sequences may inform surgical planning is unknown, in part due to the lack of measurement methods for large datasets. We hypothesized that patients with mTLE have a distinct mesial-to-lateral spike pattern that differentiates them from other epilepsy localizations. In a multicenter study at the University of Pennsylvania and the Medical University of South Carolina, we analyzed the timing and morphology of spikes and seizure high frequency energy ratio (HFER) in 75 patients with drug-resistant epilepsy. We compared these features across patients with mTLE, temporal neocortical epilepsy, and other localizations. A logistic regression model combining all features predicted a clinical localization of mTLE in unseen patients with an AUC of 0.82 (compared to an AUC of 0.70 for seizure-only features, DeLong’s test p = 0.08). Spike rate was the most important feature in the combined model. Modeled probability of mTLE was similar between patients who had a good versus a poor 12-month outcome after undergoing a mesial temporal resection or ablation (p = 0.34). These findings support quantitative spike analysis to supplement analysis of seizures for use in surgical planning.
PB  - medRxiv
PY  - 2024
ST  - Mesial-to-lateral patterns of epileptiform activity identify the seizure onset zone in mesial temporal lobe epilepsy.
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2024.10.29.24316309
ER  -


TY  - GEN
AU  - Millán, A.P.
AU  - Sun, H.
AU  - Torres, J.J.
TI  - Spatio-temporal activity patterns induced by triadic interactions in an in silico neural medium
AB  - Triadic interactions are general mechanisms by which a node or neuron can regulate directly the link or synapse between other two neurons. The regulation takes place in a familiar way by either depressing or facilitating synaptic transmission. Such interactions are ubiquitous in neural systems, accounting for axo-axonic synapses and tripartite synapses mediated by astrocytes, for instance, and have been related to neuronal and synaptic processes at different time-scales, including short and long-term synaptic plasticity. In the field of network science, triadic interactions have been shown to produce complex spatio-temporal patterns of connectivity. Here, we investigate the emergent behavior of an in silico neural medium constituted by a population of leaky integrate-and-fire neurons with triadic interactions. We observe that, depending on relevant parameters defining triadic interactions, different activity patterns emerge. These include i) a silent phase, ii) a low-activity phase in which complex spatio-temporal patterns of low neuronal firing rate emerge that propagate through the medium, iii) a high-activity phase characterized by complex spatio-temporal patterns of high neuronal firing rate that propagate through the neural medium as waves of high firing activity over a bulk of low activity neurons, and iv) a pseudo-blinking phase in which the neural medium switches between high and low activity states, in a similar fashion to up/down state transitions. Here we analyse in depth the features of such patterns and relate our findings to the recently proposed model of triadic percolation.
PB  - arXiv
PY  - 2024
ST  - Spatio-temporal activity patterns induced by triadic interactions in an in silico neural medium
Y2  - 2025/05/05/21:54:32
DO  - 10.1088/2632-072x/adbf5e
ER  -


TY  - GEN
AU  - Cultrera, L.
AU  - Becattini, F.
AU  - Berlincioni, L.
AU  - Ferrari, C.
AU  - Del Bimbo, A.
TI  - SPATIO-TEMPORAL TRANSFORMERS FOR ACTION UNIT CLASSIFICATION WITH EVENT CAMERAS
AB  - As one of the most important applications in computer vision, face analysis has been studied from different angles in order to infer emotion, poses, shapes, and landmarks. Traditionally the research has employed classical RGB cameras to collect and publish the relevant annotated data. For more fine-grained tasks however standard sensors might not be up to the task, due to their latency, making it impossible to record and detect micro-movements that carry a highly informative signal, which is necessary for inferring the true emotions of a subject. Event cameras have been increasingly gaining interest as a possible solution to this and similar high-frame rate tasks. In this paper we propose a novel spatio-temporal Vision Transformer model that uses Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA) to enhance the accuracy of Action Unit classification from event streams. We also address the lack of labeled event data in the literature, which can be considered one of the main causes of an existing gap between the maturity of RGB and neuromorphic vision models. In fact, gathering data is harder in the event domain since it cannot be crawled from the web and labeling frames should take into account event aggregation rates and the fact that static parts might not be visible in certain frames. To this end, we present FACEMORPHIC, a temporally synchronized multimodal face dataset composed of both RGB videos and event streams. The dataset is annotated at a video level with facial Action Units and also contains streams collected with a variety of possible applications, ranging from 3D shape estimation to lip-reading. We then show how temporal synchronization can allow effective neuromorphic face analysis without the need to manually annotate videos: we instead leverage cross-modal supervision bridging the domain gap by representing face shapes in a 3D space. Our proposed model outperforms baseline methods by effectively capturing spatial and temporal information, crucial for recognizing subtle facial micro-expressions.
PB  - arXiv
PY  - 2024
ST  - SPATIO-TEMPORAL TRANSFORMERS FOR ACTION UNIT CLASSIFICATION WITH EVENT CAMERAS
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/tcsvt.2025.3559299
ER  -


TY  - GEN
AU  - Xie, B.
AU  - Chen, J.
AU  - Sun, J.
TI  - Multi-Branch Fine-Grained Spatial Temporal Graph Convolution Network for Skeleton-Based Action Recognition
AB  - Skeleton-based human action recognition plays a crucial role in vision-based applications. Although existing methods based on Graph Convolutional Networks (GCNs) have demonstrated good performance, they have limitations in handling complex and similar actions. The main reasons are: (1) limited capability in capturing short-term local spatiotemporal dependencies; (2) neglect of the logical relationships between distant joints of the human body. To tackle these limitations, we propose the Multi-Branch Fine-Grained Spatiotemporal Graph Convolutional Network (MFST-GCN).MFST-GCN employs multiple branches to independently extract fine-grained features, thereby enhancing the model's sensitivity to subtle body actions. We introduce two key modules: (1) Node Reuse Spatiotemporal Graph Convolution (NRST-GC), which improves the model's ability to capture short-term local dynamics through node reuse and expansion, and (2) Graph Enhanced Self-Attention (GE-SA), which leverages learnable graph structural relative position encoding to model dependencies between distant joints. Experimental results on large-scale datasets NTU RGB+D, NTU RGB+D 120, and Northwestern-UCLA demonstrate the superior recognition accuracy of MFST-GCN compared to existing methods. Notably, our model achieves improvements of 6.4% and 3.6% in recognizing the challenging 'read' and 'write' actions, respectively, highlighting its effectiveness in practical scenarios.
PB  - SSRN
PY  - 2024
ST  - Multi-Branch Fine-Grained Spatial Temporal Graph Convolution Network for Skeleton-Based Action Recognition
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4998001
ER  -


TY  - GEN
AU  - Cheng, Y.
AU  - Mike, J.
AU  - Fang, C.
AU  - Huang, J.
AU  - Wen, Y.
TI  - Spatiotemporal Action Detection Based on Fine-Grained
AB  - In the field of computer vision, spatiotemporal action detection is a challenging task. Traditional methods such as YOWO have achieved certain results in real-time and accuracy, but still have shortcomings in dealing with missing detail information, capturing long-distance dependencies, and target overlap problems. In response to these issues, this article proposes a spatiotemporal action detection method based on fine-grained enhancement. Firstly, a dual path fine-grained enhancement module is designed to enhance the ability to extract fine-grained features; Secondly, the self-attention&convolution module was introduced, which can better capture long-distance dependencies and cross level feature relationships; Finally, in order to address the issue of target overlap, the SIoU loss function is introduced to more comprehensively evaluate the similarity between predicted boxes and real boxes. The experimental results show that the proposed method achieves 86.4% F-mAP and 52.6% V-mAP on the UCF101-24 dataset, 20.5% F-mAP on the AVA dataset, and only 44.0 GFlops with an FPS of 34, ensuring the real-time performance of the proposed method with higher accuracy than SOTA model. In addition, the ablation experiment further demonstrated the effectiveness of each module in this paper. Overall, this method significantly improves the accuracy of spatiotemporal action detection while maintaining high efficiency, providing an effective solution for real-time action detection.
PB  - SSRN
PY  - 2024
ST  - Spatiotemporal Action Detection Based on Fine-Grained
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4995638
ER  -


TY  - GEN
AU  - Wang, N.
AU  - Xiao, Y.
AU  - Peng, X.
AU  - Wang, X.
AU  - Fang, D.
TI  - ContextDet: Temporal Action Detection with Adaptive Context Aggregation
AB  - Temporal action detection (TAD), which locates and recognizes action segments, remains a challenging task in video understanding due to variable segment lengths and ambiguous boundaries. Existing methods treat neighboring contexts of an action segment indiscriminately, leading to imprecise boundary predictions. We introduce a single-stage ContextDet framework, which makes use of large-kernel convolutions in TAD for the first time. Our model features a pyramid adaptive context aggragation (ACA) architecture, capturing long context and improving action discriminability. Each ACA level consists of two novel modules. The context attention module (CAM) identifies salient contextual information, encourages context diversity, and preserves context integrity through a context gating block (CGB). The long context module (LCM) makes use of a mixture of large- and small-kernel convolutions to adaptively gather long-range context and fine-grained local features. Additionally, by varying the length of these large kernels across the ACA pyramid, our model provides lightweight yet effective context aggregation and action discrimination. We conducted extensive experiments and compared our model with a number of advanced TAD methods on six challenging TAD benchmarks: MultiThumos, Charades, FineAction, EPIC-Kitchens 100, Thumos14, and HACS, demonstrating superior accuracy at reduced inference speed.
PB  - arXiv
PY  - 2024
ST  - ContextDet
Y2  - 2025/05/05/21:54:32
DO  - 10.1609/aaai.v36i1.19900
ER  -


TY  - GEN
AU  - Li, J.
AU  - Gan, T.
AU  - Li, W.
AU  - Liu, Y.
TI  - A spatiotemporal knowledge graph-based method for identifying individual activity locations from mobile phone data
AB  - In recent years, mobile phone data has been widely used for human mobility analytics. Identifying individual activity locations is the fundamental step for mobile phone data processing. Current methods typically aggregate spatially adjacent location records over multiple days to identify activity locations. However, only considering spatial relationships while overlooking temporal ones may lead to inaccurate activity location identification, and also affect activity pattern analysis. In this study, we propose a spatiotemporal knowledge graph-based (STKG) method for identifying activity locations from mobile phone data. An STKG is designed and constructed to describe individual mobility characteristics. The spatial and temporal relationships of individual stays are inferred and transformed into a spatiotemporal graph. The modularity-optimization community detection algorithm is applied to identify stays with dense spatiotemporal relationships, which are considering as activity locations. A case study in Shanghai was conducted to verify the performance of the proposed method. The results show that compared with two baseline methods, the STKG-based method can limit an additional 45% of activity locations with the longest daytime stay within a reasonable spatial range; In addition, the STKG-based method exhibit lower variance in the start and end times of activities across different days, performing approximately 10% to 20% better than the two baseline methods. Moreover, the STKG-based method effectively distinguishes between locations that are geographically close but exhibit different temporal patterns. These findings demonstrate the effectiveness of STKG-based method in enhancing both spatial precision and temporal consistency.
PB  - arXiv
PY  - 2024
ST  - A spatiotemporal knowledge graph-based method for identifying individual activity locations from mobile phone data
Y2  - 2025/05/05/21:54:32
DO  - 10.1016/j.jtrangeo.2025.104157
ER  -


TY  - GEN
AU  - Sun, Y.
AU  - Brovman, Y.M.
TI  - CoActionGraphRec: Sequential Multi-Interest Recommendations Using Co-Action Graphs
AB  - There are unique challenges to developing item recommender systems for e-commerce platforms like eBay due to sparse data and diverse user interests. While rich user-item interactions are important, eBay’s data sparsity exceeds other e-commerce sites by an order of magnitude. To address this challenge, we propose CoActionGraphRec (CAGR), a text based two-tower deep learning model (Item Tower and User Tower) utilizing co-action graph layers. In order to enhance user and item representations, a graph-based solution tailored to eBay’s environment is utilized. For the Item Tower, we represent each item using its co-action items to capture collaborative signals in a co-action graph that is fully leveraged by the graph neural network component. For the User Tower, we build a fully connected graph of each user’s behavior sequence, with edges encoding pairwise relationships. Furthermore, an explicit interaction module learns representations capturing behavior interactions. Extensive offline and online A/B test experiments demonstrate the effectiveness of our proposed approach and results show improved performance over state-of-the-art methods on key metrics.
PB  - arXiv
PY  - 2024
ST  - CoActionGraphRec
Y2  - 2025/05/05/21:54:32
DO  - 10.1016/j.eswa.2021.115028
ER  -


TY  - GEN
AU  - Xie, J.
AU  - Zhao, Y.
AU  - Meng, Y.
AU  - Anh, N.
AU  - Zheng, Y.
TI  - Multi-Level Sparsity Spatial-Temporal Graph Convolution Network for Skeleton-Based Human Action Recognition
AB  - Spatial-temporal graph convolutional networks (ST-GCNs) have impressive perfor- mance in skeleton-based human action recognition (HAR). Although various ST-GCN models have been proposed, their recognition performance does not di↵er significantly after aligning the input settings. With this observation in mind, we hypothesized that ST-GCNs are over-parameterized for skeleton-based HAR and subsequently proved it with experiments using the lottery ticket hypothesis. To achieve the acquisition of sta- ble sparse ST-GCNs, we also proposed a novel sparse ST-GCNs generator that trains a sparse architecture from a randomly initialized dense network while maintaining per- formance comparable to the dense components. Furthermore, We generated multi-level sparsity ST-GCNs by incorporating the sparse structure at di↵erent sparsity levels and then demonstrated that the assembled model will lead to a significant enhancement in HAR performance when compared to the dense backbones. Thorough experiments on four datasets, including NTU-RGB+D 60(120), Kinetics-400, and FineGYM, demon- strate that the sparse ST-GCNs obtained from the proposed sparse ST-GCNs generator can achieve comparable performance to their dense components. Even with 95% fewer parameters, the sparse ST-GCNs only shows < 1% reduction in top-1 accuracy. Mean- while, the multi-level sparsity ST-GCNs, which require only 66% of the parameters of the dense ST-GCNs, demonstrate an improvement of > 1% in top-1 accuracy. This provided a novel approach to introduce an improvement in HAR performance without an increase in parameters. The code will be released upon acceptance.
PB  - SSRN
PY  - 2024
ST  - Multi-Level Sparsity Spatial-Temporal Graph Convolution Network for Skeleton-Based Human Action Recognition
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4986741
ER  -


TY  - GEN
AU  - Zhao, Y.
AU  - Wang, J.
AU  - Liu, M.
AU  - Ma, Y.
TI  - Enhancing Crosstransformer with Fine-Grained Spatio-Temporal Modeling for Few-Shot Action Recognition
AB  - Few-shot action recognition has emerged as a critical challenge in video understanding due to the high costs of video annotation and the difficulties associated with training models on limited samples. While previous methods inspired by few-shot image classification have made some progress, they often neglect the spatio-temporal features of video, leading to poor generalization. Besides, actions may occur at different moments and locations, making it challenging to characterize the execution process using only temporal or spatial modeling. In this paper, we propose an enhanced CrossTransformer with fine-grained spatio-temporal modeling (FSTMX) for few-shot action recognition, revisiting the problem of representing video features under limited samples through spatio-temporal attention. Our method introduces a fine-grained spatio-temporal modeling (FSTM) module, which consists of a sequentially connected temporal attention block (TAB) and spatial attention block (SAB). After decomposing the video frames into patches, TAB enhances the global dependency feature representation by learning the temporal correlation of patches at the same location across different frames. Subsequently, SAB enhances the local feature representation by learning the spatial correlation of patches within the same frame. Additionally, we apply CrossTransformer to construct query-specific class prototypes using a cross-attention mechanism. The proposed method has been tested on four well-known benchmarks and a real-world abnormal behaviour dataset. Extensive experimental results demonstrate that our method excels in few-shot action recognition.
PB  - SSRN
PY  - 2024
ST  - Enhancing Crosstransformer with Fine-Grained Spatio-Temporal Modeling for Few-Shot Action Recognition
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4984426
ER  -


TY  - GEN
AU  - Sheng, J.
AU  - Li, A.
AU  - Ge, Y.
TI  - Summarized Knowledge Guidance for Single-Frame Temporal Action Localization
AB  - Single-frame temporal action localization has elicited attention in the computer vision community. Existing methods address annotation sparsity by generating dense pseudo labels within individual videos, but disregard the variable representation from intra-class action instances, resulting in inferior completeness localization. In this paper, we propose to model intra-class relationships by using Summarized Knowledge Guidance (SKG). Specifically, we initially design a learnable memory bank to summarize annotated single-frame knowledge for each class. Then, we introduce two corresponding components, i.e., the knowledge propagation module (KPM) and the knowledge refinement module (KRM), for intra-class guidance. In KPM, we propagate summarized knowledge for feature-level enhancement through bipartite matching. In KRM, summarized knowledge is presented as confident pseudo positive samples for label-level refinement in a contrastive learning manner. Extensive experiments and ablation studies on the THUMOS14, GTEA and BEOID reveal that our method significantly outperforms state-of-the-art methods.
PB  - SSRN
PY  - 2024
ST  - Summarized Knowledge Guidance for Single-Frame Temporal Action Localization
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4980176
ER  -


TY  - GEN
AU  - Chen, B.
AU  - Ji, H.
AU  - Wang, Z.
AU  - Vanrumste, B.
AU  - Liu, H.
TI  - Language-Assisted Human Part Motion Learning for Skeleton-Based Temporal Action Segmentation
AB  - Skeleton-based Temporal Action Segmentation involves the dense action classification of variable-length skeleton sequences. Current approaches primarily apply graph-based networks to extract framewise, whole-body-level motion representations, and use one-hot encoded labels for model optimization. However, whole-body motion representations do not capture finegrained part-level motion representations and the one-hot encoded labels neglect the intrinsic semantic relationships within the language-based action definitions. To address these limitations, we propose a novel method named Language-assisted Human Part Motion Representation Learning (LPL), which contains a Disentangled Part Motion Encoder (DPE) to extract duallevel (i.e., part and whole-body) motion representations and a Language-assisted Distribution Alignment (LDA) strategy for optimizing spatial relations within representations. Specifically, after part-aware skeleton encoding via DPE, LDA generates dual-level action descriptions to construct a textual embedding space with the help of a large-scale language model. Then, LDA motivates the alignment of the embedding space between text descriptions and motions. This alignment allows LDA not only to enhance intra-class compactness but also to transfer the language-encoded semantic correlations among actions to skeleton-based motion learning. Moreover, we propose a simple yet efficient Semantic Offset Adapter to smooth the cross-domain misalignment. Our experiments indicate that LPL achieves stateof- the-art performance across various datasets (e.g., +4.4% Accuracy, +5.6% F1 on the PKU-MMD dataset). Moreover, LDA is compatible with existing methods and improves their performance (e.g., +4.8% Accuracy, +4.3% F1 on the LARa dataset) without additional inference costs.
PB  - arXiv
PY  - 2024
ST  - Language-Assisted Human Part Motion Learning for Skeleton-Based Temporal Action Segmentation
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-031-72949-2_23
ER  -


TY  - GEN
AU  - Sheng, J.
AU  - Li, A.
AU  - Ge, Y.
TI  - Summarized Knowledge Guidance for Single-Frame Temporal Action Localization
AB  - Single-frame temporal action localization has elicited attention in the computer vision community. Existing methods address annotation sparsity by generating dense pseudo labels within individual videos, but disregard the variable representation from intra-class action instances, resulting in inferior completeness localization. In this paper, we propose to model intra-class relationships by using Summarized Knowledge Guidance (SKG). Specifically, we initially design a learnable memory bank to summarize annotated single-frame knowledge for each class. Then, we introduce two corresponding components, i.e., the knowledge propagation module (KPM) and the knowledge refinement module (KRM), for intra-class guidance. In KPM, we propagate summarized knowledge for feature-level enhancement through bipartite matching. In KRM, summarized knowledge is presented as confident pseudo positive samples for label-level refinement in a contrastive learning manner. Extensive experiments and ablation studies on the THUMOS14, GTEA and BEOID reveal that our method significantly outperforms state-of-the-art methods.
PB  - SSRN
PY  - 2024
ST  - Summarized Knowledge Guidance for Single-Frame Temporal Action Localization
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4980176
ER  -


TY  - GEN
AU  - Oyem, J.C.
AU  - Heijkoop, R.
AU  - Snoeren, E.M.S.
TI  - The temporal copulatory patterns of female rat sexual behavior
AB  - Female sexual behavior is a naturally rewarding activity that plays an important role in reproduction and species survival. For female rats, regulating the timing of sexual interactions is essential for optimizing mating satisfaction and enhancing the physiological conditions needed for successful fertilization. So far, traditional research on female sexual behavior has relied on a limited set of behavioral parameters, which has certain shortcomings. To address this, our study aimed to develop a more detailed behavioral framework for assessing temporal copulatory patterns in female rats. We compared fully receptive females and less-receptive females, while also investigating the effects of (R)-(+)-8-OH-DPAT, a 5-HT1A receptor agonist known for its inhibitory impact on female sexual behavior. Additionally, we examined how sexual experience and pacing conditions influence these copulatory patterns. Our results revealed that female rats engage in structured patterns of sexual bouts and timeouts, with higher receptivity leading to more sexual bouts and shorter time-outs. This suggests that sexual bouts can be viewed as an indicator of copulatory intensity, while time-outs reflect motivation to continue mating. Sexual experience did not enhance sexual performance but did result in females receiving more copulatory events from males. Lastly, we found that the conditions under which mating occurs (paced vs. non-paced) may not significantly impact copulatory behavior in fully-receptive females but could be more relevant for less-receptive females. Despite this, paced mating conditions remain preferable for studying female sexual behavior.
PB  - bioRxiv
PY  - 2024
ST  - The temporal copulatory patterns of female rat sexual behavior
Y2  - 2025/05/05/21:54:32
DO  - 10.1016/j.beproc.2025.105148
ER  -


TY  - GEN
AU  - Han, Y.
AU  - Jiang, Q.
AU  - Mei, H.
AU  - Yang, Y.
AU  - Tang, J.
TI  - The Solution for Temporal Action Localisation Task of Perception Test Challenge 2024
AB  - This report presents our method for Temporal Action Localisation (TAL), which focuses on identifying and classifying actions within specific time intervals throughout a video sequence. We employ a data augmentation technique by expanding the training dataset using overlapping labels from the Something-SomethingV2 dataset, enhancing the model’s ability to generalize across various action classes. For feature extraction, we utilize state-of-the-art models, including UMT, VideoMAEv2 for video features, and BEATs and CAV-MAE for audio features. Our approach involves training both multimodal (video and audio) and unimodal (video only) models, followed by combining their predictions using the Weighted Box Fusion (WBF) method. This fusion strategy ensures robust action localisation. our overall approach achieves a score of 0.5498, securing first place in the competition.
PB  - arXiv
PY  - 2024
ST  - The Solution for Temporal Action Localisation Task of Perception Test Challenge 2024
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-030-46732-6_8
ER  -


TY  - GEN
AU  - Xuan, Y.
AU  - Sokol, K.
AU  - Sanderson, M.
AU  - Chan, J.
TI  - Perfect Counterfactuals in Imperfect Worlds: Modelling Noisy Implementation of Actions in Sequential Algorithmic Recourse
AB  - Algorithmic recourse provides actions to individuals who have been adversely affected by automated decision-making and helps them achieve a desired outcome. Knowing the recourse, however, does not guarantee that users would implement it perfectly, either due to environmental variability or personal choices. Recourse generation should thus anticipate its sub-optimal or noisy implementation. While several approaches have constructed recourse that accounts for robustness to small perturbation (i.e., noisy recourse implementation), they assume an entire recourse to be implemented in a single step and thus apply one-off uniform noise to it. Such assumption is unrealistic since recourse often includes multiple sequential steps which becomes harder to implement and subject to more noise. In this work, we consider recourse under plausible noise that adapts to the local data geometry and accumulates at every step of the way. We frame this problem as a Markov Decision Process and demonstrate that the distribution of our plausible noise satisfies the Markov property. We then propose the RObust SEquential (ROSE) recourse generator to output a sequence of steps that will lead to the desired outcome even under imperfect implementation. Given our plausible modelling of sub-optimal human actions and greater recourse robustness to accumulated uncertainty, ROSE can grant users higher chances of success under low recourse costs. Empirical evaluation shows our algorithm manages the inherent trade-off between recourse robustness and costs more effectively while ensuring its low sparsity and fast computation.
PB  - arXiv
PY  - 2024
ST  - Perfect Counterfactuals in Imperfect Worlds
Y2  - 2025/05/05/21:54:32
DO  - 10.1145/3442188.3445899
ER  -


TY  - GEN
AU  - Provias, V.
AU  - Schönauer, M.
AU  - Gais, S.
AU  - Bergmann, T.O.
AU  - Coffey, E.B.J.
TI  - Spatiotemporal patterns of theta-band activity during rapid-eye movement sleep: a magnetoencephalography analysis
AB  - Theta oscillations (4-8 Hz) in frontal cortical regions are present to different degrees across states of consciousness. In sleep, theta is prominent in periods of rapid eye-movement (REM) sleep. Theta has been linked to processes of memory consolidation; however, its mechanistic contribution specifically during REM sleep is not well understood. Interestingly, in the wake state, frontal theta activity increases during effortful cognitive tasks involving executive functions such as working memory, hinting at similarities in circuitry, and potentially, function. The aim of the present work is to create a spatially resolved, whole-brain characterisation of REM oscillatory activity in healthy human subjects, distinguishing theta from neighbouring frequency bands, differentiating substages of REM sleep (phasic and tonic REM), and comparing REM theta to that which is evoked during a working memory task. To that end, we analysed magneto- and electroencephalography (M/EEG) data recorded during overnight sleep in 10 healthy subjects, and similar data from 17 healthy subjects who performed a working memory task, using a novel whole-brain, source-localised MEG approach. Our results show that (i) theta activity has a frontal midline topography that is distinct from those of other prominent frequency bands in REM (delta, alpha, beta), (ii) theta activity in frontal midline regions is best observed within a focused 5-7 Hz range, separating it from occipital alpha activity, (iii) REM theta is dominant over the frontal midline but is also observed in several sub-cortical areas, (iv) theta is more widespread in tonic than phasic REM sleep, and (v) the focused frontal midline theta pattern observed in REM phasic sleep is the most similar of all observed sleep substages to theta evoked by a working memory task. These results enhance our understanding of theta physiology in REM sleep and suggest future targets for research into REM’s role in learning and memory.
PB  - bioRxiv
PY  - 2024
ST  - Spatiotemporal patterns of theta-band activity during rapid-eye movement sleep
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2024.10.01.616032
ER  -


TY  - GEN
AU  - Cao, K.
AU  - Peng, J.
AU  - Chen, J.
AU  - Hou, X.
AU  - Ma, A.J.
TI  - Adversarial Style Mixup and Improved Temporal Alignment for Cross-Domain Few-Shot Action Recognition
AB  - Few-Shot Action Recognition is a promising approach to transferring knowledge learned from base action classes to novel ones with only limited labeled training samples.Since data distributions may vary between base (source domain) and novel (target domain) classes, Cross-Domain Few-Shot Action Recognition (CDFSAR) has been recently proposed but less studied.This paper addresses the issues of insufficient style coverage for the target domain and potential temporal misalignment with chronological order in existing methods.To mitigate distribution shifts across domains, we propose expanding the search space of intermediate domain through image mixup, complemented by an Adversarial Style Mixup (ASM) module, which enhances the diversity of style distributions.The ASM mixes up the source and the target domain styles with the statistical means and variances from both domains.Through adversarial training for the mixup ratio and style noise, the style diversity is increased with a broader search range to cover the target style.Moreover, we design an Improved Temporal Alignment (ITA) module with a temporal mixer and keyframe-based alignment to better interpret the temporal context of video sequences.In the proposed ITA, keyframes are extracted as temporal alignment priors together with a temporal mixer to reduce the misalignment noise. Extensive experiments on video action recognition datasets demonstrates the superiority of our method compared with the state of the arts for the challenging problem of CDFSAR.
PB  - SSRN
PY  - 2024
ST  - Adversarial Style Mixup and Improved Temporal Alignment for Cross-Domain Few-Shot Action Recognition
Y2  - 2025/05/05/21:54:32
DO  - 10.1016/j.cviu.2025.104341
ER  -


TY  - GEN
AU  - Park, J.S.
AU  - Oh, J.S.
AU  - Park, J.N.
AU  - Chung, C.-H.
AU  - Hong, S.
TI  - Oxidation Behaviors of Nano Grain Sized Coating Layers Produced Via Sequential Two-Step Pack Cementation Coatings by B and Si of Titanbmozr High-Entropy Alloys
AB  - Herein, the oxidation behavior of TiTaNbMoZr, a high-entropy alloy, was investigated at 1300°C. The alloy was either silicon (Si) coated or sequentially coated with boron (B + Si) via pack cementation to identify the oxidation mechanism. Sequential B coating followed by Si coating produced a nanosized mixture of boride and silicide. The two-step sequentially coated specimen produced a sequence of coating layers: nanograined metal diboride (XB2) + XSi2 + T2 (X5SiB2) and XSi2 followed by an interdiffusion layer on the substrate. The mechanism for improving the oxidation resistance of the TiTaNbMoZr alloys at high temperatures was explored.
PB  - SSRN
PY  - 2024
ST  - Oxidation Behaviors of Nano Grain Sized Coating Layers Produced Via Sequential Two-Step Pack Cementation Coatings by B and Si of Titanbmozr High-Entropy Alloys
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4973832
ER  -


TY  - GEN
AU  - Khajehnejad, M.
AU  - Khajehnejad, A.
AU  - Habibollahi, F.
AU  - Kagan, B.J.
AU  - Razi, A.
TI  - TAVRNN: TEMPORAL ATTENTION-ENHANCED VARIATIONAL GRAPH RNN CAPTURES NEURAL DYNAMICS AND BEHAVIOR
AB  - We introduce Temporal Attention-enhanced Variational Graph Recurrent Neural Network (TAVRNN), a novel framework for analyzing the evolving dynamics of neuronal connectivity networks in response to external stimuli and behavioral feedback. TAVRNN captures temporal changes in network structure by modeling sequential snapshots of neuronal activity, enabling the identification of key connectivity patterns. Leveraging temporal attention mechanisms and variational graph techniques, TAVRNN uncovers how connectivity shifts align with behavior over time. We validate TAVRNN on two datasets: in vivo calcium imaging data from freely behaving rats and novel in vitro electrophysiological data from the DishBrain system, where biological neurons control a simulated environment during the game of pong. We show that TAVRNN outperforms previous baseline models in classification, clustering tasks and computational efficiency while accurately linking connectivity changes to performance variations. Crucially, TAVRNN reveals that high game performance in the DishBrain system correlates with the alignment of sensory and motor subregion channels, a relationship not evident in earlier models. This framework represents the first application of dynamic graph representation of electrophysiological (neuronal) data from DishBrain system, providing insights into the reorganization of neuronal networks during learning. TAVRNN’s ability to differentiate between neuronal states associated with successful and unsuccessful learning outcomes, offers significant implications for real-time monitoring and manipulation of biological neuronal systems.
PB  - arXiv
PY  - 2024
ST  - TAVRNN
Y2  - 2025/05/05/21:54:32
DO  - 10.1016/j.knosys.2024.112110
ER  -


TY  - GEN
AU  - Arora, R.S.
AU  - Daundkar, N.
AU  - Sarkar, S.
TI  - SECTIONAL CATEGORY WITH RESPECT TO GROUP ACTIONS AND SEQUENTIAL TOPOLOGICAL COMPLEXITY OF FIBRE BUNDLES
AB  - Let X be a G-space. In this paper, we introduce the notion of sectional category with respect to G. As a result, we obtain G-homotopy invariants: the LS category with respect to G, the sequential topological complexity with respect to G (which is same as the weak sequential equivariant topological complexity TC k,Gw(X) in the sense of Farber and Oprea), and the strong sequential topological complexity with respect to G, denoted by cat G#(X), TC k,G#(X), and TC k,G#∗(X), respectively. We explore several relationships among these invariants and well-known ones, such as the LS category, the sequential (equivariant) topological complexity, and the sequential strong equivariant topological complexity. In one of our main results, we give an additive upper bound for TCk(E) for a fibre bundle F → E → B with structure group G in terms of certain motion planning covers of the base B and the invariant TCk,G#∗(F) or cat Gk#(Fk), where the fibre F is viewed as a G-space. As applications of these results, we give bounds on the sequential topological complexity of generalized projective product spaces and mapping tori. MSC Codes 55M30, 55R91, 55S40
PB  - arXiv
PY  - 2024
ST  - SECTIONAL CATEGORY WITH RESPECT TO GROUP ACTIONS AND SEQUENTIAL TOPOLOGICAL COMPLEXITY OF FIBRE BUNDLES
Y2  - 2025/05/05/21:54:32
DO  - 10.4310/hha.2024.v26.n2.a14
ER  -


TY  - GEN
AU  - Sattler, N.J.
AU  - Wehr, M.
TI  - Cortex-wide spatiotemporal motifs of theta oscillations are coupled to freely moving behavior
AB  - Multisensory information is combined across the cortex and assimilated into the continuous production of ongoing behavior. In the hippocampus, theta oscillations (4-12 Hz) radiate as large-scale traveling waves, and serve as a scaffold for neuronal ensembles of multisensory information involved in memory and movement-related processing. An extension of such an encoding framework across the neocortex could similarly serve to bind disparate multisensory signals into ongoing, coherent, phase-coded processes. Whether the neocortex exhibits unique large-scale traveling waves distinct from that of the hippocampus however, remains unknown. Here, using cortex-wide electrocorticography in freely moving mice, we find that theta oscillations are organized into bilaterally-symmetric spatiotemporal “modes” that span virtually the entire neocortex. The dominant mode (Mode 1) is a divergent traveling wave that originates from retrosplenial cortex and whose amplitude correlates with mouse speed. Secondary modes are asynchronous spiral waves centered over primary somatosensory cortex (Modes 2 & 3), which become prominent during rapid drops in amplitude and synchrony (null spikes) and which underlie a phase reset of Mode 1. These structured cortex-wide traveling waves may provide a scaffold for large-scale phase-coding that allows the binding of multisensory information across all the regions of the cortex.
PB  - bioRxiv
PY  - 2024
ST  - Cortex-wide spatiotemporal motifs of theta oscillations are coupled to freely moving behavior
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2024.09.27.615537
ER  -


TY  - GEN
AU  - Shao, S.
AU  - Riquelme, J.L.
AU  - Gjorgjieva, J.
TI  - Spontaneous emergence and drifting of sequential neural activity in recurrent networks
AB  - Repeating sequences of neural activity exist across diverse brain regions of different animals and are thought to underlie diverse computations. However, their emergence and evolution in the presence of ongoing synaptic plasticity remain poorly understood. To gain mechanistic insights into this process, we modeled how biologically-inspired rules of activity-dependent synaptic plasticity in recurrent circuits interact to produce connectivity structures that support sequential neuronal activity. Even under unstructured inputs, our recurrent networks developed strong unidirectional connections, resulting in spontaneous repeating spiking sequences. During ongoing plasticity these sequences repeated despite turnover of individual synaptic connections, a process reminiscent of synaptic drift. The turnover process occurred over different timescales, with certain connectivity types and motif structures leading to sequences with different volatility. Structured inputs could reinforce or retrain the resulting connectivity structures underlying sequences, enabling stable but still flexible encoding of inputs. Our model unveils the interplay between synaptic plasticity and sequential activity in recurrent networks, providing insights into how brains implement reliable but flexible computations.
PB  - bioRxiv
PY  - 2024
ST  - Spontaneous emergence and drifting of sequential neural activity in recurrent networks
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2024.09.27.615499
ER  -


TY  - GEN
AU  - Ahmadpoortorkamani, M.
AU  - Cheviakov, A.
TI  - Spatiotemporal Behaviour of SIR Models with Cross-Diffusion and Vital Dynamics
AB  - Contemporary epidemiological models often involve spatial variation, providing an avenue to investigate the averaged dynamics of individual movements. In this work, we extend a recent model by Vaziry, Kolokolnikov, and Kevrekidis [Royal Society Open Science 9 (10), 2022] that included, in both infected and susceptible population dynamics equations, a cross-diffusion term with the second spatial derivative of the infected population density. Diffusion terms of this type occur, for example, in the Keller-Siegel chemotaxis model. The presented model corresponds to local orderly commute of susceptible and infected individuals, and is shown to arise in two dimensions as a limit of a discrete process. The present contribution identifies and studies specific features of the new model’s dynamics, including various types of infection waves and buffer zones protected from the infection. The model with vital dynamics additionally exhibits complex spatiotemporal behaviour that involves the generation of quasiperiodic infection waves and emergence of transient strongly heterogeneous patterns.
PB  - arXiv
PY  - 2024
ST  - Spatiotemporal Behaviour of SIR Models with Cross-Diffusion and Vital Dynamics
Y2  - 2025/05/05/21:54:32
DO  - 10.1017/s095679252400086x
ER  -


TY  - GEN
AU  - Yin, X.
AU  - Cao, W.
TI  - Spatial Hierarchy and Temporal Attention Guided Cross Masking for Self-supervised Skeleton-based Action Recognition
AB  - In self-supervised skeleton-based action recognition, the mask reconstruction paradigm is gaining interest in enhancing model refinement and robustness through effective masking. However, previous works primarily relied on a single masking criterion, resulting in the model overfitting specific features and overlooking other effective information. In this paper, we introduce a hierarchy and attention guided cross-masking framework (HA-CM) that applies masking to skeleton sequences from both spatial and temporal perspectives. Specifically, in spatial graphs, we utilize hyperbolic space to maintain joint distinctions and effectively preserve the hierarchical structure of high-dimensional skeletons, employing joint hierarchy as the masking criterion. In temporal flows, we substitute traditional distance metrics with the global attention of joints for masking, addressing the convergence of distances in high-dimensional space and the lack of a global perspective. Additionally, we incorporate cross-contrast loss based on the cross-masking framework into the loss function to enhance the model's learning of instance-level features. HA-CM shows efficiency and universality on three public large-scale datasets, NTU-60, NTU-120, and PKU-MMD. The source code of our HA-CM is available at https://github.com/YinxPeng/HA-CM-main.
PB  - arXiv
PY  - 2024
ST  - Spatial Hierarchy and Temporal Attention Guided Cross Masking for Self-supervised Skeleton-based Action Recognition
Y2  - 2025/05/05/21:54:32
DO  - 10.21203/rs.3.rs-3368402/v1
ER  -


TY  - GEN
AU  - Hori, S.
AU  - Omi, K.
AU  - Tamaki, T.
TI  - Query matching for spatio-temporal action detection with query-based object detector
AB  - In this paper, we propose a method that extends the query-based object detection model, DETR, to spatio-temporal action detection, which requires maintaining temporal consistency in videos. Our proposed method applies DETR to each frame and uses feature shift to incorporate temporal information. However, DETR’s object queries in each frame may correspond to different objects, making a simple feature shift ineffective. To overcome this issue, we propose query matching across different frames, ensuring that queries for the same object are matched and used for the feature shift. Experimental results show that performance on the JHMDB21 dataset improves significantly when query features are shifted using the proposed query matching.
PB  - arXiv
PY  - 2024
ST  - Query matching for spatio-temporal action detection with query-based object detector
Y2  - 2025/05/05/21:54:32
DO  - 10.5220/0013089500003912
ER  -


TY  - GEN
AU  - Boehm Vock, L.F.
AU  - Mossman, L.M.
AU  - Rapti, Z.
AU  - Dolezal, A.G.
AU  - Clifton, S.M.
TI  - Spatiotemporal, environmental, and behavioral predictors of Varroa mite intensity in managed honey bee apiaries
AB  - Honey bees contribute substantially to the world economy through pollination services and honey production. In the U.S. alone, honey bee pollination is estimated to contribute at least $11 billion annually, primarily through the pollination of specialty crops. However, beekeepers lose about half of their hives every season due to disease, insecticides, and other environmental factors. Here, we explore and validate a spatiotemporal statistical model of Varroa destructor mite burden (in mites/300 bees) in managed honey bee colonies, exploring the impact of both environmental factors and beekeeper behaviors. We examine risk factors for Varroa infestation using apiary inspection data collected across the state of Illinois over 2018-19, and we test the models using inspection data from 2020-21. After accounting for spatial and temporal trends, we find that environmental factors (e.g., floral quality, insecticide load) are not predictive of Varroa intensity, while several beekeeper behaviors (e.g., smaller colony density, supplemental feeding, and mite monitoring/treatment) are protective agains Varroa. Interestingly, while monitoring and treating for Varroa is protective, treating without monitoring is no more effective than not treating at all. This is an importan result supporting Integrated Pest Management (IPM) approaches.
PB  - bioRxiv
PY  - 2024
ST  - Spatiotemporal, environmental, and behavioral predictors of Varroa mite intensity in managed honey bee apiaries
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2024.09.23.614412
ER  -


TY  - GEN
AU  - Lai, X.
AU  - Fu, R.
AU  - Jiang, H.
AU  - Yao, J.
AU  - Liu, T.
TI  - A classroom behavior recognition method based on Temporal Attention Mechanism and Fragment-based Mechanism
AB  - Classroom behavior detection aims to enhance classroom efficiency by collecting information on students’ behavior and developing strategies for improved teaching methods. However, existing models suffer from low efficiency, subjectivity, and recognition accuracy. To address these issues, we propose a classroom behavior recognition model utilizing a dataset from monitoring devices, based on the Temporal Attention Mechanism (TAM) and Fragment-based mechanism of a 3D convolutional neural network. Our TAM applies attention distribution in the time dimension, combined with 3D CAM, as the Channel and temporal attention module (CTAM), resulting in a 10% accuracy improvement. We integrate 3D-CAM and 3D-SAM with TAM, validating the portability and universality of our time attention mechanism. Furthermore, we introduce the Fragment-based mechanism, synthesizing feature outputs of different time scales for improved classification. By incorporating this mechanism, the model achieves a 30% accuracy improvement on the classroom behavior dataset. Evaluation on a public dataset demonstrates significant improvements compared to the original model.
PB  - Research Square
PY  - 2024
ST  - A classroom behavior recognition method based on Temporal Attention Mechanism and Fragment-based Mechanism
Y2  - 2025/05/05/21:54:32
DO  - 10.21203/rs.3.rs-4932000/v1
ER  -


TY  - GEN
AU  - Zhu, C.
AU  - Zheng, S.
AU  - Liu, S.
AU  - Yao, Y.
AU  - Han, B.
TI  - Combined Impact of Spatial and Temporal Environment on Physical Activity: A Longitudinal, Multi-Scale Study in the Core Area of Beijing
AB  - Physical activity is closely linked to the temporal and spatial environments of cities. However, high-density urban environments and heat island effect result in a lack of spaces suitable for leisure physical activities, hindering active travel. Most studies focus only on the built environment, ignoring seasonal factors like climate and vegetation changes. This study collected data from a fitness app on exercise routes in the core area of Beijing, tracking changes every 15 days throughout 2023. Employing Pearson correlation, ordinary least squares regression, and automatic linear modeling, we accessed the impact of spatiotemporal factors. The results indicate that urban nature significantly influences walking and running activities across all seasons, especially at smaller radii around routes. The built environment impacts activities at larger radii. Seasonal climate and microclimate variations collectively affect activities, with warmer seasons and cooler local environments favoring running and walking. Our findings address the gap in the spatiotemporal analysis of physical activity and provide valuable insights for healthy urban planning.
PB  - SSRN
PY  - 2024
ST  - Combined Impact of Spatial and Temporal Environment on Physical Activity
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4962739
ER  -


TY  - GEN
AU  - Vo, K.D.
AU  - Kim, E.-J.
AU  - Lee, H.
AU  - Bansal, P.
TI  - Harnessing Household Travel Survey with Smart Card Data to Generate Spatiotemporally Heterogeneous Activity Schedules for Transit Users
AB  - Current activity-based models (ABMs) rely on household travel survey (HTS) data to generate activity schedules, but they suffer from low spatiotemporal heterogeneity due to HTS’s small sampling rate. While smart card (SC) data offers extensive spatiotemporal coverage of transit mobility, integrating it with HTS data is challenging due to differences in resolution and the lack of non-transit trip information in SC data. This study presents a novel twostage data fusion approach that leverages the strengths of HTS and SC data to create activity schedules for transit users with high spatiotemporal heterogeneity. The first stage involves two optimization problems: (i) creating clusters based on the spatiotemporal characteristics of transit trips to harmonize the resolution and information from both datasets and (ii) conditional on clusters, generating a fused distribution of spatiotemporal characteristics of transit trips and the travel mode and purpose of non-transit trips. The second stage generates the spatiotemporal characteristics of non-transit trips conditional on the distributions learned in the first stage, thereby completing the activity schedules of transit riders. The novelty of this approach lies in its ability to create consistent latent space (i.e., clusters) and maintain key distributions from both datasets while increasing heterogeneity. Using HTS and SC data from Seoul, we validate that key distributions, such as transit trips’ spatiotemporal patterns, are preserved in the fused distribution. The case study shows that integrating SC data increases spatiotemporal heterogeneity, generating 28 times more unique attribute combinations than HTS alone. Finally, the approach’s computational efficiency enables continuous ABM updates as new SC data arrives, which is critical for responding to systemlevel shocks.
PB  - SSRN
PY  - 2024
ST  - Harnessing Household Travel Survey with Smart Card Data to Generate Spatiotemporally Heterogeneous Activity Schedules for Transit Users
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4960458
ER  -


TY  - GEN
AU  - Lanzrath, H.
AU  - von Lieres, E.
AU  - Metzner, R.
AU  - Huber, G.J.
TI  - Analyzing Time Activity Curves from Spatio-Temporal Tracer Data to Determine Tracer Transport Velocity in Plants
AB  - Non-invasive methods utilizing tracers have a great potential to investigate carbon allocation in plants. Specifically, radioactive tracers, such as 11C, enable the monitoring of spatially localized transport processes on short time scales in living plants. Typically, such tracer transport experiments yield time activity curves (TACs) of tracer activity over time at various locations along a transport pathway. These TACs can exhibit different characteristic shapes that strongly depend on tracer transport dynamics, reflecting properties such as transport velocity, exchange with surrounding tissue, and tracer storage along the pathway. Various methods, either data-driven or model-based, exist to determine transport velocities from TACs. However, for some TAC shapes, the inferred carbon tracer velocity values can be inconsistent and greatly vary between analysis methods. In the present study, we review and evaluate different analysis methods for their suitability to reliably determine tracer transport velocities from typical TAC shapes. For this evaluation, we use both in silico generated and experimentally acquired TACs from positron emission tomography measurements on tomato, barley, and bean. We demonstrate that each of the compared methods can be suitable for specific TAC shapes while being less or not appropriate for others. In conclusion, we present a case-specific evaluation of methods as a reference for analyzing TACs from tracer transport experiments, which allows to ensure a robust and globally comparable determination of transport velocities.
PB  - SSRN
PY  - 2024
ST  - Analyzing Time Activity Curves from Spatio-Temporal Tracer Data to Determine Tracer Transport Velocity in Plants
Y2  - 2025/05/05/21:54:32
DO  - 10.1016/j.mbs.2025.109430
ER  -


TY  - GEN
AU  - Ao, J.
AU  - Wu, Y.
AU  - Wu, F.
AU  - Haddadin, S.
TI  - Behavior Tree Generation using Large Language Models for Sequential Manipulation Planning with Human Instructions and Feedback
AB  - Sequential manipulation planning has been a critical imperative to achieve a higher level of autonomy in robotics. Classical approaches to address task planning problems are based on symbolic formalisms, such as Planning Domain Definition Language (PDDL) [1], and search for state transition plans to reach task goals. In practice, such task plans are often programmed as Finite State Machines (FSMs), which incorporate expert knowledge specifying control and execution details. Due to its limitation of scalability [2], Behavior trees (BTs), which represent policies in a state-less, hierarchical tree structure, have gained increasing popularity for complex task planning. Its advantages of modularity, reusability and reactivity, make it a more desired formalism for long-horizon manipulation tasks.
PB  - TechRxiv
PY  - 2024
ST  - Behavior Tree Generation using Large Language Models for Sequential Manipulation Planning with Human Instructions and Feedback
Y2  - 2025/05/05/21:54:32
DO  - 10.36227/techrxiv.172668704.40528046/v1
ER  -


TY  - GEN
AU  - Ao, J.
AU  - Wu, Y.
AU  - Wu, F.
AU  - Haddadin, S.
TI  - Behavior Tree Generation using Large Language Models for Sequential Manipulation Planning with Human Instructions and Feedback
AB  - In this work, we propose an LLM-based BT generation framework to leverage thestrengths of both for sequential manipulation planning. To enable human-robotcollaborative task planning and enhance intuitive robot programming bynonexperts, the framework takes human instructions to initiate the generationof action sequences and human feedback to refine BT generation in runtime. Allpresented methods within the framework are tested on a real robotic assemblyexample, which uses a gear set model from the Siemens Robot Assembly Challenge.We use a single manipulator with a tool-changing mechanism, a common practicein flexible manufacturing, to facilitate robust grasping of a large variety ofobjects. Experimental results are evaluated regarding success rate, logicalcoherence, executability, time consumption, and token consumption. To ourknowledge, this is the first human-guided LLM-based BT generation frameworkthat unifies various plausible ways of using LLMs to fully generate BTs thatare executable on the real testbed and take into account granular knowledge oftool use.
PB  - arXiv
PY  - 2024
ST  - Behavior Tree Generation using Large Language Models for Sequential Manipulation Planning with Human Instructions and Feedback
Y2  - 2025/05/05/21:54:32
DO  - 10.36227/techrxiv.172668704.40528046/v1
ER  -


TY  - GEN
AU  - Hoffmeister, L.M.
AU  - Scassellati, B.
AU  - Rakita, D.
TI  - Sequential Discrete Action Selection via Blocking Conditions and Resolutions
AB  - In this work, we introduce a strategy that frames the sequential action selection problem for robots in terms of resolving blocking conditions, i.e., situations that impede progress on an action en route to a goal. This strategy allows a robot to make one-at-a-time decisions that take in pertinent contextual information and swiftly adapt and react to current situations. We present a first instantiation of this strategy that combines a state-transition graph and a zeroshot Large Language Model (LLM). The state-transition graph tracks which previously attempted actions are currently blocked and which candidate actions may resolve existing blocking conditions. This information from the state-transition graph is used to automatically generate a prompt for the LLM, which then uses the given context and set of possible actions to select a single action to try next. This selection process is iterative, with each chosen and executed action further refining the statetransition graph, continuing until the agent either fulfills the goal or encounters a termination condition. We demonstrate the effectiveness of our approach by comparing it to various LLM and traditional task-planning methods in a testbed of simulation experiments. We discuss the implications of our work based on our results.
PB  - arXiv
PY  - 2024
ST  - Sequential Discrete Action Selection via Blocking Conditions and Resolutions
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/iros58592.2024.10802019
ER  -


TY  - GEN
AU  - Li, W.
AU  - Chen, Z.
AU  - Bai, Y.
AU  - Lou, S.
AU  - Liao, Y.
TI  - Spatiotemporal Analysis of Wildfires in Alberta, Canada Over the Past Sixty Years: Increased Wildfire Frequency by Human Activities
AB  - Wildfires are a major socio-ecological issue in Alberta. The region’s extensive forests and grasslands provide abundant natural fuel. Since the onset of the 21st century, climate change and human activities have led to an increase in wildfire frequency. While some studies have focused on specific wildfire events and periods in Alberta, there remains a lack of comprehensive analysis addressing long-term dynamics and the multifactorial influences. This study aims to provide a thorough analysis of the historical dynamics of wildfires in Alberta, Canada (1961-2020), to examine long-term trends. Using historical datasets provided by the Alberta government, we analyzed the trends in wildfire frequency and burned areas over the past 60 years. The results reveal that although the total burned areas have declined over the last 60 years, the frequency of wildfires has significantly increased since the 21st century, with an average annual increase of approximately 0.0541 events. Spatial autocorrelation analysis shows significant clustering of human-caused (recreational and residential) fire events, while lightning-caused fires are more widely dispersed across the province. Hotspot analysis further identified specific areas with high fire activity. Finally, we incorporated 12 variables—including meteorological, human, and topographical factors (maximum temperature, precipitation, wind speed, soil moisture, snow water equivalent, lightning density, elevation, slope, aspect, population density, distance from population to roads, and land cover)—into a geographically weighted logistic regression (GWLR) model. The GWLR results highlighted the spatial variability in the influence of these factors on wildfire distribution. Population density emerged as the most significant factor affecting burned areas, underscoring the critical role of human activities in wildfire occurrences. The findings will help optimize the spatial planning of future wildfire prevention and control measures.
PB  - SSRN
PY  - 2024
ST  - Spatiotemporal Analysis of Wildfires in Alberta, Canada Over the Past Sixty Years
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5005510
ER  -


TY  - GEN
AU  - Reka, A.
AU  - Borza, D.L.
AU  - Reilly, D.
AU  - Balazia, M.
AU  - Bremond, F.
TI  - Introducing Gating and Context into Temporal Action Detection
AB  - Temporal Action Detection (TAD), the task of localizing and classifying actions in untrimmed video, remains challenging due to action overlaps and variable action durations. Recent findings suggest that TAD performance is dependent on the structural design of transformers rather than on the self-attention mechanism. Building on this insight, we propose a refined feature extraction process through lightweight, yet effective operations. First, we employ a local branch that employs parallel convolutions with varying window sizes to capture both fine-grained and coarse-grained temporal features. This branch incorporates a gating mechanism to select the most relevant features. Second, we introduce a context branch that uses boundary frames as key-value pairs to analyze their relationship with the central frame through cross-attention. The proposed method captures temporal dependencies and improves contextual understanding. Evaluations of the gating mechanism and context branch on challenging datasets (THUMOS14 and EPIC-KITCHEN 100) show a consistent improvement over the baseline and existing methods.
PB  - arXiv
PY  - 2024
ST  - Introducing Gating and Context into Temporal Action Detection
Y2  - 2025/05/05/21:54:32
DO  - 10.1609/aaai.v36i1.19900
ER  -


TY  - GEN
AU  - Fernando, P.A.
AU  - Premadasa, S.
TI  - Use of Game-Based Activities in Predicting the Sequential/Global Learning Dimension in the Felder-Silverman Model for Primary Schoolers
AB  - Modern primary schoolers, particularly those belonging to Generation Alpha, are deeply integrated with technology and gaming. Research has demonstrated numerous benefits of personalizing teaching based on students' learning styles. The Felder Silverman Learning Style Model has proven effective for digital and game-based learning environments. However, conventional learning style detection methods present several challenges for primary schoolers such as difficulty in comprehension of questions, and lack of motivation to complete questionnaires. This study introduces an automatic learning style detection approach for the sequential/global learning dimension of the Felder-Silverman model by employing a jigsaw puzzle, a reading game, and a fuzzy inference system for learning style prediction based on gameplay parameters, comprising a gameplay parameter named “order percentage”, designed to observe sequential preference of the student. An experiment involving thirty primary schoolers was conducted to evaluate the effectiveness of the proposed approach. The prototype predicted the students' prominent learning style with an accuracy of 86.7%. Feedback from post-questionnaires indicated a strong student preference for the proposed game activities over traditional questionnaires for learning style detection.
PB  - SSRN
PY  - 2024
ST  - Use of Game-Based Activities in Predicting the Sequential/Global Learning Dimension in the Felder-Silverman Model for Primary Schoolers
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4947162
ER  -


TY  - GEN
AU  - Wang, P.
AU  - Dai, W.
AU  - Xu, Y.
TI  - Temporal features of gamma activity in STN mediated by dopamine receptors in dyskinetic rat
AB  - Background Levodopa-induced dyskinesia (LID) is a challenging complication in the advanced stages of Parkinson's disease (PD). Excessive beta and gamma oscillations of PD and LID have been frequently reported in recent cross-sectional studies. Objective We investigate the temporal features of beta and gamma activity during the development of PD and LID, as well as the regulatory role of dopamine I receptors (D1R) and dopamine III receptor (D3R). Methods We collected motor behavior and electrophysiological data during the development of PD and LID, after interventions with D1R and D3R antagonists and agonists. Results We demonstrated exaggerated beta-band activity in PD state and excessive gamma-band activity during on-state dyskinesia. Subsequently, process-dependent increased beta activity correlated with bradykinesia during PD modeling, while a process-dependent increased gamma activity correlated with dyskinesia under the cumulative effects of L-dopa during on-state dyskinesia. Finally, both D1R and D3R were found to be involved in regulating dyskinesia and gamma activity. Conclusion Dynamic oscillations is closely associated with motor behavior, and mapping dynamic oscillations should be related to optimizing DBS parameters and developing personalized neurotherapeutic targeting.
PB  - Research Square
PY  - 2024
ST  - Temporal features of gamma activity in STN mediated by dopamine receptors in dyskinetic rat
Y2  - 2025/05/05/21:54:32
DO  - 10.21203/rs.3.rs-4853514/v1
ER  -


TY  - GEN
AU  - Dave, I.R.
AU  - Rizve, M.N.
AU  - Shah, M.
TI  - FinePseudo: Improving Pseudo-Labelling through Temporal-Alignablity for Semi-Supervised Fine-Grained Action Recognition
AB  - Real-life applications of action recognition often require a fine-grained understanding of subtle movements, e.g., in sports analytics, user interactions in AR/VR, and surgical videos. Although fine-grained actions are more costly to annotate, existing semi-supervised action recognition has mainly focused on coarse-grained action recognition. Since fine-grained actions are more challenging due to the absence of scene bias, classifying these actions requires an understanding of action-phases. Hence, existing coarse-grained semi-supervised methods do not work effectively. In this work, we for the first time thoroughly investigate semi-supervised fine-grained action recognition (FGAR). We observe that alignment distances like dynamic time warping (DTW) provide a suitable action-phase-aware measure for comparing fine-grained actions, a concept previously unexploited in FGAR. However, since regular DTW distance is pairwise and assumes strict alignment between pairs, it is not directly suitable for classifying fine-grained actions. To utilize such alignment distances in a limited-label setting, we propose an Alignability-Verification-based Metric learning technique to effectively discriminate between fine-grained action pairs. Our learnable alignability score provides a better phase-aware measure, which we use to refine the pseudo-labels of the primary video encoder. Our collaborative pseudo-labeling-based framework ‘FinePseudo’ significantly outperforms prior methods on four fine-grained action recognition datasets: Diving48, FineGym99, FineGym288, and FineDiving, and shows improvement on existing coarse-grained datasets: Kinetics400 and Something-SomethingV2. We also demonstrate the robustness of our collaborative pseudo-labeling in handling novel unlabeled classes in open-world semi-supervised setups.
PB  - arXiv
PY  - 2024
ST  - FinePseudo
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-031-73242-3_22
ER  -


TY  - GEN
AU  - Campana, S.
AU  - Tognetti, P.M.
AU  - Alberti, J.
AU  - Silvoso, M.C.
AU  - Yahdjian, L.
TI  - The Temporal and Spatial Stability of Plant Diversity are Disconnected from Biomass Stability in Response to Human Activities in a South American Temperate Grassland
AB  - Human activities alter biomass, nutrient availability, and species dominance in grasslands, impacting their richness, composition, and biomass production. Stability (invariability in time or space) can inform the predictability of plant communities in response to human activities. However, this measure has been simplistically analyzed for temporal (interannual) changes in live biomass, disregarding their spatial stability and the temporal stability of other plant community attributes. Moreover, the simultaneous analysis of temporal and spatial stabilities of plant communities has been scarcely assessed. Here, we test how biomass removal and nutrient addition simultaneously modify the temporal and spatial stabilities of plant richness (α diversity), composition dissimilarity (β diversity), aboveground live biomass, and the role of plant species dominance in the stability responses. We conducted a factorial experiment of biomass removal (grazing, mowing, or intact -no removal-) and nutrient addition (unfertilized or fertilized with nitrogen, phosphorus, and potassium) in a temperate grassland of Argentina, South America. We replicated the experiment in 6 blocks over 10 years to estimate the temporal and spatial stabilities of the plant community. The temporal and spatial stabilities of plant richness and composition dissimilarity decreased in the intact grassland, while the temporal stability of live biomass increased, compared to the grazed and mowed grasslands. Nutrient addition reduced the temporal and spatial stabilities of live biomass and the spatial stability of plant richness. The stability of species richness and composition dissimilarity were negatively associated with plant dominance, while the live biomass stability was not. Our results suggest that simplifying the effect of biomass removal and nutrient addition on grassland stability is not feasible, as plant diversity stability responses are not surrogates for biomass stability. The contrasting stability responses of plant diversity and biomass represent a step forward in predicting the impact of human activities over time and across space in temperate grasslands.
PB  - SSRN
PY  - 2024
ST  - The Temporal and Spatial Stability of Plant Diversity are Disconnected from Biomass Stability in Response to Human Activities in a South American Temperate Grassland
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4942602
ER  -


TY  - GEN
AU  - Azari, M.
AU  - Moridpour, S.
AU  - Hatami, M.
AU  - Hosseini, M.
TI  - Exploring the spatial-temporal arrangements of urban activity space from individual's daily commute: A Geospatial-Agent based Approach Using Empirical Data
AB  - The study explores the significance of individual mobility measures, such as activity space, in understanding how individuals interact with their daily environments. Existing measures often overlook geographical concepts like spatial-temporal arrangements of activity spaces, focusing solely on numerical assessments. To address this gap, a multi-level modeling approach combining Agent-Based Modeling (ABM) and Geographic Information Systems (GIS) is utilized to simulate activity destination selection throughout a workday in Zanjan, Iran. The model integrates individual preferences, built environment characteristics, network attributes, and travel generation data. Real-world data from Emerging Data Sources (EDSs) validate the model's reliability and accuracy. Key findings include: (1) clustering analysis identifying four types of activity destinations at different hourly intervals, (2) a central activity space acting as a hub for activity-based travel with a monocentric distribution pattern, (3) individual preference for destinations with diverse and dense built environments, and (4) a decrease in trip frequency as distance from the main activity space increases, indicating a spatial decay effect on activity-based travels.
PB  - Research Square
PY  - 2024
ST  - Exploring the spatial-temporal arrangements of urban activity space from individual's daily commute
Y2  - 2025/05/05/21:54:32
DO  - 10.21203/rs.3.rs-4835588/v1
ER  -


TY  - GEN
AU  - Kim, Y.
AU  - Kim, Y.-E.
AU  - Lee, S.-W.
TI  - Enhancing Spatio-Temporal Zero-Shot Action Recognition with Language-Driven Description Attributes
AB  - Vision-Language Models (VLMs) have demonstrated impressive capabilities in zero-shot action recognition by learning to associate video embeddings with class embeddings. However, a significant challenge arises when relying solely on action classes to provide semantic context, particularly due to the presence of multisemantic words, which can introduce ambiguity in understanding the intended concepts of actions. To address this issue, we propose an innovative approach that harnesses web-crawled descriptions, leveraging a large-language model to extract relevant keywords. This method reduces the need for human annotators and eliminates the laborious manual process of attribute data creation. Additionally, we introduce a spatio-temporal interaction module designed to focus on objects and action units, facilitating alignment between description attributes and video content. In our zero-shot experiments, our model achieves impressive results, attaining accuracies of 81.0%, 53.1%, and 68.9% on UCF-101, HMDB-51, and Kinetics-600, respectively, underscoring the model’s adaptability and effectiveness across various downstream tasks.
PB  - SSRN
PY  - 2024
ST  - Enhancing Spatio-Temporal Zero-Shot Action Recognition with Language-Driven Description Attributes
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4942030
ER  -


TY  - GEN
AU  - Tanaka, R.
AU  - Suzuki, T.
AU  - Fujii, K.
TI  - 3D Pose-Based Temporal Action Segmentation for Figure Skating: A Fine-Grained and Jump Procedure-Aware Annotation Approach
AB  - Understanding human actions from videos is essential in many domains, including sports. In figure skating, technical judgments are performed by watching skaters’ 3D movements, and its part of the judging procedure can be regarded as a Temporal Action Segmentation (TAS) task. TAS tasks in figure skating that automatically assign temporal semantics to video are actively researched. However, there is a lack of datasets and effective methods for TAS tasks requiring 3D pose data. In this study, we first created the FS-Jump3D dataset of complex and dynamic figure skating jumps using optical markerless motion capture. We also propose a new fine-grained figure skating jump TAS dataset annotation method with which TAS models can learn jump procedures. In the experimental results, we validated the usefulness of 3D pose features as input and the fine-grained dataset for the TAS model in figure skating. FS-Jump3D Dataset is available at https://github.com/ryota-skating/FS-Jump3D.
PB  - arXiv
PY  - 2024
ST  - 3D Pose-Based Temporal Action Segmentation for Figure Skating
Y2  - 2025/05/05/21:54:32
DO  - 10.1145/3689061.3689077
ER  -


TY  - GEN
AU  - Kim, J.
AU  - Lee, M.
AU  - Cho, C.-H.
AU  - Lee, J.
AU  - Heo, J.-P.
TI  - Prediction-Feedback DETR for Temporal Action Detection
AB  - Temporal Action Detection (TAD) is fundamental yet challenging for real-world video applications. Leveraging the unique benefits of transformers, various DETR-based approaches have been adopted in TAD. However, it has recently been identified that the attention collapse in self-attention causes the performance degradation of DETR for TAD. Building upon previous research, this paper newly addresses the attention collapse problem in cross-attention within DETR-based TAD methods. Moreover, our findings reveal that cross-attention exhibits patterns distinct from predictions, indicating a short-cut phenomenon. To resolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR), which utilizes predictions to restore the collapse and align the cross- and self-attention with predictions. Specifically, we devise novel prediction-feedback objectives using guidance from the relations of the predictions. As a result, Pred-DETR significantly alleviates the collapse and achieves state-of-the-art performance among DETR-based methods on various challenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, and FineAction.
PB  - arXiv
PY  - 2024
ST  - Prediction-Feedback DETR for Temporal Action Detection
Y2  - 2025/05/05/21:54:32
DO  - 10.1609/aaai.v39i4.32448
ER  -


TY  - GEN
AU  - Huang, W.-J.
AU  - Chen, M.-H.
AU  - Lai, S.-H.
TI  - Spatio-Temporal Context Prompting for Zero-Shot Action Detection
AB  - Spatio-temporal action detection encompasses the tasks of localizing and classifying individual actions within a video. Recent works aim to enhance this process by incorporating interaction modeling, which captures the relationship between people and their surrounding context. However, these approaches have primarily focused on fully-supervised learning, and the current limitation lies in the lack of generalization capability to recognize unseen action categories. In this paper, we aim to adapt the pretrained image-language models to detect unseen actions. To this end, we propose a method which can effectively leverage the rich knowledge of visual-language models to perform Person-Context Interaction. Meanwhile, our Context Prompting module will utilize contextual information to prompt labels, thereby enhancing the generation of more representative text features. Moreover, to address the challenge of recognizing distinct actions by multiple people at the same timestamp, we design the Interest Token Spotting mechanism which employs pretrained visual knowledge to find each person’s interest context tokens, and then these tokens will be used for prompting to generate text features tailored to each individual. To evaluate the ability to detect unseen actions, we propose a comprehensive benchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that our method achieves superior results compared to previous approaches and can be further extended to multi-action videos, bringing it closer to real-world applications. The code and data can be found in ST-CLIP.
PB  - arXiv
PY  - 2024
ST  - Spatio-Temporal Context Prompting for Zero-Shot Action Detection
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/wacv61041.2025.00880
ER  -


TY  - GEN
AU  - Du, J.-R.
AU  - Lin, K.-Y.
AU  - Meng, J.
AU  - Zheng, W.-S.
TI  - Towards Completeness: A Generalizable Action Proposal Generator for Zero-Shot Temporal Action Localization
AB  - To address the zero-shot temporal action localization (ZSTAL) task, existing works develop models that are generalizable to detect and classify actions from unseen categories. They typically develop a category-agnostic action detector and combine it with the Contrastive Language-Image Pre-training (CLIP) model to solve ZSTAL. However, these methods suffer from incomplete action proposals generated for unseen categories, since they follow a frame-level prediction paradigm and require hand-crafted post-processing to generate action proposals. To address this problem, in this work, we propose a novel model named Generalizable Action Proposal generator (GAP), which can interface seamlessly with CLIP and generate action proposals in a holistic way. Our GAP is built in a query-based architecture and trained with a proposallevel objective, enabling it to estimate proposal completeness and eliminate the hand-crafted post-processing. Based on this architecture, we propose an Action-aware Discrimination loss to enhance the categoryagnostic dynamic information of actions. Besides, we introduce a Static- Dynamic Rectifying module that incorporates the generalizable static information from CLIP to refine the predicted proposals, which improves proposal completeness in a generalizable manner. Our experiments show that our GAP achieves state-of-the-art performance on two challenging ZSTAL benchmarks, i.e., Thumos14 and ActivityNet1.3. Specifically, our model obtains significant performance improvement over previous works on the two benchmarks, i.e., +3.2% and +3.4% average mAP, respectively. The code is available at https://github.com/Run542968/GAP.
PB  - arXiv
PY  - 2024
ST  - Towards Completeness
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-031-78444-6_17
ER  -


TY  - GEN
AU  - Wang, F.
AU  - Wang, Q.
AU  - Wang, Y.
TI  - FMI-TAL: Few-shot Multiple Instances Temporal Action Localization by Probability Distribution Learning and Interval Cluster Refinement
AB  - The present few-shot temporal action localization model can’t handle the situation where videos contain multiple action instances. So the purpose of this paper is to achieve manifold action instances localization in a lengthy untrimmed query video using limited trimmed support videos. To address this challenging problem effectively, we proposed a novel solution involving a spatial-channel relation transformer with probability learning and cluster refinement. This method can accurately identify the start and end boundaries of actions in the query video, utilizing only a limited number of labeled videos. Our proposed method is adept at capturing both temporal and spatial contexts to effectively classify and precisely locate actions in videos, enabling a more comprehensive utilization of these crucial details. The selective cosine penalization algorithm is designed to suppress temporal boundaries that do not include action scene switches. The probability learning combined with the label generation algorithm alleviates the problem of action duration diversity and enhances the model’s ability to handle fuzzy action boundaries. The interval cluster can help us get the final results with multiple instances situations in few-shot temporal action localization. Our model achieves competitive performance through meticulous experimentation utilizing the benchmark datasets ActivityNet1.3 and THUMOS14. Our code is readily available at https://github.com/ycwfs/FMI-TAL.
PB  - arXiv
PY  - 2024
ST  - FMI-TAL
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4341961
ER  -


TY  - GEN
AU  - Osman, N.
AU  - Torki, M.
TI  - Temporal Divide-and-Conquer Anomaly Actions Localization in Semi-Supervised Videos with Hierarchical Transformer
AB  - Anomaly action detection and localization play an essential role in security and advanced surveillance systems. However, due to the tremendous amount of surveillance videos, most of the available data for the task is unlabeled or semi-labeled with the video class known, but the location of the anomaly event is unknown. In this work, we target anomaly localization in semi-supervised videos. While the mainstream direction in addressing this task is focused on segment-level multi-instance learning and the generation of pseudo labels, we aim to explore a promising yet unfulfilled direction to solve the problem by learning the temporal relations within videos in order to locate anomaly events. To this end, we propose a hierarchical transformer model designed to evaluate the significance of observed actions in anomalous videos with a divide-and-conquer strategy along the temporal axis. Our approach segments a parent video hierarchically into multiple temporal children instances and measures the influence of the children nodes in classifying the abnormality of the parent video. Evaluating our model on two well-known anomaly detection datasets, UCF-crime and ShanghaiTech, proves its ability to interpret the observed actions within videos and localize the anomalous ones. Our proposed approach outperforms previous works relying on segment-level multiple-instance learning approaches while reaching a promising performance compared to the more recent pseudo-labeling-based approaches.
PB  - arXiv
PY  - 2024
ST  - Temporal Divide-and-Conquer Anomaly Actions Localization in Semi-Supervised Videos with Hierarchical Transformer
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-031-78354-8_15
ER  -


TY  - GEN
AU  - Kim, J.
AU  - Lee, M.
AU  - Heo, J.-P.
TI  - Long-term Pre-traning for Temporal Action Detection with Transformers
AB  - Temporal action detection (TAD) is challenging, yet fundamental for real-world video applications. Recently, DETR-based models for TAD have been prevailing thanks to their unique benefits. However, transformers demand a huge dataset, and unfortunately data scarcity in TAD causes a severe degeneration. In this paper, we identify two crucial problems from data scarcity: attention collapse and imbalanced performance. To this end, we propose a new pre-training strategy, Long-Term Pre-training (LTP), tailored for transformers. LTP has two main components: 1) class-wise synthesis, 2) long-term pretext tasks. Firstly, we synthesize long-form video features by merging video snippets of a target class and non-target classes. They are analogous to untrimmed data used in TAD, despite being created from trimmed data. In addition, we devise two types of long-term pretext tasks to learn long-term dependency. They impose long-term conditions such as finding second-to-fourth or short-duration actions. Our extensive experiments show state-of-the-art performances in DETR-based methods on ActivityNet-v1.3 and THUMOS14 by a large margin. Moreover, we demonstrate that LTP significantly relieves the data scarcity issues in TAD.
PB  - arXiv
PY  - 2024
ST  - Long-term Pre-traning for Temporal Action Detection with Transformers
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.5231696
ER  -


TY  - GEN
AU  - Kim, J.
AU  - Choi, J.
AU  - Jeon, Y.
AU  - Heo, J.-P.
TI  - Boundary-Recovering Network for Temporal Action Detection
AB  - Temporal action detection (TAD) is challenging, yet fundamental for real-world video applications. Large temporal scale variation of actions is one of the most primary difficulties in TAD. Naturally, multi-scale features have potential in localizing actions of diverse lengths as widely used in object detection. Nevertheless, unlike objects in images, actions have more ambiguity in their boundaries. That is, small neighboring objects are not considered as a large one while short adjoining actions can be misunderstood as a long one. In the coarse-to-fine feature pyramid via pooling, these vague action boundaries can fade out, which we call `vanishing boundary problem'. To this end, we propose Boundary-Recovering Network (BRN) to address the vanishing boundary problem. BRN constructs scale-time features by introducing a new axis called scale dimension by interpolating multi-scale features to the same temporal length. On top of scale-time features, scale-time blocks learn to exchange features across scale levels, which can effectively settle down the issue. Our extensive experiments demonstrate that our model outperforms the state-of-the-art on the two challenging benchmarks, ActivityNet-v1.3 and THUMOS14, with remarkably reduced degree of the vanishing boundary problem.
PB  - SSRN
PY  - 2024
ST  - Boundary-Recovering Network for Temporal Action Detection
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4934024
ER  -


TY  - GEN
AU  - Zheng, Q.
AU  - Yin, Y.
AU  - Zheng, B.
TI  - Actionmamba: Action Spatial-Temporal Aggregation Network Based on Mamba and Gcn for Skeleton-Based Action Recognition
AB  - Skeleton-based action recognition networks have widely adopted the approach of Graph Convolutional Networks (GCN) due to their superior capabilities in modeling data topology, but several key issues still require further investigation. Firstly, the graph convolutional network extracts action features by applying temporal convolution to each key point, which causes the model to ignore the temporal connections between different important points. Secondly, the graph convolutional network's local receptive field limits the model's ability to understand the correlation between non-adjacent joints. Motivated by the State Space Model(SSM), we propose a Action Spatiotemporal Aggregation Network, named ActionMamba. Specifically, we introduce a novel embedding module called the Action Characteristic Encoder (ACE), which enhances the coupling of temporal and spatial information in skeletal features by combining intrinsic spatiotemporal encoding with extrinsic space encoding. Additionally, we design an Action Perception Model(APM) based on Mamba and GCN. By effectively combining the excellent feature processing capabilities of GCN with the outstanding global information modeling capabilities of Mamba, APM is able to comprehend the hidden features between different joints and selectively filter information from various joints. Extensive experimental results demonstrate that ActionMamba achieves state-of-the-art performance on three challenging benchmark datasets: NTU-RGB+D 60, NTU-RGB+D 120 and UAV-Human. The code is available at the link.
PB  - SSRN
PY  - 2024
ST  - Actionmamba
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4931394
ER  -


TY  - GEN
AU  - Ross, M.K.
AU  - Tulabandhula, T.
AU  - Nguyen, T.M.
AU  - Ajilore, O.A.
AU  - Leow, A.D.
TI  - Keying into Cognition: Temporal Smoothing of Smartphone Typing Behaviors for Passive Assessment of Processing Speed and Executive Function in Individuals with Mood Disorders
AB  - Introduction: Cognitive deficits commonly affect everyday life for individuals with mood disorders, even between mood episodes. Monitoring of these symptoms can pose several challenges due to the limitations of current methods, prompting the need for enhanced modalities to unobtrusively and objectively measure cognitive function and its fluctuations in individuals. This study explored the feasibility of passively assessing processing speed and executive function, traditionally measured by the trail-making test part B (TMT-B), using smartphone keyboard typing behaviors and assessed how diurnal patterns may impact cognitive function. Methods Through a novel method of temporal smoothing of smartphone typing behaviors via graph-regularized singular value decomposition, we engineered features to capture typing regularity as a proxy for diurnal patterns and sleep. These features were added to machine learning models constructed to predict TMT-B performance and evaluated for improvement in model performance. Results Of the models tested, a random forest model built with the addition of typing regularity features performed the best with the lowest RMSE and MAE of 0.769 and 0.644, respectively. Our findings suggest that aspects of individuals’ cognitive function, specifically processing speed and executive function, can be estimated through their smartphone typing behaviors without the need for clinical or demographic input, and these estimates are improved with additional information capturing diurnal patterns and estimated sleep. Conclusion This objective approach, passively administered in-the-wild, has the potential to supplement current methods of cognitive assessment and provide a more detailed report of cognitive fluctuations and the influence of diurnal patterns on cognitive function in individuals with mood disorders.
PB  - Research Square
PY  - 2024
ST  - Keying into Cognition
Y2  - 2025/05/05/21:54:32
DO  - 10.21203/rs.3.rs-4849283/v1
ER  -


TY  - GEN
AU  - Li, B.
AU  - Liu, M.
AU  - Wang, G.
AU  - Yu, Y.
TI  - Frame Order Matters: A Temporal Sequence-Aware Model for Few-Shot Action Recognition
AB  - In this paper, we propose a novel Temporal Sequence-Aware Model (TSAM) for few-shot action recognition (FSAR), which incorporates a sequential perceiver adapter into the pre-training framework, to integrate both the spatial information and the sequential temporal dynamics into the feature embeddings. Different from the existing fine-tuning approaches that capture temporal information by exploring the relationships among all the frames, our perceiver-based adapter recurrently captures the sequential dynamics alongside the timeline, which could perceive the order change. To obtain the discriminative representations for each class, we extend a textual corpus for each class derived from the large language models (LLMs) and enrich the visual prototypes by integrating the contextual semantic information. Besides, We introduce an unbalanced optimal transport strategy for feature matching that mitigates the impact of class-unrelated features, thereby facilitating more effective decision-making. Experimental results on five FSAR datasets demonstrate that our method set a new benchmark, beating the second-best competitors with large margins.
PB  - arXiv
PY  - 2024
ST  - Frame Order Matters
Y2  - 2025/05/05/21:54:32
DO  - 10.1609/aaai.v39i17.34004
ER  -


TY  - GEN
AU  - Wang, H.
AU  - Han, Y.
AU  - Wang, K.
AU  - Lian, D.
AU  - Chen, E.
TI  - Denoising Pre-Training and Customized Prompt Learning for Efficient Multi-Behavior Sequential Recommendation
AB  - In the realm of recommendation systems, users exhibit a diverse array of behaviors when interacting with items. This phenomenon has spurred research into learning the implicit semantic relationships between these behaviors to enhance recommendation performance. However, these methods often entail high computational complexity. To address concerns regarding efficiency, pre-training presents a viable solution. Its objective is to extract knowledge from extensive pre-training data and fine-tune the model for downstream tasks. Nevertheless, previous pre-training methods have primarily focused on single-behavior data, while multi-behavior data contains significant noise. Additionally, the fully fine-tuning strategy adopted by these methods still imposes a considerable computational burden. In response to this challenge, we propose DPCPL, the first pre-training and prompt-tuning paradigm tailored for Multi-Behavior Sequential Recommendation. Specifically, in the pre-training stage, we commence by proposing a novel Efficient Behavior Miner (EBM) to filter out the noise at multiple time scales, thereby facilitating the comprehension of the contextual semantics of multi-behavior sequences. Subsequently, we propose to tune the pre-trained model in a highly efficient manner with the proposed Customized Prompt Learning (CPL) module, which generates personalized, progressive, and diverse prompts to fully exploit the potential of the pre-trained model effectively. Extensive experiments on three real-world datasets have unequivocally demonstrated that DPCPL not only exhibits high efficiency and effectiveness, requiring minimal parameter adjustments but also surpasses the state-of-the-art performance across a diverse range of downstream tasks.
PB  - arXiv
PY  - 2024
ST  - Denoising Pre-Training and Customized Prompt Learning for Efficient Multi-Behavior Sequential Recommendation
Y2  - 2025/05/05/21:54:32
DO  - 10.1145/3543507.3583513
ER  -


TY  - GEN
AU  - Pang, Z.
AU  - Sener, F.
AU  - Ramasubramanian, S.
AU  - Yao, A.
TI  - Long-Tail Temporal Action Segmentation with Group-wise Temporal Logit Adjustment
AB  - Procedural activity videos often exhibit a long-tailed action distribution due to varying action frequencies and durations. However, state-of-the-art temporal action segmentation methods overlook the long tail and fail to recognize tail actions. Existing long-tail methods make class-independent assumptions and struggle to identify tail classes when applied to temporal segmentation frameworks. This work proposes a novel group-wise temporal logit adjustment (G-TLA) framework that combines a group-wise softmax formulation while leveraging activity information and action ordering for logit adjustment. The proposed framework significantly improves in segmenting tail actions without any performance loss on head actions. Source code is available4
PB  - arXiv
PY  - 2024
ST  - Long-Tail Temporal Action Segmentation with Group-wise Temporal Logit Adjustment
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-031-73404-5_19
ER  -


TY  - GEN
AU  - Gunasekara, S.R.
AU  - Li, W.
AU  - Yang, J.
AU  - Ogunbona, P.
TI  - Joint Temporal Pooling for Improving Skeleton-based Action Recognition
AB  - In skeleton-based human action recognition, temporal pooling is a critical step for capturing spatiotemporal relationship of joint dynamics. Conventional pooling methods overlook the preservation of motion information and treat each frame equally. However, in an action sequence, only a few segments of frames carry discriminative information related to the action. This paper presents a novel Joint Motion Adaptive Temporal Pooling (JMAP) method for improving skeleton-based action recognition. Two variants of JMAP, frame-wise pooling and joint-wise pooling, are introduced. The efficacy of JMAP has been validated through experiments on the popular NTU RGB+D 120 and PKU-MMD datasets.
PB  - arXiv
PY  - 2024
ST  - Joint Temporal Pooling for Improving Skeleton-based Action Recognition
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/dicta60407.2023.00062
ER  -


TY  - GEN
AU  - Kim, J.
AU  - Choi, J.
AU  - Jeon, Y.
AU  - Heo, J.-P.
TI  - Boundary-Recovering Network for Temporal Action Detection
AB  - Temporal action detection (TAD) is challenging, yet fundamental for real-world video applications. Large temporal scale variation of actions is one of the most primary difficulties in TAD. Naturally, multi-scale features have potential in localizing actions of diverse lengths as widely used in object detection. Nevertheless, unlike objects in images, actions have more ambiguity in their boundaries. That is, small neighboring objects are not considered as a large one while short adjoining actions can be misunderstood as a long one. In the coarse-to-fine feature pyramid via pooling, these vague action boundaries can fade out, which we call 'vanishing boundary problem'. To this end, we propose Boundary-Recovering Network (BRN) to address the vanishing boundary problem. BRN constructs scale-time features by introducing a new axis called scale dimension by interpolating multi-scale features to the same temporal length. On top of scale-time features, scale-time blocks learn to exchange features across scale levels, which can effectively settle down the issue. Our extensive experiments demonstrate that our model outperforms the state-of-the-art on the two challenging benchmarks, ActivityNet-v1.3 and THUMOS14, with remarkably reduced degree of the vanishing boundary problem.
PB  - arXiv
PY  - 2024
ST  - Boundary-Recovering Network for Temporal Action Detection
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4934024
ER  -


TY  - GEN
AU  - Wang, Y.
AU  - Jiang, X.
AU  - Cheng, D.
AU  - Li, D.
AU  - Zhao, C.
TI  - ActPrompt: In-Domain Feature Adaptation via Action Cues for Video Temporal Grounding
AB  - Video temporal grounding is an emerging topic aiming to identify specific clips within videos. In addition to pre-trained video models, contemporary methods utilize pre-trained vision-language models (VLM) to capture detailed characteristics of diverse scenes and objects from video frames. However, as pre-trained on images, VLM may struggle to distinguish action-sensitive patterns from static objects, making it necessary to adapt them to specific data domains for effective feature representation over temporal grounding. We address two primary challenges to achieve this goal. Specifically, to mitigate high adaptation costs, we propose an efficient preliminary in-domain fine-tuning paradigm for feature adaptation, where downstream-adaptive features are learned through several pretext tasks. Furthermore, to integrate action-sensitive information into VLM, we introduce Action-Cue-Injected Temporal Prompt Learning (ActPrompt), which injects action cues into the image encoder of VLM for better discovering action-sensitive patterns. Extensive experiments demonstrate that ActPrompt is an off-the-shelf training framework that can be effectively applied to various SOTA methods, resulting in notable improvements. The complete code used in this study is provided in the supplementary materials.
PB  - arXiv
PY  - 2024
ST  - ActPrompt
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/icassp49660.2025.10888960
ER  -


TY  - GEN
AU  - Lim, G.
AU  - Kim, H.
AU  - Kim, J.
AU  - Choi, Y.
TI  - Probabilistic Vision-Language Representation for Weakly Supervised Temporal Action Localization
AB  - Weakly supervised temporal action localization (WTAL) aims to detect action instances in untrimmed videos using only video-level annotations. Since many existing works optimize WTAL models based on action classification labels, they encounter the task discrepancy problem (i.e., localization-by-classification). To tackle this issue, recent studies have attempted to utilize action category names as auxiliary semantic knowledge through vision-language pre-training (VLP). However, there are still areas where existing research falls short. Previous approaches primarily focused on leveraging textual information from language models but overlooked the alignment of dynamic human action and VLP knowledge in a joint space. Furthermore, the deterministic representation employed in previous studies struggles to capture fine-grained human motions. To address these problems, we propose a novel framework that aligns human action knowledge and VLP knowledge in a probabilistic embedding space. Moreover, we propose intra- and inter-distribution contrastive learning to enhance the probabilistic embedding space based on statistical similarities. Extensive experiments and ablation studies reveal that our method significantly outperforms all previous state-of-the-art methods. Code is available at https://github.com/sejong-rcv/PVLR.
PB  - arXiv
PY  - 2024
ST  - Probabilistic Vision-Language Representation for Weakly Supervised Temporal Action Localization
Y2  - 2025/05/05/21:54:32
DO  - 10.1145/3664647.3681537
ER  -


TY  - GEN
AU  - Reza, S.
AU  - Zhang, Y.
AU  - Moghaddam, M.
AU  - Camps, O.
TI  - HAT: History-Augmented Anchor Transformer for Online Temporal Action Localization
AB  - Online video understanding often relies on individual frames, leading to frame-by-frame predictions. Recent advancements such as Online Temporal Action Localization (OnTAL), extend this approach to instance-level predictions. However, existing methods mainly focus on short-term context, neglecting historical information. To address this, we introduce the History-Augmented Anchor Transformer (HAT) Framework for OnTAL. By integrating historical context, our framework enhances the synergy between long-term and short-term information, improving the quality of anchor features crucial for classification and localization. We evaluate our model on both procedural egocentric (PREGO) datasets (EGTEA and EPIC) and standard non-PREGO OnTAL datasets (THUMOS and MUSES). Results show that our model outperforms state-of-the-art approaches significantly on PREGO datasets and achieves comparable or slightly superior performance on non-PREGO datasets, underscoring the importance of leveraging long-term history, especially in procedural and egocentric action scenarios. Code is available at: https://github.com/sakibreza/ECCV24-HAT/.
PB  - arXiv
PY  - 2024
ST  - HAT
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-031-72664-4_12
ER  -


TY  - GEN
AU  - Yang, Y.
AU  - Wang, X.
AU  - Zhang, D.
TI  - Illuminating Tandem Reactions Characterized by Temporal Separation of Catalytic Activities Via Dft Calculations: A Case Study of Ni-Catalyzed Alkyne Semi-Hydrogenation
AB  - The concept of “temporal separation of catalytic activities” outlines a scenario where multiple transformations within a catalytic tandem reaction proceed sequentially over time without mutual interference. After presenting several examples of such reactions, we specifically focus on an example of the Ni-catalyzed alkyne semi-hydrogenation as a significant case study. By performing density functional theory (DFT) calculations, we illuminate the unique dynamic character of the reaction that the intermediate remains dormant until the reactant exhausted. The insights gained from the present calculations have led us to propose a comprehensive energy landscape model for the catalytic tandem reactions with temporal separation of catalytic activities, which offers a logical explanation for the temporal dormancy of the intermediate. This class of reactions is expected to be highly valuable as it presents the opportunity to fine-tune individual reaction steps, thereby introducing fresh concepts for precise control of reactions in one-pot chemistry.
PB  - SSRN
PY  - 2024
ST  - Illuminating Tandem Reactions Characterized by Temporal Separation of Catalytic Activities Via Dft Calculations
Y2  - 2025/05/05/21:54:32
DO  - 10.1016/j.mcat.2024.114600
ER  -


TY  - GEN
AU  - Hao, Y.
AU  - Ma, C.
AU  - Lu, J.
AU  - Hao, H.
AU  - Xu, K.
TI  - Modeling Spatial-Temporal Behavior of Precipitation-Driven Karst Spring Discharge Using a Hybrid Deep Learning Model
AB  - Karst spring discharges arise from intricate hydrological processes involving spatial and temporal coupling of various hydrological factors, including precipitation, infiltration, and water flow through complex karst aquifers. As a new method for simulating karst spring discharges, deep learning yields better results compared with traditional numerical modelling methods. Existing deep learning methods, however, mainly consider temporal characteristics of data collected from different observation locations and spatial dependency of these observation locations is often ignored. In this study, we propose a hybrid deep learning model coupling a graph convolutional network (GCN) and a multi-layer long short-term memory (LSTM) network for karst spring discharge prediction. First, the GCN learns spatial hydrological correlation features of different observation locations. Temporal data are used to learn sequence spatial correlation features. Then, the sequence spatial correlation features are fed into the multi-layer LSTM, mining both spatial and temporal features of the observed data. Finally, the mined features are leveraged to predict spring discharges by a non-linear calculation. Experimental results of applying the hybrid deep learning model to Niangziguan Springs China show that simultaneously considering the spatial and temporal correlations between observed precipitation and spring discharges improves the accuracy of spring discharge simulation. The spatial hydrological correlation features from different observation locations learned from the hybrid model improve the understanding of the relation between precipitation and spring discharges. Furthermore, including historical observed spring discharges as inputs improves the accuracy of spring discharge simulation since the observed discharge history reflects the influence of human activities and aquifers on spring flow. The capability of mining intricate spatial and temporal correlation in observed data allows the hybrid model to accurately predict spring discharge several time steps into the future, presenting the model as a robust predictive tool for water resources management.
PB  - SSRN
PY  - 2024
ST  - Modeling Spatial-Temporal Behavior of Precipitation-Driven Karst Spring Discharge Using a Hybrid Deep Learning Model
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4918785
ER  -


TY  - GEN
AU  - Matringhen, C.
AU  - Vigier, A.
AU  - Bourtouli, N.
AU  - Marissal, T.
TI  - Minimally invasive activation of spared interneurons alleviates local CA1 hypersynchrony and behavioral deficits in a model of temporal lobe epilepsy
AB  - Background: Temporal lobe epilepsy (TLE) is associated with severe cognitive impairments including memory deficits. The dysfunction of hippocampal inhibitory neurons is proposed as a key mechanism and possible target for therapeutic approaches. However, the nature and extent of alterations in hippocampal inhibitory neurons remain unclear, as does their impact on behavioral impairments associated with TLE. Methods: We investigated the role of inhibitory neurons from the CA1 hippocampal region on memory deficits associated with TLE, considering both the survival and changes in the activity of a large population of interneurons. To this end, we used a combination of immunolabelling, calcium imaging, electrophysiology, human-applicable chemogenetic tools, and behavioral testing on a reliable mouse pilocarpine TLE model. Results: We show that in TLE mice with severely disturbed spatial behavior, CA1 major interneuron populations are spared from histological damages that affect the epileptic hippocampus (e.g., sclerosis). However, CA1 interneurons fire less in epileptic than in control conditions, resulting in increased synchronization and activity of the epileptic CA1 network in vitro. Restoring CA1 interneuron discharge using a chemogenetic strategy rescued CA1 activity and synchronization in vitro. In vivo, the minimally invasive chemogenetic activation of hippocampal interneurons does not affect generalized seizures but reduces behavioral alterations. Conclusions: Our data suggest that rescuing CA1 local network dynamics using interneurons as a lever could be sufficient to decrease behavioral deficits related to TLE.
PB  - bioRxiv
PY  - 2024
ST  - Minimally invasive activation of spared interneurons alleviates local CA1 hypersynchrony and behavioral deficits in a model of temporal lobe epilepsy
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2024.08.02.606307
ER  -


TY  - GEN
AU  - Song, Y.
AU  - Kim, D.
AU  - Cho, M.
AU  - Kwak, S.
TI  - Online Temporal Action Localization with Memory-Augmented Transformer
AB  - Online temporal action localization (On-TAL) is the task of identifying multiple action instances given a streaming video. Since existing methods take as input only a video segment of fixed size per iteration, they are limited in considering long-term context and require tuning the segment size carefully. To overcome these limitations, we propose memory-augmented transformer (MATR). MATR utilizes the memory queue that selectively preserves the past segment features, allowing to leverage long-term context for inference. We also propose a novel action localization method that observes the current input segment to predict the end time of the ongoing action and accesses the memory queue to estimate the start time of the action. Our method outperformed existing methods on two datasets, THUMOS14 and MUSES, surpassing not only TAL methods in the online setting but also some offline TAL methods.
PB  - arXiv
PY  - 2024
ST  - Online Temporal Action Localization with Memory-Augmented Transformer
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-031-72655-2_5
ER  -


TY  - GEN
AU  - Zheng, N.
AU  - Du, Y.
AU  - Xia, H.
AU  - Liu, D.
TI  - Signal-SGN: A Spiking Graph Convolutional Network for Skeleton Action Recognition via Learning Temporal-Frequency Dynamics
AB  - For skeleton-based action recognition, Graph Convolutional Networks (GCNs) are effective models. Still, their reliance on floating-point computations leads to high energy consumption, limiting their applicability in battery-powered devices. While energy-efficient, Spiking Neural Networks (SNNs) struggle to model skeleton dynamics, leading to suboptimal solutions. We propose Signal-SGN (Spiking Graph Convolutional Network), which utilizes the temporal dimension of skeleton sequences as the spike time steps and represents features as multi-dimensional discrete stochastic signals for temporal-frequency domain feature extraction. It combines the 1D Spiking Graph Convolution (1D-SGC) module and the Frequency Spiking Convolution (FSC) module to extract features from the skeleton represented as spiking form. Additionally, the Multi-Scale Wavelet Transform Feature Fusion (MWTF) module is proposed to extract dynamic spiking features and capture frequency-specific characteristics, enhancing classification performance. Experiments across three large-scale datasets reveal Signal-SGN exceeding state-of-the-art SNN-based methods in accuracy and computational efficiency while attaining comparable performance with GCN methods and significantly reducing theoretical energy consumption.
PB  - arXiv
PY  - 2024
ST  - Signal-SGN
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/tai.2023.3329799
ER  -


TY  - GEN
AU  - Xu, D.
AU  - Luo, Y.
AU  - Lu, T.
AU  - Zhou, Q.
AU  - Nie, B.
TI  - STDA: Spatio-Temporal Dual-Encoder Network Incorporating Driver Attention to Predict Driver Behaviors Under Safety-Critical Scenarios
AB  - Accurate behavior prediction for vehicles is essential but challenging for autonomous driving. Most existing studies show satisfying performance under regular scenarios, but most neglected safety-critical scenarios. In this study, a spatio-temporal dual-encoder network named STDA for safety-critical scenarios was developed. Considering the exceptional capabilities of human drivers in terms of situational awareness and comprehending risks, driver attention was incorporated into STDA to facilitate swift identification of the critical regions, which is expected to improve both performance and interpretability. STDA contains four parts: the driver attention prediction module, which predicts driver attention; the fusion module designed to fuse the features between driver attention and raw images; the temporary encoder module used to enhance the capability to interpret dynamic scenes; and the behavior prediction module to predict the behavior. The experiment data are used to train and validate the model. The results show that STDA improves the G-mean from 0.659 to 0.719 when incorporating driver attention and adopting a temporal encoder module. In addition, extensive experimentation has been conducted to validate that the proposed module exhibits robust generalization capabilities and can be seamlessly integrated into other mainstream models.
PB  - arXiv
PY  - 2024
ST  - STDA
Y2  - 2025/05/05/21:54:32
DO  - 10.4271/2018-01-1626
ER  -


TY  - GEN
AU  - Keum, J.-Y.
AU  - Toi, P.T.
AU  - Park, S.
AU  - Chun, H.
AU  - Park, J.-Y.
TI  - Direct imaging of neural activity reveals neural circuits via spatiotemporal activation mapping
AB  - Two years ago, our group reported direct imaging of neuronal activity (DIANA), a functional magnetic resonance imaging (fMRI) technique that directly detects neuronal activity at high spatiotemporal resolution. In this study, we successfully reproduced the DIANA response in medetomidine-anesthetized mice using forelimb electrical stimulation at 11.7 T. More importantly, we showed that multiple neural circuits can be effectively revealed by DIANA fMRI through spatiotemporal activation mapping. The spatiotemporal activation mapping proposed here utilizes the temporal information of the DIANA response, that is, the time when the DIANA response reaches its peak, which is a unique feature that distinguishes it from the activation mapping method used in existing fMRI. Based on DIANA activation areas, we identified several neural circuits involved in forelimb sensory processing in the somatosensory network, which includes multiple brain regions: ventral posterolateral nucleus of the thalamus (VPL), posteromedial thalamic nucleus (POm), forelimb primary somatosensory cortex (S1FL), secondary somatosensory cortex (S2), primary motor cortex (M1), and secondary motor cortex (M2). Additionally, we also identified a pain-related neural circuit involving brain regions of the anterior cingulate cortex (ACC) and mediodorsal nucleus (MD). Interestingly, the spatiotemporal activation mapping also allowed us to identify subregions with different DIANA response times within the same functional region (e.g., VPL, POm, S1FL, and S2). Our study highlights the potential of DIANA fMRI to advance our understanding of sensory information processing throughout the brain and to provide insight into the spatiotemporal dynamics of brain networks at the level of neural circuits.
PB  - bioRxiv
PY  - 2024
ST  - Direct imaging of neural activity reveals neural circuits via spatiotemporal activation mapping
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2024.07.31.606112
ER  -


TY  - GEN
AU  - Chen, S.
AU  - Phillips, C.M.
TI  - Nuclear Argonaute protein NRDE-3 switches small RNA partners during embryogenesis to mediate temporal-specific gene regulatory activity
AB  - RNA interference (RNAi) is a conserved gene regulation mechanism that utilizes the Argonaute protein and their associated small RNAs to exert regulatory function on complementary transcripts. While the majority of germline-expressed RNAi pathway components reside in perinuclear germ granules, it is unknown whether and how RNAi pathways are spatially organized in other cell types. Here we find that the small RNA biogenesis machinery is spatially and temporally organized during embryogenesis. Specifically, the RNAi factor, SIMR-1, forms visible concentrates during mid-embryogenesis that contain an RNA-dependent RNA polymerase, a poly-UG polymerase, and the unloaded nuclear Argonaute protein, NRDE-3. We also observe that many other RNAi factors form foci in embryonic cells distinct from “SIMR granules”, including the Argonaute protein CSR-1, underscoring a potential role for cytoplasmic concentrates of RNAi factors to promote gene regulation in embryos. Curiously, coincident with the appearance of the SIMR granules, the small RNAs bound to NRDE-3 switch from predominantly CSR-class 22G-RNAs to ERGO-dependent 22G-RNAs. Prior work has shown that NRDE-3 binds ERGO-dependent 22G-RNAs in the somatic cells of larvae and adults to silence ERGO-target genes; here we demonstrate that NRDE-3-bound, CSR-class 22G-RNAs repress transcription in oocytes. Thus, our study defines two separable roles for NRDE-3, targeting germline-expressed genes during oogenesis to promote global transcriptional repression, and switching during embryogenesis to repress recently duplicated genes and retrotransposons in somatic cells, highlighting the plasticity of Argonaute proteins and the need for more precise temporal characterization of Argonaute-small RNA interactions.
PB  - bioRxiv
PY  - 2024
ST  - Nuclear Argonaute protein NRDE-3 switches small RNA partners during embryogenesis to mediate temporal-specific gene regulatory activity
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2024.07.29.605686
ER  -


TY  - GEN
AU  - Lanzrath, H.
AU  - von Lieres, E.
AU  - Metzner, R.
AU  - Huber, G.J.
TI  - Analyzing Time Activity Curves from Spatio-Temporal Tracer Data to Determine Tracer Transport Velocity in Plants
AB  - Non-invasive methods utilizing tracers have a great potential to investigate carbon allocation in plants. Specifically, radioactive tracers, such as 11C, enable the monitoring of spatially localized transport processes on short time scales in living plants. Typically, such tracer transport experiments yield time activity curves (TACs) of tracer activity over time at various locations along a transport pathway. These TACs can exhibit different characteristic shapes that strongly depend on tracer transport dynamics, reflecting properties such as transport velocity, exchange with surrounding tissue, and tracer storage along the pathway. Various methods, either data-driven or model-based, exist to determine transport velocities from TACs. However, for some TAC shapes, the inferred carbon tracer velocity values can be inconsistent and greatly vary between analysis methods. In the present study, we review and evaluate different analysis methods for their suitability to reliably determine tracer transport velocities from typical TAC shapes. For this evaluation, we use both in silico generated and experimentally acquired TACs from positron emission tomography measurements on tomato, barley, and bean. We demonstrate that each of the compared methods can be suitable for specific TAC shapes while being less or not appropriate for others. In conclusion, we present a case-specific evaluation of methods as a reference for analyzing TACs from tracer transport experiments, which allows to ensure a robust and globally comparable determination of transport velocities.
PB  - SSRN
PY  - 2024
ST  - Analyzing Time Activity Curves from Spatio-Temporal Tracer Data to Determine Tracer Transport Velocity in Plants
Y2  - 2025/05/05/21:54:32
DO  - 10.1016/j.mbs.2025.109430
ER  -


TY  - GEN
AU  - Su, Y.
AU  - Zhao, Q.
TI  - Efficient Spatio-Temporal Network for Action Recognition
AB  - The input tensor of video data includes temporal, spatial, and channel dimensions, crucial for extracting complementary spatial, temporal, and spatiotemporal features for video action recognition. To efficiently extract and integrate these features, we propose an Efficient Spatio-Temporal Module (ESTM) with three pathways dedicated to extracting spatial, temporal, and spatio-temporal features. Each pathway uses the Cross Global Average Pooling (CGAP) module to compress the current dimension, focusing features on the remaining two dimensions. This enhances feature extraction and recognition rates for complex actions. We also introduce a Motion Excitation Module (MEM) to enrich input features by transforming correlations between adjacent frames, reducing computational complexity. Finally, ESTM and MEM are seamlessly integrated into a 2D CNN, forming the Efficient Spatio-Temporal Network (ESTN), with minimal impact on network parameters and computational costs. Extensive experiments show that ESTN outperforms state-of-the-art methods on datasets like Something V1 & V2 and HMDB51, validating its effectiveness.
PB  - Research Square
PY  - 2024
ST  - Efficient Spatio-Temporal Network for Action Recognition
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/s11554-024-01541-6
ER  -


TY  - GEN
AU  - Li, Z.
AU  - Chang, X.
AU  - Li, Y.
AU  - Su, J.
TI  - Skeleton-based Group Activity Recognition via Spatial-Temporal Panoramic Graph
AB  - Group Activity Recognition aims to understand collective activities from videos. Existing solutions primarily rely on the RGB modality, which encounters challenges such as background variations, occlusions, motion blurs, and significant computational overhead. Meanwhile, current keypoint-based methods offer a lightweight and informative representation of human motions but necessitate accurate individual annotations and specialized interaction reasoning modules. To address these limitations, we design a panoramic graph that incorporates multi-person skeletons and objects to encapsulate group activity, offering an effective alternative to RGB video. This panoramic graph enables Graph Convolutional Network (GCN) to unify intra-person, inter-person, and person-object interactive modeling through spatial-temporal graph convolutions. In practice, we develop a novel pipeline that extracts skeleton coordinates using pose estimation and tracking algorithms and employ Multi-person Panoramic GCN (MP-GCN) to predict group activities. Extensive experiments on Volleyball and NBA datasets demonstrate that the MP-GCN achieves state-of-the-art performance in both accuracy and efficiency. Notably, our method outperforms RGB-based approaches by using only estimated 2D keypoints as input. Code is available at https://github.com/mgiant/MP-GCN.
PB  - arXiv
PY  - 2024
ST  - Skeleton-based Group Activity Recognition via Spatial-Temporal Panoramic Graph
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-031-73202-7_15
ER  -


TY  - GEN
AU  - Ling, Z.
AU  - Yang, Y.
AU  - Huang, A.
TI  - An Activity-Driven Temporal Multilayer Network Framework to Support Consensus in Group Decision Making
AB  - Social network group decision-making (SNGDM) provides valuable support for describing the opinion exchange in the decision-making process by using the connected social relationships among decision makers (DMs). With the expansion of social media, DMs are interconnected through various types of links. In this cases, interaction of DMs are no longer confined to single-type binary relationships but exhibit complex multiplexing and high-order dynamic characteristics. To this end, this study develops a consensus model based on multilayer network for improving the reliability of decision-making. First, we construct an attributed multilayer network by utilizing multiple social relationships and decision information, in which attributes serve as auxiliary information to establish additional exotic connectivity patterns. Then, the natural interaction of DMs shows a specific high-order correlation, where some activities occurring over the links of a layer depend on the dynamics of certain links on other layers. We propose an interactive joint random walk model to map this co-evolution into an activity-driven network dynamics process. To accurately capture hidden collective structure, state-based non-columnar communities and physical-based overlapping communities are detected. The reinforcement effects generated in these two types of communities can identify influential nodes and communities, guiding decision aggregation to reach higher consensus level. Finally, a numerical example is presented, and simulation experiments and comparative analysis are performed to validate the effectiveness and superiority of proposed model.
PB  - SSRN
PY  - 2024
ST  - An Activity-Driven Temporal Multilayer Network Framework to Support Consensus in Group Decision Making
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4907950
ER  -


TY  - GEN
AU  - Liu, S.
AU  - Sui, L.
AU  - Zhang, C.-L.
AU  - Zhao, C.
AU  - Ghanem, B.
TI  - Harnessing Temporal Causality for Advanced Temporal Action Detection
AB  - As a fundamental task in long-form video understanding, temporal action detection (TAD) aims to capture inherent temporal relations in untrimmed videos and identify candidate actions with precise boundaries. Over the years, various networks, including convolutions, graphs, and transformers, have been explored for effective temporal modeling for TAD. However, these modules typically treat past and future information equally, overlooking the crucial fact that changes in action boundaries are essentially causal events. Inspired by this insight, we propose leveraging the temporal causality of actions to enhance TAD representation by restricting the model’s access to only past or future context. We introduce CausalTAD, which combines causal attention and causal Mamba to achieve state-of-the-art performance on multiple benchmarks. Notably, with CausalTAD, we ranked 1st in the Action Recognition, Action Detection, and Audio-Based Interaction Detection tracks at the EPIC-Kitchens Challenge 2024, as well as 1st in the Moment Queries track at the Ego4D Challenge 2024. Our code is available at https://github.com/sming256/OpenTAD.
PB  - arXiv
PY  - 2024
ST  - Harnessing Temporal Causality for Advanced Temporal Action Detection
Y2  - 2025/05/05/21:54:32
DO  - 10.59350/waswj-nma51
ER  -


TY  - GEN
AU  - Huang, W.
AU  - Zhang, J.
AU  - Qian, X.
AU  - Wang, M.
AU  - Zhang, L.
TI  - SOAP: Enhancing Spatio-Temporal Relation and Motion Information Capturing for Few-Shot Action Recognition
AB  - High frame-rate (HFR) videos of action recognition improve fine-grained expression while reducing the spatio-temporal relation and motion information density. Thus, large amounts of video samples are continuously required for traditional data-driven training. However, samples are not always sufficient in real-world scenarios, promoting few-shot action recognition (FSAR) research. We observe that most recent FSAR works build spatio-temporal relation of video samples via temporal alignment after spatial feature extraction, cutting apart spatial and temporal features within samples. They also capture motion information via narrow perspectives between adjacent frames without considering density, leading to insufficient motion information capturing. Therefore, we propose a novel plug-and-play architecture for FSAR called Spatio-tempOral frAme tuPle enhancer (SOAP) in this paper. The model we designed with such architecture refers to SOAP-Net. Temporal connections between different feature channels and spatio-temporal relation of features are considered instead of simple feature extraction. Comprehensive motion information is also captured, using frame tuples with multiple frames containing more motion information than adjacent frames. Combining frame tuples of diverse frame counts further provides a broader perspective. SOAP-Net achieves new state-of-the-art performance across well-known benchmarks such as SthSthV2, Kinetics, UCF101, and HMDB51. Extensive empirical evaluations underscore the competitiveness, pluggability, generalization, and robustness of SOAP. The code is released at https://github.com/wenbohuang1002/SOAP.
PB  - arXiv
PY  - 2024
ST  - SOAP
Y2  - 2025/05/05/21:54:32
DO  - 10.1145/3664647.3681062
ER  -


TY  - GEN
AU  - Kumazawa, T.
AU  - Ogata, Y.
TI  - Spatial and temporal variations of the 3-year earthquake swarm activities leading up to the M7.6 Noto Peninsula earthquake and interpretations of their activities
AB  - The seismic swarm activity that began and expanded in four separate regions in the northern Noto Peninsula at the end of 2020 was followed by the M6.5 earthquake in May 2023, which cascaded into the M7.6 earthquake on New Year's Day 2024. To this series of earthquake events, we estimate temporal changes in the background intensity of the nonstationary ETAS model by inversion for each region. We then interpret how this series of earthquakes was driven by subsurface fluid motion and slow slip at each stage, based on the correspondence between the background seismicity changes and the temporal changes in oblique distance and elevation differences between nearby GNSS stations, as well as the spatiotemporal earthquake patterns.
PB  - Research Square
PY  - 2024
ST  - Spatial and temporal variations of the 3-year earthquake swarm activities leading up to the M7.6 Noto Peninsula earthquake and interpretations of their activities
Y2  - 2025/05/05/21:54:32
DO  - 10.1186/s40623-024-02112-6
ER  -


TY  - GEN
AU  - Parikh, A.
AU  - Sadeghi, M.
AU  - Eskofier, B.
TI  - Exploring Facial Biomarkers for Depression through Temporal Analysis of Action Units
AB  - Depression is characterized by persistent sadness and loss of interest, significantly impairing daily functioning and now a widespread mental disorder. Traditional diagnostic methods rely on subjective assessments, necessitating objective approaches for accurate diagnosis. Our study investigates the use of facial action units (AUs) and emotions as biomarkers for depression. We analyzed facial expressions from video data of participants classified with or without depression. Our methodology involved detailed feature extraction, mean intensity comparisons of key AUs, and the application of time series classification models. Furthermore, we employed Principal Component Analysis (PCA) and various clustering algorithms to explore the variability in emotional expression patterns. Results indicate significant differences in the intensities of AUs associated with sadness and happiness between the groups, highlighting the potential of facial analysis in depression assessment.
PB  - arXiv
PY  - 2024
ST  - Exploring Facial Biomarkers for Depression through Temporal Analysis of Action Units
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/icenco.2010.5720440
ER  -


TY  - GEN
AU  - Lee, S.
AU  - Jung, J.
AU  - Oh, C.
AU  - Yun, S.
TI  - Enhancing Temporal Action Localization: Advanced S6 Modeling with Recurrent Mechanism
AB  - Temporal Action Localization (TAL) is a critical task in video analysis, identifying precise start and end times of actions. Existing methods like CNNs, RNNs, GCNs, and Transformers have limitations in capturing long-range dependencies and temporal causality. To address these challenges, we propose a novel TAL architecture leveraging the Selective State Space Model (S6). Our approach integrates the Feature Aggregated Bi-S6 block, Dual Bi-S6 structure, and a recurrent mechanism to enhance temporal and channel-wise dependency modeling without increasing parameter complexity. Extensive experiments on benchmark datasets demonstrate state-of-the-art results with mAP scores of 74.2% on THUMOS-14, 42.9% on ActivityNet, 29.6% on FineAction, and 45.8% on HACS. Ablation studies validate our method’s effectiveness, showing that the Dual structure in the Stem module and the recurrent mechanism outperform traditional approaches. Our findings demonstrate the potential of S6-based models in TAL tasks, paving the way for future research. Our code is available at https://github.com/lsy0882/RDFA-S6.
PB  - arXiv
PY  - 2024
ST  - Enhancing Temporal Action Localization
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/s00371-022-02495-1
ER  -


TY  - GEN
AU  - Zhang, S.
AU  - Yin, J.
AU  - Dang, Y.
TI  - A Generically Contrastive Spatiotemporal Representation Enhancement for 3d Skeleton Action Recognition
AB  - Skeleton-based action recognition is a central task in computer vision and human-robot interaction. However, most previous methods suffer from overlooking the explicit exploitation of the latent data distributions (i.e., the intra-class variations and inter-class relations), thereby leading to confusion about ambiguous samples and sub-optimum solutions of the skeleton encoders. To mitigate this, we propose a {Contrastive Spatiotemporal Representation Enhancement (CSRE) framework to obtain more discriminative representations from the sequences, which can be incorporated into various previous skeleton encoders and can be removed when testing. Specifically, we decompose the representation into spatial- and temporal-specific features to explore fine-grained motion patterns along corresponding dimensions. Furthermore, to explicitly exploit the latent data distributions, we employ the attentive features to contrastive learning, which models the cross-sequence semantic relations by pulling together the features from the positive pairs and pushing away the negative pairs. Extensive experiments show that CSRE with four various skeleton encoders (HCN, 2S-AGCN, CTR-GCN, and Hyperformer) achieves solid improvements on three benchmarks. The code will be released at https://github.com/Eezekiel/CSRE.
PB  - SSRN
PY  - 2024
ST  - A Generically Contrastive Spatiotemporal Representation Enhancement for 3d Skeleton Action Recognition
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4898102
ER  -


TY  - GEN
AU  - Jo, H.
AU  - Gwon, H.
AU  - Jo, S.
AU  - Jung, C.
TI  - An Empirical Study on Factors of Influence for Single-Frame Supervised Temporal Action Detection
AB  - Owing to the substantial time and labor demands associated with video annotation for fully-supervised temporal action detection (TAD), extensive research has been devoted to the domain of weakly-supervised TAD. However, existing weakly-supervised TAD approaches still suffer from severe localization errors due to the absence of fine-grained frame-level annotations. To tackle this issue, single-frame supervised TAD has been recently proposed as a potential method. This paper does not introduce a new approach. Instead, the aim of this paper is to conduct an empirical study on factors of influence for single-frame supervised TAD, which have not yet been studied and thus are still unclear. We go back to basics and investigate the effects of several fundamental components on the performance of single-frame supervised TAD: 1) feature extraction, 2) feature modeling, 3) temporal embedding, 4) classification head, and 5) video-level classification loss. In this investigation, we explore the potentials of traditional technical solutions in the task of single-frame supervised TAD and unveil the benefits of such solutions, which have not yet been reported to the research community. Based on the findings, we build a baseline detector, which achieves the state-of-the-art performance. It should be noted that, to make up for the limit of mAP (mean average precision), not only mAP but also VCCR (video-level classification correctness rate) is employed in the performance evaluation. Make a note of the fact that the VCCR is a supplementary metric supporting the mAP. We hope that our work can facilitate future research in this field.
PB  - Research Square
PY  - 2024
ST  - An Empirical Study on Factors of Influence for Single-Frame Supervised Temporal Action Detection
Y2  - 2025/05/05/21:54:32
DO  - 10.21203/rs.3.rs-4620944/v1
ER  -


TY  - GEN
AU  - Luo, Y.
AU  - Yi, J.
AU  - Farha, Y.A.
AU  - Wolter, M.
AU  - Gall, J.
TI  - RETHINKING TEMPORAL SELF-SIMILARITY FOR REPETITIVE ACTION COUNTING
AB  - Counting repetitive actions in long untrimmed videos is a challenging task that has many applications such as rehabilitation. State-of-the-art methods predict action counts by first generating a temporal self-similarity matrix (TSM) from the sampled frames and then feeding the matrix to a predictor network. The self-similarity matrix, however, is not an optimal input to a network since it discards too much information from the frame-wise embeddings. We thus rethink how a TSM can be utilized for counting repetitive actions and propose a framework that learns embeddings and predicts action start probabilities at full temporal resolution. The number of repeated actions is then inferred from the action start probabilities. In contrast to current approaches that have the TSM as an intermediate representation, we propose a novel loss based on a generated reference TSM, which enforces that the self-similarity of the learned frame-wise embeddings is consistent with the self-similarity of repeated actions. The proposed framework achieves state-of-the-art results on three datasets, i.e., RepCount, UCFRep, and Countix.
PB  - arXiv
PY  - 2024
ST  - RETHINKING TEMPORAL SELF-SIMILARITY FOR REPETITIVE ACTION COUNTING
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/icip51287.2024.10647309
ER  -


TY  - GEN
AU  - Feng, Q.
AU  - Li, W.
AU  - Lin, T.
AU  - Chen, X.
TI  - Full-Stage Pseudo Label Quality Enhancement for Weakly-supervised Temporal Action Localization
AB  - Weakly-supervised Temporal Action Localization (WSTAL) aims to localize actions in untrimmed videos using only video-level supervision. Latest methods introduce pseudo label learning framework to bridge the gap between classification-based training and inferencing targets at localization, and achieve cutting-edge results. In these frameworks, a classification-based model is used to generate pseudo labels for a regression-based student model to learn from. However, the quality of pseudo labels, which is a key factor to the final result, is not carefully studied. In this paper, we propose a set of simple yet efficient pseudo label quality enhancement mechanisms to build our FuSTAL framework. FuSTAL enhances pseudo label quality at three stages: cross-video contrastive learning at proposal Generation-Stage, prior-based filtering at proposal Selection-Stage and EMA-based distillation at Training-Stage. These designs enhance pseudo label quality at different stages in the framework, and help produce more informative, less false and smoother action proposals. With the help of these comprehensive designs at all stages, FuSTAL achieves an average mAP of 50.8% on THUMOS’14, outperforming the previous best method by 1.2%, and becomes the first method to reach the milestone of 50%. Code is available at https://github.com/fqhank/FuSTAL.
PB  - arXiv
PY  - 2024
ST  - Full-Stage Pseudo Label Quality Enhancement for Weakly-supervised Temporal Action Localization
Y2  - 2025/05/05/21:54:32
DO  - 10.1016/j.jvcir.2022.103590
ER  -


TY  - GEN
AU  - Lambert, T.
AU  - Niknejad, H.R.
AU  - Kil, D.
AU  - Montaldo, G.
AU  - Urban, A.
TI  - Functional ultrasound imaging and neuronal activity: how accurate is the spatiotemporal match?
AB  - Over the last decade, functional ultrasound (fUS) has risen as a critical tool in functional neuroimaging, leveraging hemodynamic changes to infer neural activity indirectly. Recent studies have established a strong correlation between neural spike rates (SR) and functional ultrasound signals. However, understanding their spatial distribution and variability across different brain areas is required to thoroughly interpret fUS signals. In this regard, we conducted simultaneous fUS imaging and Neuropixels recordings during stimulus-evoked activity in awake mice within three regions the visual pathway. Our findings indicate that the temporal dynamics of fUS and SR signals are linearly correlated, though the correlation coefficients vary among visual regions. Conversely, the spatial correlation between the two signals remains consistent across all regions with a spread of approximately 300 micrometers. Finally, we introduce a model that integrates the spatial and temporal components of the fUS signal, allowing for a more accurate interpretation of fUS images.
PB  - bioRxiv
PY  - 2024
ST  - Functional ultrasound imaging and neuronal activity
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2024.07.10.602912
ER  -


TY  - GEN
AU  - Zhou, F.
AU  - Williams, B.
AU  - Rahmani, H.
TI  - Towards Adaptive Pseudo-label Learning for Semi-Supervised Temporal Action Localization
AB  - Alleviating noisy pseudo labels remains a key challenge in Semi-Supervised Temporal Action Localization (SS-TAL). Existing methods often filter pseudo labels based on strict conditions, but they typically assess classification and localization quality separately, leading to suboptimal pseudo-label ranking and selection. In particular, there might be inaccurate pseudo labels within selected positives, alongside reliable counterparts erroneously assigned to negatives. To tackle these problems, we propose a novel Adaptive Pseudo-label Learning (APL) framework to facilitate better pseudo-label selection. Specifically, to improve the ranking quality, Adaptive Label Quality Assessment (ALQA) is proposed to jointly learn classification confidence and localization reliability, followed by dynamically selecting pseudo labels based on the joint score. Additionally, we propose an Instance-level Consistency Discriminator (ICD) for eliminating ambiguous positives and mining potential positives simultaneously based on inter-instance intrinsic consistency, thereby leading to a more precise selection. We further introduce a general unsupervised Action-aware Contrastive Pre-training (ACP) to enhance the discrimination both within actions and between actions and backgrounds, which benefits SS-TAL. Extensive experiments on THUMOS14 and ActivityNet v1.3 demonstrate that our method achieves state-of-the-art performance under various semi-supervised settings.
PB  - arXiv
PY  - 2024
ST  - Towards Adaptive Pseudo-label Learning for Semi-Supervised Temporal Action Localization
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-031-73033-7_18
ER  -


TY  - GEN
AU  - Hyun, J.
AU  - Han, S.H.
AU  - Kang, H.
AU  - Lee, J.-Y.
AU  - Kim, S.J.
TI  - Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action Localization
AB  - The vocabulary size in temporal action localization (TAL) is limited by the scarcity of large-scale annotated datasets. To overcome this, recent works integrate vision-language models (VLMs), such as CLIP, for open-vocabulary TAL (OV-TAL). However, despite the success of VLMs trained on extensive datasets, existing OV-TAL methods still rely on human-labeled TAL datasets of limited size to train action localizers, limiting their generalizability. In this paper, we explore the scalability of self-training with unlabeled YouTube videos for OV-TAL. Our approach consists of two stages: (1) a class-agnostic action localizer is trained on a human-labeled TAL dataset to generate pseudo-labels for unlabeled videos, and (2) the large-scale pseudo-labeled dataset is then used to train the localizer. Extensive experiments demonstrate that leveraging web-scale videos in self-training significantly enhances the generalizability of an action localizer. Additionally, we identify limitations in existing OV-TAL evaluation schemes and propose a new benchmark for thorough assessment. Finally, we showcase the TAL performance of the large multimodal model Gemini-1.5 on our new benchmark. Code is released at https://github.com/HYUNJS/STOV-TAL.
PB  - arXiv
PY  - 2024
ST  - Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action Localization
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/wacv61041.2025.00911
ER  -


TY  - GEN
AU  - Liu, T.
AU  - Huang, J.
AU  - Weng, C.
TI  - Spatio-Temporal Encoding and Decoding-Based Method for Future Human Activity Skeleton Synthesis
AB  - Inferring future activity information based on observed activity data is a crucial step to improve the accuracy of early activity prediction. Traditional methods based on generative adversarial networks(GAN) or joint learning frameworks can achieve good prediction accuracy under low observation ratios, but they usually have high computational costs. In view of this, this paper proposes a spatio-temporal encoding and decoding-based method for future human activity skeleton synthesis. Firstly, algorithms such as time control, discrete cosine transform, and low-pass filtering are used to cut or pad the skeleton sequences. Secondly, the encoder and decoder are responsible for extracting intermediate semantic encoding from observed skeleton sequences and inferring future sequences from the intermediate semantic encoding, respectively. Finally, joint displacement error, velocity error, and acceleration error, three higher-order kinematic features, are used as key components of the loss function to optimize model parameters. Experimental results show that the proposed future skeleton synthesis algorithm performs better than some existing algorithms. It generates skeleton sequences with smaller errors and fewer model parameters, effectively providing future information for early activity prediction.
PB  - arXiv
PY  - 2024
ST  - Spatio-Temporal Encoding and Decoding-Based Method for Future Human Activity Skeleton Synthesis
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4954432
ER  -


TY  - GEN
AU  - Yang, L.
AU  - Zheng, Z.
AU  - Han, Y.
AU  - Huang, G.
AU  - Li, F.
TI  - DyFADet: Dynamic Feature Aggregation for Temporal Action Detection
AB  - Recent proposed neural network-based Temporal Action Detection (TAD) models are inherently limited to extracting the discriminative representations and modeling action instances with various lengths from complex scenes by shared-weights detection heads. Inspired by the successes in dynamic neural networks, in this paper, we build a novel dynamic feature aggregation (DFA) module that can simultaneously adapt kernel weights and receptive fields at different timestamps. Based on DFA, the proposed dynamic encoder layer aggregates the temporal features within the action time ranges and guarantees the discriminability of the extracted representations. Moreover, using DFA helps to develop a Dynamic TAD head (DyHead), which adaptively aggregates the multiscale features with adjusted parameters and learned receptive fields better to detect the action instances with diverse ranges from videos. With the proposed encoder layer and DyHead, a new dynamic TAD model, Dy- FADet, achieves promising performance on a series of challenging TAD benchmarks, including HACS-Segment, THUMOS14, ActivityNet-1.3, Epic-Kitchen 100, Ego4D-Moment QueriesV1.0, and FineAction. Code is released to https://github.com/yangle15/DyFADet-pytorch.
PB  - arXiv
PY  - 2024
ST  - DyFADet
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-031-72952-2_18
ER  -


TY  - GEN
AU  - Natale, S.
AU  - Bertini, A.
AU  - Benini, E.
AU  - Bonaldo, A.
AU  - Parma, L.
TI  - First Insight into Temporal Variation of Digestive Enzyme Activities in Flathead Grey Mullet (Mugil Cephalus) During the Ongrowing Phase
AB  - Digestive enzymes break down the complex nutrients of food into easily absorbable molecules. Understanding their temporal variation in relation to the transit of food through the gastrointestinal tract is important for developing effective nutritional strategies. The present study provides the first insight on the digestive capacity of captive flathead grey mullet (Mugil cephalus), which is considered a promising omnivorous/detritivorous species for the sustainable diversification of aquaculture. Fish of 67.90 ± 15.46 g initial weight were reared in captivity and fed a low fish meal-based diet for 113 days. At the end of the trial, the activity of pancreatic and intestinal enzymes were analysed at three different times: 0, 6 and 12 hours post-prandial. Additionally, the gene expression of pept1 was determined in the intestine at all considered times to provide a reference to peptide absorption. Results of total alkaline proteases, trypsin and chymotrypsin showed a significant increase at 6 hours post-prandial, with values markedly lower than those of α-amylase. Results of bile salt-activated lipase detected a significant higher trend at 12 hours post-prandial. Results of leucine-alanine aminopeptidase showed a significant higher trend at 12 hours post-prandial. Aminopeptidase presented higher activity at 6 hours post-prandial in the anterior intestine. Alkaline phosphatase showed no significant differences. The pept1 gene showed significant higher expression at 12 hours post-prandial in the posterior intestine. This study provides foundational data on the digestive physiology of juvenile flathead grey mullet reared in captive conditions. Specifically, it offers insights into the timing of food transit, considering digestion and absorption times.
PB  - SSRN
PY  - 2024
ST  - First Insight into Temporal Variation of Digestive Enzyme Activities in Flathead Grey Mullet (Mugil Cephalus) During the Ongrowing Phase
Y2  - 2025/05/05/21:54:32
DO  - 10.1016/j.aqrep.2025.102652
ER  -


TY  - GEN
AU  - Kettlewell, L.
AU  - Sederberg, A.
AU  - Smith, G.B.
TI  - Stereotyped spatiotemporal dynamics of spontaneous activity in visual cortex prior to eye-opening
AB  - Over the course of development, functional sensory representations emerge in the visual cortex. Prior to eye-opening, modular patterns of spontaneous activity form long-range networks that may serve as a precursor for mature network organization. Although the spatial structure of these networks has been well studied, their temporal features, which may contribute to their continued plasticity and development, remain largely uncharacterized. To address this, we imaged hours of spontaneous network activity in the visual cortex of developing ferrets of both sexes utilizing a fast calcium indicator (GCaMP8m) and widefield imaging at high temporal resolution (50Hz), then segmented out spatiotemporal events. The spatial structure of this activity was highly modular, exhibiting spatially segregated active domains consistent with prior work. We found that the vast majority of events showed a clear dynamic component in which modules activated sequentially across the field of view, but only a minority of events were well-fit with a linear traveling wave. We found that spatiotemporal events occur in repeated and stereotyped motifs, reoccurring across hours of imaging. Finally, we found that the most frequently occurring single-frame spatial activity patterns were predictive of future activity patterns over hundreds of milliseconds. Together, our results demonstrate that spontaneous activity in the early developing cortex exhibits a rich spatiotemporal structure, suggesting a potential role in the maturation and refinement of future functional representations.
PB  - bioRxiv
PY  - 2024
ST  - Stereotyped spatiotemporal dynamics of spontaneous activity in visual cortex prior to eye-opening
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2024.06.25.600611
ER  -


TY  - GEN
AU  - Geenjaar, E.
AU  - Kim, D.
AU  - Calhoun, V.
TI  - Providing context: Extracting non-linear and dynamic temporal motifs from brain activity
AB  - Approaches studying the dynamics of resting-state functional magnetic resonance imaging (rs-fMRI) activity often focus on time-resolved functional connectivity (tr-FC). While many approaches have been proposed, these typically focus on linear approaches like computing the linear correlation at a timestep or within a window. In this work, we propose to use a generative non-linear deep learning model, a disentangled variational autoencoder (DSVAE), that factorizes out window-specific (context) information from timestep-specific (local) information. This has the advantage of allowing our model to capture differences at multiple temporal scales. For the timestep-specific scale, which has higher temporal precision, we find significant differences between schizophrenia patients and control subjects in their temporal step distance through our model’s latent space. We also find that window-specific embeddings, or as we refer to them, context embeddings, more accurately separate windows from schizophrenia patients and control subjects than the standard tr-FC approach. Moreover, we find that for individuals with schizophrenia, our model’s context embedding space is significantly correlated with both age and symptom severity. Interestingly, patients appear to spend more time in three clusters, one closer to controls which shows increased visual-sensorimotor, cerebellar-subcortical, and reduced cerebellar-sensorimotor functional network connectivity (FNC), an intermediate station showing increased subcortical-sensorimotor FNC, and one that shows decreased visual-sensorimotor, decreased subcortical-sensorimotor, and increased visual-subcortical domains. We verify that our model captures features that are complementary to - but not the same as - standard tr-FC features. Our model can thus help broaden the neuroimaging toolset in analyzing fMRI dynamics and shows potential as an approach for finding psychiatric links that are more sensitive to individual and group characteristics.
PB  - bioRxiv
PY  - 2024
ST  - Providing context
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2024.06.27.600937
ER  -


TY  - GEN
AU  - Zapf, J.
AU  - Roveri, M.
AU  - Martín, F.
AU  - Manzanares, J.C.
TI  - Constructing Behavior Trees from Temporal Plans for Robotic Applications
AB  - Executing temporal plans in the real and open world requires adapting to uncertainty both in the environment and in the plan actions. A plan executor must therefore be flexible to dispatch actions based on the actual execution conditions. In general, this involves considering both event and time-based constraints between the actions in the plan. A simple temporal network (STN) is a convenient framework for specifying the constraints between actions in the plan. Likewise, a behavior tree (BT) is a convenient framework for controlling the execution flow of the actions in the plan. The principle contributions of this paper are i) an algorithm for transforming a plan into an STN, and ii) an algorithm for transforming an STN into a BT. When combined, these algorithms define a systematic approach for executing total-order (time-triggered) plans in robots operating in the real world. Our approach is based on creating a graph describing a deordered (state-triggered) plan and then creating a BT representing a partial-order (determined at runtime) plan. This approach ensures the correct execution of plans, including those with required concurrency. We demonstrate the validity of our approach within the PlanSys2 framework on real robots.
PB  - arXiv
PY  - 2024
ST  - Constructing Behavior Trees from Temporal Plans for Robotic Applications
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/case56687.2023.10260363
ER  -


TY  - GEN
AU  - Hu, W.
AU  - Li, X.
AU  - Onditi, K.O.
AU  - Gao, G.
AU  - Jiang, X.
TI  - Inconsistent Effects of Human Activities on Wildlife Distributions at Different Spatiotemporal Scales: Implications for Conservation
AB  - Understanding wildlife distribution patterns in relation to human disturbance across varying spatial and temporal dimensions is crucial for advancing effective conservation strategies. Here, we employed multi-species occupancy models on an extensive dataset derived from a systematic camera trapping survey across the Gaoligong Mountain to examine the effects of human modification (settlements and farmland) and human presence (camera-detections of humans and livestock) on the diversity and distribution of large and medium-sized mammal species. We divided the mountain into northern, middle and southern sections, 12 altitude zones, and sampling years into wet and dry seasons. We then used spatial and temporal β-diversity indices to quantify changes in species assemblage. From 375 stations over 113,204 camera days, we recorded 34 large and medium-sized mammals. In the middle sections, community mean occupancy showed a strong positive response to human modification, but a significant and negative correlation with human presence in the northern section. The spatial β-diversity analysis showed that species turnover was the primary driver of diversity changes across all sections. The temporal analysis showed a predominance of species loss during dry seasons across all altitudes except the 1700-2300 m range, compared to wet seasons. Our study revealed that the effects of human activities on wildlife species diversity and distribution are scale-dependent, and stronger at local scales, and that low-altitude habitats play a crucial role in wildlife conservation. Conservation planning focuses on both the spatial dimensions of habitats and the temporal shifts in species composition, especially in areas with significant seasonal environmental variation.
PB  - SSRN
PY  - 2024
ST  - Inconsistent Effects of Human Activities on Wildlife Distributions at Different Spatiotemporal Scales
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4876387
ER  -


TY  - GEN
AU  - Patrascu, F.I.
AU  - Mostafavi, A.
TI  - Population Activity Recovery: Milestones Unfolding, Temporal Interdependencies, and Relationship with Physical and Social Vulnerability
AB  - Understanding sequential community recovery milestones is crucial for proactive recovery planning and monitoring. This study investigates these milestones related to population activities to examine their temporal interdependencies and evaluate the relationship between recovery milestones and physical (residential property damage) and social vulnerability (household income). This study leverages post-2017 Hurricane Harvey mobility data from Harris County to specify and analyze temporal recovery milestones and their interdependencies. The analysis examined four key milestones: return to evacuated areas, recovery of essential and nonessential services, and the rate of home-switch (moving out of residences). Robust linear regression validates interdependencies between across milestone lags and sequences: achieving earlier milestones accelerates subsequent recovery milestones. The study thus identifies six primary recovery milestone sequences. We found that social vulnerability accounted through the median household income level, rather than physical vulnerability to flooding accounted through the property damage extent, correlates with recovery delays between milestones. We studied variations in recovery sequences across lower and upper quantiles of property damage extent and median household income: lower property damage extent and lower household income show greater representation in the “slowest to recover” sequence, while households with greater damage and higher income are predominant in the group with the “fastest recovery sequences”. Milestone sequence variability aligns closely with social vulnerability, independent of physical vulnerability. Understanding the variation in recovery sequences, milestone interdependencies, and social vulnerability disparities provides crucial evidence for targeted interventions. This empowers emergency managers to effectively monitor and manage recovery efforts, enabling timely interventions.
PB  - SSRN
PY  - 2024
ST  - Population Activity Recovery
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4875975
ER  -


TY  - GEN
AU  - Wang, P.
AU  - Lu, S.
AU  - Dai, C.
TI  - Temporal Action Localization with State-Sensitive Mamba and Centroid Sequences Enhancement
AB  - The temporal action localization task aims to identify and localize human behaviors in unedited videos. However, most previous studies have employed sampling processing and local information aggregation to reduce the computational burden, which results in significant information loss and a weak understanding of contextual information. In this paper, we improve the localization accuracy by extending the sequence length and global information aggregation, which enhances the model's ability to capture long-range dependencies and contextual information. To this end, firstly, we propose a novel State-Sensitive Mamba module to replace self-attention mechanism. Based on its linear computational cost and high sensitivity to state boundaries, we extend the sequence length to five times that of the previous model, which helps to preserve the information integrity and leads to a more comprehensive information capture. Secondly, in order to overcome the noise and boundary ambiguity introduced by the extended sequences, we propose a centroid sequence enhancement strategy to optimize the boundary localization using central features that are closer to the motion semantics. Finally, extensive experimental results on the THUMOS'14, ActivityNet1.3 and FineAction datasets demonstrate the superior performance of the proposed method.
PB  - SSRN
PY  - 2024
ST  - Temporal Action Localization with State-Sensitive Mamba and Centroid Sequences Enhancement
Y2  - 2025/05/05/21:54:32
DO  - 10.1016/j.neucom.2024.129246
ER  -


TY  - GEN
AU  - Hervault, M.
AU  - Wessel, J.R.
TI  - Common and unique neurophysiological signatures for the stopping and revising of actions reveal the temporal dynamics of inhibitory control
AB  - Inhibitory control is a crucial cognitive-control ability for behavioral flexibility that has been extensively investigated through action-stopping tasks. Multiple neurophysiological features have been proposed to represent ‘signatures’ of inhibitory control during action-stopping, though the processes signified by these signatures are still controversially discussed. The present study aimed to disentangle these processes by comparing simple stopping situations with those in which additional action revisions were needed. Three experiments in female and male humans were performed to characterize the neurophysiological dynamics involved in action-stopping and - changing, with hypotheses derived from recently developed two-stage ‘pause-then-cancel’ models of inhibitory control. Both stopping and revising an action triggered an early broad ‘pause’-process, marked by frontal EEG β-bursts and non-selective suppression of corticospinal excitability. However, partial-EMG responses showed that motor activity was only partially inhibited by this ‘pause’, and that this activity can be further modulated during action-revision. In line with two-stage models of inhibitory control, subsequent frontocentral EEG activity after this initial ‘pause’ selectively scaled depending on the required action revisions, with more activity observed for more complex revisions. This demonstrates the presence of a selective, effector-specific ‘retune’ phase as the second process involved in action-stopping and -revision. Together, these findings show that inhibitory control is implemented over an extended period of time and in at least two phases. We are further able to align the most commonly proposed neurophysiological signatures to these phases and show that they are differentially modulated by the complexity of action-revision.
PB  - bioRxiv
PY  - 2024
ST  - Common and unique neurophysiological signatures for the stopping and revising of actions reveal the temporal dynamics of inhibitory control
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2024.06.18.597172
ER  -


TY  - GEN
AU  - Gupta, A.
AU  - Arora, A.
AU  - Narayan, S.
AU  - Khan, F.S.
AU  - Taylor, G.W.
TI  - Open-Vocabulary Temporal Action Localization using Multimodal Guidance
AB  - Open-Vocabulary Temporal Action Localization (OVTAL) enables a model to recognize any desired action category in videos without the need to explicitly curate training data for all categories. However, this flexibility poses significant challenges, as the model must recognize not only the action categories seen during training but also novel categories specified at inference. Unlike standard temporal action localization, where training and test categories are predetermined, OVTAL requires understanding contextual cues that reveal the semantics of novel categories. To address these challenges, we introduce OVFormer, a novel open-vocabulary framework extending ActionFormer with three key contributions. First, we employ task-specific prompts as input to a large language model to obtain rich class-specific descriptions for action categories. Second, we introduce a cross-attention mechanism to learn the alignment between class representations and frame-level video features, facilitating the multimodal guided features. Third, we propose a two-stage training strategy which includes training with a larger vocabulary dataset and finetuning to downstream data to generalize to novel categories. OVFormer extends existing TAL methods to open-vocabulary settings. Comprehensive evaluations on the THUMOS14 and ActivityNet-1.3 benchmarks demonstrate the effectiveness of our method. Code and pretrained models will be publicly released.
PB  - arXiv
PY  - 2024
ST  - Open-Vocabulary Temporal Action Localization using Multimodal Guidance
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/tpami.2024.3395778
ER  -


TY  - GEN
AU  - Patrascu, F.I.
AU  - Mostafavi, A.
TI  - Population Activity Recovery: Milestones Unfolding, Temporal Interdependencies, and Relationship with Physical and Social Vulnerability
AB  - Understanding sequential community recovery milestones is crucial for proactive recovery planning and monitoring. This study investigates these milestones related to population activities to examine their temporal interdependencies and evaluate the relationship between recovery milestones and physical (residential property damage) and social vulnerability (household income). This study leverages post-2017 Hurricane Harvey mobility data from Harris County to specify and analyze temporal recovery milestones and their interdependencies. The analysis examined four key milestones: return to evacuated areas, recovery of essential and nonessential services, and the rate of home-switch (moving out of residences). Robust linear regression validates interdependencies between across milestone lags and sequences: achieving earlier milestones accelerates subsequent recovery milestones. The study thus identifies six primary recovery milestone sequences. We found that social vulnerability accounted through the median household income level, rather than physical vulnerability to flooding accounted through the property damage extent, correlates with recovery delays between milestones. We studied variations in recovery sequences across lower and upper quantiles of property damage extent and median household income: lower property damage extent and lower household income show greater representation in the “slowest to recover” sequence, while households with greater damage and higher income are predominant in the group with the “fastest recovery sequences”. Milestone sequence variability aligns closely with social vulnerability, independent of physical vulnerability. Understanding the variation in recovery sequences, milestone interdependencies, and social vulnerability disparities provides crucial evidence for targeted interventions. This empowers emergency managers to effectively monitor and manage recovery efforts, enabling timely interventions.
PB  - arXiv
PY  - 2024
ST  - Population Activity Recovery
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4875975
ER  -


TY  - GEN
AU  - Nash, W.
AU  - Fahmi, R.
AU  - Ramos, V.
AU  - Crane, R.
TI  - Assessment of Ecotoxic Metal Leaching Behaviour of Coal Gangue using a Sequential Extraction Procedure
AB  - The environmental leaching characteristics of the ecotoxic metals Cr, Co, Ni, Cu, Zn, As, Se, Ag, Cd, Tl and Pb within six coal gangue samples from mines located in Poland are assessed herein using the BCR-2 sequential extraction procedure. The samples’ bulk mineralogical and chemical compositions are determined by petrographic examination, X-ray fluorescence spectroscopy, X-ray diffraction and inductively-coupled mass spectrometry (following acid digestion). In general, of the metals studied Cd (predominantly oxidizable but often exchangeable) and Pb (predominantly reducible) pose the greatest hazard to the environment. Ni (predominantly oxidizable) and Cr (predominantly reducible) are also highly abundant and potentially mobile. Zn and Cu are abundant and predominantly exchangeable in half of the samples examined. Leaching of all metals is likely to be accelerated by the samples’ acid-forming propensity, which arises from their considerable (though variable) inventories of framboidal pyrite. This is especially the case for the wastes from Bogdanka and Piast mines, since they possess an order of magnitude more sulfidic material than the other samples (up to 1.9 wt%). Management of repositories containing these wastes will require specific measures to prevent such acid generation and ecotoxic metal release into the environment. Specifically, encapsulation within acid-neutralizing (e.g. calcite-bearing) materials is essential for these wastes, but is also recommended for those from mines Jankowice, Marcel, Staszik and Chwalowice.
PB  - Research Square
PY  - 2024
ST  - Assessment of Ecotoxic Metal Leaching Behaviour of Coal Gangue using a Sequential Extraction Procedure
Y2  - 2025/05/05/21:54:32
DO  - 10.21203/rs.3.rs-4518250/v1
ER  -


TY  - GEN
AU  - Liu, C.
AU  - Lin, J.
AU  - Liu, H.
AU  - Wang, J.
AU  - Caverlee, J.
TI  - Behavior-Dependent Linear Recurrent Units for Efficient Sequential Recommendation
AB  - Sequential recommender systems aims to predict the users' next interaction through user behavior modeling with various operators like RNNs and attentions. However, existing models generally fail to achieve the three golden principles for sequential recommendation simultaneously, i.e., training efficiency, low-cost inference, and strong performance. To this end, we propose RecBLR, an Efficient Sequential Recommendation Model based on Behavior-Dependent Linear Recurrent Units to accomplish the impossible triangle of the three principles. By incorporating gating mechanisms and behaviordependent designs into linear recurrent units, our model significantly enhances user behavior modeling and recommendation performance. Furthermore, we unlock the parallelizable training as well as inference efficiency for our model by designing a hardwareaware scanning acceleration algorithm with a customized CUDA kernel. Extensive experiments on real-world datasets with varying lengths of user behavior sequences demonstrate RecBLR's remarkable effectiveness in simultaneously achieving all three golden principles - strong recommendation performance, training efficiency, and low-cost inference, while exhibiting excellent scalability to datasets with long user interaction histories.
PB  - arXiv
PY  - 2024
ST  - Behavior-Dependent Linear Recurrent Units for Efficient Sequential Recommendation
Y2  - 2025/05/05/21:54:32
DO  - 10.1145/3627673.3679717
ER  -


TY  - GEN
AU  - Rolo, I.
AU  - Mena, E.D.
AU  - Tsantaki, M.
AU  - da Silva, J.G.
TI  - The enigma of Li-rich giants and its relation with temporal variations observed in radial velocity and stellar activity signals
AB  - Context. Despite the large number of studies focused on the characterisation of Li-rich stars and understanding the mechanisms leading to such enrichment, their origin remains a mystery. Aims. Magnetic activity, particularly the phenomena usually associated with it (e.g. spots and plages), and the Li abundance (A(Li)) of stars, are in general thought to be connected. As of today, however, just how they are connected is unclear. In this work, we study a sample of young but evolved intermediate-mass red giants that are inhabitants of open clusters where planets have been searched for. Our aim is to use radial velocity (RV) and stellar activity indicator signals to look for relations between Li abundances and stellar activity or variability. Methods. We explored how the standard deviation (STD), peak-to-peak amplitude (PTP), mean, and median of typical stellar activity indicators (BIS, FWHM, Teff, and Hα index) change as a function of the Li content of 82 red giants. Furthermore, we computed weighted Pearson correlation coefficients (ρw) between time series of RV measurements and the stellar activity indicators for the stars in our sample. To aid our results, we also studied generalized Lomb-Scargle periodograms (GLSP) to capture possible significant periodic temporal variations in our data. Results. Our analysis indicates that the STD and PTP of BIS and FWHM, the mean and median of the Hα index, and vsin(i) increase exponentially with A(Li) in our sample of red giants. Significant temporal variations and correlations between RVs and activity indicators also tend to be found preferentially for stars where high A(Li) is observed. Most of the Li-rich stars in our sample either show strong correlations of RV with at least one of the stellar activity indicators or reveal significant periodic temporal variations in their GLSPs of stellar activity indicators that are consistent with those found for RV.
PB  - arXiv
PY  - 2024
ST  - The enigma of Li-rich giants and its relation with temporal variations observed in radial velocity and stellar activity signals
Y2  - 2025/05/05/21:54:32
DO  - 10.1051/0004-6361/202449873
ER  -


TY  - GEN
AU  - Gerken, F.
AU  - Darcher, A.
AU  - Gonçalves, P.J.
AU  - Mormann, F.
AU  - Leal-Taixé, L.
TI  - Decoding movie content from neuronal population activity in the human medial temporal lobe
AB  - Neurons of the medial temporal lobe (MTL) form the basis of semantic representation in the human brain. While known to contain category-selective cells, it is unclear how the MTL processes naturalistic, dynamic stimuli. We studied 2286 neurons recorded from the hippocampus, parahippocampal cortex, amygdala, and entorhinal cortex of 29 intracranially-implanted patients during a full-length movie. While few neurons responded preferentially to semantic features, we could reliably predict the presence of characters, location, and visual transitions from the neuronal populations using a recurrent neural network. We show that decoding performance differs across regions based on the feature category, and that the performance is driven by feature-selective single neurons when decoding visual transitions such as camera cuts. These findings suggest that semantic representation in the MTL varies based on semantic category, with decoding information embedded in specific subsets of neurons for event-related features or distributed across the entire population for character and location-related features.
PB  - bioRxiv
PY  - 2024
ST  - Decoding movie content from neuronal population activity in the human medial temporal lobe
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2024.06.13.598791
ER  -


TY  - GEN
AU  - Mwamasangula, J.W.
AU  - Gibore, N.S.
TI  - Service availability, readiness and health-seeking behavior of gender based violence survivor services in primary healthcare facilities in Dodoma, Tanzania. An Explanatory-sequential study design.
AB  - Background: The health sector is a key stakeholder in GBV national response network however, delivery of quality healthcare services for GBV survivors is highly neglected. Methods: This study will use a mixed-research approach by employing an explanatory sequential study design. Phase one will involve a cross-sectional survey of 61 primary healthcare facilities to examine availability and readiness of GBV survivor’s services. The second phase will involve a descriptive qualitative study among healthcare providers, community healthcare workers and clients to examine the health-seeking behavior and challenges in providing and receiving GBV survivor’s services in primary healthcare facilities. The study will be conducted between March-April 2024 in Dodoma City Council, Tanzania. Discussion: This study will have potential implications to program managers, policy makers and healthcare providers to tailor interventions that will improve the quality of healthcare service delivery especially towards integrated GBV survivor’s services. But also, understanding the health-seeking behavior and challenges associated with access and provision of GBV survivor’s services will help unveiling gaps in the clients’ point of view and the community, their needs for the GBV survivor’s services and enhancing client-healthcare provider relationship.
PB  - medRxiv
PY  - 2024
ST  - Service availability, readiness and health-seeking behavior of gender based violence survivor services in primary healthcare facilities in Dodoma, Tanzania. An Explanatory-sequential study design.
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2024.06.09.24308665
ER  -


TY  - GEN
AU  - Duan, Y.
AU  - Nurek, M.
AU  - Guan, Q.
AU  - Michalski, R.
AU  - Holme, P.
TI  - Temporal Link Prediction in Social Networks Based on Agent Behavior Synchrony and a Cognitive Mechanism
AB  - Temporality, a crucial characteristic in the formation of social relationships, was used to quantify the long-term time effects of networks for link prediction models, ignoring the heterogeneity of time effects on different time scales. In this work, we propose a novel approach to link prediction in temporal networks, extending existing methods with a cognitive mechanism that captures the dynamics of the interactions. Our approach computes the weight of the edges and their change over time, similar to memory traces in the human brain, by simulating the process of forgetting and strengthening connections depending on the intensity of interactions. We utilized five ground-truth datasets, which were used to predict social ties, missing events, and potential links. We found: (a) the cognitive mechanism enables more accurate capture of the heterogeneity of the temporal effect, leading to an average precision improvement of 9% compared to baselines with competitive AUC. (b) the local structure and synchronous agent behavior contribute differently to different types of datasets. (c) appropriately increasing the time intervals, which may reduce the negative impact from noise when dividing time windows to calculate the behavioral synchrony of agents, is effective for link prediction tasks.
PB  - arXiv
PY  - 2024
ST  - Temporal Link Prediction in Social Networks Based on Agent Behavior Synchrony and a Cognitive Mechanism
Y2  - 2025/05/05/21:54:32
DO  - 10.1109/tcss.2025.3547120
ER  -


TY  - GEN
AU  - Liang, J.
AU  - Han, X.
AU  - Zhou, Y.
AU  - Yan, L.
TI  - Investigating the Temporal Lag and Accumulation Effect of Climatic Factors on Vegetation Photosynthetic Activity Over Subtropical China
AB  - Monitoring vegetation photosynthesis in China's subtropical regions using remote sensing is challenging because of the complex ecosystems and climate variability. Previous studies often pay less attention on the influence of multiple climatic factors on the temporal effects (lag and cumulative) of vegetation photosynthesis, thereby underestimating their impact. This study utilizes a dataset comprising Solar-induced chlorophyll fluorescence (SIF) data (GOSIF product), MODIS Land Cover product (MCD12C1), and various climatic variables. Analytical methods including Theil-Sen Median trend analysis, the Mann-Kendall test, partial correlation analysis, and the optimal parameter-based geographical detector (OPGD) model were employed to explore the temporal dynamics of subtropical vegetation SIF responses to climatic factors and to identify their climate drivers in subtropical China. The study findings indicate that (1) vegetation photosynthesis, as indicated by SIF, exhibited an increasing trend in the majority of Chinese subtropical regions. These regions constitute over 80% of the study area, with particularly pronounced enhancements observed in the southern and central western parts of the Chinese subtropics. (2) Soil moisture primarily exhibits lag effects on SIF, particularly in evergreen needleleaf forests, deciduous broadleaf forests, and mixed forests. In contrast, temperature does not exhibit significant temporal effects. Solar radiation and vapor pressure deficits impact SIF through both lag and cumulative effects. Under the lag and cumulative effects, the proportion of significant correlations between climatic factors and vegetation SIF increases by 36.71%~43.8%, in the case excluding temperature. (3) Temperature is the dominant factor affecting vegetation SIF, particularly in the evergreen needleleaf forest. Interactions between climatic factors have a significantly stronger influence on SIF than individual factors. Notably, the explanatory power of the vapor pressure deficit increases substantially when it interacts with other factors. Studying the lagged and cumulative effects of climatic factors on photosynthesis aids in accurately predicting vegetation responses to climate change, thereby improving the accuracy of global carbon cycle models and guiding the development of carbon sequestration management strategies.
PB  - SSRN
PY  - 2024
ST  - Investigating the Temporal Lag and Accumulation Effect of Climatic Factors on Vegetation Photosynthetic Activity Over Subtropical China
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4858572
ER  -


TY  - GEN
AU  - Eales, O.
AU  - Teo, M.
AU  - Price, D.J.
AU  - McCaw, J.M.
AU  - Shearer, F.M.
TI  - Temporal trends in test-seeking behaviour during the COVID-19 pandemic
AB  - Background: During the COVID-19 pandemic, many countries implemented mass community testing programs, where individuals would seek tests due to (primarily) the onset of symptoms. The cases recorded by mass testing programs represent only a fraction of infected individuals, and depend on how many people seek testing. If test-seeking behaviour exhibits heterogeneities or changes over time, and this is not accounted for when analysing case data, then inferred epidemic dynamics used to inform public health decision-making can be biased. Methods: Here we describe temporal trends in COVID-19 test-seeking behaviour in Australia by symptoms, age group, test type, and jurisdiction from November 2021–September 2023. We use data from two surveillance systems: a weekly nationwide behavioural survey (NBS), established by the Australian Government to monitor a range of behavioural responses to COVID-19; and Australia’s FluTracking system, a ‘participatory surveillance system’ designed for monitoring influenza-like illness and health-care seeking behaviour, which was adapted in early 2020 to include questions relevant to COVID-19. Results: We found that peaks in test-seeking behaviour generally aligned with peaks in the rate of reported cases. Test-seeking behaviour rapidly increased in early-2022 coinciding with greater availability of rapid antigen tests. There were heterogeneities in test-seeking behaviour by jurisdiction and age-group, which were dynamic through time. Test-seeking behaviour was lowest in older individuals (60+ years) until July 2022, after which there was greater homogeneity across age-groups. Test-seeking behaviour was highest in the Australian Capital Territory and Tasmania and consistently lowest in Queensland. Over the course of the study test-seeking behaviour was highest in individuals who reported symptoms more predictive of COVID-19 infection. There was a greater probability of seeking a test for individuals in FluTracking compared to the NBS, suggesting that participatory surveillance systems such as FluTracking may include a health-conscious subset of the population. Conclusions: Our findings demonstrate the dynamism of test-seeking behaviour, highlighting the importance of the continued collection of behavioural data through dedicated surveillance systems.
PB  - medRxiv
PY  - 2024
ST  - Temporal trends in test-seeking behaviour during the COVID-19 pandemic
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2024.06.06.24308566
ER  -


TY  - GEN
AU  - Xu, N.
AU  - Yousefi, B.
AU  - Anumba, N.
AU  - Zhang, X.
AU  - Keilholz, S.
TI  - Qpplab: A Generally Applicable Software Package for Detecting, Analyzing, and Visualizing Large-Scale Quasiperiodic Spatiotemporal Patterns (Qpps) of Brain Activity
AB  - One prominent brain dynamic process detected in functional neuroimaging data is large-scale quasi-periodic patterns (QPPs) which display spatiotemporal propagations along brain cortical gradients. QPP associates with the infraslow neural activity related to attention and arousal fluctuations and has been identified in both resting and task-evoked brains across various species. Several QPP detection and analysis tools were developed for distinct applications with study-specific parameters methods. This MATLAB package provides a simplified and user-friendly generally applicable toolbox for detecting, analyzing, and visualizing QPPs from fMRI timeseries of the brain. This paper describes the software functions and presents its ease of use on any brain datasets.
PB  - SSRN
PY  - 2024
ST  - Qpplab
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4837565
ER  -


TY  - GEN
AU  - Zhang, R.
AU  - Chanthaset, N.
AU  - Sato, T.
AU  - Tanaka, T.
AU  - Ajiro, H.
TI  - Particle Behaviors of Imipenem/Cilastatin in Various Liquids Investigated by Chemical Structures and Temporal Microscope Observations
AB  - Imipenem/Cilastatin (IPM/CS) is currently used as an embolic agent in transarterial embolization for chronic musculoskeletal pain due to its efficacy of short-term blood flow occlusion. However, due to its off-label use, the development of new materials to substitute IPM/CS is needed. Therefore, investigating the behavior of IPM/CS particles becomes crucial. In this paper, we observed the changes in particle size of IPM/CS when dissolved in phosphate-buffered saline, normal saline, iodinated contrast agent and water. The results indicated that the particle size of IPM/CS ranged from 16 to 57 µm and has fastest hydrolysis rate in iodinated contrast agent. And we analyzed the possible structure of hydrolyzed product of IPM/CS in water at room temperature and 37 ℃ using 1HNMR, HPLC. The results suggested Cilastatin is stable at both temperatures, while Imipenem hydrolyzed to produce slightly different products at different temperatures.
PB  - SSRN
PY  - 2024
ST  - Particle Behaviors of Imipenem/Cilastatin in Various Liquids Investigated by Chemical Structures and Temporal Microscope Observations
Y2  - 2025/05/05/21:54:32
DO  - 10.1016/j.colsurfa.2024.134896
ER  -


TY  - GEN
AU  - Reichenbach, K.
AU  - Rothkirch, M.
AU  - Jaeckel, L.
AU  - Sterzer, P.
AU  - Weilnhammer, V.
TI  - Anterior insular activity signals perceptual conflicts induced by temporal and spatial context
AB  - The signals registered by our senses are inherently ambiguous. Subjective experience, by contrast, is informative: it portrays one interpretation of the sensory environment at a time while discarding competing alternatives. This is exemplified by bistable perception, where ambiguous sensory information induces prolonged intervals of alternating unambiguous perceptual states. According to predictive-processing accounts of bistable perception, perceptual experiences in the recent past constitute a predictive context that stabilizes perception, while sensory information that is in conflict with this predictive context evokes prediction errors. These prediction errors are thought to drive spontaneous perceptual switches. We asked whether this mechanism generalizes to conflicts between other forms of predictive context and sensory information. To this aim, we investigated the neural correlates of perceptual conflicts with temporal and spatial context during bistable perception using functional magnetic resonance imaging (fMRI). Twenty-six healthy participants viewed serial presentations of ambiguous structure-from-motion stimuli either in isolation (conflict with temporal context) or embedded in a similar but unambiguous surround stimulus (conflict with spatial context). The neural correlates of conflicts with temporal and spatial context overlapped in the anterior insula bilaterally. Model-based analyses similarly yielded common prediction error signals in the anterior insula bilaterally, right inferior frontal gyrus and right inferior parietal lobe. Together, these findings point to a generic role of these frontoparietal regions in detecting perceptual conflict and thus in the construction of unambiguous perceptual experiences.
PB  - bioRxiv
PY  - 2024
ST  - Anterior insular activity signals perceptual conflicts induced by temporal and spatial context
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2024.05.29.595872
ER  -


TY  - GEN
AU  - Sooter, J.S.
AU  - Fontenele, A.J.
AU  - Ly, C.
AU  - Barreiro, A.K.
AU  - Shew, W.L.
TI  - Cortex deviates from criticality during action and deep sleep: a temporal renormalization group approach
AB  - The hypothesis that the brain operates near criticality explains observations of complex, often scale-invariant, neural activity. However, the brain is not static, its dynamical state varies depending on what an organism is doing. Neurons often become more synchronized (ordered) during unconsciousness and more desynchronized (disordered) in highly active awake conditions. Are all these states equidistant from criticality; if not, which is closest? The fundamental physics of how systems behave near criticality came from renormalization group (RG) theory, but RG for neural systems remains largely undeveloped. Here we developed a temporal RG (tRG) theory for analysis of typical neuroscience data. We mathematically identified multiple types of criticality (tRG fixed points) and developed tRG-driven data analytic methods to assess proximity to each fixed point based on relatively short time series. Unlike traditional methods for studying criticality in neural systems, our tRG approach allows time-resolved measurements of distance from criticality in experiments at behaviorally relevant timescales. We apply our approach to recordings of spike activity in mouse visual cortex, showing that the relaxed, awake state is closest to criticality. When arousal shifts away from this state – either increasing in more active awake states or decreasing in deep sleep – cortical dynamics deviate from criticality.
PB  - bioRxiv
PY  - 2024
ST  - Cortex deviates from criticality during action and deep sleep
Y2  - 2025/05/05/21:54:32
DO  - 10.1101/2024.05.29.596499
ER  -


TY  - GEN
AU  - Gao, Z.
AU  - Duberg, K.
AU  - Warren, S.L.
AU  - Menon, V.
AU  - Cai, W.
TI  - Reduced temporal and spatial stability of neural activity patterns predict cognitive control deficits in children with ADHD
AB  - This study explores the neural underpinnings of cognitive control deficits in ADHD, focusing on overlooked aspects of trial-level variability of neural coding. We employed a novel computational approach to neural decoding on a single-trial basis alongside a cued stop-signal task which allowed us to distinctly probe both proactive and reactive cognitive control. Typically developing (TD) children exhibited stable neural response patterns for efficient proactive and reactive dual control mechanisms. However, neural coding was compromised in children with ADHD. Children with ADHD showed increased temporal variability and diminished spatial stability in neural responses in salience and frontal-parietal network regions, indicating disrupted neural coding during both proactive and reactive control. Moreover, this variability correlated with fluctuating task performance and with more severe symptoms of ADHD. These findings underscore the significance of modeling single-trial variability and representational similarity in understanding distinct components of cognitive control in ADHD, highlighting new perspectives on neurocognitive dysfunction in psychiatric disorders.
PB  - bioRxiv
PY  - 2024
ST  - Reduced temporal and spatial stability of neural activity patterns predict cognitive control deficits in children with ADHD
Y2  - 2025/05/05/21:54:32
DO  - 10.1038/s41467-025-57685-x
ER  -


TY  - GEN
AU  - Song, Y.C.
TI  - Temporal Grounding of Activities using Multimodal Large Language Models
AB  - Temporal grounding of activities, the identification of specific time intervals of actions within a larger event context, is a critical task in video understanding. Recent advancements in multimodal large language models (LLMs) offer new opportunities for enhancing temporal reasoning capabilities. In this paper, we evaluate the effectiveness of combining image-based and text-based large language models (LLMs) in a two-stage approach for temporal activity localization. We demonstrate that our method outperforms existing video-based LLMs. Furthermore, we explore the impact of instruction-tuning on a smaller multimodal LLM, showing that refining its ability to process action queries leads to more expressive and informative outputs, thereby enhancing its performance in identifying specific time intervals of activities. Our experimental results on the Charades-STA dataset highlight the potential of this approach in advancing the field of temporal activity localization and video understanding.
PB  - arXiv
PY  - 2024
ST  - Temporal Grounding of Activities using Multimodal Large Language Models
Y2  - 2025/05/05/21:54:32
DO  - 10.18653/v1/2024.emnlp-main.556
ER  -


TY  - GEN
AU  - Miller, E.M.
AU  - Chan, T.C.D.
AU  - Montes-Matamoros, C.
AU  - Pujo-Menjouet, L.
AU  - Lindstrom, M.R.
TI  - Oscillations in neuronal activity: A neuron-centered spatiotemporal model of the Unfolded Protein Response in prion diseases
AB  - Many neurodegenerative diseases (NDs) are characterized by the slow spatial spread of toxic protein species in the brain. The toxic proteins can induce neuronal stress, triggering the Unfolded Protein Response (UPR), which slows or stops protein translation and can indirectly reduce the toxic load. However, the UPR may also trigger processes leading to apoptotic cell death and the UPR is implicated in the progression of several NDs. In this paper, we develop a novel mathematical model to describe the spatiotemporal dynamics of the UPR mechanism for prion diseases. Our model is centered around a single neuron, with representative proteins P (healthy) and S (toxic) interacting with heterodimer dynamics (S interacts with P to form two S's). The model takes the form of a coupled system of nonlinear reaction-diffusion equations with a delayed, nonlinear flux for P (delay from the UPR). Through the delay, we find parameter regimes that exhibit oscillations in the P- and S-protein levels. We find that oscillations are more pronounced when the S-clearance rate and S-diffusivity are small in comparison to the P-clearance rate and P-diffusivity, respectively. The oscillations become more pronounced as delays in initiating the UPR increase. We also consider quasi-realistic clinical parameters to understand how possible drug therapies can alter the course of a prion disease. We find that decreasing the production of P, decreasing the recruitment rate, increasing the diffusivity of S, increasing the UPR S-threshold, and increasing the S clearance rate appear to be the most powerful modifications to reduce the mean UPR intensity and potentially moderate the disease progression.
PB  - arXiv
PY  - 2024
ST  - Oscillations in neuronal activity
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/s11538-024-01307-y
ER  -


TY  - GEN
AU  - Miyazaki, Y.
AU  - Švábenský, V.
AU  - Taniguchi, Y.
AU  - Minematsu, T.
AU  - Shimada, A.
TI  - E2Vec: Feature Embedding with Temporal Information for Analyzing Student Actions in E-Book Systems
AB  - Digital textbook (e-book) systems record student interactions with textbooks as a sequence of events called EventStream data. In the past, researchers extracted meaningful features from EventStream, and utilized them as inputs for downstream tasks such as grade prediction and modeling of student behavior. Previous research evaluated models that mainly used statistical-based features derived from EventStream logs, such as the number of operation types or access frequencies. While these features are useful for providing certain insights, they lack temporal information that captures fine-grained differences in learning behaviors among different students. This study proposes E2Vec, a novel feature representation method based on word embeddings. The proposed method regards operation logs and their time intervals for each student as a string sequence of characters and generates a student vector of learning activity features that incorporates time information. We applied fastText to generate an embedding vector for each of 305 students in a dataset from two years of computer science courses. Then, we investigated the effectiveness of E2Vec in an at-risk detection task, demonstrating potential for generalizability and performance.
PB  - arXiv
PY  - 2024
ST  - E2Vec
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-540-39592-8_37
ER  -


TY  - GEN
AU  - Wang, P.
AU  - Lin, Y.
AU  - Blasch, E.
AU  - Wei, J.
AU  - Ling, H.
TI  - Efficient Temporal Action Segmentation via Boundary-aware Query Voting
AB  - Although the performance of Temporal Action Segmentation (TAS) has improved in recent years, achieving promising results often comes with a high computational cost due to dense inputs, complex model structures, and resource-intensive post-processing requirements. To improve the efficiency while keeping the performance, we present a novel perspective centered on per-segment classification. By harnessing the capabilities of Transformers, we tokenize each video segment as an instance token, endowed with intrinsic instance segmentation. To realize efficient action segmentation, we introduce BaFormer, a boundary-aware Transformer network. It employs instance queries for instance segmentation and a global query for class-agnostic boundary prediction, yielding continuous segment proposals. During inference, BaFormer employs a simple yet effective voting strategy to classify boundary-wise segments based on instance segmentation. Remarkably, as a single-stage approach, BaFormer significantly reduces the computational costs, utilizing only ∼6% of the running time compared to state-of-the-art method DiffAct, while producing better or comparable accuracy over several popular benchmarks. The code for this project is publicly available at https://github.com/peiyao-w/BaFormer.
PB  - arXiv
PY  - 2024
ST  - Efficient Temporal Action Segmentation via Boundary-aware Query Voting
Y2  - 2025/05/05/21:54:32
DO  - 10.1007/978-3-030-58595-2_3
ER  -


TY  - GEN
AU  - Li, N.
AU  - Ding, C.
AU  - Zhang, Y.
AU  - Shi, W.
AU  - Fan, C.
TI  - Long-Term Motion Feature Extraction Based Temporal Action Proposal Generation for Video Understanding
AB  - Temporal Action Detection (TAD) is a critical topic in video understanding and Temporal Action Proposal Generation (TAPG) plays a key role in TAD by generating high-quality action proposals to identify the boundaries of actions. However, most of the existing temporal modeling methods mainly capture short-term motion information based on the changes between temporal adjacent frames, but lack of considering the scalable collection of the long-term motion information. In this paper, we propose a scalable TAPG algorithm based on a plug-and-play Long-term Motion Feature Extraction (LMFE) module. Firstly, a framework is developed to enhance the existing TAPG methods with the lightweight long-term motion feature extracted by the LMFE module. Secondly, the LMFE module is introduced to adaptively collect the long-term motion feature between two frames over a period in terms of the action (i.e., action score) and attention mechanism. Thirdly, the long-term motion feature is combined with the original feature at different semantic levels. Comprehensive experiments on widely recognized video benchmarks demonstrated that the LMFE module is plug-and-play and can be flexibly combined with the existing TAPG methods. Moreover, the proposed LMFE can improve the Average Recall at Average Number (AR@AN) of proposals by 3.2% for TAPG tasks and improve the mean Average Precision (mAP) by 2.9% for TAD tasks.
PB  - SSRN
PY  - 2024
ST  - Long-Term Motion Feature Extraction Based Temporal Action Proposal Generation for Video Understanding
Y2  - 2025/05/05/21:54:32
DO  - 10.2139/ssrn.4837901
ER  -