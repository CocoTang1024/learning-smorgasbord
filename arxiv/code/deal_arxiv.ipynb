{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已成功处理 9 个文件，共导出 1635 条文献。\n"
     ]
    }
   ],
   "source": [
    "# 1.将SingleFile获取得到的文件提取出当前的具体ris文件 1635条->66.4s\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# 清洗文本\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# 处理单个 HTML 文件，提取文献信息\n",
    "def process_html_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    current_time = datetime.now().strftime(\"%Y/%m/%d/%H:%M:%S\")\n",
    "    documents = []\n",
    "    rows = soup.find_all('tr', class_='TableItems-module__m0Z0b')\n",
    "\n",
    "    for row in rows:\n",
    "        preprint_span = row.find('span', string=re.compile('Preprint.*开放获取'))\n",
    "        if preprint_span and not row.find('h3'):\n",
    "            continue\n",
    "\n",
    "        title_div = row.find('div', class_='TableItems-module__sHEzP')\n",
    "        if title_div and title_div.find('h3'):\n",
    "            doc = {}\n",
    "            title_span = title_div.find('h3').find('span')\n",
    "            doc['title'] = clean_text(title_span.get_text(strip=True)) if title_span else ''\n",
    "\n",
    "            author_div = row.find('div', class_='author-list')\n",
    "            authors = []\n",
    "            if author_div:\n",
    "                author_buttons = author_div.find_all('button')\n",
    "                for button in author_buttons:\n",
    "                    author_name = button.find('span', class_='Typography-module__lVnit')\n",
    "                    if author_name:\n",
    "                        authors.append(clean_text(author_name.get_text(strip=True)))\n",
    "            doc['authors'] = authors\n",
    "\n",
    "            source_div = row.find('div', class_='DocumentResultsList-module__tqiI3')\n",
    "            doc['publisher'] = clean_text(source_div.find('span').get_text(strip=True)) if source_div else ''\n",
    "\n",
    "            year_div = row.find('div', class_='TableItems-module__TpdzW')\n",
    "            doc['year'] = clean_text(year_div.find('span').get_text(strip=True)) if year_div else ''\n",
    "\n",
    "            abstract = ''\n",
    "            current_row = row\n",
    "            while True:\n",
    "                next_row = current_row.find_next('tr')\n",
    "                if not next_row:\n",
    "                    break\n",
    "                abstract_div = next_row.find('div', class_='Abstract-module__ukTwj')\n",
    "                if abstract_div:\n",
    "                    abstract = clean_text(abstract_div.get_text(strip=True))\n",
    "                    break\n",
    "                current_row = next_row\n",
    "            doc['abstract'] = abstract\n",
    "            doc['current_time'] = current_time\n",
    "            documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "# 将文献信息写入 RIS 格式\n",
    "def write_ris(documents, output_path):\n",
    "    with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for doc in documents:\n",
    "            f.write(\"TY  - GEN\\n\")\n",
    "            for author in doc['authors']:\n",
    "                f.write(f\"AU  - {author}\\n\")\n",
    "            f.write(f\"TI  - {doc['title']}\\n\")\n",
    "            if doc['abstract']:\n",
    "                f.write(f\"AB  - {doc['abstract']}\\n\")\n",
    "            if doc['publisher']:\n",
    "                f.write(f\"PB  - {doc['publisher']}\\n\")\n",
    "            if doc['year']:\n",
    "                f.write(f\"PY  - {doc['year']}\\n\")\n",
    "            st = doc['title'].split(\":\")[0] if \":\" in doc['title'] else doc['title']\n",
    "            f.write(f\"ST  - {st}\\n\")\n",
    "            f.write(f\"Y2  - {doc['current_time']}\\n\")\n",
    "            f.write(\"ER  -\\n\\n\")\n",
    "\n",
    "# 主函数：多线程读取多个文件并合并写入 RIS 文件\n",
    "def main():\n",
    "    html_dir = R\"D:\\Users\\tang\\Downloads\"\n",
    "    ris_output_path = \"../data/arxiv_results_multi_20250508_scopus_arxiv_1635_tad_tal.ris\"\n",
    "    \n",
    "    filenames = [\n",
    "        R\"20250508_scopus_arxiv_0-200_tad_tal.html\",\n",
    "        R\"20250508_scopus_arxiv_201-400_tad_tal.html\",\n",
    "        R\"20250508_scopus_arxiv_401-600_tad_tal.html\",\n",
    "        R\"20250508_scopus_arxiv_601-800_tad_tal.html\",\n",
    "        R\"20250508_scopus_arxiv_801-1000_tad_tal.html\",\n",
    "        R\"20250508_scopus_arxiv_1001-1200_tad_tal.html\",\n",
    "        R\"20250508_scopus_arxiv_1201-1400_tad_tal.html\",\n",
    "        R\"20250508_scopus_arxiv_1401-1600_tad_tal.html\",\n",
    "        R\"20250508_scopus_arxiv_1601-1635_tad_tal.html\",\n",
    "    ]\n",
    "\n",
    "    filepaths = [os.path.join(html_dir, name) for name in filenames]\n",
    "\n",
    "    # 多线程处理文件\n",
    "    all_documents = []\n",
    "    with ThreadPoolExecutor(max_workers=9) as executor:\n",
    "        results = list(executor.map(process_html_file, filepaths))\n",
    "        for docs in results:\n",
    "            all_documents.extend(docs)\n",
    "\n",
    "    # 写入 RIS\n",
    "    write_ris(all_documents, ris_output_path)\n",
    "    print(f\"已成功处理 {len(filepaths)} 个文件，共导出 {len(all_documents)} 条文献。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error querying CrossRef for title `Injecting Explainability and Lightweight Design into Weakly Supervised Video Anomaly Detection Systems`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `MARINE: A Computer Vision Model for Detecting Rare Predator-Prey Interactions in Animal Videos`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `A Simulated Two-Stream Network via Multi-level Distillation of Reviewed Features and Decoupled Logits for Video Action Recognition`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `Structure-Aware Human Body Reshaping with Adaptive Affinity-Graph Network`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `Segment Anything Model Can Not Segment Anything: Assessing AI Foundation Model's Generalizability in Permafrost Mapping`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `Dense Video Captioning: A Survey of Techniques, Datasets and Evaluation Protocols`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `VideoPro: A Visual Analytics Approach for Interactive Video Programming`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `MRSN: MULTI-RELATION SUPPORT NETWORK FOR VIDEO ACTION DETECTION`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `A Two-Stream Network Using Global Feature Pyramid and Spatial Attention Blockfor Human Action Recognition`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `Distill and Collect for Semi-Supervised Temporal Action Segmentation`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `ReAct: Temporal Action Detection with Relational Queries`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `Forcing the Whole Video as Background: An Adversarial Learning Strategy for Weakly Temporal Action Localization`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `Graph convolutional module for temporal action localization in videos`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `OadTR: Online action detection with transformers`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `Two-Stream Consensus Network: Submission to HACS Challenge 2021 Weakly-Supervised Learning Track`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `Proposal relation network for temporal action detection`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `Weakly-supervised temporal action localization through local-global background modeling`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `Equivariance-bridged SO(2)-Invariant representation learning using graph convolutional network`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `Discerning generic event boundaries in long-form wild videos`: HTTPSConnectionPool(host='api.crossref.org', port=443): Max retries exceeded with url: /works?query.title=Discerning+generic+event+boundaries+in+long-form+wild+videos&rows=1 (Caused by SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:1133)')))\n",
      "Error querying CrossRef for title `End-to-end Temporal Action Detection with Transformer`: HTTPSConnectionPool(host='api.crossref.org', port=443): Max retries exceeded with url: /works?query.title=End-to-end+Temporal+Action+Detection+with+Transformer&rows=1 (Caused by SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:1133)')))\n",
      "Error querying CrossRef for title `C3: Compositional Counterfactual Contrastive Learning for Video-grounded Dialogues`: HTTPSConnectionPool(host='api.crossref.org', port=443): Max retries exceeded with url: /works?query.title=C3%3A+Compositional+Counterfactual+Contrastive+Learning+for+Video-grounded+Dialogues&rows=1 (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))\n",
      "Error querying CrossRef for title `JRDB-Act: A large-scale dataset for spatio-temporal action, social group and activity detection`: HTTPSConnectionPool(host='api.crossref.org', port=443): Max retries exceeded with url: /works?query.title=JRDB-Act%3A+A+large-scale+dataset+for+spatio-temporal+action%2C+social+group+and+activity+detection&rows=1 (Caused by SSLError(SSLZeroReturnError(6, 'TLS/SSL connection has been closed (EOF) (_ssl.c:1133)')))\n",
      "Error querying CrossRef for title `Aerial scene understanding in the wild: Multi-scene recognition via prototype-based memory networks`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `Privileged knowledge distillation for online action detection`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `Improved soccer action spotting using both audio and video streams`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `Weakly-supervised multi-level attentional reconstruction network for grounding textual queries in videos`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `AU R-CNN: Encoding expert prior knowledge into R-CNN for action unit detection`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `TAN: Temporal Aggregation Network for Dense Multi-label Action Recognition`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `To find where you talk: Temporal sentence localization in video with attention based location regression`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `Learning to anonymize faces for privacy preserving action detection`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "Error querying CrossRef for title `UntrimmedNets for weakly supervised action recognition and detection`: HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
      "已生成带 DOI 的 RIS: ../data/arxiv_results_multi_20250508_scopus_arxiv_1635_tad_tal_with_doi.ris\n"
     ]
    }
   ],
   "source": [
    "# 2.后期为了评定当前arxiv的文献是否值得看，需要给定DOI号，然后看一下它的引用在进行分析\n",
    "# 需要自己手动补充因为SSL失败而导入DOI失败的文件\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "def query_doi(title):\n",
    "    \"\"\"\n",
    "    通过 CrossRef API 根据论文标题查询 DOI，返回 DOI 字符串或 None。\n",
    "    \"\"\"\n",
    "    url = 'https://api.crossref.org/works'\n",
    "    params = {\n",
    "        'query.title': title,\n",
    "        'rows': 1\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        items = r.json().get('message', {}).get('items', [])\n",
    "        if items:\n",
    "            return items[0].get('DOI')\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying CrossRef for title `{title}`: {e}\")\n",
    "    return None\n",
    "\n",
    "def insert_doi_lines(ris_text):\n",
    "    \"\"\"\n",
    "    对一整个 RIS 文本，拆分成多条记录，\n",
    "    对每条记录提取 TI 字段，查询 DOI，\n",
    "    并在 ER 之前插入 DO 字段。\n",
    "    返回所有记录拼接后的新文本。\n",
    "    \"\"\"\n",
    "    records = re.split(r'\\nER  -.*', ris_text)\n",
    "    endings = re.findall(r'(?:\\nER  -.*)', ris_text)\n",
    "    new_records = []\n",
    "\n",
    "    for rec, ending in zip(records, endings):\n",
    "        # 跳过空记录\n",
    "        if not rec.strip():\n",
    "            continue\n",
    "\n",
    "        # 提取标题（TI  - 开头的一行）\n",
    "        m = re.search(r'^TI  - (.+)$', rec, flags=re.MULTILINE)\n",
    "        title = m.group(1).strip() if m else None\n",
    "\n",
    "        doi = None\n",
    "        if title:\n",
    "            doi = query_doi(title)\n",
    "            # time.sleep(1)  # 避免过快请求限流\n",
    "\n",
    "        # 如果查到了 DOI，就插入 DO  - 行\n",
    "        if doi:\n",
    "            rec = rec + f'\\nDO  - {doi}'\n",
    "\n",
    "        # 把 ER 行放回\n",
    "        new_records.append(rec + ending)\n",
    "\n",
    "    return \"\\n\".join(new_records)\n",
    "\n",
    "def main():\n",
    "    infile = R'../data/arxiv_results_multi_20250508_scopus_arxiv_1635_tad_tal.ris'\n",
    "    outfile = '../data/arxiv_results_multi_20250508_scopus_arxiv_1635_tad_tal_with_doi.ris'\n",
    "\n",
    "    # 读入原始 RIS\n",
    "    with open(infile, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # 插入 DOI\n",
    "    new_text = insert_doi_lines(text)\n",
    "\n",
    "    # 写入新文件\n",
    "    with open(outfile, 'w', encoding='utf-8') as f:\n",
    "        f.write(new_text)\n",
    "\n",
    "    print(f\"已生成带 DOI 的 RIS: {outfile}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated RIS file with new DOIs: ../data/arxiv_results_multi_20250508_scopus_arxiv_1635_tad_tal_with_doi_updated.ris\n"
     ]
    }
   ],
   "source": [
    "# 2.2 处理没有DOI查询失败的文献进行再次查询\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "def setup_session():\n",
    "    \"\"\"\n",
    "    Create a requests session with retry logic for handling transient errors.\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "    return session\n",
    "\n",
    "def query_doi(title, session):\n",
    "    \"\"\"\n",
    "    Query DOI via CrossRef API using the paper title, return DOI string or None.\n",
    "    \"\"\"\n",
    "    url = 'https://api.crossref.org/works'\n",
    "    params = {\n",
    "        'query.title': title,\n",
    "        'rows': 1\n",
    "    }\n",
    "    try:\n",
    "        r = session.get(url, params=params, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        items = r.json().get('message', {}).get('items', [])\n",
    "        if items:\n",
    "            return items[0].get('DOI')\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying CrossRef for title `{title}`: {e}\")\n",
    "    return None\n",
    "\n",
    "def update_doi_lines(ris_text):\n",
    "    \"\"\"\n",
    "    Process RIS text, re-query DOIs for records without DO field,\n",
    "    and insert DO field before ER.\n",
    "    \"\"\"\n",
    "    # Split into records and preserve endings\n",
    "    records = re.split(r'\\nER  -.*', ris_text)\n",
    "    endings = re.findall(r'(?:\\nER  -.*)', ris_text)\n",
    "    new_records = []\n",
    "\n",
    "    session = setup_session()\n",
    "\n",
    "    for rec, ending in zip(records, endings):\n",
    "        # Skip empty records\n",
    "        if not rec.strip():\n",
    "            continue\n",
    "\n",
    "        # Extract title (TI field)\n",
    "        m = re.search(r'^TI  - (.+)$', rec, flags=re.MULTILINE)\n",
    "        title = m.group(1).strip() if m else None\n",
    "\n",
    "        # Check if record already has a DO field\n",
    "        has_doi = re.search(r'^DO  - ', rec, flags=re.MULTILINE) is not None\n",
    "\n",
    "        # If no DOI and title exists, re-query\n",
    "        if not has_doi and title:\n",
    "            doi = query_doi(title, session)\n",
    "            if doi:\n",
    "                rec = rec + f'\\nDO  - {doi}'\n",
    "            time.sleep(1)  # Avoid rate limiting\n",
    "\n",
    "        new_records.append(rec + ending)\n",
    "\n",
    "    return \"\\n\".join(new_records)\n",
    "\n",
    "def main():\n",
    "    infile = '../data/arxiv_results_multi_20250508_scopus_arxiv_1635_tad_tal_with_doi.ris'\n",
    "    outfile = '../data/arxiv_results_multi_20250508_scopus_arxiv_1635_tad_tal_with_doi_updated.ris'\n",
    "\n",
    "    # Read the existing RIS file\n",
    "    with open(infile, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Update with new DOIs\n",
    "    new_text = update_doi_lines(text)\n",
    "\n",
    "    # Write the updated RIS file\n",
    "    with open(outfile, 'w', encoding='utf-8') as f:\n",
    "        f.write(new_text)\n",
    "\n",
    "    print(f\"Updated RIS file with new DOIs: {outfile}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication complete. Output written to D:\\Programs\\Codes\\Skill-Up\\learning-smorgasbord\\arxiv\\data\\arxiv_results_multi_20250508_scopus_arxiv_1635_tad_tal_output_deplication.ris\n",
      "Log written to deduplication_log_*.txt\n"
     ]
    }
   ],
   "source": [
    "# 3.处理重复的条目，保留最详细的，并且在日志里面输出当前去重过的期刊 二重 去重逻辑 题目 DOI 现在存在bug -和空格区分不开\n",
    "# 20250508最好的条目是arxiv\\data\\20250508_scopus_3837_tad_tal 20250508_wos_886_tad_tal_3891-3_3888_deduplication_end.ris\n",
    "import uuid\n",
    "import logging\n",
    "import datetime\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def normalize_title(title):\n",
    "    \"\"\"Normalize title for comparison by removing case and punctuation.\"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', title.lower()).strip()\n",
    "\n",
    "def parse_ris_file(file_path):\n",
    "    \"\"\"Parse RIS file and return a list of entries.\"\"\"\n",
    "    entries = []\n",
    "    current_entry = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line == 'ER  -':\n",
    "                if current_entry:\n",
    "                    entries.append(current_entry)\n",
    "                    current_entry = {}\n",
    "            elif line:\n",
    "                tag, value = line.split('  - ', 1) if '  - ' in line else (line, '')\n",
    "                current_entry[tag] = current_entry.get(tag, []) + [value]\n",
    "    return entries\n",
    "\n",
    "def count_entry_fields(entry):\n",
    "    \"\"\"Count the number of fields (data lines) in an entry.\"\"\"\n",
    "    return sum(len(values) for values in entry.values())\n",
    "\n",
    "def deduplicate_by_field(entries, field, normalize=False):\n",
    "    \"\"\"Deduplicate entries based on a specified field, keeping the one with most fields.\"\"\"\n",
    "    field_to_entries = defaultdict(list)\n",
    "    for entry in entries:\n",
    "        field_value = entry.get(field, [''])[0]\n",
    "        if field_value:  # Only process entries with the field\n",
    "            key = normalize_title(field_value) if normalize else field_value\n",
    "            field_to_entries[key].append(entry)\n",
    "    \n",
    "    deduplicated = []\n",
    "    log_messages = []\n",
    "    \n",
    "    for key, entries_group in field_to_entries.items():\n",
    "        if len(entries_group) > 1:\n",
    "            # Sort by number of fields (descending) and keep the one with most fields\n",
    "            entries_group.sort(key=count_entry_fields, reverse=True)\n",
    "            kept_entry = entries_group[0]\n",
    "            deduplicated.append(kept_entry)\n",
    "            # Log removed entries\n",
    "            for removed_entry in entries_group[1:]:\n",
    "                log_messages.append(\n",
    "                    f\"Removed duplicate entry with {field} '{key}' \"\n",
    "                    f\"(kept {count_entry_fields(kept_entry)} fields, \"\n",
    "                    f\"removed {count_entry_fields(removed_entry)} fields, \"\n",
    "                    f\"title: '{removed_entry.get('TI', [''])[0]}')\"\n",
    "                )\n",
    "        else:\n",
    "            deduplicated.append(entries_group[0])\n",
    "    \n",
    "    # Add entries that didn't have the field\n",
    "    for entry in entries:\n",
    "        if not entry.get(field, [''])[0]:\n",
    "            deduplicated.append(entry)\n",
    "    \n",
    "    return deduplicated, log_messages\n",
    "\n",
    "def deduplicate_entries(entries):\n",
    "    \"\"\"Deduplicate entries first by TI, then by DO.\"\"\"\n",
    "    # Step 1: Deduplicate by TI\n",
    "    entries, ti_log_messages = deduplicate_by_field(entries, 'TI', normalize=True)\n",
    "    \n",
    "    # Step 2: Deduplicate by DO\n",
    "    entries, do_log_messages = deduplicate_by_field(entries, 'DO', normalize=False)\n",
    "    \n",
    "    return entries, ti_log_messages + do_log_messages\n",
    "\n",
    "def write_ris_file(entries, output_path):\n",
    "    \"\"\"Write deduplicated entries to a new RIS file with a blank line between entries.\"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        for i, entry in enumerate(entries):\n",
    "            for tag, values in entry.items():\n",
    "                for value in values:\n",
    "                    file.write(f\"{tag}  - {value}\\n\")\n",
    "            file.write(\"ER  -\\n\")\n",
    "            if i < len(entries) - 1:  # Add blank line between entries, but not after the last\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Set up logging to a file.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        filename=f'deduplication_log_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt',\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(message)s'\n",
    "    )\n",
    "\n",
    "def main(input_file, output_file):\n",
    "    setup_logging()\n",
    "    \n",
    "    # Parse RIS file\n",
    "    entries = parse_ris_file(input_file)\n",
    "    \n",
    "    # Deduplicate entries\n",
    "    deduplicated_entries, log_messages = deduplicate_entries(entries)\n",
    "    \n",
    "    # Write to output file\n",
    "    write_ris_file(deduplicated_entries, output_file)\n",
    "    \n",
    "    # Log results\n",
    "    for message in log_messages:\n",
    "        logging.info(message)\n",
    "    \n",
    "    logging.info(f\"Processed {len(entries)} entries, kept {len(deduplicated_entries)} entries\")\n",
    "    print(f\"Deduplication complete. Output written to {output_file}\")\n",
    "    print(f\"Log written to deduplication_log_*.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = R\"D:\\Programs\\Codes\\Skill-Up\\learning-smorgasbord\\arxiv\\data\\arxiv_results_multi_20250508_scopus_arxiv_1635_tad_tal.ris\"  # Replace with your input RIS file path\n",
    "    output_file = R\"D:\\Programs\\Codes\\Skill-Up\\learning-smorgasbord\\arxiv\\data\\arxiv_results_multi_20250508_scopus_arxiv_1635_tad_tal_output_deplication.ris\"  # Replace with your desired output RIS file path\n",
    "    main(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed RIS file saved to D:\\Programs\\Codes\\Skill-Up\\learning-smorgasbord\\arxiv\\data\\20250510_scopus_LNCS_tad_tal_303_processed.ris\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# 映射字典\n",
    "conference_map = {\n",
    "    'European Conference on Computer Vision': 'ECCV',\n",
    "    'Asian Conference on Computer Vision': 'ACCV',\n",
    "    'Asian Conference on Pattern Recognition': 'ACPR',\n",
    "    'Computer Graphics International': 'CGI',\n",
    "    'German Conference on Pattern Recognition': 'DAGM-GCPR',\n",
    "    'International Conference on Artificial Neural Networks': 'ICANN',\n",
    "    'International Conference on Image Analysis and Processing': 'ICIAP',\n",
    "    'International Conference on Intelligent Computing': 'ICIC',\n",
    "    'International Conference on Image and Graphics': 'ICIG',\n",
    "    'International Conference on Intelligent Robotics and Applications': 'ICIRA',\n",
    "    'International Conference on Neural Information Processing': 'ICONIP',\n",
    "    'International Conference on Pattern Recognition': 'ICPR',\n",
    "    'International Symposium on Visual Computing': 'ISVC',\n",
    "    'International Conference on Multimedia Modeling': 'MMM',\n",
    "    'Pacific-Rim Conference on Multimedia': 'PCM',\n",
    "    'Chinese Conference on Pattern Recognition and Computer Vision': 'PRCV',\n",
    "    'Pacific Rim International Conference on Artificial Intelligence': 'PRICAI',\n",
    "    'Scandinavian Conference on Image Analysis': 'SCIA',\n",
    "}\n",
    "\n",
    "# 处理单个 RIS 条目的函数\n",
    "def process_ris_entry(entry: str) -> str:\n",
    "    t2_match = re.search(r'T2  - (.+)', entry)\n",
    "    n1_match = re.search(r'N1  - <p>(.*?)</p>', entry, re.DOTALL)\n",
    "\n",
    "    if t2_match and n1_match:\n",
    "        t2_value = t2_match.group(1)\n",
    "        n1_content = n1_match.group(1)\n",
    "\n",
    "        if \"Lecture Notes in Computer Science\" in t2_value:\n",
    "            for conf_name, conf_abbr in conference_map.items():\n",
    "                if conf_name in n1_content:\n",
    "                    eccv_code_match = re.search(rf'{re.escape(conf_abbr)}\\s+\\d{{4}}', n1_content)\n",
    "                    if eccv_code_match:\n",
    "                        new_t2 = f'{conf_name}, {eccv_code_match.group()}; Conference date'\n",
    "                        entry = re.sub(r'(T2  - ).+', f\"\\\\1{new_t2}\", entry)\n",
    "                        break\n",
    "\n",
    "    return entry\n",
    "\n",
    "# 主处理函数\n",
    "def process_ris_file(input_path: str, output_path: str):\n",
    "    text = Path(input_path).read_text(encoding='utf-8')\n",
    "    entries = text.strip().split('\\nER  -')\n",
    "\n",
    "    processed_entries = [process_ris_entry(entry.strip()) + '\\nER  -' for entry in entries if entry.strip()]\n",
    "    Path(output_path).write_text('\\n\\n'.join(processed_entries), encoding='utf-8')\n",
    "\n",
    "# 示例使用\n",
    "if __name__ == '__main__':\n",
    "    input_ris = R'D:\\Programs\\Codes\\Skill-Up\\learning-smorgasbord\\arxiv\\data\\20250510_scopus_LNCS_tad_tal_303.ris'\n",
    "    output_ris = R'D:\\Programs\\Codes\\Skill-Up\\learning-smorgasbord\\arxiv\\data\\20250510_scopus_LNCS_tad_tal_303_processed.ris'\n",
    "    process_ris_file(input_ris, output_ris)\n",
    "\n",
    "    print(f\"Processed RIS file saved to {output_ris}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed RIS file saved to D:\\Programs\\Codes\\Skill-Up\\learning-smorgasbord\u0007rxiv\\data50510_scopus_LNCS_tad_tal_303_processed.ris\n"
     ]
    }
   ],
   "source": [
    "# 去除Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)的杂项\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# 扩展会议缩写列表\n",
    "abbrs = [\n",
    "    'ECCV', 'ACCV', 'ACPR', 'CGI', 'DAGM-GCPR', 'ICANN', 'ICIAP', 'ICIC',\n",
    "    'ICIG', 'ICIRA', 'ICONIP', 'ICPR', 'ISVC', 'MMM', 'PCM', 'PRCV',\n",
    "    'PRICAI', 'SCIA', 'CAAI', 'NPC', 'ADMA', 'CVM', 'HCC', 'ICSI',\n",
    "    'CCBR', 'ICFEM', 'MICCAI', 'DMAH', 'MICCAI', 'CAAI',\n",
    "    'ICR', 'Euro-Par', 'MLDM', 'IbPRIA', 'ICPRAI',\n",
    "    'ICCSA', 'ICPRAI', 'CAIP', 'ICDAR', 'CICAI'\n",
    "    \n",
    "]\n",
    "\n",
    "# 处理单个 RIS 条目的函数\n",
    "def process_ris_entry(entry: str) -> str:\n",
    "    # 匹配 T2 和 N1 字段\n",
    "    t2_match = re.search(r'^(T2  - .+)$', entry, re.MULTILINE)\n",
    "    n1_match = re.search(r'^N1  - <p>(.*?)</p>$', entry, re.MULTILINE | re.DOTALL)\n",
    "\n",
    "    if t2_match and n1_match:\n",
    "        t2_value = t2_match.group(1)\n",
    "        n1_content = n1_match.group(1)\n",
    "\n",
    "        # 仅处理 Lecture Notes in Computer Science 条目\n",
    "        if 'Lecture Notes in Computer Science' in t2_value:\n",
    "            # 在 N1 中寻找缩写和年份，例如 'ECCV 2018'\n",
    "            pattern = r'\\b(' + '|'.join(map(re.escape, abbrs)) + r')\\s+(\\d{4})\\b'\n",
    "            match = re.search(pattern, n1_content)\n",
    "            if match:\n",
    "                abbr = match.group(1)\n",
    "                year = match.group(2)\n",
    "                new_t2 = f'T2  - {abbr} {year}'\n",
    "                # 替换原有 T2 行\n",
    "                entry = re.sub(r'^(T2  - .+)$', new_t2, entry, flags=re.MULTILINE)\n",
    "\n",
    "    return entry\n",
    "\n",
    "# 主处理函数\n",
    "def process_ris_file(input_path: str, output_path: str):\n",
    "    text = Path(input_path).read_text(encoding='utf-8')\n",
    "    # 保留 ER 结束标记并分割条目\n",
    "    raw_entries = re.split(r'\\nER  -\\s*\\n', text.strip(), flags=re.DOTALL)\n",
    "    processed = []\n",
    "\n",
    "    for raw in raw_entries:\n",
    "        entry = raw.strip()\n",
    "        if not entry:\n",
    "            continue\n",
    "        # 恢复 ER 标记，处理后再添加\n",
    "        processed_entry = process_ris_entry(entry + '\\nER  -')\n",
    "        processed.append(processed_entry)\n",
    "\n",
    "    # 写入输出文件\n",
    "    result = '\\n'.join(processed).strip() + '\\n'\n",
    "    Path(output_path).write_text(result, encoding='utf-8')\n",
    "\n",
    "# 命令行执行示例\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "\n",
    "    # parser = argparse.ArgumentParser(description='Process RIS file, updating T2 based on conference abbreviations.')\n",
    "    # parser.add_argument('input', help='Input RIS file path')\n",
    "    # parser.add_argument('output', help='Output RIS file path')\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    process_ris_file(R'D:\\Programs\\Codes\\Skill-Up\\learning-smorgasbord\\arxiv\\data\\20250510_scopus_LNCS_tad_tal_303.ris', R'D:\\Programs\\Codes\\Skill-Up\\learning-smorgasbord\\arxiv\\data\\20250510_scopus_LNCS_tad_tal_303_processed.ris')\n",
    "    print(f\"Processed RIS file saved to D:\\Programs\\Codes\\Skill-Up\\learning-smorgasbord\\arxiv\\data\\20250510_scopus_LNCS_tad_tal_303_processed.ris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "两个 RIS 文件中相同的 TI 条目有：\n",
      "- Dimensionality Deduction for Action Proposals: To Extract or to Select?\n",
      "- Fast Video Clip Retrieval Method via Language Query\n",
      "- Get the whole action event by action stage classification\n",
      "- Improving saliency models by predicting human fixation patches\n",
      "- Marine Vertebrate Predator Detection and Recognition in Underwater Videos by Region Convolutional Neural Network\n",
      "- Modeling the temporality of saliency\n",
      "- Multi-label discriminative weakly-supervised human activity recognition and localization\n",
      "- Online Aggregated-Event Representation for Multiple Event Detection in Videos\n",
      "- PON: Proposal Optimization Network for Temporal Action Proposal Generation\n",
      "- Periodic Action Temporal Localization Method Based on Two-Path Architecture for Product Counting in Sewing Video\n",
      "- STN-BA: Weakly-Supervised Few-Shot Temporal Action Localization\n",
      "- TadML: A Fast Temporal Action Detection with Mechanics-MLP\n",
      "- Temporal Relation-Aware Global Attention Network for Temporal Action Detection\n"
     ]
    }
   ],
   "source": [
    "# 比较不同ris相同的条目\n",
    "def extract_titles_from_ris(path):\n",
    "    \"\"\"\n",
    "    从 RIS 文件中读取所有 TI 字段的值并返回一个集合。\n",
    "    \"\"\"\n",
    "    titles = set()\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('TI  -'):\n",
    "                # 去掉开头 'TI  - ' 并去除两端空白\n",
    "                title = line[len('TI  -'):].strip()\n",
    "                titles.add(title)\n",
    "    return titles\n",
    "\n",
    "def find_common_titles(ris_path1, ris_path2):\n",
    "    \"\"\"\n",
    "    比较两个 RIS 文件，返回它们共有的 TI 字段值列表。\n",
    "    \"\"\"\n",
    "    titles1 = extract_titles_from_ris(ris_path1)\n",
    "    titles2 = extract_titles_from_ris(ris_path2)\n",
    "    common = titles1 & titles2  # 取交集\n",
    "    return sorted(common)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 替换成你的 RIS 文件路径\n",
    "    file1 = R'D:\\Programs\\Codes\\Skill-Up\\learning-smorgasbord\\arxiv\\data\\20250510_scopus_LNCS_tad_tal_303_processed_backup.ris'\n",
    "    file2 = R\"D:\\Users\\tang\\Desktop\\CCFC\\CCFC.ris\"\n",
    "\n",
    "    common_titles = find_common_titles(file1, file2)\n",
    "    if common_titles:\n",
    "        print(\"两个 RIS 文件中相同的 TI 条目有：\")\n",
    "        for t in common_titles:\n",
    "            print(f\"- {t}\")\n",
    "    else:\n",
    "        print(\"未发现相同的 TI 条目。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
